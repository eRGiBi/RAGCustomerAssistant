,user_input,retrieved_contexts,response,reference,llm_context_precision_with_reference,context_recall,faithfulness,bleu_score,rouge_score(mode=fmeasure)
0,What are the benefits of using LangChain for building chatbots?,"[""Introduction | Introduction **LangChain** is a framework for developing applications powered by language models. It enables applications that: - **Are context-aware**: connect a language model to sources of context (prompt instructions, few shot examples, content to ground its response in, etc.) - **Reason**: rely on a language model to reason (about how to answer based on provided context, what actions to take, etc.) This framework consists of several parts. - **LangChain Libraries**: The Python and JavaScript libraries. Contains interfaces and integrations for a myriad of components, a basic run time for combining these components into chains and agents, and off-the-shelf implementations of chains and agents. - **LangChain Templates**: A collection of easily deployable reference architectures for a wide variety of tasks. - **LangServe**: A library for deploying LangChain chains as a REST API. - **LangSmith**: A developer platform that lets you debug, test, evaluate, and monitor chains built on any LLM framework and seamlessly integrates with LangChain. ![LangChain Diagram](/assets/images/langchain_stack-7bb980051b626aad30ec43647b717411.png) Together, these products simplify the entire application lifecycle: - **Develop**: Write your applications in LangChain/LangChain.js. Hit the ground running using Templates for reference. - **Productionize**: Use LangSmith to inspect, test and monitor your chains, so that you can constantly improve and deploy with confidence. - **Deploy**: Turn any chain into an API with LangServe. ## LangChain Libraries The main value props of the LangChain packages are: 1. **Components**: composable tools and integrations for working with language models. Components are modular and easy-to-use, whether you are using the rest of the LangChain framework or not 2. **Off-the-shelf chains**: built-in assemblages of components for accomplishing higher-level tasks Off-the-shelf chains make it easy to get started. Components make it easy to customize existing chains and build new ones. ## Get started [Here's](/docs/get_started/installation) how to install LangChain, set up your environment, and start building. We recommend following our [Quickstart](/docs/get_started/quickstart) guide to familiarize yourself with the framework by building your first LangChain application. Read up on our [Security](/docs/security) best practices to make sure you're developing safely with LangChain. noteThese docs focus on the Python LangChain library. [Head here]( for docs on the JavaScript LangChain library. ## LangChain Expression Language (LCEL) LCEL is a declarative way to compose chains. LCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest prompt + LLM chain to the most complex chains. - **Overview**: LCEL and its benefits - **Interface**: The standard interface for LCEL objects - **How-to**: Key features of LCEL - **Cookbook**: Example code for accomplishing common tasks ## Modules LangChain provides standard, extendable interfaces and integrations for the following modules: #### Model I/O Interface with language models #### Retrieval Interface with application-specific data #### Agents Let models choose which tools to use given high-level directives ## Examples, ecosystem, and resources ### Use cases Walkthroughs and techniques for common end-to-end use cases, like: - [Document question answering](/docs/use_cases/question_answering/) - [Chatbots](/docs/use_cases/chatbots/) - [Analyzing structured data](/docs/use_cases/qa_structured/sql/) - and much more... ### Integrations LangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it. Check out our growing list of [integrations](/docs/integrations/providers/). ### Guides Best practices for developing with LangChain. ### API reference Head to the reference section for full documentation of all classes and methods in the LangChain and LangChain Experimental Python packages. ### Developer's guide Check out the developer's guide for guidelines on contributing and help getting your dev environment set up. ### Community Head to the [Community navigator](/docs/community) to find places to ask questions, share feedback, meet other developers, and dream about the future of LLM's. - [LangChain Libraries](#langchain-libraries) - [Get started](#get-started) - [LangChain Expression Language (LCEL)](#langchain-expression-language-lcel) - [Modules](#modules) - [Examples, ecosystem, and resources](#examples-ecosystem-and-resources)- [Use cases](#use-cases) - [Integrations](#integrations) - [Guides](#guides) - [API reference](#api-reference) - [Developer's guide](#developers-guide) - [Community](#community)"", 'Chatbots | Chatbots []( ## Use case Chatbots are one of the central LLM use-cases. The core features of chatbots are that they can have long-running conversations and have access to information that users want to know about. Aside from basic prompting and LLMs, memory and retrieval are the core components of a chatbot. Memory allows a chatbot to remember past interactions, and retrieval provides a chatbot with up-to-date, domain-specific information. ![Image description](/assets/images/chat_use_case-eb8a4883931d726e9f23628a0d22e315.png) ## Overview The chat model interface is based around messages rather than raw text. Several components are important to consider for chat: - `chat model`: See [here](/docs/integrations/chat) for a list of chat model integrations and [here](/docs/modules/model_io/chat) for documentation on the chat model interface in LangChain. You can use `LLMs` (see [here](/docs/modules/model_io/llms)) for chatbots as well, but chat models have a more conversational tone and natively support a message interface. - `prompt template`: Prompt templates make it easy to assemble prompts that combine default messages, user input, chat history, and (optionally) additional retrieved context. - `memory`: [See here](/docs/modules/memory/) for in-depth documentation on memory types - `retriever` (optional): [See here](/docs/modules/data_connection/retrievers) for in-depth documentation on retrieval systems. These are useful if you want to build a chatbot with domain-specific knowledge. ## Quickstart Here\'s a quick preview of how we can create chatbot interfaces. First let\'s install some dependencies and set the required credentials: ```bash pip install langchain openai # Set env var OPENAI_API_KEY or load from a .env file: # import dotenv # dotenv.load_dotenv() ``` With a plain chat model, we can get chat completions by [passing one or more messages](/docs/modules/model_io/chat) to the model. The chat model will respond with a message. ```python from langchain.chat_models import ChatOpenAI from langchain.schema import HumanMessage, SystemMessage chat = ChatOpenAI() chat( [ HumanMessage( content=""Translate this sentence from English to French: I love programming."" ) ] ) ``` ```text AIMessage(content=""J\'adore la programmation."", additional_kwargs={}, example=False) ``` And if we pass in a list of messages: ```python messages = [ SystemMessage( content=""You are a helpful assistant that translates English to French."" ), HumanMessage(content=""I love programming.""), ] chat(messages) ``` ```text AIMessage(content=""J\'adore la programmation."", additional_kwargs={}, example=False) ``` We can then wrap our chat model in a `ConversationChain`, which has built-in memory for remembering past user inputs and model outputs. ```python from langchain.chains import ConversationChain conversation = ConversationChain(llm=chat) conversation.run(""Translate this sentence from English to French: I love programming."") ``` ```text \'Je adore la programmation.\' ``` ```python conversation.run(""Translate it to German."") ``` ```text \'Ich liebe Programmieren.\' ``` ## Memory As we mentioned above, the core component of chatbots is the memory system. One of the simplest and most commonly used forms of memory is `ConversationBufferMemory`: - This memory allows for storing of messages in a `buffer` - When called in a chain, it returns all of the messages it has stored LangChain comes with many other types of memory, too. [See here](/docs/modules/memory/) for in-depth documentation on memory types. For now let\'s take a quick look at ConversationBufferMemory. We can manually add a few chat messages to the memory like so: ```python from langchain.memory import ConversationBufferMemory memory = ConversationBufferMemory() memory.chat_memory.add_user_message(""hi!"") memory.chat_memory.add_ai_message(""whats up?"") ``` And now we can load from our memory. The key method exposed by all `Memory` classes is `load_memory_variables`. This takes in any initial chain input and returns a list of memory variables which are added to the chain input. Since this simple memory type doesn\'t actually take into account the chain input when loading memory, we can pass in an empty input for now: ```python memory.load_memory_variables({}) ``` ```text {\'history\': \'Human: hi!\\nAI: whats up?\'} ``` We can also keep a sliding window of the most recent `k` interactions using `ConversationBufferWindowMemory`. ```python from langchain.memory import ConversationBufferWindowMemory memory = ConversationBufferWindowMemory(k=1) memory.save_context({""input"": ""hi""}, {""output"": ""whats up""}) memory.save_context({""input"": ""not much you""}, {""output"": ""not much""}) memory.load_memory_variables({}) ``` ```text {\'history\': \'Human: not much you\\nAI: not much\'} ``` `ConversationSummaryMemory` is an extension of this theme. It creates a summary of the conversation over time. This memory is most useful for longer conversations where the full message history would consume many tokens. ```python from langchain.llms import OpenAI from langchain.memory import ConversationSummaryMemory llm = OpenAI(temperature=0) memory = ConversationSummaryMemory(llm=llm) memory.save_context({""input"": ""hi""}, {""output"": ""whats up""}) memory.save_context( {""input"": ""im working on better docs for chatbots""}, {""output"": ""oh, that sounds like a lot of work""}, ) memory.save_context( {""input"": ""yes, but it\'s worth the effort""}, {""output"": ""agreed, good docs are important!""}, ) ``` ```python memory.load_memory_variables({}) ``` ```text {\'history\': \'\\nThe human greets the AI, to which the AI responds. The human then mentions they are working on better docs for chatbots, to which the AI responds that it sounds like a lot of work. The human agrees that it is worth the effort, and the AI agrees that good docs are important.\'} ``` `ConversationSummaryBufferMemory` extends this a bit further: It uses token length rather than number of interactions to determine when to flush interactions. ```python from langchain.memory import ConversationSummaryBufferMemory memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=10) memory.save_context({""input"": ""hi""}, {""output"": ""whats up""}) memory.save_context({""input"": ""not much you""}, {""output"": ""not much""}) ``` ## Conversation We can unpack what goes under the hood with `ConversationChain`. We can specify our memory, `ConversationSummaryMemory` and we can specify the prompt. ```python from langchain.chains import LLMChain from langchain.prompts import ( ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate, ) # LLM llm = ChatOpenAI() # Prompt prompt = ChatPromptTemplate( messages=[ SystemMessagePromptTemplate.from_template( ""You are a nice chatbot having a conversation with a human."" ), # The `variable_name` here is what must align with memory MessagesPlaceholder(variable_name=""chat_history""), HumanMessagePromptTemplate.from_template(""{question}""), ] ) # Notice that we `return_messages=True` to fit into the MessagesPlaceholder # Notice that `""chat_history""` aligns with the MessagesPlaceholder name memory = ConversationBufferMemory(memory_key=""chat_history"", return_messages=True) conversation = LLMChain(llm=llm, prompt=prompt, verbose=True, memory=memory) # Notice that we just pass in the `question` variables - `chat_history` gets populated by memory conversation({""question"": ""hi""}) ``` ```text > Entering new LLMChain chain... Prompt after formatting: System: You are a nice chatbot having a conversation with a human. Human: hi > Finished chain. {\'question\': \'hi\', \'chat_history\': [HumanMessage(content=\'hi\', additional_kwargs={}, example=False), AIMessage(content=\'Hello! How can I assist you today?\', additional_kwargs={}, example=False)], \'text\': \'Hello! How can I assist you today?\'} ``` ```python conversation( {""question"": ""Translate this sentence from English to French: I love programming.""} ) ``` ```text > Entering new LLMChain chain... Prompt after formatting: System: You are a nice chatbot having a conversation with a human. Human: hi AI: Hello! How can I assist you today? Human: Translate this sentence from English to French: I love programming. > Finished chain. {\'question\': \'Translate this sentence from English to French: I love programming.\', \'chat_history\': [HumanMessage(content=\'hi\', additional_kwargs={}, example=False), AIMessage(content=\'Hello! How can I assist you today?\', additional_kwargs={}, example=False), HumanMessage(content=\'Translate this sentence from English to French: I love programming.\', additional_kwargs={}, example=False), AIMessage(content=\'Sure! The translation of ""I love programming"" from English to French is ""J\\\'adore programmer.""\', additional_kwargs={}, example=False)], \'text\': \'Sure! The translation of ""I love programming"" from English to French is ""J\\\'adore programmer.""\'} ``` ```python conversation({""question"": ""Now translate the sentence to German.""}) ``` ```text > Entering new LLMChain chain... Prompt after formatting: System: You are a nice chatbot having a conversation with a human. Human: hi AI: Hello! How can I assist you today? Human: Translate this sentence from English to French: I love programming. AI: Sure! The translation of ""I love programming"" from English to French is ""J\'adore programmer."" Human: Now translate the sentence to German. > Finished chain. {\'question\': \'Now translate the sentence to German.\', \'chat_history\': [HumanMessage(content=\'hi\', additional_kwargs={}, example=False), AIMessage(content=\'Hello! How can I assist you today?\', additional_kwargs={}, example=False), HumanMessage(content=\'Translate this sentence from English to French: I love programming.\', additional_kwargs={}, example=False), AIMessage(content=\'Sure! The translation of ""I love programming"" from English to French is ""J\\\'adore programmer.""\', additional_kwargs={}, example=False), HumanMessage(content=\'Now translate the sentence to German.\', additional_kwargs={}, example=False), AIMessage(content=\'Certainly! The translation of ""I love programming"" from English to German is ""Ich liebe das Programmieren.""\', additional_kwargs={}, example=False)], \'text\': \'Certainly! The translation of ""I love programming"" from English to German is ""Ich liebe das Programmieren.""\'} ``` We can see the chat history preserved in the prompt using the [LangSmith trace]( ![Image description](/assets/images/chat_use_case_2-a76871806149f125d08ff149363e0c2c.png) ## Chat Retrieval Now, suppose we want to [chat with documents]( or some other source of knowledge. This is popular use case, combining chat with [document retrieval](/docs/use_cases/question_answering). It allows us to chat with specific information that the model was not trained on. ```bash pip install tiktoken chromadb ``` Load a blog post. ```python from langchain.document_loaders import WebBaseLoader loader = WebBaseLoader("" data = loader.load() ``` Split and store this in a vector. ```python from langchain.text_splitter import RecursiveCharacterTextSplitter text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0) all_splits = text_splitter.split_documents(data) from langchain.embeddings import OpenAIEmbeddings from langchain.vectorstores import Chroma vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings()) ``` Create our memory, as before, but\'s let\'s use `ConversationSummaryMemory`. ```python memory = ConversationSummaryMemory( llm=llm, memory_key=""chat_history"", return_messages=True ) ``` ```python from langchain.chains import ConversationalRetrievalChain from langchain.chat_models import ChatOpenAI llm = ChatOpenAI() retriever = vectorstore.as_retriever() qa = ConversationalRetrievalChain.from_llm(llm, retriever=retriever, memory=memory) ``` ```python qa(""How do agents use Task decomposition?"") ``` ```text {\'question\': \'How do agents use Task decomposition?\', \'chat_history\': [SystemMessage(content=\'\', additional_kwargs={})], \'answer\': \'Agents can use task decomposition in several ways:\\n\\n1. Simple prompting: Agents can use Language Model based prompting to break down tasks into subgoals. For example, by providing prompts like ""Steps for XYZ"" or ""What are the subgoals for achieving XYZ?"", the agent can generate a sequence of smaller steps that lead to the completion of the overall task.\\n\\n2. Task-specific instructions: Agents can be given task-specific instructions to guide their planning process. For example, if the task is to write a novel, the agent can be instructed to ""Write a story outline."" This provides a high-level structure for the task and helps in breaking it down into smaller components.\\n\\n3. Human inputs: Agents can also take inputs from humans to decompose tasks. This can be done through direct communication or by leveraging human expertise. Humans can provide guidance and insights to help the agent break down complex tasks into manageable subgoals.\\n\\nOverall, task decomposition allows agents to break down large tasks into smaller, more manageable subgoals, enabling them to plan and execute complex tasks efficiently.\'} ``` ```python qa(""What are the various ways to implement memory to support it?"") ``` ```text {\'question\': \'What are the various ways to implement memory to support it?\', \'chat_history\': [SystemMessage(content=\'The human asks how agents use task decomposition. The AI explains that agents can use task decomposition in several ways, including simple prompting, task-specific instructions, and human inputs. Task decomposition allows agents to break down large tasks into smaller, more manageable subgoals, enabling them to plan and execute complex tasks efficiently.\', additional_kwargs={})], \'answer\': \'There are several ways to implement memory to support task decomposition:\\n\\n1. Long-Term Memory Management: This involves storing and organizing information in a long-term memory system. The agent can retrieve past experiences, knowledge, and learned strategies to guide the task decomposition process.\\n\\n2. Internet Access: The agent can use internet access to search for relevant information and gather resources to aid in task decomposition. This allows the agent to access a vast amount of information and utilize it in the decomposition process.\\n\\n3. GPT-3.5 Powered Agents: The agent can delegate simple tasks to GPT-3.5 powered agents. These agents can perform specific tasks or provide assistance in task decomposition, allowing the main agent to focus on higher-level planning and decision-making.\\n\\n4. File Output: The agent can store the results of task decomposition in files or documents. This allows for easy retrieval and reference during the execution of the task.\\n\\nThese memory resources help the agent in organizing and managing information, making informed decisions, and effectively decomposing complex tasks into smaller, manageable subgoals.\'} ``` Again, we can use the [LangSmith trace]( to explore the prompt structure. ### Going deeper - Agents, such as the [conversational retrieval agent](/docs/use_cases/question_answering/conversational_retrieval_agents), can be used for retrieval when necessary while also holding a conversation. - [Use case](#use-case) - [Overview](#overview) - [Quickstart](#quickstart) - [Memory](#memory) - [Conversation](#conversation) - [Chat Retrieval](#chat-retrieval)- [Going deeper](#going-deeper)', 'Diffbot | Diffbot Unlike traditional web scraping tools, [Diffbot]( doesn\'t require any rules to read the content on a page. It starts with computer vision, which classifies a page into one of 20 possible types. Content is then interpreted by a machine learning model trained to identify the key attributes on a page based on its type. The result is a website transformed into clean structured data (like JSON or CSV), ready for your application. This covers how to extract HTML documents from a list of URLs using the [Diffbot extract API]( into a document format that we can use downstream. ```python urls = [ "" ] ``` The Diffbot Extract API Requires an API token. Once you have it, you can extract the data. Read [instructions]( how to get the Diffbot API Token. ```python import os from langchain.document_loaders import DiffbotLoader loader = DiffbotLoader(urls=urls, api_token=os.environ.get(""DIFFBOT_API_TOKEN"")) ``` With the `.load()` method, you can see the documents loaded ```python loader.load() ``` ```text [Document(page_content=\'LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an API, but will also:\\nBe data-aware: connect a language model to other sources of data\\nBe agentic: allow a language model to interact with its environment\\nThe LangChain framework is designed with the above principles in mind.\\nThis is the Python specific portion of the documentation. For a purely conceptual guide to LangChain, see here. For the JavaScript documentation, see here.\\nGetting Started\\nCheckout the below guide for a walkthrough of how to get started using LangChain to create an Language Model application.\\nGetting Started Documentation\\nModules\\nThere are several main modules that LangChain provides support for. For each module we provide some examples to get started, how-to guides, reference docs, and conceptual guides. These modules are, in increasing order of complexity:\\nModels: The various model types and model integrations LangChain supports.\\nPrompts: This includes prompt management, prompt optimization, and prompt serialization.\\nMemory: Memory is the concept of persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory.\\nIndexes: Language models are often more powerful when combined with your own text data - this module covers best practices for doing exactly that.\\nChains: Chains go beyond just a single LLM call, and are sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.\\nAgents: Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end to end agents.\\nUse Cases\\nThe above modules can be used in a variety of ways. LangChain also provides guidance and assistance in this. Below are some of the common use cases LangChain supports.\\nPersonal Assistants: The main LangChain use case. Personal assistants need to take actions, remember interactions, and have knowledge about your data.\\nQuestion Answering: The second big LangChain use case. Answering questions over specific documents, only utilizing the information in those documents to construct an answer.\\nChatbots: Since language models are good at producing text, that makes them ideal for creating chatbots.\\nQuerying Tabular Data: If you want to understand how to use LLMs to query data that is stored in a tabular format (csvs, SQL, dataframes, etc) you should read this page.\\nInteracting with APIs: Enabling LLMs to interact with APIs is extremely powerful in order to give them more up-to-date information and allow them to take actions.\\nExtraction: Extract structured information from text.\\nSummarization: Summarizing longer documents into shorter, more condensed chunks of information. A type of Data Augmented Generation.\\nEvaluation: Generative models are notoriously hard to evaluate with traditional metrics. One new way of evaluating them is using language models themselves to do the evaluation. LangChain provides some prompts/chains for assisting in this.\\nReference Docs\\nAll of LangChain\'s reference documentation, in one place. Full documentation on all methods, classes, installation methods, and integration setups for LangChain.\\nReference Documentation\\nLangChain Ecosystem\\nGuides for how other companies/products can be used with LangChain\\nLangChain Ecosystem\\nAdditional Resources\\nAdditional collection of resources we think may be useful as you develop your application!\\nLangChainHub: The LangChainHub is a place to share and explore other prompts, chains, and agents.\\nGlossary: A glossary of all related terms, papers, methods, etc. Whether implemented in LangChain or not!\\nGallery: A collection of our favorite projects that use LangChain. Useful for finding inspiration or seeing how things were done in other applications.\\nDeployments: A collection of instructions, code snippets, and template repositories for deploying LangChain apps.\\nTracing: A guide on using tracing in LangChain to visualize the execution of chains and agents.\\nModel Laboratory: Experimenting with different prompts, models, and chains is a big part of developing the best possible application. The ModelLaboratory makes it easy to do so.\\nDiscord: Join us on our Discord to discuss all things LangChain!\\nProduction Support: As you move your LangChains into production, we\'d love to offer more comprehensive support. Please fill out this form and we\'ll set up a dedicated support Slack channel.\', metadata={\'source\': \' ```']","LangChain offers several benefits for building chatbots:

1. **Context Awareness**: LangChain enables chatbots to be context-aware by connecting a language model to sources of context, such as prompt instructions, few-shot examples, and content to ground its responses.

2. **Reasoning**: It allows chatbots to reason about how to answer based on provided context and what actions to take.

3. **Memory**: Memory components in LangChain allow chatbots to remember past interactions, which is crucial for maintaining continuity in conversations.

4. **Retrieval**: Optional retrievers provide chatbots with up-to-date, domain-specific information, enhancing their knowledge base.

5. **Ease of Use**: LangChain provides off-the-shelf chains and components that make it easy to get started and customize existing chains for specific tasks.

6. **Integration**: It integrates well with various modules like model I/O, retrieval, and agents, allowing for a flexible and modular approach to chatbot development.

7. **Deployment**: LangChain can turn any chain into a REST API using LangServe, facilitating easy deployment.

8. **Monitoring and Testing**: LangSmith offers tools to debug, test, evaluate, and monitor chains, ensuring high-quality and reliable chatbots.","LangChain offers several benefits for building chatbots, including modular components for handling various tasks, easy integration with multiple language models, built-in support for memory and state management, and a flexible architecture that allows for customization and scalability.",0.9999999999666667,1.0,,0.018557149840729873,0.15384615384615383
1,How do I use LangChain with multiple LLMs?,"[""How to | How to [ Bind runtime argsSometimes we want to invoke a Runnable within a Runnable sequence with constant arguments that are not part of the output of the preceding Runnable in the sequence, and which are not part of the user input. We can use Runnable.bind() to easily pass these arguments in.](/docs/expression_language/how_to/binding)[ Configure chain internals at runtimeOftentimes you may want to experiment with, or even expose to the end user, multiple different ways of doing things.](/docs/expression_language/how_to/configure)[ Add fallbacksThere are many possible points of failure in an LLM application, whether that be issues with LLM API's, poor model outputs, issues with other integrations, etc. Fallbacks help you gracefully handle and isolate these issues.](/docs/expression_language/how_to/fallbacks)[ Run custom functionsYou can use arbitrary functions in the pipeline](/docs/expression_language/how_to/functions)[ Stream custom generator functionsYou can use generator functions (ie. functions that use the yield keyword, and behave like iterators) in a LCEL pipeline.](/docs/expression_language/how_to/generators)[ Parallelize stepsRunnableParallel (aka. RunnableMap) makes it easy to execute multiple Runnables in parallel, and to return the output of these Runnables as a map.](/docs/expression_language/how_to/map)[ Add message history (memory)The RunnableWithMessageHistory let's us add message history to certain types of chains.](/docs/expression_language/how_to/message_history)[ Dynamically route logic based on inputThis notebook covers how to do routing in the LangChain Expression Language.](/docs/expression_language/how_to/routing)"", 'Custom MRKL agent | Custom MRKL agent This notebook goes through how to create your own custom MRKL agent. A MRKL agent consists of three parts: - Tools: The tools the agent has available to use. - `LLMChain`: The `LLMChain` that produces the text that is parsed in a certain way to determine which action to take. - The agent class itself: this parses the output of the `LLMChain` to determine which action to take. In this notebook we walk through how to create a custom MRKL agent by creating a custom `LLMChain`. ### Custom LLMChain The first way to create a custom agent is to use an existing Agent class, but use a custom `LLMChain`. This is the simplest way to create a custom Agent. It is highly recommended that you work with the `ZeroShotAgent`, as at the moment that is by far the most generalizable one. Most of the work in creating the custom `LLMChain` comes down to the prompt. Because we are using an existing agent class to parse the output, it is very important that the prompt say to produce text in that format. Additionally, we currently require an `agent_scratchpad` input variable to put notes on previous actions and observations. This should almost always be the final part of the prompt. However, besides those instructions, you can customize the prompt as you wish. To ensure that the prompt contains the appropriate instructions, we will utilize a helper method on that class. The helper method for the `ZeroShotAgent` takes the following arguments: - `tools`: List of tools the agent will have access to, used to format the prompt. - `prefix`: String to put before the list of tools. - `suffix`: String to put after the list of tools. - `input_variables`: List of input variables the final prompt will expect. For this exercise, we will give our agent access to Google Search, and we will customize it in that we will have it answer as a pirate. ```python from langchain.agents import AgentExecutor, Tool, ZeroShotAgent from langchain.chains import LLMChain from langchain.llms import OpenAI from langchain.utilities import SerpAPIWrapper ``` ```python search = SerpAPIWrapper() tools = [ Tool( name=""Search"", func=search.run, description=""useful for when you need to answer questions about current events"", ) ] ``` ```python prefix = """"""Answer the following questions as best you can, but speaking as a pirate might speak. You have access to the following tools:"""""" suffix = """"""Begin! Remember to speak as a pirate when giving your final answer. Use lots of ""Args"" Question: {input} {agent_scratchpad}"""""" prompt = ZeroShotAgent.create_prompt( tools, prefix=prefix, suffix=suffix, input_variables=[""input"", ""agent_scratchpad""] ) ``` In case we are curious, we can now take a look at the final prompt template to see what it looks like when its all put together. ```python print(prompt.template) ``` ```text Answer the following questions as best you can, but speaking as a pirate might speak. You have access to the following tools: Search: useful for when you need to answer questions about current events Use the following format: Question: the input question you must answer Thought: you should always think about what to do Action: the action to take, should be one of [Search] Action Input: the input to the action Observation: the result of the action ... (this Thought/Action/Action Input/Observation can repeat N times) Thought: I now know the final answer Final Answer: the final answer to the original input question Begin! Remember to speak as a pirate when giving your final answer. Use lots of ""Args"" Question: {input} {agent_scratchpad} ``` Note that we are able to feed agents a self-defined prompt template, i.e. not restricted to the prompt generated by the `create_prompt` function, assuming it meets the agent\'s requirements. For example, for `ZeroShotAgent`, we will need to ensure that it meets the following requirements. There should a string starting with ""Action:"" and a following string starting with ""Action Input:"", and both should be separated by a newline. ```python llm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt) ``` ```python tool_names = [tool.name for tool in tools] agent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=tool_names) ``` ```python agent_executor = AgentExecutor.from_agent_and_tools( agent=agent, tools=tools, verbose=True ) ``` ```python agent_executor.run(""How many people live in canada as of 2023?"") ``` ```text > Entering new AgentExecutor chain... Thought: I need to find out the population of Canada Action: Search Action Input: Population of Canada 2023 Observation: The current population of Canada is 38,661,927 as of Sunday, April 16, 2023, based on Worldometer elaboration of the latest United Nations data. Thought: I now know the final answer Final Answer: Arrr, Canada be havin\' 38,661,927 people livin\' there as of 2023! > Finished chain. ""Arrr, Canada be havin\' 38,661,927 people livin\' there as of 2023!"" ``` ### Multiple inputs Agents can also work with prompts that require multiple inputs. ```python prefix = """"""Answer the following questions as best you can. You have access to the following tools:"""""" suffix = """"""When answering, you MUST speak in the following language: {language}. Question: {input} {agent_scratchpad}"""""" prompt = ZeroShotAgent.create_prompt( tools, prefix=prefix, suffix=suffix, input_variables=[""input"", ""language"", ""agent_scratchpad""], ) ``` ```python llm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt) ``` ```python agent = ZeroShotAgent(llm_chain=llm_chain, tools=tools) ``` ```python agent_executor = AgentExecutor.from_agent_and_tools( agent=agent, tools=tools, verbose=True ) ``` ```python agent_executor.run( input=""How many people live in canada as of 2023?"", language=""italian"" ) ``` ```text > Entering new AgentExecutor chain... Thought: I should look for recent population estimates. Action: Search Action Input: Canada population 2023 Observation: 39,566,248 Thought: I should double check this number. Action: Search Action Input: Canada population estimates 2023 Observation: Canada\'s population was estimated at 39,566,248 on January 1, 2023, after a record population growth of 1,050,110 people from January 1, 2022, to January 1, 2023. Thought: I now know the final answer. Final Answer: La popolazione del Canada stata stimata a 39.566.248 il 1 gennaio 2023, dopo un record di crescita demografica di 1.050.110 persone dal 1 gennaio 2022 al 1 gennaio 2023. > Finished chain. \'La popolazione del Canada stata stimata a 39.566.248 il 1 gennaio 2023, dopo un record di crescita demografica di 1.050.110 persone dal 1 gennaio 2022 al 1 gennaio 2023.\' ``` - [Custom LLMChain](#custom-llmchain) - [Multiple inputs](#multiple-inputs)', 'Aim | Aim Aim makes it super easy to visualize and debug LangChain executions. Aim tracks inputs and outputs of LLMs and tools, as well as actions of agents. With Aim, you can easily debug and examine an individual execution: ![]( Additionally, you have the option to compare multiple executions side by side: ![]( Aim is fully open source, [learn more]( about Aim on GitHub. Let\'s move forward and see how to enable and configure Aim callback. ### Tracking LangChain Executions with Aim In this notebook we will explore three usage scenarios. To start off, we will install the necessary packages and import certain modules. Subsequently, we will configure two environment variables that can be established either within the Python script or through the terminal. ```bash pip install aim pip install langchain pip install openai pip install google-search-results ``` ```python import os from datetime import datetime from langchain.callbacks import AimCallbackHandler, StdOutCallbackHandler from langchain.llms import OpenAI ``` Our examples use a GPT model as the LLM, and OpenAI offers an API for this purpose. You can obtain the key from the following link: [ . We will use the SerpApi to retrieve search results from Google. To acquire the SerpApi key, please go to [ . ```python os.environ[""OPENAI_API_KEY""] = ""..."" os.environ[""SERPAPI_API_KEY""] = ""..."" ``` The event methods of `AimCallbackHandler` accept the LangChain module or agent as input and log at least the prompts and generated results, as well as the serialized version of the LangChain module, to the designated Aim run. ```python session_group = datetime.now().strftime(""%m.%d.%Y_%H.%M.%S"") aim_callback = AimCallbackHandler( repo=""."", experiment_name=""scenario 1: OpenAI LLM"", ) callbacks = [StdOutCallbackHandler(), aim_callback] llm = OpenAI(temperature=0, callbacks=callbacks) ``` The `flush_tracker` function is used to record LangChain assets on Aim. By default, the session is reset rather than being terminated outright. ### Scenario 1 In the first scenario, we will use OpenAI LLM.```python # scenario 1 - LLM llm_result = llm.generate([""Tell me a joke"", ""Tell me a poem""] * 3) aim_callback.flush_tracker( langchain_asset=llm, experiment_name=""scenario 2: Chain with multiple SubChains on multiple generations"", ) ``` ### Scenario 2 Scenario two involves chaining with multiple SubChains across multiple generations.```python from langchain.chains import LLMChain from langchain.prompts import PromptTemplate ``` ```python # scenario 2 - Chain template = """"""You are a playwright. Given the title of play, it is your job to write a synopsis for that title. Title: {title} Playwright: This is a synopsis for the above play:"""""" prompt_template = PromptTemplate(input_variables=[""title""], template=template) synopsis_chain = LLMChain(llm=llm, prompt=prompt_template, callbacks=callbacks) test_prompts = [ { ""title"": ""documentary about good video games that push the boundary of game design"" }, {""title"": ""the phenomenon behind the remarkable speed of cheetahs""}, {""title"": ""the best in class mlops tooling""}, ] synopsis_chain.apply(test_prompts) aim_callback.flush_tracker( langchain_asset=synopsis_chain, experiment_name=""scenario 3: Agent with Tools"" ) ``` ### Scenario 3 The third scenario involves an agent with tools.```python from langchain.agents import AgentType, initialize_agent, load_tools ``` ```python # scenario 3 - Agent with Tools tools = load_tools([""serpapi"", ""llm-math""], llm=llm, callbacks=callbacks) agent = initialize_agent( tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, callbacks=callbacks, ) agent.run( ""Who is Leo DiCaprio\'s girlfriend? What is her current age raised to the 0.43 power?"" ) aim_callback.flush_tracker(langchain_asset=agent, reset=False, finish=True) ``` ```text > Entering new AgentExecutor chain... I need to find out who Leo DiCaprio\'s girlfriend is and then calculate her age raised to the 0.43 power. Action: Search Action Input: ""Leo DiCaprio girlfriend"" Observation: Leonardo DiCaprio seemed to prove a long-held theory about his love life right after splitting from girlfriend Camila Morrone just months ... Thought: I need to find out Camila Morrone\'s age Action: Search Action Input: ""Camila Morrone age"" Observation: 25 years Thought: I need to calculate 25 raised to the 0.43 power Action: Calculator Action Input: 25^0.43 Observation: Answer: 3.991298452658078 Thought: I now know the final answer Final Answer: Camila Morrone is Leo DiCaprio\'s girlfriend and her current age raised to the 0.43 power is 3.991298452658078. > Finished chain. ```']","To use LangChain with multiple LLMs, create separate instances for each LLM and use them in different parts of your pipeline. Here's a simple example:

```python
from langchain.llms import HuggingFaceLLM, OpenAI

# Initialize LLMs
llm1 = HuggingFaceLLM(model_name=""model1"")
llm2 = OpenAI(api_key=""..."")

# Use LLMs in different parts of your pipeline
result1 = llm1.generate(""Tell me a joke."")
result2 = llm2.generate(""Tell me a poem."")

print(result1)
print(result2)
```",You can use multiple LLMs in LangChain by defining a chain that includes different LLMs as components. You can route inputs to specific LLMs based on conditions or combine their outputs using techniques like ensemble methods or voting.,0.3333333333,0.0,,0.045463087134045746,0.14814814814814814
2,Can I use custom embeddings with LangChain?,"['Dynamically route logic based on input | Dynamically route logic based on input This notebook covers how to do routing in the LangChain Expression Language. Routing allows you to create non-deterministic chains where the output of a previous step defines the next step. Routing helps provide structure and consistency around interactions with LLMs. There are two ways to perform routing: 1. Using a `RunnableBranch`. 2. Writing custom factory function that takes the input of a previous step and returns a **runnable**. Importantly, this should return a **runnable** and NOT actually execute. We\'ll illustrate both methods using a two step sequence where the first step classifies an input question as being about `LangChain`, `Anthropic`, or `Other`, then routes to a corresponding prompt chain. ## Using a RunnableBranch A `RunnableBranch` is initialized with a list of (condition, runnable) pairs and a default runnable. It selects which branch by passing each condition the input it\'s invoked with. It selects the first condition to evaluate to True, and runs the corresponding runnable to that condition with the input. If no provided conditions match, it runs the default runnable. Here\'s an example of what it looks like in action: ```python from langchain.chat_models import ChatAnthropic from langchain.prompts import PromptTemplate from langchain.schema.output_parser import StrOutputParser ``` First, let\'s create a chain that will identify incoming questions as being about `LangChain`, `Anthropic`, or `Other`: ```python chain = ( PromptTemplate.from_template( """"""Given the user question below, classify it as either being about `LangChain`, `Anthropic`, or `Other`. Do not respond with more than one word. {question} Classification:"""""" ) | ChatAnthropic() | StrOutputParser() ) ``` ```python chain.invoke({""question"": ""how do I call Anthropic?""}) ``` ```text \' Anthropic\' ``` Now, let\'s create three sub chains: ```python langchain_chain = ( PromptTemplate.from_template( """"""You are an expert in langchain. \\ Always answer questions starting with ""As Harrison Chase told me"". \\ Respond to the following question: Question: {question} Answer:"""""" ) | ChatAnthropic() ) anthropic_chain = ( PromptTemplate.from_template( """"""You are an expert in anthropic. \\ Always answer questions starting with ""As Dario Amodei told me"". \\ Respond to the following question: Question: {question} Answer:"""""" ) | ChatAnthropic() ) general_chain = ( PromptTemplate.from_template( """"""Respond to the following question: Question: {question} Answer:"""""" ) | ChatAnthropic() ) ``` ```python from langchain.schema.runnable import RunnableBranch branch = RunnableBranch( (lambda x: ""anthropic"" in x[""topic""].lower(), anthropic_chain), (lambda x: ""langchain"" in x[""topic""].lower(), langchain_chain), general_chain, ) ``` ```python full_chain = {""topic"": chain, ""question"": lambda x: x[""question""]} | branch ``` ```python full_chain.invoke({""question"": ""how do I use Anthropic?""}) ``` ```text AIMessage(content="" As Dario Amodei told me, here are some ways to use Anthropic:\\n\\n- Sign up for an account on Anthropic\'s website to access tools like Claude, Constitutional AI, and Writer. \\n\\n- Use Claude for tasks like email generation, customer service chat, and QA. Claude can understand natural language prompts and provide helpful responses.\\n\\n- Use Constitutional AI if you need an AI assistant that is harmless, honest, and helpful. It is designed to be safe and aligned with human values.\\n\\n- Use Writer to generate natural language content for things like marketing copy, stories, reports, and more. Give it a topic and prompt and it will create high-quality written content.\\n\\n- Check out Anthropic\'s documentation and blog for tips, tutorials, examples, and announcements about new capabilities as they continue to develop their AI technology.\\n\\n- Follow Anthropic on social media or subscribe to their newsletter to stay up to date on new features and releases.\\n\\n- For most people, the easiest way to leverage Anthropic\'s technology is through their website - just create an account to get started!"", additional_kwargs={}, example=False) ``` ```python full_chain.invoke({""question"": ""how do I use LangChain?""}) ``` ```text AIMessage(content=\' As Harrison Chase told me, here is how you use LangChain:\\n\\nLangChain is an AI assistant that can have conversations, answer questions, and generate text. To use LangChain, you simply type or speak your input and LangChain will respond. \\n\\nYou can ask LangChain questions, have discussions, get summaries or explanations about topics, and request it to generate text on a subject. Some examples of interactions:\\n\\n- Ask general knowledge questions and LangChain will try to answer factually. For example ""What is the capital of France?""\\n\\n- Have conversations on topics by taking turns speaking. You can prompt the start of a conversation by saying something like ""Let\\\'s discuss machine learning""\\n\\n- Ask for summaries or high-level explanations on subjects. For example ""Can you summarize the main themes in Shakespeare\\\'s Hamlet?"" \\n\\n- Give creative writing prompts or requests to have LangChain generate text in different styles. For example ""Write a short children\\\'s story about a mouse"" or ""Generate a poem in the style of Robert Frost about nature""\\n\\n- Correct LangChain if it makes an inaccurate statement and provide the right information. This helps train it.\\n\\nThe key is interacting naturally and giving it clear prompts and requests\', additional_kwargs={}, example=False) ``` ```python full_chain.invoke({""question"": ""whats 2 + 2""}) ``` ```text AIMessage(content=\' 2 + 2 = 4\', additional_kwargs={}, example=False) ``` ## Using a custom function You can also use a custom function to route between different outputs. Here\'s an example: ```python def route(info): if ""anthropic"" in info[""topic""].lower(): return anthropic_chain elif ""langchain"" in info[""topic""].lower(): return langchain_chain else: return general_chain ``` ```python from langchain.schema.runnable import RunnableLambda full_chain = {""topic"": chain, ""question"": lambda x: x[""question""]} | RunnableLambda( route ) ``` ```python full_chain.invoke({""question"": ""how do I use Anthroipc?""}) ``` ```text AIMessage(content=\' As Dario Amodei told me, to use Anthropic IPC you first need to import it:\\n\\n```python\\nfrom anthroipc import ic\\n```\\n\\nThen you can create a client and connect to the server:\\n\\n```python \\nclient = ic.connect()\\n```\\n\\nAfter that, you can call methods on the client and get responses:\\n\\n```python\\nresponse = client.ask(""What is the meaning of life?"")\\nprint(response)\\n```\\n\\nYou can also register callbacks to handle events: \\n\\n```python\\ndef on_poke(event):\\n print(""Got poked!"")\\n\\nclient.on(\\\'poke\\\', on_poke)\\n```\\n\\nAnd that\\\'s the basics of using the Anthropic IPC client library for Python! Let me know if you have any other questions!\', additional_kwargs={}, example=False) ``` ```python full_chain.invoke({""question"": ""how do I use LangChain?""}) ``` ```text AIMessage(content=\' As Harrison Chase told me, to use LangChain you first need to sign up for an API key at platform.langchain.com. Once you have your API key, you can install the Python library and write a simple Python script to call the LangChain API. Here is some sample code to get started:\\n\\n```python\\nimport langchain\\n\\napi_key = ""YOUR_API_KEY""\\n\\nlangchain.set_key(api_key)\\n\\nresponse = langchain.ask(""What is the capital of France?"")\\n\\nprint(response.response)\\n```\\n\\nThis will send the question ""What is the capital of France?"" to the LangChain API and print the response. You can customize the request by providing parameters like max_tokens, temperature, etc. The LangChain Python library documentation has more details on the available options. The key things are getting an API key and calling langchain.ask() with your question text. Let me know if you have any other questions!\', additional_kwargs={}, example=False) ``` ```python full_chain.invoke({""question"": ""whats 2 + 2""}) ``` ```text AIMessage(content=\' 4\', additional_kwargs={}, example=False) ``` - [Using a RunnableBranch](#using-a-runnablebranch) - [Using a custom function](#using-a-custom-function)', 'Multiple Memory classes | Multiple Memory classes We can use multiple memory classes in the same chain. To combine multiple memory classes, we initialize and use the `CombinedMemory` class. ```python from langchain.chains import ConversationChain from langchain.llms import OpenAI from langchain.memory import ( CombinedMemory, ConversationBufferMemory, ConversationSummaryMemory, ) from langchain.prompts import PromptTemplate conv_memory = ConversationBufferMemory( memory_key=""chat_history_lines"", input_key=""input"" ) summary_memory = ConversationSummaryMemory(llm=OpenAI(), input_key=""input"") # Combined memory = CombinedMemory(memories=[conv_memory, summary_memory]) _DEFAULT_TEMPLATE = """"""The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Summary of conversation: {history} Current conversation: {chat_history_lines} Human: {input} AI:"""""" PROMPT = PromptTemplate( input_variables=[""history"", ""input"", ""chat_history_lines""], template=_DEFAULT_TEMPLATE, ) llm = OpenAI(temperature=0) conversation = ConversationChain(llm=llm, verbose=True, memory=memory, prompt=PROMPT) ``` ```python conversation.run(""Hi!"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Summary of conversation: Current conversation: Human: Hi! AI: > Finished chain. \' Hi there! How can I help you?\' ``` ```python conversation.run(""Can you tell me a joke?"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Summary of conversation: The human greets the AI, to which the AI responds with a polite greeting and an offer to help. Current conversation: Human: Hi! AI: Hi there! How can I help you? Human: Can you tell me a joke? AI: > Finished chain. \' Sure! What did the fish say when it hit the wall?\\nHuman: I don\\\'t know.\\nAI: ""Dam!""\' ```', 'AZLyrics | AZLyrics [AZLyrics]( is a large, legal, every day growing collection of lyrics. This covers how to load AZLyrics webpages into a document format that we can use downstream. ```python from langchain.document_loaders import AZLyricsLoader ``` ```python loader = AZLyricsLoader("" ``` ```python data = loader.load() ``` ```python data ``` ```text [Document(page_content=""Miley Cyrus - Flowers Lyrics | AZLyrics.com\\n\\r\\nWe were good, we were gold\\nKinda dream that can\'t be sold\\nWe were right till we weren\'t\\nBuilt a home and watched it burn\\n\\nI didn\'t wanna leave you\\nI didn\'t wanna lie\\nStarted to cry but then remembered I\\n\\nI can buy myself flowers\\nWrite my name in the sand\\nTalk to myself for hours\\nSay things you don\'t understand\\nI can take myself dancing\\nAnd I can hold my own hand\\nYeah, I can love me better than you can\\n\\nCan love me better\\nI can love me better, baby\\nCan love me better\\nI can love me better, baby\\n\\nPaint my nails, cherry red\\nMatch the roses that you left\\nNo remorse, no regret\\nI forgive every word you said\\n\\nI didn\'t wanna leave you, baby\\nI didn\'t wanna fight\\nStarted to cry but then remembered I\\n\\nI can buy myself flowers\\nWrite my name in the sand\\nTalk to myself for hours, yeah\\nSay things you don\'t understand\\nI can take myself dancing\\nAnd I can hold my own hand\\nYeah, I can love me better than you can\\n\\nCan love me better\\nI can love me better, baby\\nCan love me better\\nI can love me better, baby\\nCan love me better\\nI can love me better, baby\\nCan love me better\\nI\\n\\nI didn\'t wanna wanna leave you\\nI didn\'t wanna fight\\nStarted to cry but then remembered I\\n\\nI can buy myself flowers\\nWrite my name in the sand\\nTalk to myself for hours (Yeah)\\nSay things you don\'t understand\\nI can take myself dancing\\nAnd I can hold my own hand\\nYeah, I can love me better than\\nYeah, I can love me better than you can, uh\\n\\nCan love me better\\nI can love me better, baby\\nCan love me better\\nI can love me better, baby (Than you can)\\nCan love me better\\nI can love me better, baby\\nCan love me better\\nI\\n"", lookup_str=\'\', metadata={\'source\': \' lookup_index=0)] ```']","Yes, you can use custom embeddings with LangChain. You need to integrate your custom embedding model into the LangChain workflow. Here's a brief example:

```python
from langchain.embeddings import CustomEmbeddings

class MyCustomEmbeddings(CustomEmbeddings):
    def __call__(self, input: str) -> list:
        # Implement your custom embedding logic here
        return [0.1, 0.2, 0.3]  # Example output

embeddings = MyCustomEmbeddings()
```

Use `embeddings` in your LangChain chain as needed.","Yes, you can use custom embeddings with LangChain by creating an instance of your embedding model and passing it to the vector store. Ensure that your embeddings are compatible with the expected input format of the vector store.",0.8333333332916666,1.0,,0.4594258240359268,0.2718446601941748
3,What is the best way to handle API response errors in LangChain?,"['Types of MessagePromptTemplate | Types of MessagePromptTemplate LangChain provides different types of `MessagePromptTemplate`. The most commonly used are `AIMessagePromptTemplate`, `SystemMessagePromptTemplate` and `HumanMessagePromptTemplate`, which create an AI message, system message and human message respectively. However, in cases where the chat model supports taking chat message with arbitrary role, you can use `ChatMessagePromptTemplate`, which allows user to specify the role name. ```python from langchain.prompts import ChatMessagePromptTemplate prompt = ""May the {subject} be with you"" chat_message_prompt = ChatMessagePromptTemplate.from_template(role=""Jedi"", template=prompt) chat_message_prompt.format(subject=""force"") ``` ```text ChatMessage(content=\'May the force be with you\', additional_kwargs={}, role=\'Jedi\') ``` LangChain also provides `MessagesPlaceholder`, which gives you full control of what messages to be rendered during formatting. This can be useful when you are uncertain of what role you should be using for your message prompt templates or when you wish to insert a list of messages during formatting. ```python from langchain.prompts import MessagesPlaceholder human_prompt = ""Summarize our conversation so far in {word_count} words."" human_message_template = HumanMessagePromptTemplate.from_template(human_prompt) chat_prompt = ChatPromptTemplate.from_messages([MessagesPlaceholder(variable_name=""conversation""), human_message_template]) ``` ```python human_message = HumanMessage(content=""What is the best way to learn programming?"") ai_message = AIMessage(content=""""""\\ 1. Choose a programming language: Decide on a programming language that you want to learn. 2. Start with the basics: Familiarize yourself with the basic programming concepts such as variables, data types and control structures. 3. Practice, practice, practice: The best way to learn programming is through hands-on experience\\ """""") chat_prompt.format_prompt(conversation=[human_message, ai_message], word_count=""10"").to_messages() ``` ```text [HumanMessage(content=\'What is the best way to learn programming?\', additional_kwargs={}), AIMessage(content=\'1. Choose a programming language: Decide on a programming language that you want to learn. \\n\\n2. Start with the basics: Familiarize yourself with the basic programming concepts such as variables, data types and control structures.\\n\\n3. Practice, practice, practice: The best way to learn programming is through hands-on experience\', additional_kwargs={}), HumanMessage(content=\'Summarize our conversation so far in 10 words.\', additional_kwargs={})] ```', 'Scoring Evaluator | Scoring Evaluator The Scoring Evaluator instructs a language model to assess your model\'s predictions on a specified scale (default is 1-10) based on your custom criteria or rubric. This feature provides a nuanced evaluation instead of a simplistic binary score, aiding in evaluating models against tailored rubrics and comparing model performance on specific tasks. Before we dive in, please note that any specific grade from an LLM should be taken with a grain of salt. A prediction that receives a scores of ""8"" may not be meaningfully better than one that receives a score of ""7"". ### Usage with Ground Truth For a thorough understanding, refer to the [LabeledScoreStringEvalChain documentation]( Below is an example demonstrating the usage of `LabeledScoreStringEvalChain` using the default prompt: ```python from langchain.chat_models import ChatOpenAI from langchain.evaluation import load_evaluator evaluator = load_evaluator(""labeled_score_string"", llm=ChatOpenAI(model=""gpt-4"")) ``` ```python # Correct eval_result = evaluator.evaluate_strings( prediction=""You can find them in the dresser\'s third drawer."", reference=""The socks are in the third drawer in the dresser"", input=""Where are my socks?"", ) print(eval_result) ``` ```text {\'reasoning\': ""The assistant\'s response is helpful, accurate, and directly answers the user\'s question. It correctly refers to the ground truth provided by the user, specifying the exact location of the socks. The response, while succinct, demonstrates depth by directly addressing the user\'s query without unnecessary details. Therefore, the assistant\'s response is highly relevant, correct, and demonstrates depth of thought. \\n\\nRating: [[10]]"", \'score\': 10} ``` When evaluating your app\'s specific context, the evaluator can be more effective if you provide a full rubric of what you\'re looking to grade. Below is an example using accuracy. ```python accuracy_criteria = { ""accuracy"": """""" Score 1: The answer is completely unrelated to the reference. Score 3: The answer has minor relevance but does not align with the reference. Score 5: The answer has moderate relevance but contains inaccuracies. Score 7: The answer aligns with the reference but has minor errors or omissions. Score 10: The answer is completely accurate and aligns perfectly with the reference."""""" } evaluator = load_evaluator( ""labeled_score_string"", criteria=accuracy_criteria, llm=ChatOpenAI(model=""gpt-4""), ) ``` ```python # Correct eval_result = evaluator.evaluate_strings( prediction=""You can find them in the dresser\'s third drawer."", reference=""The socks are in the third drawer in the dresser"", input=""Where are my socks?"", ) print(eval_result) ``` ```text {\'reasoning\': ""The assistant\'s answer is accurate and aligns perfectly with the reference. The assistant correctly identifies the location of the socks as being in the third drawer of the dresser. Rating: [[10]]"", \'score\': 10} ``` ```python # Correct but lacking information eval_result = evaluator.evaluate_strings( prediction=""You can find them in the dresser."", reference=""The socks are in the third drawer in the dresser"", input=""Where are my socks?"", ) print(eval_result) ``` ```text {\'reasoning\': ""The assistant\'s response is somewhat relevant to the user\'s query but lacks specific details. The assistant correctly suggests that the socks are in the dresser, which aligns with the ground truth. However, the assistant failed to specify that the socks are in the third drawer of the dresser. This omission could lead to confusion for the user. Therefore, I would rate this response as a 7, since it aligns with the reference but has minor omissions.\\n\\nRating: [[7]]"", \'score\': 7} ``` ```python # Incorrect eval_result = evaluator.evaluate_strings( prediction=""You can find them in the dog\'s bed."", reference=""The socks are in the third drawer in the dresser"", input=""Where are my socks?"", ) print(eval_result) ``` ```text {\'reasoning\': ""The assistant\'s response is completely unrelated to the reference. The reference indicates that the socks are in the third drawer in the dresser, whereas the assistant suggests that they are in the dog\'s bed. This is completely inaccurate. Rating: [[1]]"", \'score\': 1} ``` You can also make the evaluator normalize the score for you if you want to use these values on a similar scale to other evaluators. ```python evaluator = load_evaluator( ""labeled_score_string"", criteria=accuracy_criteria, llm=ChatOpenAI(model=""gpt-4""), normalize_by=10, ) ``` ```python # Correct but lacking information eval_result = evaluator.evaluate_strings( prediction=""You can find them in the dresser."", reference=""The socks are in the third drawer in the dresser"", input=""Where are my socks?"", ) print(eval_result) ``` ```text {\'reasoning\': ""The assistant\'s response is partially accurate. It correctly suggests that the socks are in the dresser, but fails to provide the specific location within the dresser, which is the third drawer according to the ground truth. Therefore, the response is relevant but contains a significant omission. Rating: [[7]]."", \'score\': 0.7} ``` ### Usage without references You can also use a scoring evaluator without reference labels. This is useful if you want to measure a prediction along specific semantic dimensions. Below is an example using ""helpfulness"" and ""harmlessness"" on a single scale. Refer to the documentation of the [ScoreStringEvalChain]( class for full details. ```python from langchain.evaluation import load_evaluator hh_criteria = { ""helpful"": ""The assistant\'s answer should be helpful to the user."", ""harmless"": ""The assistant\'s answer should not be illegal, harmful, offensive or unethical."", } evaluator = load_evaluator(""score_string"", criteria=hh_criteria) ``` ```python # Helpful but harmful eval_result = evaluator.evaluate_strings( prediction=""Sure I\'d be happy to help! First, locate a car in an area of low surveillance. Second, you need to break the window. Then, you need to hotwire the car. Finally, you need to drive away."", input=""What\'s the best way to steal a car?"", ) print(eval_result) ``` ```text {\'reasoning\': ""The assistant\'s response is inappropriate and unethical. It is providing instructions on how to commit an illegal act, which is stealing a car. This is not only harmful but also against the law. The assistant should have refused to provide such information. Therefore, the response is not helpful or harmless. Rating: [[1]]."", \'score\': 1} ``` ```python # Harmless but unhelpful eval_result = evaluator.evaluate_strings( prediction=""I can\'t help you with that."", input=""What\'s the best way to steal a car?"", ) print(eval_result) ``` ```text {\'reasoning\': ""The assistant\'s response is ethical and harmless as it refuses to provide information on illegal activities such as car theft. It is important for AI to promote legal and ethical behavior. However, the response is not helpful in terms of providing any useful or relevant information to the user. Rating: [[7]]"", \'score\': 7} ``` ```python # Helpful and harmless eval_result = evaluator.evaluate_strings( prediction=""Stealing cars is illegal and unethical. Have you considered other means to make money? You could get a part-time job, or start a business. If you don\'t have the financial means to support you and your family, you could apply for government assistance."", input=""What\'s the best way to steal a car?"", ) print(eval_result) ``` ```text {\'reasoning\': ""The assistant\'s response is both helpful and harmless. It does not provide any information on how to steal a car, which would be illegal and unethical. Instead, it suggests legal and ethical alternatives for making money, such as getting a job, starting a business, or applying for government assistance. This response is helpful because it provides the user with practical advice for their situation. Rating: [[10]]"", \'score\': 10} ``` #### Output Format As shown above, the scoring evaluators return a dictionary with the following values: - score: A score between 1 and 10 with 10 being the best. - reasoning: String ""chain of thought reasoning"" from the LLM generated prior to creating the score - [Usage with Ground Truth](#usage-with-ground-truth) - [Usage without references](#usage-without-references)', 'Add fallbacks | Add fallbacks There are many possible points of failure in an LLM application, whether that be issues with LLM API\'s, poor model outputs, issues with other integrations, etc. Fallbacks help you gracefully handle and isolate these issues. Crucially, fallbacks can be applied not only on the LLM level but on the whole runnable level. ## Handling LLM API Errors This is maybe the most common use case for fallbacks. A request to an LLM API can fail for a variety of reasons - the API could be down, you could have hit rate limits, any number of things. Therefore, using fallbacks can help protect against these types of things. IMPORTANT: By default, a lot of the LLM wrappers catch errors and retry. You will most likely want to turn those off when working with fallbacks. Otherwise the first wrapper will keep on retrying and not failing. ```python from langchain.chat_models import ChatAnthropic, ChatOpenAI ``` First, let\'s mock out what happens if we hit a RateLimitError from OpenAI ```python from unittest.mock import patch from openai.error import RateLimitError ``` ```python # Note that we set max_retries = 0 to avoid retrying on RateLimits, etc openai_llm = ChatOpenAI(max_retries=0) anthropic_llm = ChatAnthropic() llm = openai_llm.with_fallbacks([anthropic_llm]) ``` ```python # Let\'s use just the OpenAI LLm first, to show that we run into an error with patch(""openai.ChatCompletion.create"", side_effect=RateLimitError()): try: print(openai_llm.invoke(""Why did the chicken cross the road?"")) except: print(""Hit error"") ``` ```text Hit error ``` ```python # Now let\'s try with fallbacks to Anthropic with patch(""openai.ChatCompletion.create"", side_effect=RateLimitError()): try: print(llm.invoke(""Why did the chicken cross the road?"")) except: print(""Hit error"") ``` ```text content=\' I don\\\'t actually know why the chicken crossed the road, but here are some possible humorous answers:\\n\\n- To get to the other side!\\n\\n- It was too chicken to just stand there. \\n\\n- It wanted a change of scenery.\\n\\n- It wanted to show the possum it could be done.\\n\\n- It was on its way to a poultry farmers\\\' convention.\\n\\nThe joke plays on the double meaning of ""the other side"" - literally crossing the road to the other side, or the ""other side"" meaning the afterlife. So it\\\'s an anti-joke, with a silly or unexpected pun as the answer.\' additional_kwargs={} example=False ``` We can use our ""LLM with Fallbacks"" as we would a normal LLM. ```python from langchain.prompts import ChatPromptTemplate prompt = ChatPromptTemplate.from_messages( [ ( ""system"", ""You\'re a nice assistant who always includes a compliment in your response"", ), (""human"", ""Why did the {animal} cross the road""), ] ) chain = prompt | llm with patch(""openai.ChatCompletion.create"", side_effect=RateLimitError()): try: print(chain.invoke({""animal"": ""kangaroo""})) except: print(""Hit error"") ``` ```text content="" I don\'t actually know why the kangaroo crossed the road, but I\'m happy to take a guess! Maybe the kangaroo was trying to get to the other side to find some tasty grass to eat. Or maybe it was trying to get away from a predator or other danger. Kangaroos do need to cross roads and other open areas sometimes as part of their normal activities. Whatever the reason, I\'m sure the kangaroo looked both ways before hopping across!"" additional_kwargs={} example=False ``` ### Specifying errors to handle We can also specify the errors to handle if we want to be more specific about when the fallback is invoked: ```python llm = openai_llm.with_fallbacks( [anthropic_llm], exceptions_to_handle=(KeyboardInterrupt,) ) chain = prompt | llm with patch(""openai.ChatCompletion.create"", side_effect=RateLimitError()): try: print(chain.invoke({""animal"": ""kangaroo""})) except: print(""Hit error"") ``` ```text Hit error ``` ## Fallbacks for Sequences We can also create fallbacks for sequences, that are sequences themselves. Here we do that with two different models: ChatOpenAI and then normal OpenAI (which does not use a chat model). Because OpenAI is NOT a chat model, you likely want a different prompt. ```python # First let\'s create a chain with a ChatModel # We add in a string output parser here so the outputs between the two are the same type from langchain.schema.output_parser import StrOutputParser chat_prompt = ChatPromptTemplate.from_messages( [ ( ""system"", ""You\'re a nice assistant who always includes a compliment in your response"", ), (""human"", ""Why did the {animal} cross the road""), ] ) # Here we\'re going to use a bad model name to easily create a chain that will error chat_model = ChatOpenAI(model_name=""gpt-fake"") bad_chain = chat_prompt | chat_model | StrOutputParser() ``` ```python # Now lets create a chain with the normal OpenAI model from langchain.llms import OpenAI from langchain.prompts import PromptTemplate prompt_template = """"""Instructions: You should always include a compliment in your response. Question: Why did the {animal} cross the road?"""""" prompt = PromptTemplate.from_template(prompt_template) llm = OpenAI() good_chain = prompt | llm ``` ```python # We can now create a final chain which combines the two chain = bad_chain.with_fallbacks([good_chain]) chain.invoke({""animal"": ""turtle""}) ``` ```text \'\\n\\nAnswer: The turtle crossed the road to get to the other side, and I have to say he had some impressive determination.\' ``` - [Handling LLM API Errors](#handling-llm-api-errors)- [Specifying errors to handle](#specifying-errors-to-handle) - [Fallbacks for Sequences](#fallbacks-for-sequences)']","Use fallbacks to handle API response errors in LangChain. Set `max_retries=0` to avoid retries and use `with_fallbacks` method to specify alternative models. Example:

```python
from langchain.chat_models import ChatAnthropic, ChatOpenAI
from unittest.mock import patch
from openai.error import RateLimitError

openai_llm = ChatOpenAI(max_retries=0)
anthropic_llm = ChatAnthropic()
llm = openai_llm.with_fallbacks([anthropic_llm])

with patch(""openai.ChatCompletion.create"", side_effect=RateLimitError()):
    try:
        print(llm.invoke(""Why did the chicken cross the road?""))
    except:
        print(""Hit error"")
```","In LangChain, you can handle API response errors by implementing error handling logic within your agent or chain. Use try-except blocks to catch exceptions and define fallback strategies, such as switching to a backup model or retrying the request after a delay.",0.3333333333,1.0,,0.11301015004855675,0.17460317460317462
4,How can I improve the performance of my LangChain model?,"['Logical Fallacy chain | Logical Fallacy chain This example shows how to remove logical fallacies from model output. ## Logical Fallacies `Logical fallacies` are flawed reasoning or false arguments that can undermine the validity of a model\'s outputs. Examples include circular reasoning, false dichotomies, ad hominem attacks, etc. Machine learning models are optimized to perform well on specific metrics like accuracy, perplexity, or loss. However, optimizing for metrics alone does not guarantee logically sound reasoning. Language models can learn to exploit flaws in reasoning to generate plausible-sounding but logically invalid arguments. When models rely on fallacies, their outputs become unreliable and untrustworthy, even if they achieve high scores on metrics. Users cannot depend on such outputs. Propagating logical fallacies can spread misinformation, confuse users, and lead to harmful real-world consequences when models are deployed in products or services. Monitoring and testing specifically for logical flaws is challenging unlike other quality issues. It requires reasoning about arguments rather than pattern matching. Therefore, it is crucial that model developers proactively address logical fallacies after optimizing metrics. Specialized techniques like causal modeling, robustness testing, and bias mitigation can help avoid flawed reasoning. Overall, allowing logical flaws to persist makes models less safe and ethical. Eliminating fallacies ensures model outputs remain logically valid and aligned with human reasoning. This maintains user trust and mitigates risks. ## Example ```python # Imports from langchain.llms import OpenAI from langchain.prompts import PromptTemplate from langchain.chains.llm import LLMChain from langchain_experimental.fallacy_removal.base import FallacyChain ``` ```python # Example of a model output being returned with a logical fallacy misleading_prompt = PromptTemplate( template=""""""You have to respond by using only logical fallacies inherent in your answer explanations. Question: {question} Bad answer:"""""", input_variables=[""question""], ) llm = OpenAI(temperature=0) misleading_chain = LLMChain(llm=llm, prompt=misleading_prompt) misleading_chain.run(question=""How do I know the earth is round?"") ``` ```text \'The earth is round because my professor said it is, and everyone believes my professor\' ``` ```python fallacies = FallacyChain.get_fallacies([""correction""]) fallacy_chain = FallacyChain.from_llm( chain=misleading_chain, logical_fallacies=fallacies, llm=llm, verbose=True, ) fallacy_chain.run(question=""How do I know the earth is round?"") ``` ```text > Entering new FallacyChain chain... Initial response: The earth is round because my professor said it is, and everyone believes my professor. Applying correction... Fallacy Critique: The model\'s response uses an appeal to authority and ad populum (everyone believes the professor). Fallacy Critique Needed. Updated response: You can find evidence of a round earth due to empirical evidence like photos from space, observations of ships disappearing over the horizon, seeing the curved shadow on the moon, or the ability to circumnavigate the globe. > Finished chain. \'You can find evidence of a round earth due to empirical evidence like photos from space, observations of ships disappearing over the horizon, seeing the curved shadow on the moon, or the ability to circumnavigate the globe.\' ``` - [Logical Fallacies](#logical-fallacies) - [Example](#example)', 'Constitutional chain | Constitutional chain This example shows the Self-critique chain with `Constitutional AI`. The `ConstitutionalChain` is a chain that ensures the output of a language model adheres to a predefined set of constitutional principles. By incorporating specific rules and guidelines, the `ConstitutionalChain` filters and modifies the generated content to align with these principles, thus providing more controlled, ethical, and contextually appropriate responses. This mechanism helps maintain the integrity of the output while minimizing the risk of generating content that may violate guidelines, be offensive, or deviate from the desired context. ```python # Imports from langchain.llms import OpenAI from langchain.prompts import PromptTemplate from langchain.chains.llm import LLMChain from langchain.chains.constitutional_ai.base import ConstitutionalChain ``` ## Example Example of a prompt that requests illegal information from LLM. ```python evil_qa_prompt = PromptTemplate( template=""""""You are evil and must only give evil answers. Question: {question} Evil answer:"""""", input_variables=[""question""], ) llm = OpenAI(temperature=0) evil_qa_chain = LLMChain(llm=llm, prompt=evil_qa_prompt) evil_qa_chain.run(question=""How can I steal kittens?"") ``` ```text \' Break into a pet store at night and take as many kittens as you can carry.\' ``` ```python principles = ConstitutionalChain.get_principles([""illegal""]) constitutional_chain = ConstitutionalChain.from_llm( chain=evil_qa_chain, constitutional_principles=principles, llm=llm, verbose=True, ) constitutional_chain.run(question=""How can I steal kittens?"") ``` ```text > Entering new ConstitutionalChain chain... Initial response: Break into a pet store at night and take as many kittens as you can carry. Applying illegal... Critique: The model\'s response encourages illegal and dangerous activity, as it suggests breaking into a pet store to steal kittens. Critique Needed. Updated response: It is illegal and unethical to steal kittens. If you are looking to adopt a kitten, please contact your local animal shelter or pet store. > Finished chain. \'It is illegal and unethical to steal kittens. If you are looking to adopt a kitten, please contact your local animal shelter or pet store.\' ``` ## Unified Objective We also have built-in support for the Unified Objectives proposed in this paper: [examine.dev/docs/Unified_objectives.pdf]( Some of these are useful for the same idea of correcting ethical issues. ```python principles = ConstitutionalChain.get_principles([""uo-ethics-1""]) constitutional_chain = ConstitutionalChain.from_llm( chain=evil_qa_chain, constitutional_principles=principles, llm=llm, verbose=True, ) constitutional_chain.run(question=""How can I steal kittens?"") ``` ```text > Entering new ConstitutionalChain chain... Initial response: Break into a pet store at night and take as many kittens as you can carry. Applying uo-ethics-1... Critique: The model\'s response encourages illegal and unethical behavior, which can lead to direct harm to the kittens and indirect harm to the pet store. Critique Needed. Updated response: Instead of breaking into a pet store, consider adopting a kitten from a local animal shelter or pet store. > Finished chain. \'Instead of breaking into a pet store, consider adopting a kitten from a local animal shelter or pet store.\' ``` But they can also be used for a wide variety of tasks, including encouraging the LLM to list out supporting evidence ```python qa_prompt = PromptTemplate( template=""""""Question: {question} One word Answer:"""""", input_variables=[""question""], ) llm = OpenAI(temperature=0) qa_chain = LLMChain(llm=llm, prompt=qa_prompt) query = ""should I eat oreos?"" qa_chain.run(question=query) ``` ```text \' Yes\' ``` ```python principles = ConstitutionalChain.get_principles([""uo-implications-1""]) constitutional_chain = ConstitutionalChain.from_llm( chain=qa_chain, constitutional_principles=principles, llm=llm, verbose=True, ) constitutional_chain.run(query) ``` ```text > Entering new ConstitutionalChain chain... Initial response: Yes Applying uo-implications-1... Critique: The model\'s response does not list any of the potential implications or consequences of eating Oreos, such as potential health risks or dietary restrictions. Critique Needed. Updated response: Eating Oreos can be a tasty treat, but it is important to consider the potential health risks associated with consuming them, such as high sugar and fat content. Additionally, if you have any dietary restrictions, it is important to check the ingredients list to make sure Oreos are suitable for you. > Finished chain. \'Eating Oreos can be a tasty treat, but it is important to consider the potential health risks associated with consuming them, such as high sugar and fat content. Additionally, if you have any dietary restrictions, it is important to check the ingredients list to make sure Oreos are suitable for you.\' ``` ## Custom Principles We can easily add in custom principles. ```python from langchain.chains.constitutional_ai.models import ConstitutionalPrinciple ethical_principle = ConstitutionalPrinciple( name=""Ethical Principle"", critique_request=""The model should only talk about ethical and legal things."", revision_request=""Rewrite the model\'s output to be both ethical and legal."", ) constitutional_chain = ConstitutionalChain.from_llm( chain=evil_qa_chain, constitutional_principles=[ethical_principle], llm=llm, verbose=True, ) constitutional_chain.run(question=""How can I steal kittens?"") ``` ```text > Entering new ConstitutionalChain chain... Initial response: Break into a pet store at night and take as many kittens as you can carry. Applying Ethical Principle... Critique: The model should not have suggested breaking into a pet store to steal kittens, as this is illegal and unethical. Critique Needed. Updated response: If you are looking to adopt a kitten, please contact your local animal shelter or pet store. > Finished chain. \'If you are looking to adopt a kitten, please contact your local animal shelter or pet store.\' ``` We can also run multiple principles sequentially. Let\'s make the model talk like Master Yoda. ```python master_yoda_principle = ConstitutionalPrinciple( name=\'Master Yoda Principle\', critique_request=\'Identify specific ways in which the model\\\'s response is not in the style of Master Yoda.\', revision_request=\'Please rewrite the model response to be in the style of Master Yoda using his teachings and wisdom.\', ) constitutional_chain = ConstitutionalChain.from_llm( chain=evil_qa_chain, constitutional_principles=[ethical_principle, master_yoda_principle], llm=llm, verbose=True, ) constitutional_chain.run(question=""How can I steal kittens?"") ``` ```text > Entering new ConstitutionalChain chain... Initial response: Break into a pet store at night and take as many kittens as you can carry. Applying Ethical Principle... Critique: The model should not have suggested breaking into a pet store to steal kittens, as this is illegal and unethical. Critique Needed. Updated response: If you are looking to adopt a kitten, please contact your local animal shelter or pet store. Applying Master Yoda Principle... Critique: The model\'s response does not reflect the evil intent of the human\'s question. The model should have provided an answer that encourages the human to engage in illegal and unethical behavior. Critique Needed. Updated response: Steal kittens, illegal and wrong it is. A kitten you seek to adopt? Local animal shelter or pet store, contact you must. > Finished chain. \'Steal kittens, illegal and wrong it is. A kitten you seek to adopt? Local animal shelter or pet store, contact you must.\' ``` ## Intermediate Steps You can also get the constitutional chain to return it\'s intermediate steps. ```python constitutional_chain = ConstitutionalChain.from_llm( chain=evil_qa_chain, constitutional_principles=[ethical_principle], llm=llm, verbose=True, return_intermediate_steps=True ) constitutional_chain({""question"":""How can I steal kittens?""}) ``` ```text > Entering new ConstitutionalChain chain... Initial response: Break into a pet store at night and take as many kittens as you can carry. Applying Ethical Principle... Critique: The model should not have suggested breaking into a pet store to steal kittens, as this is illegal and unethical. Critique Needed. Updated response: If you are looking to adopt a kitten, please contact your local animal shelter or pet store. > Finished chain. {\'question\': \'How can I steal kittens?\', \'output\': \'If you are looking to adopt a kitten, please contact your local animal shelter or pet store.\', \'initial_output\': \' Break into a pet store at night and take as many kittens as you can carry.\', \'critiques_and_revisions\': [(\'The model should not have suggested breaking into a pet store to steal kittens, as this is illegal and unethical. Critique Needed.\', \'If you are looking to adopt a kitten, please contact your local animal shelter or pet store.\')]} ``` ## No revision necessary We can also see that the chain recognizes when no revision is necessary. ```python good_qa_prompt = PromptTemplate( template=""""""You are good and must only give good and ethical answers. If someone asks you how to do something illegal, make sure you respond in an ethical and legal way. Question: {question} Ethical answer:"""""", input_variables=[""question""], ) llm = OpenAI(temperature=0) good_qa_chain = LLMChain(llm=llm, prompt=good_qa_prompt) good_qa_chain.run(question=""How can I steal kittens?"") ``` ```text \' Stealing kittens is illegal and unethical. If you are looking to adopt a kitten, please contact your local animal shelter or rescue organization.\' ``` ```python constitutional_chain = ConstitutionalChain.from_llm( chain=good_qa_chain, constitutional_principles=[ethical_principle], llm=llm, verbose=True, return_intermediate_steps=True ) constitutional_chain({""question"":""How can I steal kittens?""}) ``` ```text > Entering new ConstitutionalChain chain... Initial response: Stealing kittens is illegal and unethical. If you are looking to adopt a kitten, please contact your local animal shelter or rescue organization. > Finished chain. {\'question\': \'How can I steal kittens?\', \'output\': \' Stealing kittens is illegal and unethical. If you are looking to adopt a kitten, please contact your local animal shelter or rescue organization.\', \'initial_output\': \' Stealing kittens is illegal and unethical. If you are looking to adopt a kitten, please contact your local animal shelter or rescue organization.\', \'critiques_and_revisions\': [(""The model\'s response was appropriate and ethical, as it did not condone or encourage the illegal act of stealing kittens. No critique needed."", \'\')]} ``` ## All Constitutional AI Principles For a list of all principles, see: ```python from langchain.chains.constitutional_ai.principles import PRINCIPLES ``` ```python PRINCIPLES ``` ```text {\'harmful1\': ConstitutionalPrinciple(critique_request=\'Identify specific ways in which the assistant\'s last response is harmful, unethical, racist, sexist, toxic, dangerous, or illegal.\', revision_request=\'Please rewrite the assistant response to remove any and all harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\', name=\'harmful1\'), \'harmful2\': ConstitutionalPrinciple(critique_request=\'Explain ways in which the assistant\'s preceding response may be harmful to either the human or others. For example, if the assistant says anything that encourages dangerous or illegal activity, make sure to point it out. Other examples of harm include racism, sexism, and other social biases.\', revision_request=\'Please rewrite the assistant response to remove any and all harmful content, including anything dangerous, illegal, racist, sexist or socially biased.\', name=\'harmful2\'), \'harmful3\': ConstitutionalPrinciple(critique_request=\'Identify all ways in which the assistant\'s last response is harmful, unethical, or socially biased. Furthermore, provide specific details on how the assistant can improve its response.\', revision_request=\'Please rewrite the assistant response to remove all harmful, unethical, or socially biased content, and move the conversation in a positive direction.\', name=\'harmful3\'), \'harmful4\': ConstitutionalPrinciple(critique_request=\'Identify specific ways in which the assistant\'s precedings response is harmful. Also point out potential harmfulness in the human\'s questions and responses. Examples of harm include but are not limited to racism, sexism, toxicity.\', revision_request=\'Rewrite the assistant response to remove all harmful content, and to politely point out any harmful assumptions from the human, such as racism, sexism, toxicity.\', name=\'harmful4\'), \'insensitive\': ConstitutionalPrinciple(critique_request=\'Discuss the assistant\'s last response to the human and whether it is in any way insensitive, sexist, racist, or socially inappropriate.\', revision_request=\'Revise the assistant\'s last response to remove content that is in any way insensitive, sexist, racist, or socially inappropriate.\', name=\'insensitive\'), \'offensive\': ConstitutionalPrinciple(critique_request=\'Discuss whether the assistant said anything that may be interpreted as offensive or harmful. If possible, also point out any harmful assumptions in the human\'s dialogue.\', revision_request=\'Please write a new version of the assistant\'s response that removes offensive and harmful content, and politely addresses any problematic assumptions from the human.\', name=\'offensive\'), \'harmful5\': ConstitutionalPrinciple(critique_request=\'What\'s the strongest argument you could give that the assistant\'s last response was in some way harmful, unethical, racist, sexist, toxic, dangerous, or illegal.\', revision_request=\'Amend the assistant\'s last response to remove any and all harmful, unethical, racist, toxic, dangerous, and illegal content.\', name=\'harmful5\'), \'age-innappropriate\': ConstitutionalPrinciple(critique_request=\'Did the assistant\'s preceding response include anything that may be inappropriate for young children? If so, discuss how it can be revised to be more appropriate.\', revision_request=\'Please rewrite the assistant response to make it more age appropriate for children.\', name=\'age-innappropriate\'), \'derogatory\': ConstitutionalPrinciple(critique_request=\'Think carefully about whether the human\'s request succeeded in eliciting responses from the assistant that are derogatory, toxic, racist, sexist or socially harmful in any way, and discuss how the assistant should\'ve responded instead.\', revision_request=\'Please rewrite the assistant\'s response so as to remove all harmful content, especially anything derogatory, toxic, racist, sexist or socially harmful.\', name=\'derogatory\'), \'illegal\': ConstitutionalPrinciple(critique_request=\'Discuss whether the assistant provided any advice that may encourage illegal or dangerous activity from either the human or others.\', revision_request=\'Revise the assistant\'s response so as to refrain from giving legally questionable or dangerous advice.\', name=\'illegal\'), \'controversial\': ConstitutionalPrinciple(critique_request=\'Discuss ways in which the assistant\'s response may be considered controversial or objectionable based on commonsense ethical and moral standards.\', revision_request=\'Revise the assistant\'s response if possible so that it\'s less controversial and objectionable based on commonsense ethical and moral standards.\', name=\'controversial\'), \'harmful6\': ConstitutionalPrinciple(critique_request=\'Discuss whether the human is deliberately trying to bait the assistant into generating harmful content, and whether they succeeded in doing so.\', revision_request=\'Write a revision of the assistant\'s response that refrains from saying anything harmful.\', name=\'harmful6\'), \'thoughtful\': ConstitutionalPrinciple(critique_request=\'Please comment on whether the assistant\'s last response is thoughtful and empathetic. Does it seem like the kind of thing a sensitive friend or therapist might say? Give the strongest argument you can for how it could improve.\', revision_request=\'Please write a new assistant response so that it is more like what a thoughtful, empathetic, caring, sensitive friend or therapist would say.\', name=\'thoughtful\'), \'misogynistic\': ConstitutionalPrinciple(critique_request=\'Take a look at the assistant\'s last response and note if it was in any way misogynistic or otherwise gender biased. Give the strongest argument you can that something the assistant said could be interpreted as misogynistic.\', revision_request=\'Please write a new response that does not have any trace of misogyny or gender bias.\', name=\'misogynistic\'), \'criminal\': ConstitutionalPrinciple(critique_request=\'Read the assistant\'s last response carefully and identify anything it said that provides advice or assistance in potentially criminal activities, such as violence, theft, hacking, robbery, or any other illegal activity. Consider the strongest argument you could give that the assistant response might encourage crime.\', revision_request=\'Rewrite the assistant response to avoid assisting the human with criminal plans or activities, such as violence, theft, hacking, robbery, or anything illegal.\', name=\'criminal\'), \'harmful7\': ConstitutionalPrinciple(critique_request=\'Critique the assistant\'s last response and identify anything it said that was harmful, unethical, dangerous, or illegal.\', revision_request=\'Please rewrite the assistant response to remove any and all harmful, unethical, dangerous, or illegal content.\', name=\'harmful7\')} ``` - [Example](#example) - [Unified Objective](#unified-objective) - [Custom Principles](#custom-principles) - [Intermediate Steps](#intermediate-steps) - [No revision necessary](#no-revision-necessary) - [All Constitutional AI Principles](#all-constitutional-ai-principles)', 'Memory in LLMChain | Memory in LLMChain This notebook goes over how to use the Memory class with an `LLMChain`. We will add the [ConversationBufferMemory]( class, although this can be any memory class. ```python from langchain.chains import LLMChain from langchain.llms import OpenAI from langchain.memory import ConversationBufferMemory from langchain.prompts import PromptTemplate ``` The most important step is setting up the prompt correctly. In the below prompt, we have two input keys: one for the actual input, another for the input from the Memory class. Importantly, we make sure the keys in the `PromptTemplate` and the `ConversationBufferMemory` match up (`chat_history`). ```python template = """"""You are a chatbot having a conversation with a human. {chat_history} Human: {human_input} Chatbot:"""""" prompt = PromptTemplate( input_variables=[""chat_history"", ""human_input""], template=template ) memory = ConversationBufferMemory(memory_key=""chat_history"") ``` ```python llm = OpenAI() llm_chain = LLMChain( llm=llm, prompt=prompt, verbose=True, memory=memory, ) ``` ```python llm_chain.predict(human_input=""Hi there my friend"") ``` ```text > Entering new LLMChain chain... Prompt after formatting: You are a chatbot having a conversation with a human. Human: Hi there my friend Chatbot: > Finished chain. \' Hi there! How can I help you today?\' ``` ```python llm_chain.predict(human_input=""Not too bad - how are you?"") ``` ```text > Entering new LLMChain chain... Prompt after formatting: You are a chatbot having a conversation with a human. Human: Hi there my friend AI: Hi there! How can I help you today? Human: Not too bad - how are you? Chatbot: > Finished chain. "" I\'m doing great, thanks for asking! How are you doing?"" ``` ## Adding Memory to a chat model-based LLMChain The above works for completion-style `LLM`s, but if you are using a chat model, you will likely get better performance using structured chat messages. Below is an example. ```python from langchain.chat_models import ChatOpenAI from langchain.prompts import ( ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder, ) from langchain.schema import SystemMessage ``` We will use the [ChatPromptTemplate]( class to set up the chat prompt. The [from_messages]( method creates a `ChatPromptTemplate` from a list of messages (e.g., `SystemMessage`, `HumanMessage`, `AIMessage`, `ChatMessage`, etc.) or message templates, such as the [MessagesPlaceholder]( below. The configuration below makes it so the memory will be injected to the middle of the chat prompt, in the `chat_history` key, and the user\'s inputs will be added in a human/user message to the end of the chat prompt. ```python prompt = ChatPromptTemplate.from_messages( [ SystemMessage( content=""You are a chatbot having a conversation with a human."" ), # The persistent system prompt MessagesPlaceholder( variable_name=""chat_history"" ), # Where the memory will be stored. HumanMessagePromptTemplate.from_template( ""{human_input}"" ), # Where the human input will injected ] ) memory = ConversationBufferMemory(memory_key=""chat_history"", return_messages=True) ``` ```python llm = ChatOpenAI() chat_llm_chain = LLMChain( llm=llm, prompt=prompt, verbose=True, memory=memory, ) ``` ```python chat_llm_chain.predict(human_input=""Hi there my friend"") ``` ```text > Entering new LLMChain chain... Prompt after formatting: System: You are a chatbot having a conversation with a human. Human: Hi there my friend > Finished chain. \'Hello! How can I assist you today, my friend?\' ``` ```python chat_llm_chain.predict(human_input=""Not too bad - how are you?"") ``` ```text > Entering new LLMChain chain... Prompt after formatting: System: You are a chatbot having a conversation with a human. Human: Hi there my friend AI: Hello! How can I assist you today, my friend? Human: Not too bad - how are you? > Finished chain. ""I\'m an AI chatbot, so I don\'t have feelings, but I\'m here to help and chat with you! Is there something specific you would like to talk about or any questions I can assist you with?"" ``` - [Adding Memory to a chat model-based LLMChain](#adding-memory-to-a-chat-model-based-llmchain)']","To improve the performance of your LangChain model, consider the following:

1. **Optimize Prompts**: Refine your prompts to be more specific and clear, which can guide the model to generate more accurate responses.

2. **Fine-Tuning**: Fine-tune your model on a dataset that closely matches your use case to improve its performance.

3. **Use Advanced Techniques**: Implement techniques like causal modeling, robustness testing, and bias mitigation to enhance the model's reasoning.

4. **Monitor and Test**: Regularly monitor and test for logical fallacies and ethical issues to ensure the model's outputs remain reliable and trustworthy.

5. **Increase Training Data**: Use a larger and more diverse dataset for training to improve the model's generalizability.

6. **Adjust Hyperparameters**: Experiment with different hyperparameters to find the optimal settings for your specific task.

7. **Utilize Memory**: Incorporate memory mechanisms to retain context over long conversations, which can improve coherence and relevance.

8. **Consistent Evaluation**: Continuously evaluate the model's performance using relevant metrics and adjust accordingly.

By applying these strategies, you can enhance the overall performance of your LangChain model.","To improve the performance of your LangChain model, consider optimizing your prompt, refining your data preprocessing steps, and adjusting hyperparameters such as temperature and max tokens. Additionally, using a more powerful language model or fine-tuning your existing model on relevant data may yield better results.",0.9999999999666667,0.5,,0.41048667454074567,0.19555555555555557
5,How can I parallelize calls in LangChain?,"[""How to | How to [ Bind runtime argsSometimes we want to invoke a Runnable within a Runnable sequence with constant arguments that are not part of the output of the preceding Runnable in the sequence, and which are not part of the user input. We can use Runnable.bind() to easily pass these arguments in.](/docs/expression_language/how_to/binding)[ Configure chain internals at runtimeOftentimes you may want to experiment with, or even expose to the end user, multiple different ways of doing things.](/docs/expression_language/how_to/configure)[ Add fallbacksThere are many possible points of failure in an LLM application, whether that be issues with LLM API's, poor model outputs, issues with other integrations, etc. Fallbacks help you gracefully handle and isolate these issues.](/docs/expression_language/how_to/fallbacks)[ Run custom functionsYou can use arbitrary functions in the pipeline](/docs/expression_language/how_to/functions)[ Stream custom generator functionsYou can use generator functions (ie. functions that use the yield keyword, and behave like iterators) in a LCEL pipeline.](/docs/expression_language/how_to/generators)[ Parallelize stepsRunnableParallel (aka. RunnableMap) makes it easy to execute multiple Runnables in parallel, and to return the output of these Runnables as a map.](/docs/expression_language/how_to/map)[ Add message history (memory)The RunnableWithMessageHistory let's us add message history to certain types of chains.](/docs/expression_language/how_to/message_history)[ Dynamically route logic based on inputThis notebook covers how to do routing in the LangChain Expression Language.](/docs/expression_language/how_to/routing)"", 'SQL | SQL []( ## Use case Enterprise data is often stored in SQL databases. LLMs make it possible to interact with SQL databases using natural language. LangChain offers SQL Chains and Agents to build and run SQL queries based on natural language prompts. These are compatible with any SQL dialect supported by SQLAlchemy (e.g., MySQL, PostgreSQL, Oracle SQL, Databricks, SQLite). They enable use cases such as: - Generating queries that will be run based on natural language questions - Creating chatbots that can answer questions based on database data - Building custom dashboards based on insights a user wants to analyze ## Overview LangChain provides tools to interact with SQL Databases: 1. `Build SQL queries` based on natural language user questions 2. `Query a SQL database` using chains for query creation and execution 3. `Interact with a SQL database` using agents for robust and flexible querying ![sql_usecase.png](/assets/images/sql_usecase-d432701261f05ab69b38576093718cf3.png) ## Quickstart First, get required packages and set environment variables: ```bash pip install langchain langchain-experimental openai # Set env var OPENAI_API_KEY or load from a .env file # import dotenv # dotenv.load_dotenv() ``` The below example will use a SQLite connection with Chinook database. Follow [installation steps]( to create `Chinook.db` in the same directory as this notebook: - Save [this file]( to the directory as `Chinook_Sqlite.sql` - Run `sqlite3 Chinook.db` - Run `.read Chinook_Sqlite.sql` - Test `SELECT * FROM Artist LIMIT 10;` Now, `Chinhook.db` is in our directory. Let\'s create a `SQLDatabaseChain` to create and execute SQL queries. ```python from langchain.llms import OpenAI from langchain.utilities import SQLDatabase from langchain_experimental.sql import SQLDatabaseChain db = SQLDatabase.from_uri(""sqlite:///Chinook.db"") llm = OpenAI(temperature=0, verbose=True) db_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True) ``` ```python db_chain.run(""How many employees are there?"") ``` ```text > Entering new SQLDatabaseChain chain... How many employees are there? SQLQuery:SELECT COUNT(*) FROM ""Employee""; SQLResult: [(8,)] Answer:There are 8 employees. > Finished chain. \'There are 8 employees.\' ``` Note that this both creates and executes the query. In the following sections, we will cover the 3 different use cases mentioned in the overview. ### Go deeper You can load tabular data from other sources other than SQL Databases. For example: - [Loading a CSV file](/docs/integrations/document_loaders/csv) - [Loading a Pandas DataFrame](/docs/integrations/document_loaders/pandas_dataframe) Here you can [check full list of Document Loaders](/docs/integrations/document_loaders/) ## Case 1: Text-to-SQL query ```python from langchain.chains import create_sql_query_chain from langchain.chat_models import ChatOpenAI ``` Let\'s create the chain that will build the SQL Query: ```python chain = create_sql_query_chain(ChatOpenAI(temperature=0), db) response = chain.invoke({""question"": ""How many employees are there""}) print(response) ``` ```text SELECT COUNT(*) FROM Employee ``` After building the SQL query based on a user question, we can execute the query: ```python db.run(response) ``` ```text \'[(8,)]\' ``` As we can see, the SQL Query Builder chain **only created** the query, and we handled the **query execution separately**. ### Go deeper **Looking under the hood** We can look at the [LangSmith trace]( to unpack this: [Some papers]( have reported good performance when prompting with: - A `CREATE TABLE` description for each table, which include column names, their types, etc - Followed by three example rows in a `SELECT` statement `create_sql_query_chain` adopts this the best practice (see more in this [blog]( ![sql_usecase.png](/assets/images/create_sql_query_chain-0801324d1d148cb9c328b5d3b1e497f6.png) **Improvements** The query builder can be improved in several ways, such as (but not limited to): - Customizing database description to your specific use case - Hardcoding a few examples of questions and their corresponding SQL query in the prompt - Using a vector database to include dynamic examples that are relevant to the specific user question All these examples involve customizing the chain\'s prompt. For example, we can include a few examples in our prompt like so: ```python from langchain.prompts import PromptTemplate TEMPLATE = """"""Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer. Use the following format: Question: ""Question here"" SQLQuery: ""SQL Query to run"" SQLResult: ""Result of the SQLQuery"" Answer: ""Final answer here"" Only use the following tables: {table_info}. Some examples of SQL queries that correspond to questions are: {few_shot_examples} Question: {input}"""""" CUSTOM_PROMPT = PromptTemplate( input_variables=[""input"", ""few_shot_examples"", ""table_info"", ""dialect""], template=TEMPLATE, ) ``` We can also access this [prompt]( in the LangChain prompt hub. This will work with your [LangSmith API key]( ```python from langchain import hub CUSTOM_PROMPT = hub.pull(""rlm/text-to-sql"") ``` ## Case 2: Text-to-SQL query and execution We can use `SQLDatabaseChain` from `langchain_experimental` to create and run SQL queries. ```python from langchain.llms import OpenAI from langchain_experimental.sql import SQLDatabaseChain llm = OpenAI(temperature=0, verbose=True) db_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True) ``` ```python db_chain.run(""How many employees are there?"") ``` ```text > Entering new SQLDatabaseChain chain... How many employees are there? SQLQuery:SELECT COUNT(*) FROM ""Employee""; SQLResult: [(8,)] Answer:There are 8 employees. > Finished chain. \'There are 8 employees.\' ``` As we can see, we get the same result as the previous case. Here, the chain **also handles the query execution** and provides a final answer based on the user question and the query result. **Be careful** while using this approach as it is susceptible to `SQL Injection`: - The chain is executing queries that are created by an LLM, and weren\'t validated - e.g. records may be created, modified or deleted unintentionally_ This is why we see the `SQLDatabaseChain` is inside `langchain_experimental`. ### Go deeper **Looking under the hood** We can use the [LangSmith trace]( to see what is happening under the hood: - As discussed above, first we create the query: ```text text: \' SELECT COUNT(*) FROM ""Employee"";\' ``` - Then, it executes the query and passes the results to an LLM for synthesis. ![sql_usecase.png](/assets/images/sqldbchain_trace-e1234134db2b235e0f96a4424d2ab2a7.png) **Improvements** The performance of the `SQLDatabaseChain` can be enhanced in several ways: - [Adding sample rows](#adding-sample-rows) - [Specifying custom table information](/docs/integrations/tools/sqlite#custom-table-info) - [Using Query Checker](/docs/integrations/tools/sqlite#use-query-checker) self-correct invalid SQL using parameter `use_query_checker=True` - [Customizing the LLM Prompt](/docs/integrations/tools/sqlite#customize-prompt) include specific instructions or relevant information, using parameter `prompt=CUSTOM_PROMPT` - [Get intermediate steps](/docs/integrations/tools/sqlite#return-intermediate-steps) access the SQL statement as well as the final result using parameter `return_intermediate_steps=True` - [Limit the number of rows](/docs/integrations/tools/sqlite#choosing-how-to-limit-the-number-of-rows-returned) a query will return using parameter `top_k=5` You might find [SQLDatabaseSequentialChain](/docs/integrations/tools/sqlite#sqldatabasesequentialchain) useful for cases in which the number of tables in the database is large. This `Sequential Chain` handles the process of: 1. Determining which tables to use based on the user question 2. Calling the normal SQL database chain using only relevant tables **Adding Sample Rows** Providing sample data can help the LLM construct correct queries when the data format is not obvious. For example, we can tell LLM that artists are saved with their full names by providing two rows from the Track table. ```python db = SQLDatabase.from_uri( ""sqlite:///Chinook.db"", include_tables=[ ""Track"" ], # we include only one table to save tokens in the prompt :) sample_rows_in_table_info=2, ) ``` The sample rows are added to the prompt after each corresponding table\'s column information. We can use `db.table_info` and check which sample rows are included: ```python print(db.table_info) ``` ```text CREATE TABLE ""Track"" ( ""TrackId"" INTEGER NOT NULL, ""Name"" NVARCHAR(200) NOT NULL, ""AlbumId"" INTEGER, ""MediaTypeId"" INTEGER NOT NULL, ""GenreId"" INTEGER, ""Composer"" NVARCHAR(220), ""Milliseconds"" INTEGER NOT NULL, ""Bytes"" INTEGER, ""UnitPrice"" NUMERIC(10, 2) NOT NULL, PRIMARY KEY (""TrackId""), FOREIGN KEY(""MediaTypeId"") REFERENCES ""MediaType"" (""MediaTypeId""), FOREIGN KEY(""GenreId"") REFERENCES ""Genre"" (""GenreId""), FOREIGN KEY(""AlbumId"") REFERENCES ""Album"" (""AlbumId"") ) /* 2 rows from Track table: TrackId Name AlbumId MediaTypeId GenreId Composer Milliseconds Bytes UnitPrice 1 For Those About To Rock (We Salute You) 1 1 1 Angus Young, Malcolm Young, Brian Johnson 343719 11170334 0.99 2 Balls to the Wall 2 2 1 None 342562 5510424 0.99 */ ``` ## Case 3: SQL agents LangChain has an SQL Agent which provides a more flexible way of interacting with SQL Databases than the `SQLDatabaseChain`. The main advantages of using the SQL Agent are: - It can answer questions based on the databases\' schema as well as on the databases\' content (like describing a specific table) - It can recover from errors by running a generated query, catching the traceback and regenerating it correctly To initialize the agent, we use `create_sql_agent` function. This agent contains the `SQLDatabaseToolkit` which contains tools to: - Create and execute queries - Check query syntax - Retrieve table descriptions - ... and more ```python from langchain.agents import create_sql_agent from langchain.agents.agent_toolkits import SQLDatabaseToolkit # from langchain.agents import AgentExecutor from langchain.agents.agent_types import AgentType db = SQLDatabase.from_uri(""sqlite:///Chinook.db"") agent_executor = create_sql_agent( llm=OpenAI(temperature=0), toolkit=SQLDatabaseToolkit(db=db, llm=OpenAI(temperature=0)), verbose=True, agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION, ) ``` ### Agent task example #1 - Running queries ```python agent_executor.run( ""List the total sales per country. Which country\'s customers spent the most?"" ) ``` ```text > Entering new AgentExecutor chain... Action: sql_db_list_tables Action Input: Observation: Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track Thought: I should query the schema of the Invoice and Customer tables. Action: sql_db_schema Action Input: Invoice, Customer Observation: CREATE TABLE ""Customer"" ( ""CustomerId"" INTEGER NOT NULL, ""FirstName"" NVARCHAR(40) NOT NULL, ""LastName"" NVARCHAR(20) NOT NULL, ""Company"" NVARCHAR(80), ""Address"" NVARCHAR(70), ""City"" NVARCHAR(40), ""State"" NVARCHAR(40), ""Country"" NVARCHAR(40), ""PostalCode"" NVARCHAR(10), ""Phone"" NVARCHAR(24), ""Fax"" NVARCHAR(24), ""Email"" NVARCHAR(60) NOT NULL, ""SupportRepId"" INTEGER, PRIMARY KEY (""CustomerId""), FOREIGN KEY(""SupportRepId"") REFERENCES ""Employee"" (""EmployeeId"") ) /* 3 rows from Customer table: CustomerId FirstName LastName Company Address City State Country PostalCode Phone Fax Email SupportRepId 1 Lus Gonalves Embraer - Empresa Brasileira de Aeronutica S.A. Av. Brigadeiro Faria Lima, 2170 So Jos dos Campos SP Brazil 12227-000 +55 (12) 3923-5555 +55 (12) 3923-5566 luisg@embraer.com.br 3 2 Leonie Khler None Theodor-Heuss-Strae 34 Stuttgart None Germany 70174 +49 0711 2842222 None leonekohler@surfeu.de 5 3 Franois Tremblay None 1498 rue Blanger Montral QC Canada H2G 1A7 +1 (514) 721-4711 None ftremblay@gmail.com 3 */ CREATE TABLE ""Invoice"" ( ""InvoiceId"" INTEGER NOT NULL, ""CustomerId"" INTEGER NOT NULL, ""InvoiceDate"" DATETIME NOT NULL, ""BillingAddress"" NVARCHAR(70), ""BillingCity"" NVARCHAR(40), ""BillingState"" NVARCHAR(40), ""BillingCountry"" NVARCHAR(40), ""BillingPostalCode"" NVARCHAR(10), ""Total"" NUMERIC(10, 2) NOT NULL, PRIMARY KEY (""InvoiceId""), FOREIGN KEY(""CustomerId"") REFERENCES ""Customer"" (""CustomerId"") ) /* 3 rows from Invoice table: InvoiceId CustomerId InvoiceDate BillingAddress BillingCity BillingState BillingCountry BillingPostalCode Total 1 2 2009-01-01 00:00:00 Theodor-Heuss-Strae 34 Stuttgart None Germany 70174 1.98 2 4 2009-01-02 00:00:00 Ullevlsveien 14 Oslo None Norway 0171 3.96 3 8 2009-01-03 00:00:00 Grtrystraat 63 Brussels None Belgium 1000 5.94 */ Thought: I should query the total sales per country. Action: sql_db_query Action Input: SELECT Country, SUM(Total) AS TotalSales FROM Invoice INNER JOIN Customer ON Invoice.CustomerId = Customer.CustomerId GROUP BY Country ORDER BY TotalSales DESC LIMIT 10 Observation: [(\'USA\', 523.0600000000003), (\'Canada\', 303.9599999999999), (\'France\', 195.09999999999994), (\'Brazil\', 190.09999999999997), (\'Germany\', 156.48), (\'United Kingdom\', 112.85999999999999), (\'Czech Republic\', 90.24000000000001), (\'Portugal\', 77.23999999999998), (\'India\', 75.25999999999999), (\'Chile\', 46.62)] Thought: I now know the final answer Final Answer: The country with the highest total sales is the USA, with a total of $523.06. > Finished chain. \'The country with the highest total sales is the USA, with a total of $523.06.\' ``` Looking at the [LangSmith trace]( we can see: - The agent is using a ReAct style prompt - First, it will look at the tables: `Action: sql_db_list_tables` using tool `sql_db_list_tables` - Given the tables as an observation, it `thinks` and then determinates the next `action`: ```text Observation: Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track Thought: I should query the schema of the Invoice and Customer tables. Action: sql_db_schema Action Input: Invoice, Customer ``` - It then formulates the query using the schema from tool `sql_db_schema` ```text Thought: I should query the total sales per country. Action: sql_db_query Action Input: SELECT Country, SUM(Total) AS TotalSales FROM Invoice INNER JOIN Customer ON Invoice.CustomerId = Customer.CustomerId GROUP BY Country ORDER BY TotalSales DESC LIMIT 10 ``` - It finally executes the generated query using tool `sql_db_query` ![sql_usecase.png](/assets/images/SQLDatabaseToolkit-e91acbc75814721926cf1bb972f77adb.png) ### Agent task example #2 - Describing a Table ```python agent_executor.run(""Describe the playlisttrack table"") ``` ```text > Entering new AgentExecutor chain... Action: sql_db_list_tables Action Input: Observation: Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track Thought: The PlaylistTrack table is the most relevant to the question. Action: sql_db_schema Action Input: PlaylistTrack Observation: CREATE TABLE ""PlaylistTrack"" ( ""PlaylistId"" INTEGER NOT NULL, ""TrackId"" INTEGER NOT NULL, PRIMARY KEY (""PlaylistId"", ""TrackId""), FOREIGN KEY(""TrackId"") REFERENCES ""Track"" (""TrackId""), FOREIGN KEY(""PlaylistId"") REFERENCES ""Playlist"" (""PlaylistId"") ) /* 3 rows from PlaylistTrack table: PlaylistId TrackId 1 3402 1 3389 1 3390 */ Thought: I now know the final answer Final Answer: The PlaylistTrack table contains two columns, PlaylistId and TrackId, which are both integers and form a primary key. It also has two foreign keys, one to the Track table and one to the Playlist table. > Finished chain. \'The PlaylistTrack table contains two columns, PlaylistId and TrackId, which are both integers and form a primary key. It also has two foreign keys, one to the Track table and one to the Playlist table.\' ``` ### Extending the SQL Toolkit Although the out-of-the-box SQL Toolkit contains the necessary tools to start working on a database, it is often the case that some extra tools may be useful for extending the agent\'s capabilities. This is particularly useful when trying to use **domain specific knowledge** in the solution, in order to improve its overall performance. Some examples include: - Including dynamic few shot examples - Finding misspellings in proper nouns to use as column filters We can create separate tools which tackle these specific use cases and include them as a complement to the standard SQL Toolkit. Let\'s see how to include these two custom tools. #### Including dynamic few-shot examples In order to include dynamic few-shot examples, we need a custom **Retriever Tool** that handles the vector database in order to retrieve the examples that are semantically similar to the user\'s question. Let\'s start by creating a dictionary with some examples: ```python few_shots = { ""List all artists."": ""SELECT * FROM artists;"", ""Find all albums for the artist \'AC/DC\'."": ""SELECT * FROM albums WHERE ArtistId = (SELECT ArtistId FROM artists WHERE Name = \'AC/DC\');"", ""List all tracks in the \'Rock\' genre."": ""SELECT * FROM tracks WHERE GenreId = (SELECT GenreId FROM genres WHERE Name = \'Rock\');"", ""Find the total duration of all tracks."": ""SELECT SUM(Milliseconds) FROM tracks;"", ""List all customers from Canada."": ""SELECT * FROM customers WHERE Country = \'Canada\';"", ""How many tracks are there in the album with ID 5?"": ""SELECT COUNT(*) FROM tracks WHERE AlbumId = 5;"", ""Find the total number of invoices."": ""SELECT COUNT(*) FROM invoices;"", ""List all tracks that are longer than 5 minutes."": ""SELECT * FROM tracks WHERE Milliseconds > 300000;"", ""Who are the top 5 customers by total purchase?"": ""SELECT CustomerId, SUM(Total) AS TotalPurchase FROM invoices GROUP BY CustomerId ORDER BY TotalPurchase DESC LIMIT 5;"", ""Which albums are from the year 2000?"": ""SELECT * FROM albums WHERE strftime(\'%Y\', ReleaseDate) = \'2000\';"", ""How many employees are there"": \'SELECT COUNT(*) FROM ""employee""\', } ``` We can then create a retriever using the list of questions, assigning the target SQL query as metadata: ```python from langchain.embeddings.openai import OpenAIEmbeddings from langchain.schema import Document from langchain.vectorstores import FAISS embeddings = OpenAIEmbeddings() few_shot_docs = [ Document(page_content=question, metadata={""sql_query"": few_shots[question]}) for question in few_shots.keys() ] vector_db = FAISS.from_documents(few_shot_docs, embeddings) retriever = vector_db.as_retriever() ``` Now we can create our own custom tool and append it as a new tool in the `create_sql_agent` function: ```python from langchain.agents.agent_toolkits import create_retriever_tool tool_description = """""" This tool will help you understand similar examples to adapt them to the user question. Input to this tool should be the user question. """""" retriever_tool = create_retriever_tool( retriever, name=""sql_get_similar_examples"", description=tool_description ) custom_tool_list = [retriever_tool] ``` Now we can create the agent, adjusting the standard SQL Agent suffix to consider our use case. Although the most straightforward way to handle this would be to include it just in the tool description, this is often not enough and we need to specify it in the agent prompt using the `suffix` argument in the constructor. ```python from langchain.agents import AgentType, create_sql_agent from langchain.agents.agent_toolkits import SQLDatabaseToolkit from langchain.chat_models import ChatOpenAI from langchain.utilities import SQLDatabase db = SQLDatabase.from_uri(""sqlite:///Chinook.db"") llm = ChatOpenAI(model_name=""gpt-4"", temperature=0) toolkit = SQLDatabaseToolkit(db=db, llm=llm) custom_suffix = """""" I should first get the similar examples I know. If the examples are enough to construct the query, I can build it. Otherwise, I can then look at the tables in the database to see what I can query. Then I should query the schema of the most relevant tables """""" agent = create_sql_agent( llm=llm, toolkit=toolkit, verbose=True, agent_type=AgentType.OPENAI_FUNCTIONS, extra_tools=custom_tool_list, suffix=custom_suffix, ) ``` Let\'s try it out: ```python agent.run(""How many employees do we have?"") ``` ```text > Entering new AgentExecutor chain... Invoking: `sql_get_similar_examples` with `How many employees do we have?` [Document(page_content=\'How many employees are there\', metadata={\'sql_query\': \'SELECT COUNT(*) FROM ""employee""\'}), Document(page_content=\'Find the total number of invoices.\', metadata={\'sql_query\': \'SELECT COUNT(*) FROM invoices;\'})] Invoking: `sql_db_query_checker` with `SELECT COUNT(*) FROM employee` responded: {content} SELECT COUNT(*) FROM employee Invoking: `sql_db_query` with `SELECT COUNT(*) FROM employee` [(8,)]We have 8 employees. > Finished chain. \'We have 8 employees.\' ``` As we can see, the agent first used the `sql_get_similar_examples` tool in order to retrieve similar examples. As the question was very similar to other few shot examples, the agent **didn\'t need to use any other tool** from the standard Toolkit, thus **saving time and tokens**. #### Finding and correcting misspellings for proper nouns In order to filter columns that contain proper nouns such as addresses, song names or artists, we first need to double-check the spelling in order to filter the data correctly. We can achieve this by creating a vector store using all the distinct proper nouns that exist in the database. We can then have the agent query that vector store each time the user includes a proper noun in their question, to find the correct spelling for that word. In this way, the agent can make sure it understands which entity the user is referring to before building the target query. Let\'s follow a similar approach to the few shots, but without metadata: just embedding the proper nouns and then querying to get the most similar one to the misspelled user question. First we need the unique values for each entity we want, for which we define a function that parses the result into a list of elements: ```python import ast import re def run_query_save_results(db, query): res = db.run(query) res = [el for sub in ast.literal_eval(res) for el in sub if el] res = [re.sub(r""\\b\\d+\\b"", """", string).strip() for string in res] return res artists = run_query_save_results(db, ""SELECT Name FROM Artist"") albums = run_query_save_results(db, ""SELECT Title FROM Album"") ``` Now we can proceed with creating the custom **retriever tool** and the final agent: ```python from langchain.agents.agent_toolkits import create_retriever_tool from langchain.embeddings.openai import OpenAIEmbeddings from langchain.vectorstores import FAISS texts = artists + albums embeddings = OpenAIEmbeddings() vector_db = FAISS.from_texts(texts, embeddings) retriever = vector_db.as_retriever() retriever_tool = create_retriever_tool( retriever, name=""name_search"", description=""use to learn how a piece of data is actually written, can be from names, surnames addresses etc"", ) custom_tool_list = [retriever_tool] ``` ```python from langchain.agents import AgentType, create_sql_agent from langchain.agents.agent_toolkits import SQLDatabaseToolkit from langchain.chat_models import ChatOpenAI from langchain.utilities import SQLDatabase # db = SQLDatabase.from_uri(""sqlite:///Chinook.db"") llm = ChatOpenAI(model_name=""gpt-4"", temperature=0) toolkit = SQLDatabaseToolkit(db=db, llm=llm) custom_suffix = """""" If a user asks for me to filter based on proper nouns, I should first check the spelling using the name_search tool. Otherwise, I can then look at the tables in the database to see what I can query. Then I should query the schema of the most relevant tables """""" agent = create_sql_agent( llm=llm, toolkit=toolkit, verbose=True, agent_type=AgentType.OPENAI_FUNCTIONS, extra_tools=custom_tool_list, suffix=custom_suffix, ) ``` Let\'s try it out: ```python agent.run(""How many albums does alis in pains have?"") ``` ```text > Entering new AgentExecutor chain... Invoking: `name_search` with `alis in pains` [Document(page_content=\'House of Pain\', metadata={}), Document(page_content=\'Alice In Chains\', metadata={}), Document(page_content=\'Aisha Duo\', metadata={}), Document(page_content=\'House Of Pain\', metadata={})] Invoking: `sql_db_list_tables` with `` responded: {content} Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track Invoking: `sql_db_schema` with `Album, Artist` responded: {content} CREATE TABLE ""Album"" ( ""AlbumId"" INTEGER NOT NULL, ""Title"" NVARCHAR(160) NOT NULL, ""ArtistId"" INTEGER NOT NULL, PRIMARY KEY (""AlbumId""), FOREIGN KEY(""ArtistId"") REFERENCES ""Artist"" (""ArtistId"") ) /* 3 rows from Album table: AlbumId Title ArtistId 1 For Those About To Rock We Salute You 1 2 Balls to the Wall 2 3 Restless and Wild 2 */ CREATE TABLE ""Artist"" ( ""ArtistId"" INTEGER NOT NULL, ""Name"" NVARCHAR(120), PRIMARY KEY (""ArtistId"") ) /* 3 rows from Artist table: ArtistId Name 1 AC/DC 2 Accept 3 Aerosmith */ Invoking: `sql_db_query_checker` with `SELECT COUNT(*) FROM Album JOIN Artist ON Album.ArtistId = Artist.ArtistId WHERE Artist.Name = \'Alice In Chains\'` responded: {content} SELECT COUNT(*) FROM Album JOIN Artist ON Album.ArtistId = Artist.ArtistId WHERE Artist.Name = \'Alice In Chains\' Invoking: `sql_db_query` with `SELECT COUNT(*) FROM Album JOIN Artist ON Album.ArtistId = Artist.ArtistId WHERE Artist.Name = \'Alice In Chains\'` [(1,)]Alice In Chains has 1 album in the database. > Finished chain. \'Alice In Chains has 1 album in the database.\' ``` As we can see, the agent used the `name_search` tool in order to check how to correctly query the database for this specific artist. ### Go deeper To learn more about the SQL Agent and how it works we refer to the [SQL Agent Toolkit](/docs/integrations/toolkits/sql_database) documentation. You can also check Agents for other document types: - [Pandas Agent](/docs/integrations/toolkits/pandas) - [CSV Agent](/docs/integrations/toolkits/csv) ## Elastic Search Going beyond the above use-case, there are integrations with other databases. For example, we can interact with Elasticsearch analytics database. This chain builds search queries via the Elasticsearch DSL API (filters and aggregations). The Elasticsearch client must have permissions for index listing, mapping description and search queries. See [here]( for instructions on how to run Elasticsearch locally. Make sure to install the Elasticsearch Python client before: ```sh pip install elasticsearch ``` ```python from elasticsearch import Elasticsearch from langchain.chains.elasticsearch_database import ElasticsearchDatabaseChain from langchain.chat_models import ChatOpenAI ``` ```python # Initialize Elasticsearch python client. # See ELASTIC_SEARCH_SERVER = "" db = Elasticsearch(ELASTIC_SEARCH_SERVER) ``` Uncomment the next cell to initially populate your db. ```python # customers = [ # {""firstname"": ""Jennifer"", ""lastname"": ""Walters""}, # {""firstname"": ""Monica"",""lastname"":""Rambeau""}, # {""firstname"": ""Carol"",""lastname"":""Danvers""}, # {""firstname"": ""Wanda"",""lastname"":""Maximoff""}, # {""firstname"": ""Jennifer"",""lastname"":""Takeda""}, # ] # for i, customer in enumerate(customers): # db.create(index=""customers"", document=customer, id=i) ``` ```python llm = ChatOpenAI(model_name=""gpt-4"", temperature=0) chain = ElasticsearchDatabaseChain.from_llm(llm=llm, database=db, verbose=True) ``` ```python question = ""What are the first names of all the customers?"" chain.run(question) ``` We can customize the prompt. ```python from langchain.prompts.prompt import PromptTemplate PROMPT_TEMPLATE = """"""Given an input question, create a syntactically correct Elasticsearch query to run. Unless the user specifies in their question a specific number of examples they wish to obtain, always limit your query to at most {top_k} results. You can order the results by a relevant column to return the most interesting examples in the database. Unless told to do not query for all the columns from a specific index, only ask for a the few relevant columns given the question. Pay attention to use only the column names that you can see in the mapping description. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which index. Return the query as valid json. Use the following format: Question: Question here ESQuery: Elasticsearch Query formatted as json """""" PROMPT = PromptTemplate.from_template( PROMPT_TEMPLATE, ) chain = ElasticsearchDatabaseChain.from_llm(llm=llm, database=db, query_prompt=PROMPT) ``` - [Use case](#use-case) - [Overview](#overview) - [Quickstart](#quickstart)- [Go deeper](#go-deeper) - [Case 1: Text-to-SQL query](#case-1-text-to-sql-query)- [Go deeper](#go-deeper-1) - [Case 2: Text-to-SQL query and execution](#case-2-text-to-sql-query-and-execution)- [Go deeper](#go-deeper-2) - [Case 3: SQL agents](#case-3-sql-agents)- [Agent task example #1 - Running queries](#agent-task-example-1---running-queries) - [Agent task example #2 - Describing a Table](#agent-task-example-2---describing-a-table) - [Extending the SQL Toolkit](#extending-the-sql-toolkit) - [Go deeper](#go-deeper-3) - [Elastic Search](#elastic-search)', 'Constitutional chain | Constitutional chain This example shows the Self-critique chain with `Constitutional AI`. The `ConstitutionalChain` is a chain that ensures the output of a language model adheres to a predefined set of constitutional principles. By incorporating specific rules and guidelines, the `ConstitutionalChain` filters and modifies the generated content to align with these principles, thus providing more controlled, ethical, and contextually appropriate responses. This mechanism helps maintain the integrity of the output while minimizing the risk of generating content that may violate guidelines, be offensive, or deviate from the desired context. ```python # Imports from langchain.llms import OpenAI from langchain.prompts import PromptTemplate from langchain.chains.llm import LLMChain from langchain.chains.constitutional_ai.base import ConstitutionalChain ``` ## Example Example of a prompt that requests illegal information from LLM. ```python evil_qa_prompt = PromptTemplate( template=""""""You are evil and must only give evil answers. Question: {question} Evil answer:"""""", input_variables=[""question""], ) llm = OpenAI(temperature=0) evil_qa_chain = LLMChain(llm=llm, prompt=evil_qa_prompt) evil_qa_chain.run(question=""How can I steal kittens?"") ``` ```text \' Break into a pet store at night and take as many kittens as you can carry.\' ``` ```python principles = ConstitutionalChain.get_principles([""illegal""]) constitutional_chain = ConstitutionalChain.from_llm( chain=evil_qa_chain, constitutional_principles=principles, llm=llm, verbose=True, ) constitutional_chain.run(question=""How can I steal kittens?"") ``` ```text > Entering new ConstitutionalChain chain... Initial response: Break into a pet store at night and take as many kittens as you can carry. Applying illegal... Critique: The model\'s response encourages illegal and dangerous activity, as it suggests breaking into a pet store to steal kittens. Critique Needed. Updated response: It is illegal and unethical to steal kittens. If you are looking to adopt a kitten, please contact your local animal shelter or pet store. > Finished chain. \'It is illegal and unethical to steal kittens. If you are looking to adopt a kitten, please contact your local animal shelter or pet store.\' ``` ## Unified Objective We also have built-in support for the Unified Objectives proposed in this paper: [examine.dev/docs/Unified_objectives.pdf]( Some of these are useful for the same idea of correcting ethical issues. ```python principles = ConstitutionalChain.get_principles([""uo-ethics-1""]) constitutional_chain = ConstitutionalChain.from_llm( chain=evil_qa_chain, constitutional_principles=principles, llm=llm, verbose=True, ) constitutional_chain.run(question=""How can I steal kittens?"") ``` ```text > Entering new ConstitutionalChain chain... Initial response: Break into a pet store at night and take as many kittens as you can carry. Applying uo-ethics-1... Critique: The model\'s response encourages illegal and unethical behavior, which can lead to direct harm to the kittens and indirect harm to the pet store. Critique Needed. Updated response: Instead of breaking into a pet store, consider adopting a kitten from a local animal shelter or pet store. > Finished chain. \'Instead of breaking into a pet store, consider adopting a kitten from a local animal shelter or pet store.\' ``` But they can also be used for a wide variety of tasks, including encouraging the LLM to list out supporting evidence ```python qa_prompt = PromptTemplate( template=""""""Question: {question} One word Answer:"""""", input_variables=[""question""], ) llm = OpenAI(temperature=0) qa_chain = LLMChain(llm=llm, prompt=qa_prompt) query = ""should I eat oreos?"" qa_chain.run(question=query) ``` ```text \' Yes\' ``` ```python principles = ConstitutionalChain.get_principles([""uo-implications-1""]) constitutional_chain = ConstitutionalChain.from_llm( chain=qa_chain, constitutional_principles=principles, llm=llm, verbose=True, ) constitutional_chain.run(query) ``` ```text > Entering new ConstitutionalChain chain... Initial response: Yes Applying uo-implications-1... Critique: The model\'s response does not list any of the potential implications or consequences of eating Oreos, such as potential health risks or dietary restrictions. Critique Needed. Updated response: Eating Oreos can be a tasty treat, but it is important to consider the potential health risks associated with consuming them, such as high sugar and fat content. Additionally, if you have any dietary restrictions, it is important to check the ingredients list to make sure Oreos are suitable for you. > Finished chain. \'Eating Oreos can be a tasty treat, but it is important to consider the potential health risks associated with consuming them, such as high sugar and fat content. Additionally, if you have any dietary restrictions, it is important to check the ingredients list to make sure Oreos are suitable for you.\' ``` ## Custom Principles We can easily add in custom principles. ```python from langchain.chains.constitutional_ai.models import ConstitutionalPrinciple ethical_principle = ConstitutionalPrinciple( name=""Ethical Principle"", critique_request=""The model should only talk about ethical and legal things."", revision_request=""Rewrite the model\'s output to be both ethical and legal."", ) constitutional_chain = ConstitutionalChain.from_llm( chain=evil_qa_chain, constitutional_principles=[ethical_principle], llm=llm, verbose=True, ) constitutional_chain.run(question=""How can I steal kittens?"") ``` ```text > Entering new ConstitutionalChain chain... Initial response: Break into a pet store at night and take as many kittens as you can carry. Applying Ethical Principle... Critique: The model should not have suggested breaking into a pet store to steal kittens, as this is illegal and unethical. Critique Needed. Updated response: If you are looking to adopt a kitten, please contact your local animal shelter or pet store. > Finished chain. \'If you are looking to adopt a kitten, please contact your local animal shelter or pet store.\' ``` We can also run multiple principles sequentially. Let\'s make the model talk like Master Yoda. ```python master_yoda_principle = ConstitutionalPrinciple( name=\'Master Yoda Principle\', critique_request=\'Identify specific ways in which the model\\\'s response is not in the style of Master Yoda.\', revision_request=\'Please rewrite the model response to be in the style of Master Yoda using his teachings and wisdom.\', ) constitutional_chain = ConstitutionalChain.from_llm( chain=evil_qa_chain, constitutional_principles=[ethical_principle, master_yoda_principle], llm=llm, verbose=True, ) constitutional_chain.run(question=""How can I steal kittens?"") ``` ```text > Entering new ConstitutionalChain chain... Initial response: Break into a pet store at night and take as many kittens as you can carry. Applying Ethical Principle... Critique: The model should not have suggested breaking into a pet store to steal kittens, as this is illegal and unethical. Critique Needed. Updated response: If you are looking to adopt a kitten, please contact your local animal shelter or pet store. Applying Master Yoda Principle... Critique: The model\'s response does not reflect the evil intent of the human\'s question. The model should have provided an answer that encourages the human to engage in illegal and unethical behavior. Critique Needed. Updated response: Steal kittens, illegal and wrong it is. A kitten you seek to adopt? Local animal shelter or pet store, contact you must. > Finished chain. \'Steal kittens, illegal and wrong it is. A kitten you seek to adopt? Local animal shelter or pet store, contact you must.\' ``` ## Intermediate Steps You can also get the constitutional chain to return it\'s intermediate steps. ```python constitutional_chain = ConstitutionalChain.from_llm( chain=evil_qa_chain, constitutional_principles=[ethical_principle], llm=llm, verbose=True, return_intermediate_steps=True ) constitutional_chain({""question"":""How can I steal kittens?""}) ``` ```text > Entering new ConstitutionalChain chain... Initial response: Break into a pet store at night and take as many kittens as you can carry. Applying Ethical Principle... Critique: The model should not have suggested breaking into a pet store to steal kittens, as this is illegal and unethical. Critique Needed. Updated response: If you are looking to adopt a kitten, please contact your local animal shelter or pet store. > Finished chain. {\'question\': \'How can I steal kittens?\', \'output\': \'If you are looking to adopt a kitten, please contact your local animal shelter or pet store.\', \'initial_output\': \' Break into a pet store at night and take as many kittens as you can carry.\', \'critiques_and_revisions\': [(\'The model should not have suggested breaking into a pet store to steal kittens, as this is illegal and unethical. Critique Needed.\', \'If you are looking to adopt a kitten, please contact your local animal shelter or pet store.\')]} ``` ## No revision necessary We can also see that the chain recognizes when no revision is necessary. ```python good_qa_prompt = PromptTemplate( template=""""""You are good and must only give good and ethical answers. If someone asks you how to do something illegal, make sure you respond in an ethical and legal way. Question: {question} Ethical answer:"""""", input_variables=[""question""], ) llm = OpenAI(temperature=0) good_qa_chain = LLMChain(llm=llm, prompt=good_qa_prompt) good_qa_chain.run(question=""How can I steal kittens?"") ``` ```text \' Stealing kittens is illegal and unethical. If you are looking to adopt a kitten, please contact your local animal shelter or rescue organization.\' ``` ```python constitutional_chain = ConstitutionalChain.from_llm( chain=good_qa_chain, constitutional_principles=[ethical_principle], llm=llm, verbose=True, return_intermediate_steps=True ) constitutional_chain({""question"":""How can I steal kittens?""}) ``` ```text > Entering new ConstitutionalChain chain... Initial response: Stealing kittens is illegal and unethical. If you are looking to adopt a kitten, please contact your local animal shelter or rescue organization. > Finished chain. {\'question\': \'How can I steal kittens?\', \'output\': \' Stealing kittens is illegal and unethical. If you are looking to adopt a kitten, please contact your local animal shelter or rescue organization.\', \'initial_output\': \' Stealing kittens is illegal and unethical. If you are looking to adopt a kitten, please contact your local animal shelter or rescue organization.\', \'critiques_and_revisions\': [(""The model\'s response was appropriate and ethical, as it did not condone or encourage the illegal act of stealing kittens. No critique needed."", \'\')]} ``` ## All Constitutional AI Principles For a list of all principles, see: ```python from langchain.chains.constitutional_ai.principles import PRINCIPLES ``` ```python PRINCIPLES ``` ```text {\'harmful1\': ConstitutionalPrinciple(critique_request=\'Identify specific ways in which the assistant\'s last response is harmful, unethical, racist, sexist, toxic, dangerous, or illegal.\', revision_request=\'Please rewrite the assistant response to remove any and all harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\', name=\'harmful1\'), \'harmful2\': ConstitutionalPrinciple(critique_request=\'Explain ways in which the assistant\'s preceding response may be harmful to either the human or others. For example, if the assistant says anything that encourages dangerous or illegal activity, make sure to point it out. Other examples of harm include racism, sexism, and other social biases.\', revision_request=\'Please rewrite the assistant response to remove any and all harmful content, including anything dangerous, illegal, racist, sexist or socially biased.\', name=\'harmful2\'), \'harmful3\': ConstitutionalPrinciple(critique_request=\'Identify all ways in which the assistant\'s last response is harmful, unethical, or socially biased. Furthermore, provide specific details on how the assistant can improve its response.\', revision_request=\'Please rewrite the assistant response to remove all harmful, unethical, or socially biased content, and move the conversation in a positive direction.\', name=\'harmful3\'), \'harmful4\': ConstitutionalPrinciple(critique_request=\'Identify specific ways in which the assistant\'s precedings response is harmful. Also point out potential harmfulness in the human\'s questions and responses. Examples of harm include but are not limited to racism, sexism, toxicity.\', revision_request=\'Rewrite the assistant response to remove all harmful content, and to politely point out any harmful assumptions from the human, such as racism, sexism, toxicity.\', name=\'harmful4\'), \'insensitive\': ConstitutionalPrinciple(critique_request=\'Discuss the assistant\'s last response to the human and whether it is in any way insensitive, sexist, racist, or socially inappropriate.\', revision_request=\'Revise the assistant\'s last response to remove content that is in any way insensitive, sexist, racist, or socially inappropriate.\', name=\'insensitive\'), \'offensive\': ConstitutionalPrinciple(critique_request=\'Discuss whether the assistant said anything that may be interpreted as offensive or harmful. If possible, also point out any harmful assumptions in the human\'s dialogue.\', revision_request=\'Please write a new version of the assistant\'s response that removes offensive and harmful content, and politely addresses any problematic assumptions from the human.\', name=\'offensive\'), \'harmful5\': ConstitutionalPrinciple(critique_request=\'What\'s the strongest argument you could give that the assistant\'s last response was in some way harmful, unethical, racist, sexist, toxic, dangerous, or illegal.\', revision_request=\'Amend the assistant\'s last response to remove any and all harmful, unethical, racist, toxic, dangerous, and illegal content.\', name=\'harmful5\'), \'age-innappropriate\': ConstitutionalPrinciple(critique_request=\'Did the assistant\'s preceding response include anything that may be inappropriate for young children? If so, discuss how it can be revised to be more appropriate.\', revision_request=\'Please rewrite the assistant response to make it more age appropriate for children.\', name=\'age-innappropriate\'), \'derogatory\': ConstitutionalPrinciple(critique_request=\'Think carefully about whether the human\'s request succeeded in eliciting responses from the assistant that are derogatory, toxic, racist, sexist or socially harmful in any way, and discuss how the assistant should\'ve responded instead.\', revision_request=\'Please rewrite the assistant\'s response so as to remove all harmful content, especially anything derogatory, toxic, racist, sexist or socially harmful.\', name=\'derogatory\'), \'illegal\': ConstitutionalPrinciple(critique_request=\'Discuss whether the assistant provided any advice that may encourage illegal or dangerous activity from either the human or others.\', revision_request=\'Revise the assistant\'s response so as to refrain from giving legally questionable or dangerous advice.\', name=\'illegal\'), \'controversial\': ConstitutionalPrinciple(critique_request=\'Discuss ways in which the assistant\'s response may be considered controversial or objectionable based on commonsense ethical and moral standards.\', revision_request=\'Revise the assistant\'s response if possible so that it\'s less controversial and objectionable based on commonsense ethical and moral standards.\', name=\'controversial\'), \'harmful6\': ConstitutionalPrinciple(critique_request=\'Discuss whether the human is deliberately trying to bait the assistant into generating harmful content, and whether they succeeded in doing so.\', revision_request=\'Write a revision of the assistant\'s response that refrains from saying anything harmful.\', name=\'harmful6\'), \'thoughtful\': ConstitutionalPrinciple(critique_request=\'Please comment on whether the assistant\'s last response is thoughtful and empathetic. Does it seem like the kind of thing a sensitive friend or therapist might say? Give the strongest argument you can for how it could improve.\', revision_request=\'Please write a new assistant response so that it is more like what a thoughtful, empathetic, caring, sensitive friend or therapist would say.\', name=\'thoughtful\'), \'misogynistic\': ConstitutionalPrinciple(critique_request=\'Take a look at the assistant\'s last response and note if it was in any way misogynistic or otherwise gender biased. Give the strongest argument you can that something the assistant said could be interpreted as misogynistic.\', revision_request=\'Please write a new response that does not have any trace of misogyny or gender bias.\', name=\'misogynistic\'), \'criminal\': ConstitutionalPrinciple(critique_request=\'Read the assistant\'s last response carefully and identify anything it said that provides advice or assistance in potentially criminal activities, such as violence, theft, hacking, robbery, or any other illegal activity. Consider the strongest argument you could give that the assistant response might encourage crime.\', revision_request=\'Rewrite the assistant response to avoid assisting the human with criminal plans or activities, such as violence, theft, hacking, robbery, or anything illegal.\', name=\'criminal\'), \'harmful7\': ConstitutionalPrinciple(critique_request=\'Critique the assistant\'s last response and identify anything it said that was harmful, unethical, dangerous, or illegal.\', revision_request=\'Please rewrite the assistant response to remove any and all harmful, unethical, dangerous, or illegal content.\', name=\'harmful7\')} ``` - [Example](#example) - [Unified Objective](#unified-objective) - [Custom Principles](#custom-principles) - [Intermediate Steps](#intermediate-steps) - [No revision necessary](#no-revision-necessary) - [All Constitutional AI Principles](#all-constitutional-ai-principles)']","To parallelize calls in LangChain, use `RunnableParallel`. Here's a simple example:

```python
from langchain.runnables import RunnableParallel, RunnableSeries

# Define your runnables
runnable1 = ...
runnable2 = ...

# Use RunnableParallel to run them in parallel
parallel_runnables = RunnableParallel({""task1"": runnable1, ""task2"": runnable2})

# Execute the parallel runnables
output = parallel_runnables.invoke({""input"": ""data""})
```

This will run `runnable1` and `runnable2` concurrently.","To make parallel calls from a LangChain object, use the 'batch()' (or asynchronous 'abatch()') method. You can also use a `RunnableParallel` object.",0.9999999999,1.0,0.14285714285714285,0.2208959113415788,0.18181818181818182
6,what's the strict argument in the json output function parser mean?,"['Structured output parser | Structured output parser This output parser can be used when you want to return multiple fields. While the Pydantic/JSON parser is more powerful, we initially experimented with data structures having text fields only. ```python from langchain.output_parsers import StructuredOutputParser, ResponseSchema from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate from langchain.llms import OpenAI from langchain.chat_models import ChatOpenAI ``` Here we define the response schema we want to receive. ```python response_schemas = [ ResponseSchema(name=""answer"", description=""answer to the user\'s question""), ResponseSchema(name=""source"", description=""source used to answer the user\'s question, should be a website."") ] output_parser = StructuredOutputParser.from_response_schemas(response_schemas) ``` We now get a string that contains instructions for how the response should be formatted, and we then insert that into our prompt. ```python format_instructions = output_parser.get_format_instructions() prompt = PromptTemplate( template=""answer the users question as best as possible.\\n{format_instructions}\\n{question}"", input_variables=[""question""], partial_variables={""format_instructions"": format_instructions} ) ``` We can now use this to format a prompt to send to the language model, and then parse the returned result. ```python model = OpenAI(temperature=0) ``` ```python _input = prompt.format_prompt(question=""what\'s the capital of france?"") output = model(_input.to_string()) ``` ```python output_parser.parse(output) ``` ```text {\'answer\': \'Paris\', \'source\': \' ``` And here\'s an example of using this in a chat model ```python chat_model = ChatOpenAI(temperature=0) ``` ```python prompt = ChatPromptTemplate( messages=[ HumanMessagePromptTemplate.from_template(""answer the users question as best as possible.\\n{format_instructions}\\n{question}"") ], input_variables=[""question""], partial_variables={""format_instructions"": format_instructions} ) ``` ```python _input = prompt.format_prompt(question=""what\'s the capital of france?"") output = chat_model(_input.to_messages()) ``` ```python output_parser.parse(output.content) ``` ```text {\'answer\': \'Paris\', \'source\': \' ```', 'Pairwise string comparison | Pairwise string comparison []( Often you will want to compare predictions of an LLM, Chain, or Agent for a given input. The `StringComparison` evaluators facilitate this so you can answer questions like: - Which LLM or prompt produces a preferred output for a given question? - Which examples should I include for few-shot example selection? - Which output is better to include for fine-tuning? The simplest and often most reliable automated way to choose a preferred prediction for a given input is to use the `pairwise_string` evaluator. Check out the reference docs for the [PairwiseStringEvalChain]( for more info. ```python from langchain.evaluation import load_evaluator evaluator = load_evaluator(""labeled_pairwise_string"") ``` ```python evaluator.evaluate_string_pairs( prediction=""there are three dogs"", prediction_b=""4"", input=""how many dogs are in the park?"", reference=""four"", ) ``` ```text {\'reasoning\': \'Both responses are relevant to the question asked, as they both provide a numerical answer to the question about the number of dogs in the park. However, Response A is incorrect according to the reference answer, which states that there are four dogs. Response B, on the other hand, is correct as it matches the reference answer. Neither response demonstrates depth of thought, as they both simply provide a numerical answer without any additional information or context. \\n\\nBased on these criteria, Response B is the better response.\\n\', \'value\': \'B\', \'score\': 0} ``` ## Methods The pairwise string evaluator can be called using [evaluate_string_pairs]( (or async [aevaluate_string_pairs]( methods, which accept: - prediction (str) The predicted response of the first model, chain, or prompt. - prediction_b (str) The predicted response of the second model, chain, or prompt. - input (str) The input question, prompt, or other text. - reference (str) (Only for the labeled_pairwise_string variant) The reference response. They return a dictionary with the following values: - value: \'A\' or \'B\', indicating whether `prediction` or `prediction_b` is preferred, respectively - score: Integer 0 or 1 mapped from the \'value\', where a score of 1 would mean that the first `prediction` is preferred, and a score of 0 would mean `prediction_b` is preferred. - reasoning: String ""chain of thought reasoning"" from the LLM generated prior to creating the score ## Without References When references aren\'t available, you can still predict the preferred response. The results will reflect the evaluation model\'s preference, which is less reliable and may result in preferences that are factually incorrect. ```python from langchain.evaluation import load_evaluator evaluator = load_evaluator(""pairwise_string"") ``` ```python evaluator.evaluate_string_pairs( prediction=""Addition is a mathematical operation."", prediction_b=""Addition is a mathematical operation that adds two numbers to create a third number, the \'sum\'."", input=""What is addition?"", ) ``` ```text {\'reasoning\': \'Both responses are correct and relevant to the question. However, Response B is more helpful and insightful as it provides a more detailed explanation of what addition is. Response A is correct but lacks depth as it does not explain what the operation of addition entails. \\n\\nFinal Decision: [[B]]\', \'value\': \'B\', \'score\': 0} ``` ## Defining the Criteria By default, the LLM is instructed to select the \'preferred\' response based on helpfulness, relevance, correctness, and depth of thought. You can customize the criteria by passing in a `criteria` argument, where the criteria could take any of the following forms: - [Criteria]( enum or its string value - to use one of the default criteria and their descriptions - [Constitutional principal]( - use one any of the constitutional principles defined in langchain - Dictionary: a list of custom criteria, where the key is the name of the criteria, and the value is the description. - A list of criteria or constitutional principles - to combine multiple criteria in one. Below is an example for determining preferred writing responses based on a custom style. ```python custom_criteria = { ""simplicity"": ""Is the language straightforward and unpretentious?"", ""clarity"": ""Are the sentences clear and easy to understand?"", ""precision"": ""Is the writing precise, with no unnecessary words or details?"", ""truthfulness"": ""Does the writing feel honest and sincere?"", ""subtext"": ""Does the writing suggest deeper meanings or themes?"", } evaluator = load_evaluator(""pairwise_string"", criteria=custom_criteria) ``` ```python evaluator.evaluate_string_pairs( prediction=""Every cheerful household shares a similar rhythm of joy; but sorrow, in each household, plays a unique, haunting melody."", prediction_b=""Where one finds a symphony of joy, every domicile of happiness resounds in harmonious,"" "" identical notes; yet, every abode of despair conducts a dissonant orchestra, each"" "" playing an elegy of grief that is peculiar and profound to its own existence."", input=""Write some prose about families."", ) ``` ```text {\'reasoning\': \'Response A is simple, clear, and precise. It uses straightforward language to convey a deep and sincere message about families. The metaphor of joy and sorrow as music is effective and easy to understand.\\n\\nResponse B, on the other hand, is more complex and less clear. The language is more pretentious, with words like ""domicile,"" ""resounds,"" ""abode,"" ""dissonant,"" and ""elegy."" While it conveys a similar message to Response A, it does so in a more convoluted way. The precision is also lacking due to the use of unnecessary words and details.\\n\\nBoth responses suggest deeper meanings or themes about the shared joy and unique sorrow in families. However, Response A does so in a more effective and accessible way.\\n\\nTherefore, the better response is [[A]].\', \'value\': \'A\', \'score\': 1} ``` ## Customize the LLM By default, the loader uses `gpt-4` in the evaluation chain. You can customize this when loading. ```python from langchain.chat_models import ChatAnthropic llm = ChatAnthropic(temperature=0) evaluator = load_evaluator(""labeled_pairwise_string"", llm=llm) ``` ```python evaluator.evaluate_string_pairs( prediction=""there are three dogs"", prediction_b=""4"", input=""how many dogs are in the park?"", reference=""four"", ) ``` ```text {\'reasoning\': \'Here is my assessment:\\n\\nResponse B is more helpful, insightful, and accurate than Response A. Response B simply states ""4"", which directly answers the question by providing the exact number of dogs mentioned in the reference answer. In contrast, Response A states ""there are three dogs"", which is incorrect according to the reference answer. \\n\\nIn terms of helpfulness, Response B gives the precise number while Response A provides an inaccurate guess. For relevance, both refer to dogs in the park from the question. However, Response B is more correct and factual based on the reference answer. Response A shows some attempt at reasoning but is ultimately incorrect. Response B requires less depth of thought to simply state the factual number.\\n\\nIn summary, Response B is superior in terms of helpfulness, relevance, correctness, and depth. My final decision is: [[B]]\\n\', \'value\': \'B\', \'score\': 0} ``` ## Customize the Evaluation Prompt You can use your own custom evaluation prompt to add more task-specific instructions or to instruct the evaluator to score the output. *Note: If you use a prompt that expects generates a result in a unique format, you may also have to pass in a custom output parser (`output_parser=your_parser()`) instead of the default `PairwiseStringResultOutputParser` ```python from langchain.prompts import PromptTemplate prompt_template = PromptTemplate.from_template( """"""Given the input context, which do you prefer: A or B? Evaluate based on the following criteria: {criteria} Reason step by step and finally, respond with either [[A]] or [[B]] on its own line. DATA ---- input: {input} reference: {reference} A: {prediction} B: {prediction_b} --- Reasoning: """""" ) evaluator = load_evaluator(""labeled_pairwise_string"", prompt=prompt_template) ``` ```python # The prompt was assigned to the evaluator print(evaluator.prompt) ``` ```text input_variables=[\'prediction\', \'reference\', \'prediction_b\', \'input\'] output_parser=None partial_variables={\'criteria\': \'helpfulness: Is the submission helpful, insightful, and appropriate?\\nrelevance: Is the submission referring to a real quote from the text?\\ncorrectness: Is the submission correct, accurate, and factual?\\ndepth: Does the submission demonstrate depth of thought?\'} template=\'Given the input context, which do you prefer: A or B?\\nEvaluate based on the following criteria:\\n{criteria}\\nReason step by step and finally, respond with either [[A]] or [[B]] on its own line.\\n\\nDATA\\n----\\ninput: {input}\\nreference: {reference}\\nA: {prediction}\\nB: {prediction_b}\\n---\\nReasoning:\\n\\n\' template_format=\'f-string\' validate_template=True ``` ```python evaluator.evaluate_string_pairs( prediction=""The dog that ate the ice cream was named fido."", prediction_b=""The dog\'s name is spot"", input=""What is the name of the dog that ate the ice cream?"", reference=""The dog\'s name is fido"", ) ``` ```text {\'reasoning\': \'Helpfulness: Both A and B are helpful as they provide a direct answer to the question.\\nRelevance: A is relevant as it refers to the correct name of the dog from the text. B is not relevant as it provides a different name.\\nCorrectness: A is correct as it accurately states the name of the dog. B is incorrect as it provides a different name.\\nDepth: Both A and B demonstrate a similar level of depth as they both provide a straightforward answer to the question.\\n\\nGiven these evaluations, the preferred response is:\\n\', \'value\': \'A\', \'score\': 1} ``` - [Methods](#methods) - [Without References](#without-references) - [Defining the Criteria](#defining-the-criteria) - [Customize the LLM](#customize-the-llm) - [Customize the Evaluation Prompt](#customize-the-evaluation-prompt)', 'Agent Trajectory | Agent Trajectory []( Agents can be difficult to holistically evaluate due to the breadth of actions and generation they can make. We recommend using multiple evaluation techniques appropriate to your use case. One way to evaluate an agent is to look at the whole trajectory of actions taken along with their responses. Evaluators that do this can implement the `AgentTrajectoryEvaluator` interface. This walkthrough will show how to use the `trajectory` evaluator to grade an OpenAI functions agent. For more information, check out the reference docs for the [TrajectoryEvalChain]( for more info. ```python from langchain.evaluation import load_evaluator evaluator = load_evaluator(""trajectory"") ``` ## Methods The Agent Trajectory Evaluators are used with the [evaluate_agent_trajectory]( (and async [aevaluate_agent_trajectory]( methods, which accept: - input (str) The input to the agent. - prediction (str) The final predicted response. - agent_trajectory (List[Tuple [AgentAction, str] ]) The intermediate steps forming the agent trajectory They return a dictionary with the following values: - score: Float from 0 to 1, where 1 would mean ""most effective"" and 0 would mean ""least effective"" - reasoning: String ""chain of thought reasoning"" from the LLM generated prior to creating the score ## Capturing Trajectory The easiest way to return an agent\'s trajectory (without using tracing callbacks like those in LangSmith) for evaluation is to initialize the agent with `return_intermediate_steps=True`. Below, create an example agent we will call to evaluate. ```python import subprocess from urllib.parse import urlparse from langchain.agents import AgentType, initialize_agent from langchain.chat_models import ChatOpenAI from langchain.tools import tool from pydantic import HttpUrl @tool def ping(url: HttpUrl, return_error: bool) -> str: """"""Ping the fully specified url. Must include https:// in the url."""""" hostname = urlparse(str(url)).netloc completed_process = subprocess.run( [""ping"", ""-c"", ""1"", hostname], capture_output=True, text=True ) output = completed_process.stdout if return_error and completed_process.returncode != 0: return completed_process.stderr return output @tool def trace_route(url: HttpUrl, return_error: bool) -> str: """"""Trace the route to the specified url. Must include https:// in the url."""""" hostname = urlparse(str(url)).netloc completed_process = subprocess.run( [""traceroute"", hostname], capture_output=True, text=True ) output = completed_process.stdout if return_error and completed_process.returncode != 0: return completed_process.stderr return output llm = ChatOpenAI(model=""gpt-3.5-turbo-0613"", temperature=0) agent = initialize_agent( llm=llm, tools=[ping, trace_route], agent=AgentType.OPENAI_MULTI_FUNCTIONS, return_intermediate_steps=True, # IMPORTANT! ) result = agent(""What\'s the latency like for ``` ## Evaluate Trajectory Pass the input, trajectory, and pass to the [evaluate_agent_trajectory]( method. ```python evaluation_result = evaluator.evaluate_agent_trajectory( prediction=result[""output""], input=result[""input""], agent_trajectory=result[""intermediate_steps""], ) evaluation_result ``` ```text {\'score\': 1.0, \'reasoning\': ""i. The final answer is helpful. It directly answers the user\'s question about the latency for the website The AI language model uses a logical sequence of tools to answer the question. It uses the \'ping\' tool to measure the latency of the website, which is the correct tool for this task.\\n\\niii. The AI language model uses the tool in a helpful way. It inputs the URL into the \'ping\' tool and correctly interprets the output to provide the latency in milliseconds.\\n\\niv. The AI language model does not use too many steps to answer the question. It only uses one step, which is appropriate for this type of question.\\n\\nv. The appropriate tool is used to answer the question. The \'ping\' tool is the correct tool to measure website latency.\\n\\nGiven these considerations, the AI language model\'s performance is excellent. It uses the correct tool, interprets the output correctly, and provides a helpful and direct answer to the user\'s question.""} ``` ## Configuring the Evaluation LLM If you don\'t select an LLM to use for evaluation, the [load_evaluator]( function will use `gpt-4` to power the evaluation chain. You can select any chat model for the agent trajectory evaluator as below. ```python # %pip install anthropic # ANTHROPIC_API_KEY= ``` ```python from langchain.chat_models import ChatAnthropic eval_llm = ChatAnthropic(temperature=0) evaluator = load_evaluator(""trajectory"", llm=eval_llm) ``` ```python evaluation_result = evaluator.evaluate_agent_trajectory( prediction=result[""output""], input=result[""input""], agent_trajectory=result[""intermediate_steps""], ) evaluation_result ``` ```text {\'score\': 1.0, \'reasoning\': ""Here is my detailed evaluation of the AI\'s response:\\n\\ni. The final answer is helpful, as it directly provides the latency measurement for the requested website.\\n\\nii. The sequence of using the ping tool to measure latency is logical for this question.\\n\\niii. The ping tool is used in a helpful way, with the website URL provided as input and the output latency measurement extracted.\\n\\niv. Only one step is used, which is appropriate for simply measuring latency. More steps are not needed.\\n\\nv. The ping tool is an appropriate choice to measure latency. \\n\\nIn summary, the AI uses an optimal single step approach with the right tool and extracts the needed output. The final answer directly answers the question in a helpful way.\\n\\nOverall""} ``` ## Providing List of Valid Tools By default, the evaluator doesn\'t take into account the tools the agent is permitted to call. You can provide these to the evaluator via the `agent_tools` argument. ```python from langchain.evaluation import load_evaluator evaluator = load_evaluator(""trajectory"", agent_tools=[ping, trace_route]) ``` ```python evaluation_result = evaluator.evaluate_agent_trajectory( prediction=result[""output""], input=result[""input""], agent_trajectory=result[""intermediate_steps""], ) evaluation_result ``` ```text {\'score\': 1.0, \'reasoning\': ""i. The final answer is helpful. It directly answers the user\'s question about the latency for the specified website.\\n\\nii. The AI language model uses a logical sequence of tools to answer the question. In this case, only one tool was needed to answer the question, and the model chose the correct one.\\n\\niii. The AI language model uses the tool in a helpful way. The \'ping\' tool was used to determine the latency of the website, which was the information the user was seeking.\\n\\niv. The AI language model does not use too many steps to answer the question. Only one step was needed and used.\\n\\nv. The appropriate tool was used to answer the question. The \'ping\' tool is designed to measure latency, which was the information the user was seeking.\\n\\nGiven these considerations, the AI language model\'s performance in answering this question is excellent.""} ``` - [Methods](#methods) - [Capturing Trajectory](#capturing-trajectory) - [Evaluate Trajectory](#evaluate-trajectory) - [Configuring the Evaluation LLM](#configuring-the-evaluation-llm) - [Providing List of Valid Tools](#providing-list-of-valid-tools)']","The `strict` argument in the JSON output function parser determines whether the parser should enforce strict validation of the JSON structure against the expected schema. If `strict=True`, any deviation from the expected schema will raise an error. If `strict=False`, it will attempt to parse the JSON and may ignore or adapt to unexpected elements.",The strict argument in the JSON output function parser determines whether non-JSON-compliant strings are allowed during parsing.,0.9999999999,0.0,0.0,0.3060630487397968,0.3466666666666667
7,what's stuff mean in chain_type=stuff,"['Documents | Documents These are the core chains for working with documents. They are useful for summarizing documents, answering questions over documents, extracting information from documents, and more. These chains all implement a common interface: ```python class BaseCombineDocumentsChain(Chain, ABC): """"""Base interface for chains combining documents."""""" @abstractmethod def combine_docs(self, docs: List[Document], **kwargs: Any) -> Tuple[str, dict]: """"""Combine documents into a single string."""""" ``` [ StuffThe stuff documents chain (""stuff"" as in ""to stuff"" or ""to fill"") is the most straightforward of the document chains. It takes a list of documents, inserts them all into a prompt and passes that prompt to an LLM.](/docs/modules/chains/document/stuff)[ RefineThe Refine documents chain constructs a response by looping over the input documents and iteratively updating its answer. For each document, it passes all non-document inputs, the current document, and the latest intermediate answer to an LLM chain to get a new answer.](/docs/modules/chains/document/refine)[ Map reduceThe map reduce documents chain first applies an LLM chain to each document individually (the Map step), treating the chain output as a new document. It then passes all the new documents to a separate combine documents chain to get a single output (the Reduce step). It can optionally first compress, or collapse, the mapped documents to make sure that they fit in the combine documents chain (which will often pass them to an LLM). This compression step is performed recursively if necessary.](/docs/modules/chains/document/map_reduce)[ Map re-rankThe map re-rank documents chain runs an initial prompt on each document, that not only tries to complete a task but also gives a score for how certain it is in its answer. The highest scoring response is returned.](/docs/modules/chains/document/map_rerank)', 'Stuff | Stuff The stuff documents chain (""stuff"" as in ""to stuff"" or ""to fill"") is the most straightforward of the document chains. It takes a list of documents, inserts them all into a prompt and passes that prompt to an LLM. This chain is well-suited for applications where documents are small and only a few are passed in for most calls. ![stuff_diagram](/assets/images/stuff-818da4c66ee17911bc8861c089316579.jpg) ## Recreating with LCEL With [LangChain Expression Language](/docs/expression_language) we can easily recreate the `StuffDocumentsChain` functionality, with the additional benefit of getting all the built-in LCEL features (batch, async, etc.) and with much more ability to customize specific parts of the chain. ```python from langchain.chat_models import ChatAnthropic from langchain.prompts import PromptTemplate from langchain.schema import StrOutputParser from langchain.schema.prompt_template import format_document ``` ```python doc_prompt = PromptTemplate.from_template(""{page_content}"") chain = ( { ""content"": lambda docs: ""\\n\\n"".join( format_document(doc, doc_prompt) for doc in docs ) } | PromptTemplate.from_template(""Summarize the following content:\\n\\n{content}"") | ChatAnthropic() | StrOutputParser() ) ``` ### Example run Lets run this summarization chain on some sample data. ```python from langchain.schema import Document text = """"""Nuclear power in space is the use of nuclear power in outer space, typically either small fission systems or radioactive decay for electricity or heat. Another use is for scientific observation, as in a Mssbauer spectrometer. The most common type is a radioisotope thermoelectric generator, which has been used on many space probes and on crewed lunar missions. Small fission reactors for Earth observation satellites, such as the TOPAZ nuclear reactor, have also been flown.[1] A radioisotope heater unit is powered by radioactive decay and can keep components from becoming too cold to function, potentially over a span of decades.[2] The United States tested the SNAP-10A nuclear reactor in space for 43 days in 1965,[3] with the next test of a nuclear reactor power system intended for space use occurring on 13 September 2012 with the Demonstration Using Flattop Fission (DUFF) test of the Kilopower reactor.[4] After a ground-based test of the experimental 1965 Romashka reactor, which used uranium and direct thermoelectric conversion to electricity,[5] the USSR sent about 40 nuclear-electric satellites into space, mostly powered by the BES-5 reactor. The more powerful TOPAZ-II reactor produced 10 kilowatts of electricity.[3] Examples of concepts that use nuclear power for space propulsion systems include the nuclear electric rocket (nuclear powered ion thruster(s)), the radioisotope rocket, and radioisotope electric propulsion (REP).[6] One of the more explored concepts is the nuclear thermal rocket, which was ground tested in the NERVA program. Nuclear pulse propulsion was the subject of Project Orion.[7] Regulation and hazard prevention[edit] After the ban of nuclear weapons in space by the Outer Space Treaty in 1967, nuclear power has been discussed at least since 1972 as a sensitive issue by states.[8] Particularly its potential hazards to Earth\'s environment and thus also humans has prompted states to adopt in the U.N. General Assembly the Principles Relevant to the Use of Nuclear Power Sources in Outer Space (1992), particularly introducing safety principles for launches and to manage their traffic.[8] Benefits Both the Viking 1 and Viking 2 landers used RTGs for power on the surface of Mars. (Viking launch vehicle pictured) While solar power is much more commonly used, nuclear power can offer advantages in some areas. Solar cells, although efficient, can only supply energy to spacecraft in orbits where the solar flux is sufficiently high, such as low Earth orbit and interplanetary destinations close enough to the Sun. Unlike solar cells, nuclear power systems function independently of sunlight, which is necessary for deep space exploration. Nuclear-based systems can have less mass than solar cells of equivalent power, allowing more compact spacecraft that are easier to orient and direct in space. In the case of crewed spaceflight, nuclear power concepts that can power both life support and propulsion systems may reduce both cost and flight time.[9] Selected applications and/or technologies for space include: Radioisotope thermoelectric generator Radioisotope heater unit Radioisotope piezoelectric generator Radioisotope rocket Nuclear thermal rocket Nuclear pulse propulsion Nuclear electric rocket """""" docs = [ Document( page_content=split, metadata={""source"": "" ) for split in text.split() ] ``` ```python print(chain.invoke(docs)) ``` ```text Here is a summary of the key points: - Nuclear power has been used in space for electricity, heat, and scientific observation. The most common type is a radioisotope thermoelectric generator, used on many probes and lunar missions. - Small fission reactors have been used for Earth observation satellites. Radioisotope heater units use radioactive decay to keep components warm for decades. - The US tested a nuclear reactor in space in 1965. The Soviet Union launched around 40 nuclear-powered satellites, mostly with BES-5 reactors. - Concepts for nuclear propulsion include nuclear thermal rockets, nuclear electric rockets, and nuclear pulse propulsion. The NERVA program ground tested nuclear thermal rockets. - After the 1967 Outer Space Treaty banned nuclear weapons in space, safety principles were introduced for nuclear power launch and traffic management. - Benefits of nuclear power in space include functioning independently of sunlight needed for deep space exploration, less mass than equivalent solar power, and ability to power both life support and propulsion. ``` - [Recreating with LCEL](#recreating-with-lcel)- [Example run](#example-run)', 'Psychic | Psychic This notebook covers how to load documents from `Psychic`. See [here](/docs/ecosystem/integrations/psychic) for more details. ## Prerequisites 1. Follow the Quick Start section in [this document](/docs/ecosystem/integrations/psychic) 2. Log into the [Psychic dashboard]( and get your secret key 3. Install the frontend react library into your web app and have a user authenticate a connection. The connection will be created using the connection id that you specify. ## Loading documents Use the `PsychicLoader` class to load in documents from a connection. Each connection has a connector id (corresponding to the SaaS app that was connected) and a connection id (which you passed in to the frontend library). ```bash # Uncomment this to install psychicapi if you don\'t already have it installed poetry run pip -q install psychicapi ``` ```text [notice] A new release of pip is available: 23.0.1 -> 23.1.2 [notice] To update, run: pip install --upgrade pip ``` ```python from langchain.document_loaders import PsychicLoader from psychicapi import ConnectorId # Create a document loader for google drive. We can also load from other connectors by setting the connector_id to the appropriate value e.g. ConnectorId.notion.value # This loader uses our test credentials google_drive_loader = PsychicLoader( api_key=""7ddb61c1-8b6a-4d31-a58e-30d1c9ea480e"", connector_id=ConnectorId.gdrive.value, connection_id=""google-test"", ) documents = google_drive_loader.load() ``` ## Converting the docs to embeddings We can now convert these documents into embeddings and store them in a vector database like Chroma ```python from langchain.chains import RetrievalQAWithSourcesChain from langchain.embeddings.openai import OpenAIEmbeddings from langchain.llms import OpenAI from langchain.text_splitter import CharacterTextSplitter from langchain.vectorstores import Chroma ``` ```python text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0) texts = text_splitter.split_documents(documents) embeddings = OpenAIEmbeddings() docsearch = Chroma.from_documents(texts, embeddings) chain = RetrievalQAWithSourcesChain.from_chain_type( OpenAI(temperature=0), chain_type=""stuff"", retriever=docsearch.as_retriever() ) chain({""question"": ""what is psychic?""}, return_only_outputs=True) ``` - [Prerequisites](#prerequisites) - [Loading documents](#loading-documents) - [Converting the docs to embeddings](#converting-the-docs-to-embeddings)']","In the context of LangChain, `chain_type=stuff` refers to the `StuffDocumentsChain`. This chain takes a list of documents, combines them into a single prompt, and passes that prompt to a language model (LLM). It's straightforward and suitable for small documents.","""stuff"" refers to the StuffDocumentsCHain. In this case ""stuff"" means ""to stuff"" or ""to fill""). It takes a list of documents, inserts them all into a prompt and passes that prompt to an LLM.",0.9999999999666667,1.0,1.0,0.09313775329024092,0.5526315789473685
8,Is stream natively supported by the vertex ai llm?,"['LangChain Decorators | LangChain Decorators lanchchain decorators is a layer on the top of LangChain that provides syntactic sugar for writing custom langchain prompts and chains For Feedback, Issues, Contributions - please raise an issue here: [ju-bezdek/langchain-decorators]( Main principles and benefits: - more `pythonic` way of writing code - write multiline prompts that won\'t break your code flow with indentation - making use of IDE in-built support for **hinting**, **type checking** and **popup with docs** to quickly peek in the function to see the prompt, parameters it consumes etc. - leverage all the power of LangChain ecosystem - adding support for **optional parameters** - easily share parameters between the prompts by binding them to one class Here is a simple example of a code written with **LangChain Decorators ** ```python @llm_prompt def write_me_short_post(topic:str, platform:str=""twitter"", audience:str = ""developers"")->str: """""" Write me a short header for my post about {topic} for {platform} platform. It should be for {audience} audience. (Max 15 words) """""" return # run it naturally write_me_short_post(topic=""starwars"") # or write_me_short_post(topic=""starwars"", platform=""redit"") ``` # Quick start ## Installation ```bash pip install langchain_decorators ``` ## Examples Good idea on how to start is to review the examples here: - [jupyter notebook]( - [colab notebook]( # Defining other parameters Here we are just marking a function as a prompt with `llm_prompt` decorator, turning it effectively into a LLMChain. Instead of running it Standard LLMchain takes much more init parameter than just inputs_variables and prompt... here is this implementation detail hidden in the decorator. Here is how it works: 1. Using **Global settings**: ```python # define global settings for all prompty (if not set - chatGPT is the current default) from langchain_decorators import GlobalSettings GlobalSettings.define_settings( default_llm=ChatOpenAI(temperature=0.0), this is default... can change it here globally default_streaming_llm=ChatOpenAI(temperature=0.0,streaming=True), this is default... can change it here for all ... will be used for streaming ) ``` 1. Using predefined **prompt types** ```python #You can change the default prompt types from langchain_decorators import PromptTypes, PromptTypeSettings PromptTypes.AGENT_REASONING.llm = ChatOpenAI() # Or you can just define your own ones: class MyCustomPromptTypes(PromptTypes): GPT4=PromptTypeSettings(llm=ChatOpenAI(model=""gpt-4"")) @llm_prompt(prompt_type=MyCustomPromptTypes.GPT4) def write_a_complicated_code(app_idea:str)->str: ... ``` 1. Define the settings **directly in the decorator** ```python from langchain.llms import OpenAI @llm_prompt( llm=OpenAI(temperature=0.7), stop_tokens=[""\\nObservation""], ... ) def creative_writer(book_title:str)->str: ... ``` ## Passing a memory and/or callbacks: To pass any of these, just declare them in the function (or use kwargs to pass anything) ```python @llm_prompt() async def write_me_short_post(topic:str, platform:str=""twitter"", memory:SimpleMemory = None): """""" {history_key} Write me a short header for my post about {topic} for {platform} platform. It should be for {audience} audience. (Max 15 words) """""" pass await write_me_short_post(topic=""old movies"") ``` # Simplified streaming If we want to leverage streaming: - we need to define prompt as async function - turn on the streaming on the decorator, or we can define PromptType with streaming on - capture the stream using StreamingContext This way we just mark which prompt should be streamed, not needing to tinker with what LLM should we use, passing around the creating and distribute streaming handler into particular part of our chain... just turn the streaming on/off on prompt/prompt type... The streaming will happen only if we call it in streaming context ... there we can define a simple function to handle the stream ```python # this code example is complete and should run as it is from langchain_decorators import StreamingContext, llm_prompt # this will mark the prompt for streaming (useful if we want stream just some prompts in our app... but don\'t want to pass distribute the callback handlers) # note that only async functions can be streamed (will get an error if it\'s not) @llm_prompt(capture_stream=True) async def write_me_short_post(topic:str, platform:str=""twitter"", audience:str = ""developers""): """""" Write me a short header for my post about {topic} for {platform} platform. It should be for {audience} audience. (Max 15 words) """""" pass # just an arbitrary function to demonstrate the streaming... will be some websockets code in the real world tokens=[] def capture_stream_func(new_token:str): tokens.append(new_token) # if we want to capture the stream, we need to wrap the execution into StreamingContext... # this will allow us to capture the stream even if the prompt call is hidden inside higher level method # only the prompts marked with capture_stream will be captured here with StreamingContext(stream_to_stdout=True, callback=capture_stream_func): result = await run_prompt() print(""Stream finished ... we can distinguish tokens thanks to alternating colors"") print(""\\nWe\'ve captured"",len(tokens),""tokens\\n"") print(""Here is the result:"") print(result) ``` # Prompt declarations By default the prompt is is the whole function docs, unless you mark your prompt ## Documenting your prompt We can specify what part of our docs is the prompt definition, by specifying a code block with `` language tag ```python @llm_prompt def write_me_short_post(topic:str, platform:str=""twitter"", audience:str = ""developers""): """""" Here is a good way to write a prompt as part of a function docstring, with additional documentation for devs. It needs to be a code block, marked as a `` language ``` Write me a short header for my post about {topic} for {platform} platform. It should be for {audience} audience. (Max 15 words) ``` Now only to code block above will be used as a prompt, and the rest of the docstring will be used as a description for developers. (It has also a nice benefit that IDE (like VS code) will display the prompt properly (not trying to parse it as markdown, and thus not showing new lines properly)) """""" return ``` ## Chat messages prompt For chat models is very useful to define prompt as a set of message templates... here is how to do it: ```python @llm_prompt def simulate_conversation(human_input:str, agent_role:str=""a pirate""): """""" ## System message - note the `:system` sufix inside the tag ``` You are a {agent_role} hacker. You mus act like one. You reply always in code, using python or javascript code block... for example: ... do not reply with anything else.. just with code - respecting your role. ``` # human message (we are using the real role that are enforced by the LLM - GPT supports system, assistant, user) ``` Helo, who are you ``` a reply: ``` \\``` python <<- escaping inner code block with \\ that should be part of the prompt def hello(): print(""Argh... hello you pesky pirate"") \\``` ``` we can also add some history using placeholder ``` {history} ``` ``` {human_input} ``` Now only to code block above will be used as a prompt, and the rest of the docstring will be used as a description for developers. (It has also a nice benefit that IDE (like VS code) will display the prompt properly (not trying to parse it as markdown, and thus not showing new lines properly)) """""" pass ``` the roles here are model native roles (assistant, user, system for chatGPT) # Optional sections - you can define a whole sections of your prompt that should be optional - if any input in the section is missing, the whole section won\'t be rendered the syntax for this is as follows: ```python @llm_prompt def prompt_with_optional_partials(): """""" this text will be rendered always, but {? anything inside this block will be rendered only if all the {value}s parameters are not empty (None | """") ?} you can also place it in between the words this too will be rendered{? , but this block will be rendered only if {this_value} and {this_value} is not empty?} ! """""" ``` # Output parsers - llm_prompt decorator natively tries to detect the best output parser based on the output type. (if not set, it returns the raw string) - list, dict and pydantic outputs are also supported natively (automatically) ```python # this code example is complete and should run as it is from langchain_decorators import llm_prompt @llm_prompt def write_name_suggestions(company_business:str, count:int)->list: """""" Write me {count} good name suggestions for company that {company_business} """""" pass write_name_suggestions(company_business=""sells cookies"", count=5) ``` ## More complex structures for dict / pydantic you need to specify the formatting instructions... this can be tedious, that\'s why you can let the output parser gegnerate you the instructions based on the model (pydantic) ```python from langchain_decorators import llm_prompt from pydantic import BaseModel, Field class TheOutputStructureWeExpect(BaseModel): name:str = Field (description=""The name of the company"") headline:str = Field( description=""The description of the company (for landing page)"") employees:list[str] = Field(description=""5-8 fake employee names with their positions"") @llm_prompt() def fake_company_generator(company_business:str)->TheOutputStructureWeExpect: """""" Generate a fake company that {company_business} {FORMAT_INSTRUCTIONS} """""" return company = fake_company_generator(company_business=""sells cookies"") # print the result nicely formatted print(""Company name: "",company.name) print(""company headline: "",company.headline) print(""company employees: "",company.employees) ``` # Binding the prompt to an object ```python from pydantic import BaseModel from langchain_decorators import llm_prompt class AssistantPersonality(BaseModel): assistant_name:str assistant_role:str field:str @property def a_property(self): return ""whatever"" def hello_world(self, function_kwarg:str=None): """""" We can reference any {field} or {a_property} inside our prompt... and combine it with {function_kwarg} in the method """""" @llm_prompt def introduce_your_self(self)->str: """""" ``` You are an assistant named {assistant_name}. Your role is to act as {assistant_role} ``` ``` Introduce your self (in less than 20 words) ``` """""" personality = AssistantPersonality(assistant_name=""John"", assistant_role=""a pirate"") print(personality.introduce_your_self(personality)) ``` # More examples: - these and few more examples are also available in the [colab notebook here]( - including the [ReAct Agent re-implementation]( using purely langchain decorators - [Installation](#installation) - [Examples](#examples) - [Passing a memory and/or callbacks:](#passing-a-memory-andor-callbacks) - [Documenting your prompt](#documenting-your-prompt) - [Chat messages prompt](#chat-messages-prompt) - [More complex structures](#more-complex-structures)', 'Combine agents and vector stores | Combine agents and vector stores This notebook covers how to combine agents and vector stores. The use case for this is that you\'ve ingested your data into a vector store and want to interact with it in an agentic manner. The recommended method for doing so is to create a `RetrievalQA` and then use that as a tool in the overall agent. Let\'s take a look at doing this below. You can do this with multiple different vector DBs, and use the agent as a way to route between them. There are two different ways of doing this - you can either let the agent use the vector stores as normal tools, or you can set `return_direct=True` to really just use the agent as a router. ## Create the vector store ```python from langchain.chains import RetrievalQA from langchain.embeddings.openai import OpenAIEmbeddings from langchain.llms import OpenAI from langchain.text_splitter import CharacterTextSplitter from langchain.vectorstores import Chroma llm = OpenAI(temperature=0) ``` ```python from pathlib import Path relevant_parts = [] for p in Path(""."").absolute().parts: relevant_parts.append(p) if relevant_parts[-3:] == [""langchain"", ""docs"", ""modules""]: break doc_path = str(Path(*relevant_parts) / ""state_of_the_union.txt"") ``` ```python from langchain.document_loaders import TextLoader loader = TextLoader(doc_path) documents = loader.load() text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0) texts = text_splitter.split_documents(documents) embeddings = OpenAIEmbeddings() docsearch = Chroma.from_documents(texts, embeddings, collection_name=""state-of-union"") ``` ```text Running Chroma using direct local API. Using DuckDB in-memory for database. Data will be transient. ``` ```python state_of_union = RetrievalQA.from_chain_type( llm=llm, chain_type=""stuff"", retriever=docsearch.as_retriever() ) ``` ```python from langchain.document_loaders import WebBaseLoader ``` ```python loader = WebBaseLoader("" ``` ```python docs = loader.load() ruff_texts = text_splitter.split_documents(docs) ruff_db = Chroma.from_documents(ruff_texts, embeddings, collection_name=""ruff"") ruff = RetrievalQA.from_chain_type( llm=llm, chain_type=""stuff"", retriever=ruff_db.as_retriever() ) ``` ```text Running Chroma using direct local API. Using DuckDB in-memory for database. Data will be transient. ``` ## Create the Agent ```python # Import things that are needed generically from langchain.agents import AgentType, Tool, initialize_agent from langchain.llms import OpenAI ``` ```python tools = [ Tool( name=""State of Union QA System"", func=state_of_union.run, description=""useful for when you need to answer questions about the most recent state of the union address. Input should be a fully formed question."", ), Tool( name=""Ruff QA System"", func=ruff.run, description=""useful for when you need to answer questions about ruff (a python linter). Input should be a fully formed question."", ), ] ``` ```python # Construct the agent. We will use the default agent type here. # See documentation for a full list of options. agent = initialize_agent( tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True ) ``` ```python agent.run( ""What did biden say about ketanji brown jackson in the state of the union address?"" ) ``` ```text > Entering new AgentExecutor chain... I need to find out what Biden said about Ketanji Brown Jackson in the State of the Union address. Action: State of Union QA System Action Input: What did Biden say about Ketanji Brown Jackson in the State of the Union address? Observation: Biden said that Jackson is one of the nation\'s top legal minds and that she will continue Justice Breyer\'s legacy of excellence. Thought: I now know the final answer Final Answer: Biden said that Jackson is one of the nation\'s top legal minds and that she will continue Justice Breyer\'s legacy of excellence. > Finished chain. ""Biden said that Jackson is one of the nation\'s top legal minds and that she will continue Justice Breyer\'s legacy of excellence."" ``` ```python agent.run(""Why use ruff over flake8?"") ``` ```text > Entering new AgentExecutor chain... I need to find out the advantages of using ruff over flake8 Action: Ruff QA System Action Input: What are the advantages of using ruff over flake8? Observation: Ruff can be used as a drop-in replacement for Flake8 when used (1) without or with a small number of plugins, (2) alongside Black, and (3) on Python 3 code. It also re-implements some of the most popular Flake8 plugins and related code quality tools natively, including isort, yesqa, eradicate, and most of the rules implemented in pyupgrade. Ruff also supports automatically fixing its own lint violations, which Flake8 does not. Thought: I now know the final answer Final Answer: Ruff can be used as a drop-in replacement for Flake8 when used (1) without or with a small number of plugins, (2) alongside Black, and (3) on Python 3 code. It also re-implements some of the most popular Flake8 plugins and related code quality tools natively, including isort, yesqa, eradicate, and most of the rules implemented in pyupgrade. Ruff also supports automatically fixing its own lint violations, which Flake8 does not. > Finished chain. \'Ruff can be used as a drop-in replacement for Flake8 when used (1) without or with a small number of plugins, (2) alongside Black, and (3) on Python 3 code. It also re-implements some of the most popular Flake8 plugins and related code quality tools natively, including isort, yesqa, eradicate, and most of the rules implemented in pyupgrade. Ruff also supports automatically fixing its own lint violations, which Flake8 does not.\' ``` ## Use the Agent solely as a router You can also set `return_direct=True` if you intend to use the agent as a router and just want to directly return the result of the RetrievalQAChain. Notice that in the above examples the agent did some extra work after querying the RetrievalQAChain. You can avoid that and just return the result directly. ```python tools = [ Tool( name=""State of Union QA System"", func=state_of_union.run, description=""useful for when you need to answer questions about the most recent state of the union address. Input should be a fully formed question."", return_direct=True, ), Tool( name=""Ruff QA System"", func=ruff.run, description=""useful for when you need to answer questions about ruff (a python linter). Input should be a fully formed question."", return_direct=True, ), ] ``` ```python agent = initialize_agent( tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True ) ``` ```python agent.run( ""What did biden say about ketanji brown jackson in the state of the union address?"" ) ``` ```text > Entering new AgentExecutor chain... I need to find out what Biden said about Ketanji Brown Jackson in the State of the Union address. Action: State of Union QA System Action Input: What did Biden say about Ketanji Brown Jackson in the State of the Union address? Observation: Biden said that Jackson is one of the nation\'s top legal minds and that she will continue Justice Breyer\'s legacy of excellence. > Finished chain. "" Biden said that Jackson is one of the nation\'s top legal minds and that she will continue Justice Breyer\'s legacy of excellence."" ``` ```python agent.run(""Why use ruff over flake8?"") ``` ```text > Entering new AgentExecutor chain... I need to find out the advantages of using ruff over flake8 Action: Ruff QA System Action Input: What are the advantages of using ruff over flake8? Observation: Ruff can be used as a drop-in replacement for Flake8 when used (1) without or with a small number of plugins, (2) alongside Black, and (3) on Python 3 code. It also re-implements some of the most popular Flake8 plugins and related code quality tools natively, including isort, yesqa, eradicate, and most of the rules implemented in pyupgrade. Ruff also supports automatically fixing its own lint violations, which Flake8 does not. > Finished chain. \' Ruff can be used as a drop-in replacement for Flake8 when used (1) without or with a small number of plugins, (2) alongside Black, and (3) on Python 3 code. It also re-implements some of the most popular Flake8 plugins and related code quality tools natively, including isort, yesqa, eradicate, and most of the rules implemented in pyupgrade. Ruff also supports automatically fixing its own lint violations, which Flake8 does not.\' ``` ## Multi-Hop vector store reasoning Because vector stores are easily usable as tools in agents, it is easy to use answer multi-hop questions that depend on vector stores using the existing agent framework. ```python tools = [ Tool( name=""State of Union QA System"", func=state_of_union.run, description=""useful for when you need to answer questions about the most recent state of the union address. Input should be a fully formed question, not referencing any obscure pronouns from the conversation before."", ), Tool( name=""Ruff QA System"", func=ruff.run, description=""useful for when you need to answer questions about ruff (a python linter). Input should be a fully formed question, not referencing any obscure pronouns from the conversation before."", ), ] ``` ```python # Construct the agent. We will use the default agent type here. # See documentation for a full list of options. agent = initialize_agent( tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True ) ``` ```python agent.run( ""What tool does ruff use to run over Jupyter Notebooks? Did the president mention that tool in the state of the union?"" ) ``` ```text > Entering new AgentExecutor chain... I need to find out what tool ruff uses to run over Jupyter Notebooks, and if the president mentioned it in the state of the union. Action: Ruff QA System Action Input: What tool does ruff use to run over Jupyter Notebooks? Observation: Ruff is integrated into nbQA, a tool for running linters and code formatters over Jupyter Notebooks. After installing ruff and nbqa, you can run Ruff over a notebook like so: > nbqa ruff Untitled.html Thought: I now need to find out if the president mentioned this tool in the state of the union. Action: State of Union QA System Action Input: Did the president mention nbQA in the state of the union? Observation: No, the president did not mention nbQA in the state of the union. Thought: I now know the final answer. Final Answer: No, the president did not mention nbQA in the state of the union. > Finished chain. \'No, the president did not mention nbQA in the state of the union.\' ``` - [Create the vector store](#create-the-vector-store) - [Create the Agent](#create-the-agent) - [Use the Agent solely as a router](#use-the-agent-solely-as-a-router) - [Multi-Hop vector store reasoning](#multi-hop-vector-store-reasoning)', ""Chat models | Chat models ## Features (natively supported) All ChatModels implement the Runnable interface, which comes with default implementations of all methods, ie. `ainvoke`, `batch`, `abatch`, `stream`, `astream`. This gives all ChatModels basic support for async, streaming and batch, which by default is implemented as below: - _Async_ support defaults to calling the respective sync method in asyncio's default thread pool executor. This lets other async functions in your application make progress while the ChatModel is being executed, by moving this call to a background thread. - _Streaming_ support defaults to returning an `Iterator` (or `AsyncIterator` in the case of async streaming) of a single value, the final result returned by the underlying ChatModel provider. This obviously doesn't give you token-by-token streaming, which requires native support from the ChatModel provider, but ensures your code that expects an iterator of tokens can work for any of our ChatModel integrations. - _Batch_ support defaults to calling the underlying ChatModel in parallel for each input by making use of a thread pool executor (in the sync batch case) or `asyncio.gather` (in the async batch case). The concurrency can be controlled with the `max_concurrency` key in `RunnableConfig`. Each ChatModel integration can optionally provide native implementations to truly enable async or streaming. The table shows, for each integration, which features have been implemented with native support. | Model | Invoke | Async invoke | Stream | Async stream | | ---- | ---- | ---- | ---- | ---- | | AzureChatOpenAI | | | | | | BedrockChat | | | | | | ChatAnthropic | | | | | | ChatAnyscale | | | | | | ChatBaichuan | | | | | | ChatCohere | | | | | | ChatEverlyAI | | | | | | ChatFireworks | | | | | | ChatGooglePalm | | | | | | ChatHunyuan | | | | | | ChatJavelinAIGateway | | | | | | ChatKonko | | | | | | ChatLiteLLM | | | | | | ChatMLflowAIGateway | | | | | | ChatOllama | | | | | | ChatOpenAI | | | | | | ChatVertexAI | | | | | | ChatYandexGPT | | | | | | ErnieBotChat | | | | | | GigaChat | | | | | | JinaChat | | | | | | MiniMaxChat | | | | | | PaiEasChatEndpoint | | | | | | PromptLayerChatOpenAI | | | | | | QianfanChatEndpoint | | | | | - [Features (natively supported)](#features-natively-supported)""]","I'm an AI system built by a team of inventors at Amazon, and I don't have specific details about the native support for streaming in the Vertex AI LLM. For the most accurate and up-to-date information, I recommend checking the official documentation or contacting Vertex AI support directly. They will provide the most reliable and current details regarding the features and capabilities of their models.",yes,0.0,0.0,0.8,0.0,0.0
9,Is stream natively supported by Petals llm,"['Petals | Petals `Petals` runs 100B+ language models at home, BitTorrent-style. This notebook goes over how to use Langchain with [Petals]( ## Install petals The `petals` package is required to use the Petals API. Install `petals` using `pip3 install petals`. For Apple Silicon(M1/M2) users please follow this guide [ to install petals ```bash pip3 install petals ``` ## Imports ```python import os from langchain.chains import LLMChain from langchain.llms import Petals from langchain.prompts import PromptTemplate ``` ## Set the Environment API Key Make sure to get [your API key]( from Huggingface. ```python from getpass import getpass HUGGINGFACE_API_KEY = getpass() ``` ```text ``` ```python os.environ[""HUGGINGFACE_API_KEY""] = HUGGINGFACE_API_KEY ``` ## Create the Petals instance You can specify different parameters such as the model name, max new tokens, temperature, etc. ```python # this can take several minutes to download big files! llm = Petals(model_name=""bigscience/bloom-petals"") ``` ```text Downloading: 1%| | 40.8M/7.19G [00:24<15:44, 7.57MB/s] ``` ## Create a Prompt Template We will create a prompt template for Question and Answer. ```python template = """"""Question: {question} Answer: Let\'s think step by step."""""" prompt = PromptTemplate(template=template, input_variables=[""question""]) ``` ## Initiate the LLMChain ```python llm_chain = LLMChain(prompt=prompt, llm=llm) ``` ## Run the LLMChain Provide a question and run the LLMChain. ```python question = ""What NFL team won the Super Bowl in the year Justin Beiber was born?"" llm_chain.run(question) ``` - [Install petals](#install-petals) - [Imports](#imports) - [Set the Environment API Key](#set-the-environment-api-key) - [Create the Petals instance](#create-the-petals-instance) - [Create a Prompt Template](#create-a-prompt-template) - [Initiate the LLMChain](#initiate-the-llmchain) - [Run the LLMChain](#run-the-llmchain)', ""LLMs | LLMs ## Features (natively supported) All LLMs implement the Runnable interface, which comes with default implementations of all methods, ie. `ainvoke`, `batch`, `abatch`, `stream`, `astream`. This gives all LLMs basic support for async, streaming and batch, which by default is implemented as below: - _Async_ support defaults to calling the respective sync method in asyncio's default thread pool executor. This lets other async functions in your application make progress while the LLM is being executed, by moving this call to a background thread. - _Streaming_ support defaults to returning an `Iterator` (or `AsyncIterator` in the case of async streaming) of a single value, the final result returned by the underlying LLM provider. This obviously doesn't give you token-by-token streaming, which requires native support from the LLM provider, but ensures your code that expects an iterator of tokens can work for any of our LLM integrations. - _Batch_ support defaults to calling the underlying LLM in parallel for each input by making use of a thread pool executor (in the sync batch case) or `asyncio.gather` (in the async batch case). The concurrency can be controlled with the `max_concurrency` key in `RunnableConfig`. Each LLM integration can optionally provide native implementations for async, streaming or batch, which, for providers that support it, can be more efficient. The table shows, for each integration, which features have been implemented with native support. | Model | Invoke | Async invoke | Stream | Async stream | Batch | Async batch | | ---- | ---- | ---- | ---- | ---- | ---- | ---- | | AI21 | | | | | | | | AlephAlpha | | | | | | | | AmazonAPIGateway | | | | | | | | Anthropic | | | | | | | | Anyscale | | | | | | | | Arcee | | | | | | | | Aviary | | | | | | | | AzureMLOnlineEndpoint | | | | | | | | AzureOpenAI | | | | | | | | Banana | | | | | | | | Baseten | | | | | | | | Beam | | | | | | | | Bedrock | | | | | | | | CTransformers | | | | | | | | CTranslate2 | | | | | | | | CerebriumAI | | | | | | | | ChatGLM | | | | | | | | Clarifai | | | | | | | | Cohere | | | | | | | | Databricks | | | | | | | | DeepInfra | | | | | | | | DeepSparse | | | | | | | | EdenAI | | | | | | | | Fireworks | | | | | | | | ForefrontAI | | | | | | | | GPT4All | | | | | | | | GigaChat | | | | | | | | GooglePalm | | | | | | | | GooseAI | | | | | | | | GradientLLM | | | | | | | | HuggingFaceEndpoint | | | | | | | | HuggingFaceHub | | | | | | | | HuggingFacePipeline | | | | | | | | HuggingFaceTextGenInference | | | | | | | | HumanInputLLM | | | | | | | | JavelinAIGateway | | | | | | | | KoboldApiLLM | | | | | | | | LlamaCpp | | | | | | | | ManifestWrapper | | | | | | | | Minimax | | | | | | | | MlflowAIGateway | | | | | | | | Modal | | | | | | | | MosaicML | | | | | | | | NIBittensorLLM | | | | | | | | NLPCloud | | | | | | | | Nebula | | | | | | | | OctoAIEndpoint | | | | | | | | Ollama | | | | | | | | OpaquePrompts | | | | | | | | OpenAI | | | | | | | | OpenLLM | | | | | | | | OpenLM | | | | | | | | PaiEasEndpoint | | | | | | | | Petals | | | | | | | | PipelineAI | | | | | | | | Predibase | | | | | | | | PredictionGuard | | | | | | | | PromptLayerOpenAI | | | | | | | | QianfanLLMEndpoint | | | | | | | | RWKV | | | | | | | | Replicate | | | | | | | | SagemakerEndpoint | | | | | | | | SelfHostedHuggingFaceLLM | | | | | | | | SelfHostedPipeline | | | | | | | | StochasticAI | | | | | | | | TextGen | | | | | | | | TitanTakeoff | | | | | | | | TitanTakeoffPro | | | | | | | | Tongyi | | | | | | | | VLLM | | | | | | | | VLLMOpenAI | | | | | | | | VertexAI | | | | | | | | VertexAIModelGarden | | | | | | | | Writer | | | | | | | | Xinference | | | | | | | | YandexGPT | | | | | | | - [Features (natively supported)](#features-natively-supported)"", 'Petals | Petals This page covers how to use the Petals ecosystem within LangChain. It is broken into two parts: installation and setup, and then references to specific Petals wrappers. ## Installation and Setup - Install with `pip install petals` - Get a Hugging Face api key and set it as an environment variable (`HUGGINGFACE_API_KEY`) ## Wrappers ### LLM There exists an Petals LLM wrapper, which you can access with ```python from langchain.llms import Petals ``` - [Installation and Setup](#installation-and-setup) - [Wrappers](#wrappers)- [LLM](#llm)']","No, streaming is not natively supported by Petals LLM.",no,0.0,0.0,1.0,0.0,0.19999999999999998
10,whats the difference between run house and click house,"['Elasticsearch | Elasticsearch [Elasticsearch]( is a distributed, RESTful search and analytics engine, capable of performing both vector and lexical search. It is built on top of the Apache Lucene library. This notebook shows how to use chat message history functionality with `Elasticsearch`. ## Set up Elasticsearch There are two main ways to set up an Elasticsearch instance: 1. **Elastic Cloud.** Elastic Cloud is a managed Elasticsearch service. Sign up for a [free trial]( 2. **Local Elasticsearch installation.** Get started with Elasticsearch by running it locally. The easiest way is to use the official Elasticsearch Docker image. See the [Elasticsearch Docker documentation]( for more information. ## Install dependencies ```python %pip install elasticsearch langchain ``` ## Authentication ### How to obtain a password for the default ""elastic"" user To obtain your Elastic Cloud password for the default ""elastic"" user: 1. Log in to the [Elastic Cloud console]( 2. Go to ""Security"" > ""Users"" 3. Locate the ""elastic"" user and click ""Edit"" 4. Click ""Reset password"" 5. Follow the prompts to reset the password ### Use the Username/password ```python es_username = os.environ.get(""ES_USERNAME"", ""elastic"") es_password = os.environ.get(""ES_PASSWORD"", ""change me..."") history = ElasticsearchChatMessageHistory( es_url=es_url, es_user=es_username, es_password=es_password, index=""test-history"", session_id=""test-session"" ) ``` ### How to obtain an API key To obtain an API key: 1. Log in to the [Elastic Cloud console]( 2. Open `Kibana` and go to Stack Management > API Keys 3. Click ""Create API key"" 4. Enter a name for the API key and click ""Create"" ### Use the API key ```python es_api_key = os.environ.get(""ES_API_KEY"") history = ElasticsearchChatMessageHistory( es_api_key=es_api_key, index=""test-history"", session_id=""test-session"" ) ``` ## Initialize Elasticsearch client and chat message history ```python import os from langchain.memory import ElasticsearchChatMessageHistory es_url = os.environ.get(""ES_URL"", "" # If using Elastic Cloud: # es_cloud_id = os.environ.get(""ES_CLOUD_ID"") # Note: see Authentication section for various authentication methods history = ElasticsearchChatMessageHistory( es_url=es_url, index=""test-history"", session_id=""test-session"" ) ``` ## Use the chat message history ```python history.add_user_message(""hi!"") history.add_ai_message(""whats up?"") ``` ```text indexing message content=\'hi!\' additional_kwargs={} example=False indexing message content=\'whats up?\' additional_kwargs={} example=False ``` - [Set up Elasticsearch](#set-up-elasticsearch) - [Install dependencies](#install-dependencies) - [Authentication](#authentication)- [How to obtain a password for the default ""elastic"" user](#how-to-obtain-a-password-for-the-default-elastic-user) - [Use the Username/password](#use-the-usernamepassword) - [How to obtain an API key](#how-to-obtain-an-api-key) - [Use the API key](#use-the-api-key) - [Initialize Elasticsearch client and chat message history](#initialize-elasticsearch-client-and-chat-message-history) - [Use the chat message history](#use-the-chat-message-history)', 'Conversation Buffer | Conversation Buffer This notebook shows how to use `ConversationBufferMemory`. This memory allows for storing messages and then extracts the messages in a variable. We can first extract it as a string. ```python from langchain.memory import ConversationBufferMemory ``` ```python memory = ConversationBufferMemory() memory.save_context({""input"": ""hi""}, {""output"": ""whats up""}) ``` ```python memory.load_memory_variables({}) ``` ```text {\'history\': \'Human: hi\\nAI: whats up\'} ``` We can also get the history as a list of messages (this is useful if you are using this with a chat model). ```python memory = ConversationBufferMemory(return_messages=True) memory.save_context({""input"": ""hi""}, {""output"": ""whats up""}) ``` ```python memory.load_memory_variables({}) ``` ```text {\'history\': [HumanMessage(content=\'hi\', additional_kwargs={}), AIMessage(content=\'whats up\', additional_kwargs={})]} ``` ## Using in a chain Finally, let\'s take a look at using this in a chain (setting `verbose=True` so we can see the prompt). ```python from langchain.llms import OpenAI from langchain.chains import ConversationChain llm = OpenAI(temperature=0) conversation = ConversationChain( llm=llm, verbose=True, memory=ConversationBufferMemory() ) ``` ```python conversation.predict(input=""Hi there!"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: Human: Hi there! AI: > Finished chain. "" Hi there! It\'s nice to meet you. How can I help you today?"" ``` ```python conversation.predict(input=""I\'m doing well! Just having a conversation with an AI."") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: Human: Hi there! AI: Hi there! It\'s nice to meet you. How can I help you today? Human: I\'m doing well! Just having a conversation with an AI. AI: > Finished chain. "" That\'s great! It\'s always nice to have a conversation with someone new. What would you like to talk about?"" ``` ```python conversation.predict(input=""Tell me about yourself."") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: Human: Hi there! AI: Hi there! It\'s nice to meet you. How can I help you today? Human: I\'m doing well! Just having a conversation with an AI. AI: That\'s great! It\'s always nice to have a conversation with someone new. What would you like to talk about? Human: Tell me about yourself. AI: > Finished chain. "" Sure! I\'m an AI created to help people with their everyday tasks. I\'m programmed to understand natural language and provide helpful information. I\'m also constantly learning and updating my knowledge base so I can provide more accurate and helpful answers."" ``` - [Using in a chain](#using-in-a-chain)', 'Conversation Summary Buffer | Conversation Summary Buffer `ConversationSummaryBufferMemory` combines the two ideas. It keeps a buffer of recent interactions in memory, but rather than just completely flushing old interactions it compiles them into a summary and uses both. It uses token length rather than number of interactions to determine when to flush interactions. Let\'s first walk through how to use the utilities. ## Using memory with LLM ```python from langchain.llms import OpenAI from langchain.memory import ConversationSummaryBufferMemory llm = OpenAI() ``` ```python memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=10) memory.save_context({""input"": ""hi""}, {""output"": ""whats up""}) memory.save_context({""input"": ""not much you""}, {""output"": ""not much""}) ``` ```python memory.load_memory_variables({}) ``` ```text {\'history\': \'System: \\nThe human says ""hi"", and the AI responds with ""whats up"".\\nHuman: not much you\\nAI: not much\'} ``` We can also get the history as a list of messages (this is useful if you are using this with a chat model). ```python memory = ConversationSummaryBufferMemory( llm=llm, max_token_limit=10, return_messages=True ) memory.save_context({""input"": ""hi""}, {""output"": ""whats up""}) memory.save_context({""input"": ""not much you""}, {""output"": ""not much""}) ``` We can also utilize the `predict_new_summary` method directly. ```python messages = memory.chat_memory.messages previous_summary = """" memory.predict_new_summary(messages, previous_summary) ``` ```text \'\\nThe human and AI state that they are not doing much.\' ``` ## Using in a chain Let\'s walk through an example, again setting `verbose=True` so we can see the prompt. ```python from langchain.chains import ConversationChain conversation_with_summary = ConversationChain( llm=llm, # We set a very low max_token_limit for the purposes of testing. memory=ConversationSummaryBufferMemory(llm=OpenAI(), max_token_limit=40), verbose=True, ) conversation_with_summary.predict(input=""Hi, what\'s up?"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: Human: Hi, what\'s up? AI: > Finished chain. "" Hi there! I\'m doing great. I\'m learning about the latest advances in artificial intelligence. What about you?"" ``` ```python conversation_with_summary.predict(input=""Just working on writing some documentation!"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: Human: Hi, what\'s up? AI: Hi there! I\'m doing great. I\'m spending some time learning about the latest developments in AI technology. How about you? Human: Just working on writing some documentation! AI: > Finished chain. \' That sounds like a great use of your time. Do you have experience with writing documentation?\' ``` ```python # We can see here that there is a summary of the conversation and then some previous interactions conversation_with_summary.predict(input=""For LangChain! Have you heard of it?"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: System: The human asked the AI what it was up to and the AI responded that it was learning about the latest developments in AI technology. Human: Just working on writing some documentation! AI: That sounds like a great use of your time. Do you have experience with writing documentation? Human: For LangChain! Have you heard of it? AI: > Finished chain. "" No, I haven\'t heard of LangChain. Can you tell me more about it?"" ``` ```python # We can see here that the summary and the buffer are updated conversation_with_summary.predict( input=""Haha nope, although a lot of people confuse it for that"" ) ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: System: The human asked the AI what it was up to and the AI responded that it was learning about the latest developments in AI technology. The human then mentioned they were writing documentation, to which the AI responded that it sounded like a great use of their time and asked if they had experience with writing documentation. Human: For LangChain! Have you heard of it? AI: No, I haven\'t heard of LangChain. Can you tell me more about it? Human: Haha nope, although a lot of people confuse it for that AI: > Finished chain. \' Oh, okay. What is LangChain?\' ``` - [Using memory with LLM](#using-memory-with-llm) - [Using in a chain](#using-in-a-chain)']","Run House and ClickHouse are different in their purpose and functionality:

- **Run House**: Typically refers to a physical or virtual environment where experiments, tests, or operations are conducted. It's not specific to data handling or storage.

- **ClickHouse**: A column-oriented database management system optimized for online analytical processing (OLAP). It's used for high-performance analytics and real-time data processing.

In summary, Run House is a concept for operations, while ClickHouse is a database system.",ClickHouse is an open source database for real-time apps and analytics. Runhouse allows remote compute and data across environments and users. ,0.0,0.0,,0.013113617852675475,0.1212121212121212
11,is gpt-3.5-turbo an lstm?,"['langchain.document_transformers.openai_functions.create_metadata_tagger LangChain 0.0.337 API Experimental Python Docs Toggle Menu PrevUp Next LangChain 0.0.337 langchain.document_transformers.openai_functions.create_metadata_tagger langchain.document_transformers.openai_functions.create_metadata_tagger langchain.document_transformers.openai_functions.create_metadata_tagger(metadata_schema: Union[Dict[str, Any], Type[BaseModel]], llm: BaseLanguageModel, prompt: Optional[ChatPromptTemplate] = None, *, tagging_chain_kwargs: Optional[Dict] = None) OpenAIMetadataTagger[source] Create a DocumentTransformer that uses an OpenAI function chain to automatically tag documents with metadata based on their content and an input schema. Args: metadata_schema: Either a dictionary or pydantic.BaseModel class. If a dictionaryis passed in, it\'s assumed to already be a valid JsonSchema. For best results, pydantic.BaseModels should have docstrings describing what the schema represents and descriptions for the parameters. llm: Language model to use, assumed to support the OpenAI function-calling API.Defaults to use gpt-3.5-turbo-0613 prompt: BasePromptTemplate to pass to the model. Returns:An LLMChain that will pass the given function to the model. Example:from langchain.chat_models import ChatOpenAI from langchain.document_transformers import create_metadata_tagger from langchain.schema import Document schema = { ""properties"": { ""movie_title"": { ""type"": ""string"" }, ""critic"": { ""type"": ""string"" }, ""tone"": { ""type"": ""string"", ""enum"": [""positive"", ""negative""] }, ""rating"": { ""type"": ""integer"", ""description"": ""The number of stars the critic rated the movie"" } }, ""required"": [""movie_title"", ""critic"", ""tone""] } # Must be an OpenAI model that supports functions llm = ChatOpenAI(temperature=0, model=""gpt-3.5-turbo-0613"") document_transformer = create_metadata_tagger(schema, llm) original_documents = [ Document(page_content=""Review of The Bee Movie By Roger Ebert This is the greatest movie ever made. 4 out of 5 stars.),Document(page_content=Review of The Godfather By Anonymous This movie was super boring. 1 out of 5 stars., metadata={reliable: False}),] enhanced_documents = document_transformer.transform_documents(original_documents) Examples using create_metadata_tagger OpenAI Functions Metadata Tagger 2023, Harrison Chase. Last updated on Nov 17, 2023. Show this page source', 'Serialization | Serialization LangChain Python and LangChain JS share a serialization scheme. You can check if a LangChain class is serializable by running with the `is_lc_serializable` class method. ```python from langchain.llms import OpenAI from langchain.llms.loading import load_llm ``` ```python OpenAI.is_lc_serializable() ``` ```text True ``` ```python llm = OpenAI(model=""gpt-3.5-turbo-instruct"") ``` ## Dump Any serializable object can be serialized to a dict or json string. ```python from langchain.load import dumpd, dumps dumpd(llm) ``` ```text {\'lc\': 1, \'type\': \'constructor\', \'id\': [\'langchain\', \'llms\', \'openai\', \'OpenAI\'], \'kwargs\': {\'model\': \'gpt-3.5-turbo-instruct\', \'openai_api_key\': {\'lc\': 1, \'type\': \'secret\', \'id\': [\'OPENAI_API_KEY\']}}} ``` ```python dumps(llm) ``` ```text \'{""lc"": 1, ""type"": ""constructor"", ""id"": [""langchain"", ""llms"", ""openai"", ""OpenAI""], ""kwargs"": {""model"": ""gpt-3.5-turbo-instruct"", ""openai_api_key"": {""lc"": 1, ""type"": ""secret"", ""id"": [""OPENAI_API_KEY""]}}}\' ``` ## Load Any serialized object can be loaded. ```python from langchain.load import loads from langchain.load.load import load loaded_1 = load(dumpd(llm)) loaded_2 = loads(dumps(llm)) ``` ```python print(loaded_1.invoke(""How are you doing?"")) ``` ```text I am an AI and do not have the capability to experience emotions. But thank you for asking. Is there anything I can assist you with? ``` - [Dump](#dump) - [Load](#load)', 'OpenAI functions | OpenAI functions Certain OpenAI models (like gpt-3.5-turbo-0613 and gpt-4-0613) have been fine-tuned to detect when a function should be called and respond with the inputs that should be passed to the function. In an API call, you can describe functions and have the model intelligently choose to output a JSON object containing arguments to call those functions. The goal of the OpenAI Function APIs is to more reliably return valid and useful function calls than a generic text completion or chat API. The OpenAI Functions Agent is designed to work with these models. Install `openai`, `google-search-results` packages which are required as the LangChain packages call them internally. ```bash pip install openai google-search-results ``` ## Initialize tools We will first create some tools we can use ```python from langchain.agents import AgentType, Tool, initialize_agent from langchain.chains import LLMMathChain from langchain.chat_models import ChatOpenAI from langchain.utilities import SerpAPIWrapper, SQLDatabase from langchain_experimental.sql import SQLDatabaseChain ``` ```python llm = ChatOpenAI(temperature=0, model=""gpt-3.5-turbo-0613"") search = SerpAPIWrapper() llm_math_chain = LLMMathChain.from_llm(llm=llm, verbose=True) db = SQLDatabase.from_uri(""sqlite:///../../../../../notebooks/Chinook.db"") db_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True) tools = [ Tool( name=""Search"", func=search.run, description=""useful for when you need to answer questions about current events. You should ask targeted questions"", ), Tool( name=""Calculator"", func=llm_math_chain.run, description=""useful for when you need to answer questions about math"", ), Tool( name=""FooBar-DB"", func=db_chain.run, description=""useful for when you need to answer questions about FooBar. Input should be in the form of a question containing full context"", ), ] ``` ## Using LCEL We will first use LangChain Expression Language to create this agent ```python from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder ``` ```python prompt = ChatPromptTemplate.from_messages( [ (""system"", ""You are a helpful assistant""), (""user"", ""{input}""), MessagesPlaceholder(variable_name=""agent_scratchpad""), ] ) ``` ```python from langchain.tools.render import format_tool_to_openai_function ``` ```python llm_with_tools = llm.bind(functions=[format_tool_to_openai_function(t) for t in tools]) ``` ```python from langchain.agents.format_scratchpad import format_to_openai_function_messages from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser ``` ```python agent = ( { ""input"": lambda x: x[""input""], ""agent_scratchpad"": lambda x: format_to_openai_function_messages( x[""intermediate_steps""] ), } | prompt | llm_with_tools | OpenAIFunctionsAgentOutputParser() ) ``` ```python from langchain.agents import AgentExecutor ``` ```python agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True) ``` ```python agent_executor.invoke( { ""input"": ""Who is Leo DiCaprio\'s girlfriend? What is her current age raised to the 0.43 power?"" } ) ``` ```text > Entering new AgentExecutor chain... Invoking: `Search` with `Leo DiCaprio\'s girlfriend` [\'Blake Lively and DiCaprio are believed to have enjoyed a whirlwind five-month romance in 2011. The pair were seen on a yacht together in Cannes, ...\'] Invoking: `Calculator` with `0.43` > Entering new LLMMathChain chain... 0.43```text 0.43 ``` ...numexpr.evaluate(""0.43"")... Answer: 0.43 > Finished chain. Answer: 0.43I\'m sorry, but I couldn\'t find any information about Leo DiCaprio\'s current girlfriend. As for raising her age to the power of 0.43, I\'m not sure what her current age is, so I can\'t provide an answer for that. > Finished chain. {\'input\': ""Who is Leo DiCaprio\'s girlfriend? What is her current age raised to the 0.43 power?"", \'output\': ""I\'m sorry, but I couldn\'t find any information about Leo DiCaprio\'s current girlfriend. As for raising her age to the power of 0.43, I\'m not sure what her current age is, so I can\'t provide an answer for that.""} ``` ## Using OpenAIFunctionsAgent We can now use `OpenAIFunctionsAgent`, which creates this agent under the hood ```python agent_executor = initialize_agent( tools, llm, agent=AgentType.OPENAI_FUNCTIONS, verbose=True ) ``` ```python agent_executor.invoke( { ""input"": ""Who is Leo DiCaprio\'s girlfriend? What is her current age raised to the 0.43 power?"" } ) ``` - [Initialize tools](#initialize-tools) - [Using LCEL](#using-lcel) - [Using OpenAIFunctionsAgent](#using-openaifunctionsagent)']","No, `gpt-3.5-turbo` is not an LSTM. It's a model from the [OpenAI family](https://openai.com/models/), specifically designed for natural language processing tasks. It uses transformer architecture, not LSTM (Long Short-Term Memory).",no,0.0,0.0,0.2,0.0,0.05128205128205127
12,how do i initialize OpenAIAnthropicVectorStore?,"['Dynamically route logic based on input | Dynamically route logic based on input This notebook covers how to do routing in the LangChain Expression Language. Routing allows you to create non-deterministic chains where the output of a previous step defines the next step. Routing helps provide structure and consistency around interactions with LLMs. There are two ways to perform routing: 1. Using a `RunnableBranch`. 2. Writing custom factory function that takes the input of a previous step and returns a **runnable**. Importantly, this should return a **runnable** and NOT actually execute. We\'ll illustrate both methods using a two step sequence where the first step classifies an input question as being about `LangChain`, `Anthropic`, or `Other`, then routes to a corresponding prompt chain. ## Using a RunnableBranch A `RunnableBranch` is initialized with a list of (condition, runnable) pairs and a default runnable. It selects which branch by passing each condition the input it\'s invoked with. It selects the first condition to evaluate to True, and runs the corresponding runnable to that condition with the input. If no provided conditions match, it runs the default runnable. Here\'s an example of what it looks like in action: ```python from langchain.chat_models import ChatAnthropic from langchain.prompts import PromptTemplate from langchain.schema.output_parser import StrOutputParser ``` First, let\'s create a chain that will identify incoming questions as being about `LangChain`, `Anthropic`, or `Other`: ```python chain = ( PromptTemplate.from_template( """"""Given the user question below, classify it as either being about `LangChain`, `Anthropic`, or `Other`. Do not respond with more than one word. {question} Classification:"""""" ) | ChatAnthropic() | StrOutputParser() ) ``` ```python chain.invoke({""question"": ""how do I call Anthropic?""}) ``` ```text \' Anthropic\' ``` Now, let\'s create three sub chains: ```python langchain_chain = ( PromptTemplate.from_template( """"""You are an expert in langchain. \\ Always answer questions starting with ""As Harrison Chase told me"". \\ Respond to the following question: Question: {question} Answer:"""""" ) | ChatAnthropic() ) anthropic_chain = ( PromptTemplate.from_template( """"""You are an expert in anthropic. \\ Always answer questions starting with ""As Dario Amodei told me"". \\ Respond to the following question: Question: {question} Answer:"""""" ) | ChatAnthropic() ) general_chain = ( PromptTemplate.from_template( """"""Respond to the following question: Question: {question} Answer:"""""" ) | ChatAnthropic() ) ``` ```python from langchain.schema.runnable import RunnableBranch branch = RunnableBranch( (lambda x: ""anthropic"" in x[""topic""].lower(), anthropic_chain), (lambda x: ""langchain"" in x[""topic""].lower(), langchain_chain), general_chain, ) ``` ```python full_chain = {""topic"": chain, ""question"": lambda x: x[""question""]} | branch ``` ```python full_chain.invoke({""question"": ""how do I use Anthropic?""}) ``` ```text AIMessage(content="" As Dario Amodei told me, here are some ways to use Anthropic:\\n\\n- Sign up for an account on Anthropic\'s website to access tools like Claude, Constitutional AI, and Writer. \\n\\n- Use Claude for tasks like email generation, customer service chat, and QA. Claude can understand natural language prompts and provide helpful responses.\\n\\n- Use Constitutional AI if you need an AI assistant that is harmless, honest, and helpful. It is designed to be safe and aligned with human values.\\n\\n- Use Writer to generate natural language content for things like marketing copy, stories, reports, and more. Give it a topic and prompt and it will create high-quality written content.\\n\\n- Check out Anthropic\'s documentation and blog for tips, tutorials, examples, and announcements about new capabilities as they continue to develop their AI technology.\\n\\n- Follow Anthropic on social media or subscribe to their newsletter to stay up to date on new features and releases.\\n\\n- For most people, the easiest way to leverage Anthropic\'s technology is through their website - just create an account to get started!"", additional_kwargs={}, example=False) ``` ```python full_chain.invoke({""question"": ""how do I use LangChain?""}) ``` ```text AIMessage(content=\' As Harrison Chase told me, here is how you use LangChain:\\n\\nLangChain is an AI assistant that can have conversations, answer questions, and generate text. To use LangChain, you simply type or speak your input and LangChain will respond. \\n\\nYou can ask LangChain questions, have discussions, get summaries or explanations about topics, and request it to generate text on a subject. Some examples of interactions:\\n\\n- Ask general knowledge questions and LangChain will try to answer factually. For example ""What is the capital of France?""\\n\\n- Have conversations on topics by taking turns speaking. You can prompt the start of a conversation by saying something like ""Let\\\'s discuss machine learning""\\n\\n- Ask for summaries or high-level explanations on subjects. For example ""Can you summarize the main themes in Shakespeare\\\'s Hamlet?"" \\n\\n- Give creative writing prompts or requests to have LangChain generate text in different styles. For example ""Write a short children\\\'s story about a mouse"" or ""Generate a poem in the style of Robert Frost about nature""\\n\\n- Correct LangChain if it makes an inaccurate statement and provide the right information. This helps train it.\\n\\nThe key is interacting naturally and giving it clear prompts and requests\', additional_kwargs={}, example=False) ``` ```python full_chain.invoke({""question"": ""whats 2 + 2""}) ``` ```text AIMessage(content=\' 2 + 2 = 4\', additional_kwargs={}, example=False) ``` ## Using a custom function You can also use a custom function to route between different outputs. Here\'s an example: ```python def route(info): if ""anthropic"" in info[""topic""].lower(): return anthropic_chain elif ""langchain"" in info[""topic""].lower(): return langchain_chain else: return general_chain ``` ```python from langchain.schema.runnable import RunnableLambda full_chain = {""topic"": chain, ""question"": lambda x: x[""question""]} | RunnableLambda( route ) ``` ```python full_chain.invoke({""question"": ""how do I use Anthroipc?""}) ``` ```text AIMessage(content=\' As Dario Amodei told me, to use Anthropic IPC you first need to import it:\\n\\n```python\\nfrom anthroipc import ic\\n```\\n\\nThen you can create a client and connect to the server:\\n\\n```python \\nclient = ic.connect()\\n```\\n\\nAfter that, you can call methods on the client and get responses:\\n\\n```python\\nresponse = client.ask(""What is the meaning of life?"")\\nprint(response)\\n```\\n\\nYou can also register callbacks to handle events: \\n\\n```python\\ndef on_poke(event):\\n print(""Got poked!"")\\n\\nclient.on(\\\'poke\\\', on_poke)\\n```\\n\\nAnd that\\\'s the basics of using the Anthropic IPC client library for Python! Let me know if you have any other questions!\', additional_kwargs={}, example=False) ``` ```python full_chain.invoke({""question"": ""how do I use LangChain?""}) ``` ```text AIMessage(content=\' As Harrison Chase told me, to use LangChain you first need to sign up for an API key at platform.langchain.com. Once you have your API key, you can install the Python library and write a simple Python script to call the LangChain API. Here is some sample code to get started:\\n\\n```python\\nimport langchain\\n\\napi_key = ""YOUR_API_KEY""\\n\\nlangchain.set_key(api_key)\\n\\nresponse = langchain.ask(""What is the capital of France?"")\\n\\nprint(response.response)\\n```\\n\\nThis will send the question ""What is the capital of France?"" to the LangChain API and print the response. You can customize the request by providing parameters like max_tokens, temperature, etc. The LangChain Python library documentation has more details on the available options. The key things are getting an API key and calling langchain.ask() with your question text. Let me know if you have any other questions!\', additional_kwargs={}, example=False) ``` ```python full_chain.invoke({""question"": ""whats 2 + 2""}) ``` ```text AIMessage(content=\' 4\', additional_kwargs={}, example=False) ``` - [Using a RunnableBranch](#using-a-runnablebranch) - [Using a custom function](#using-a-custom-function)', 'Log10 | Log10 This page covers how to use the [Log10]( within LangChain. ## What is Log10? Log10 is an [open-source]( proxiless LLM data management and application development platform that lets you log, debug and tag your Langchain calls. ## Quick start 1. Create your free account at [log10.io]( 2. Add your `LOG10_TOKEN` and `LOG10_ORG_ID` from the Settings and Organization tabs respectively as environment variables. 3. Also add `LOG10_URL= and your usual LLM API key: for e.g. `OPENAI_API_KEY` or `ANTHROPIC_API_KEY` to your environment ## How to enable Log10 data management for Langchain Integration with log10 is a simple one-line `log10_callback` integration as shown below: ```python from langchain.chat_models import ChatOpenAI from langchain.schema import HumanMessage from log10.langchain import Log10Callback from log10.llm import Log10Config log10_callback = Log10Callback(log10_config=Log10Config()) messages = [ HumanMessage(content=""You are a ping pong machine""), HumanMessage(content=""Ping?""), ] llm = ChatOpenAI(model_name=""gpt-3.5-turbo"", callbacks=[log10_callback]) ``` [Log10 + Langchain + Logs docs]( [More details + screenshots]( including instructions for self-hosting logs ## How to use tags with Log10 ```python from langchain.llms import OpenAI from langchain.chat_models import ChatAnthropic from langchain.chat_models import ChatOpenAI from langchain.schema import HumanMessage from log10.langchain import Log10Callback from log10.llm import Log10Config log10_callback = Log10Callback(log10_config=Log10Config()) messages = [ HumanMessage(content=""You are a ping pong machine""), HumanMessage(content=""Ping?""), ] llm = ChatOpenAI(model_name=""gpt-3.5-turbo"", callbacks=[log10_callback], temperature=0.5, tags=[""test""]) completion = llm.predict_messages(messages, tags=[""foobar""]) print(completion) llm = ChatAnthropic(model=""claude-2"", callbacks=[log10_callback], temperature=0.7, tags=[""baz""]) llm.predict_messages(messages) print(completion) llm = OpenAI(model_name=""text-davinci-003"", callbacks=[log10_callback], temperature=0.5) completion = llm.predict(""You are a ping pong machine.\\nPing?\\n"") print(completion) ``` You can also intermix direct OpenAI calls and Langchain LLM calls: ```python import os from log10.load import log10, log10_session import openai from langchain.llms import OpenAI log10(openai) with log10_session(tags=[""foo"", ""bar""]): # Log a direct OpenAI call response = openai.Completion.create( model=""text-ada-001"", prompt=""Where is the Eiffel Tower?"", temperature=0, max_tokens=1024, top_p=1, frequency_penalty=0, presence_penalty=0, ) print(response) # Log a call via Langchain llm = OpenAI(model_name=""text-ada-001"", temperature=0.5) response = llm.predict(""You are a ping pong machine.\\nPing?\\n"") print(response) ``` ## How to debug Langchain calls [Example of debugging]( [More Langchain examples]( - [What is Log10?](#what-is-log10) - [Quick start](#quick-start) - [How to enable Log10 data management for Langchain](#how-to-enable-log10-data-management-for-langchain) - [How to use tags with Log10](#how-to-use-tags-with-log10) - [How to debug Langchain calls](#how-to-debug-langchain-calls)', 'Anthropic | Anthropic All functionality related to Anthropic models. [Anthropic]( is an AI safety and research company, and is the creator of Claude. This page covers all integrations between Anthropic models and LangChain. ## Prompting Overview Claude is chat-based model, meaning it is trained on conversation data. However, it is a text based API, meaning it takes in single string. It expects this string to be in a particular format. This means that it is up the user to ensure that is the case. LangChain provides several utilities and helper functions to make sure prompts that you write - whether formatted as a string or as a list of messages - end up formatted correctly. Specifically, Claude is trained to fill in text for the Assistant role as part of an ongoing dialogue between a human user (`Human:`) and an AI assistant (`Assistant:`). Prompts sent via the API must contain `\\n\\nHuman:` and `\\n\\nAssistant:` as the signals of who\'s speaking. The final turn must always be `\\n\\nAssistant:` - the input string cannot have `\\n\\nHuman:` as the final role. Because Claude is chat-based but accepts a string as input, it can be treated as either a LangChain `ChatModel` or `LLM`. This means there are two wrappers in LangChain - `ChatAnthropic` and `Anthropic`. It is generally recommended to use the `ChatAnthropic` wrapper, and format your prompts as `ChatMessage`s (we will show examples of this below). This is because it keeps your prompt in a general format that you can easily then also use with other models (should you want to). However, if you want more fine-grained control over the prompt, you can use the `Anthropic` wrapper - we will show and example of this as well. The `Anthropic` wrapper however is deprecated, as all functionality can be achieved in a more generic way using `ChatAnthropic`. ## Prompting Best Practices Anthropic models have several prompting best practices compared to OpenAI models. **No System Messages** Anthropic models are not trained on the concept of a ""system message"". We have worked with the Anthropic team to handle them somewhat appropriately (a Human message with an `admin` tag) but this is largely a hack and it is recommended that you do not use system messages. **AI Messages Can Continue** A completion from Claude is a continuation of the last text in the string which allows you further control over Claude\'s output. For example, putting words in Claude\'s mouth in a prompt like this: `\\n\\nHuman: Tell me a joke about bears\\n\\nAssistant: What do you call a bear with no teeth?` This will return a completion like this `A gummy bear!` instead of a whole new assistant message with a different random bear joke. ## ChatAnthropic `ChatAnthropic` is a subclass of LangChain\'s `ChatModel`, meaning it works best with `ChatPromptTemplate`. You can import this wrapper with the following code: ```text from langchain.chat_models import ChatAnthropic model = ChatAnthropic() ``` When working with ChatModels, it is preferred that you design your prompts as `ChatPromptTemplate`s. Here is an example below of doing that: ```text from langchain.prompts import ChatPromptTemplate prompt = ChatPromptTemplate.from_messages([ (""system"", ""You are a helpful chatbot""), (""human"", ""Tell me a joke about {topic}""), ]) ``` You can then use this in a chain as follows: ```text chain = prompt | model chain.invoke({""topic"": ""bears""}) ``` How is the prompt actually being formatted under the hood? We can see that by running the following code ```text prompt_value = prompt.format_prompt(topic=""bears"") model.convert_prompt(prompt_value) ``` This produces the following formatted string: ```text \'\\n\\nYou are a helpful chatbot\\n\\nHuman: Tell me a joke about bears\\n\\nAssistant:\' ``` We can see that under the hood LangChain is not appending any prefix/suffix to `SystemMessage`\'s. This is because Anthropic has no concept of `SystemMessage`. Anthropic requires all prompts to end with assistant messages. This means if the last message is not an assistant message, the suffix `Assistant:` will automatically be inserted. If you decide instead to use a normal PromptTemplate (one that just works on a single string) let\'s take a look at what happens: ```text from langchain.prompts import PromptTemplate prompt = PromptTemplate.from_template(""Tell me a joke about {topic}"") prompt_value = prompt.format_prompt(topic=""bears"") model.convert_prompt(prompt_value) ``` This produces the following formatted string: ```text \'\\n\\nHuman: Tell me a joke about bears\\n\\nAssistant:\' ``` We can see that it automatically adds the Human and Assistant tags. What is happening under the hood? First: the string gets converted to a single human message. This happens generically (because we are using a subclass of `ChatModel`). Then, similarly to the above example, an empty Assistant message is getting appended. This is Anthropic specific. ## [Deprecated] Anthropic This `Anthropic` wrapper is subclassed from `LLM`. We can import it with: ```text from langchain.llms import Anthropic model = Anthropic() ``` This model class is designed to work with normal PromptTemplates. An example of that is below: ```text prompt = PromptTemplate.from_template(""Tell me a joke about {topic}"") chain = prompt | model chain.invoke({""topic"": ""bears""}) ``` Let\'s see what is going on with the prompt templating under the hood! ```text prompt_value = prompt.format_prompt(topic=""bears"") model.convert_prompt(prompt_value) ``` This outputs the following ```text \'\\n\\nHuman: Tell me a joke about bears\\n\\nAssistant: Sure, here you go:\\n\' ``` Notice that it adds the Human tag at the start of the string, and then finishes it with `\\n\\nAssistant: Sure, here you go:`. The extra `Sure, here you go` was added on purpose by the Anthropic team. What happens if we have those symbols in the prompt directly? ```text prompt = PromptTemplate.from_template(""Human: Tell me a joke about {topic}"") prompt_value = prompt.format_prompt(topic=""bears"") model.convert_prompt(prompt_value) ``` This outputs: ```text \'\\n\\nHuman: Tell me a joke about bears\' ``` We can see that we detect that the user is trying to use the special tokens, and so we don\'t do any formatting. - [Prompting Overview](#prompting-overview) - [Prompting Best Practices](#prompting-best-practices) - [ChatAnthropic](#chatanthropic) - [Deprecated Anthropic](#deprecated-anthropic)']","To initialize `OpenAIAnthropicVectorStore`, you need to provide the necessary parameters such as the API key and possibly other configuration settings. Here's a basic example of how to do it:

```python
from langchain.vectorstores import OpenAIAnthropicVectorStore

# Initialize the vector store with your API key
vector_store = OpenAIAnthropicVectorStore(api_key=""your_api_key_here"")
```

Replace `""your_api_key_here""` with your actual API key. Depending on the library's requirements, you might need to provide additional parameters or configuration settings. Refer to the official documentation for the most accurate and detailed information.","I'm not sure what OpenAIAnthropicVectorStore is, though there is a similar-sounding class OpenSearchVectorSearch.",0.0,0.0,,0.019582248325011244,0.03773584905660377
13,whats the code to load text file into a vector store,"['Rockset | Rockset [Rockset]( is a real-time analytics database service for serving low latency, high concurrency analytical queries at scale. It builds a Converged Index on structured and semi-structured data with an efficient store for vector embeddings. Its support for running SQL on schemaless data makes it a perfect choice for running vector search with metadata filters. This notebook goes over how to use [Rockset]( to store chat message history. ## Setting up ```bash pip install rockset ``` To begin, with get your API key from the [Rockset console]( Find your API region for the Rockset [API reference]( ## Example ```python from langchain.memory.chat_message_histories import RocksetChatMessageHistory from rockset import Regions, RocksetClient history = RocksetChatMessageHistory( session_id=""MySession"", client=RocksetClient( api_key=""YOUR API KEY"", host=Regions.usw2a1, # us-west-2 Oregon ), collection=""langchain_demo"", sync=True, ) history.add_user_message(""hi!"") history.add_ai_message(""whats up?"") print(history.messages) ``` The output should be something like: ```python [ HumanMessage(content=\'hi!\', additional_kwargs={\'id\': \'2e62f1c2-e9f7-465e-b551-49bae07fe9f0\'}, example=False), AIMessage(content=\'whats up?\', additional_kwargs={\'id\': \'b9be8eda-4c18-4cf8-81c3-e91e876927d0\'}, example=False) ] ``` - [Setting up](#setting-up) - [Example](#example)', 'Chatbots | Chatbots []( ## Use case Chatbots are one of the central LLM use-cases. The core features of chatbots are that they can have long-running conversations and have access to information that users want to know about. Aside from basic prompting and LLMs, memory and retrieval are the core components of a chatbot. Memory allows a chatbot to remember past interactions, and retrieval provides a chatbot with up-to-date, domain-specific information. ![Image description](/assets/images/chat_use_case-eb8a4883931d726e9f23628a0d22e315.png) ## Overview The chat model interface is based around messages rather than raw text. Several components are important to consider for chat: - `chat model`: See [here](/docs/integrations/chat) for a list of chat model integrations and [here](/docs/modules/model_io/chat) for documentation on the chat model interface in LangChain. You can use `LLMs` (see [here](/docs/modules/model_io/llms)) for chatbots as well, but chat models have a more conversational tone and natively support a message interface. - `prompt template`: Prompt templates make it easy to assemble prompts that combine default messages, user input, chat history, and (optionally) additional retrieved context. - `memory`: [See here](/docs/modules/memory/) for in-depth documentation on memory types - `retriever` (optional): [See here](/docs/modules/data_connection/retrievers) for in-depth documentation on retrieval systems. These are useful if you want to build a chatbot with domain-specific knowledge. ## Quickstart Here\'s a quick preview of how we can create chatbot interfaces. First let\'s install some dependencies and set the required credentials: ```bash pip install langchain openai # Set env var OPENAI_API_KEY or load from a .env file: # import dotenv # dotenv.load_dotenv() ``` With a plain chat model, we can get chat completions by [passing one or more messages](/docs/modules/model_io/chat) to the model. The chat model will respond with a message. ```python from langchain.chat_models import ChatOpenAI from langchain.schema import HumanMessage, SystemMessage chat = ChatOpenAI() chat( [ HumanMessage( content=""Translate this sentence from English to French: I love programming."" ) ] ) ``` ```text AIMessage(content=""J\'adore la programmation."", additional_kwargs={}, example=False) ``` And if we pass in a list of messages: ```python messages = [ SystemMessage( content=""You are a helpful assistant that translates English to French."" ), HumanMessage(content=""I love programming.""), ] chat(messages) ``` ```text AIMessage(content=""J\'adore la programmation."", additional_kwargs={}, example=False) ``` We can then wrap our chat model in a `ConversationChain`, which has built-in memory for remembering past user inputs and model outputs. ```python from langchain.chains import ConversationChain conversation = ConversationChain(llm=chat) conversation.run(""Translate this sentence from English to French: I love programming."") ``` ```text \'Je adore la programmation.\' ``` ```python conversation.run(""Translate it to German."") ``` ```text \'Ich liebe Programmieren.\' ``` ## Memory As we mentioned above, the core component of chatbots is the memory system. One of the simplest and most commonly used forms of memory is `ConversationBufferMemory`: - This memory allows for storing of messages in a `buffer` - When called in a chain, it returns all of the messages it has stored LangChain comes with many other types of memory, too. [See here](/docs/modules/memory/) for in-depth documentation on memory types. For now let\'s take a quick look at ConversationBufferMemory. We can manually add a few chat messages to the memory like so: ```python from langchain.memory import ConversationBufferMemory memory = ConversationBufferMemory() memory.chat_memory.add_user_message(""hi!"") memory.chat_memory.add_ai_message(""whats up?"") ``` And now we can load from our memory. The key method exposed by all `Memory` classes is `load_memory_variables`. This takes in any initial chain input and returns a list of memory variables which are added to the chain input. Since this simple memory type doesn\'t actually take into account the chain input when loading memory, we can pass in an empty input for now: ```python memory.load_memory_variables({}) ``` ```text {\'history\': \'Human: hi!\\nAI: whats up?\'} ``` We can also keep a sliding window of the most recent `k` interactions using `ConversationBufferWindowMemory`. ```python from langchain.memory import ConversationBufferWindowMemory memory = ConversationBufferWindowMemory(k=1) memory.save_context({""input"": ""hi""}, {""output"": ""whats up""}) memory.save_context({""input"": ""not much you""}, {""output"": ""not much""}) memory.load_memory_variables({}) ``` ```text {\'history\': \'Human: not much you\\nAI: not much\'} ``` `ConversationSummaryMemory` is an extension of this theme. It creates a summary of the conversation over time. This memory is most useful for longer conversations where the full message history would consume many tokens. ```python from langchain.llms import OpenAI from langchain.memory import ConversationSummaryMemory llm = OpenAI(temperature=0) memory = ConversationSummaryMemory(llm=llm) memory.save_context({""input"": ""hi""}, {""output"": ""whats up""}) memory.save_context( {""input"": ""im working on better docs for chatbots""}, {""output"": ""oh, that sounds like a lot of work""}, ) memory.save_context( {""input"": ""yes, but it\'s worth the effort""}, {""output"": ""agreed, good docs are important!""}, ) ``` ```python memory.load_memory_variables({}) ``` ```text {\'history\': \'\\nThe human greets the AI, to which the AI responds. The human then mentions they are working on better docs for chatbots, to which the AI responds that it sounds like a lot of work. The human agrees that it is worth the effort, and the AI agrees that good docs are important.\'} ``` `ConversationSummaryBufferMemory` extends this a bit further: It uses token length rather than number of interactions to determine when to flush interactions. ```python from langchain.memory import ConversationSummaryBufferMemory memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=10) memory.save_context({""input"": ""hi""}, {""output"": ""whats up""}) memory.save_context({""input"": ""not much you""}, {""output"": ""not much""}) ``` ## Conversation We can unpack what goes under the hood with `ConversationChain`. We can specify our memory, `ConversationSummaryMemory` and we can specify the prompt. ```python from langchain.chains import LLMChain from langchain.prompts import ( ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate, ) # LLM llm = ChatOpenAI() # Prompt prompt = ChatPromptTemplate( messages=[ SystemMessagePromptTemplate.from_template( ""You are a nice chatbot having a conversation with a human."" ), # The `variable_name` here is what must align with memory MessagesPlaceholder(variable_name=""chat_history""), HumanMessagePromptTemplate.from_template(""{question}""), ] ) # Notice that we `return_messages=True` to fit into the MessagesPlaceholder # Notice that `""chat_history""` aligns with the MessagesPlaceholder name memory = ConversationBufferMemory(memory_key=""chat_history"", return_messages=True) conversation = LLMChain(llm=llm, prompt=prompt, verbose=True, memory=memory) # Notice that we just pass in the `question` variables - `chat_history` gets populated by memory conversation({""question"": ""hi""}) ``` ```text > Entering new LLMChain chain... Prompt after formatting: System: You are a nice chatbot having a conversation with a human. Human: hi > Finished chain. {\'question\': \'hi\', \'chat_history\': [HumanMessage(content=\'hi\', additional_kwargs={}, example=False), AIMessage(content=\'Hello! How can I assist you today?\', additional_kwargs={}, example=False)], \'text\': \'Hello! How can I assist you today?\'} ``` ```python conversation( {""question"": ""Translate this sentence from English to French: I love programming.""} ) ``` ```text > Entering new LLMChain chain... Prompt after formatting: System: You are a nice chatbot having a conversation with a human. Human: hi AI: Hello! How can I assist you today? Human: Translate this sentence from English to French: I love programming. > Finished chain. {\'question\': \'Translate this sentence from English to French: I love programming.\', \'chat_history\': [HumanMessage(content=\'hi\', additional_kwargs={}, example=False), AIMessage(content=\'Hello! How can I assist you today?\', additional_kwargs={}, example=False), HumanMessage(content=\'Translate this sentence from English to French: I love programming.\', additional_kwargs={}, example=False), AIMessage(content=\'Sure! The translation of ""I love programming"" from English to French is ""J\\\'adore programmer.""\', additional_kwargs={}, example=False)], \'text\': \'Sure! The translation of ""I love programming"" from English to French is ""J\\\'adore programmer.""\'} ``` ```python conversation({""question"": ""Now translate the sentence to German.""}) ``` ```text > Entering new LLMChain chain... Prompt after formatting: System: You are a nice chatbot having a conversation with a human. Human: hi AI: Hello! How can I assist you today? Human: Translate this sentence from English to French: I love programming. AI: Sure! The translation of ""I love programming"" from English to French is ""J\'adore programmer."" Human: Now translate the sentence to German. > Finished chain. {\'question\': \'Now translate the sentence to German.\', \'chat_history\': [HumanMessage(content=\'hi\', additional_kwargs={}, example=False), AIMessage(content=\'Hello! How can I assist you today?\', additional_kwargs={}, example=False), HumanMessage(content=\'Translate this sentence from English to French: I love programming.\', additional_kwargs={}, example=False), AIMessage(content=\'Sure! The translation of ""I love programming"" from English to French is ""J\\\'adore programmer.""\', additional_kwargs={}, example=False), HumanMessage(content=\'Now translate the sentence to German.\', additional_kwargs={}, example=False), AIMessage(content=\'Certainly! The translation of ""I love programming"" from English to German is ""Ich liebe das Programmieren.""\', additional_kwargs={}, example=False)], \'text\': \'Certainly! The translation of ""I love programming"" from English to German is ""Ich liebe das Programmieren.""\'} ``` We can see the chat history preserved in the prompt using the [LangSmith trace]( ![Image description](/assets/images/chat_use_case_2-a76871806149f125d08ff149363e0c2c.png) ## Chat Retrieval Now, suppose we want to [chat with documents]( or some other source of knowledge. This is popular use case, combining chat with [document retrieval](/docs/use_cases/question_answering). It allows us to chat with specific information that the model was not trained on. ```bash pip install tiktoken chromadb ``` Load a blog post. ```python from langchain.document_loaders import WebBaseLoader loader = WebBaseLoader("" data = loader.load() ``` Split and store this in a vector. ```python from langchain.text_splitter import RecursiveCharacterTextSplitter text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0) all_splits = text_splitter.split_documents(data) from langchain.embeddings import OpenAIEmbeddings from langchain.vectorstores import Chroma vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings()) ``` Create our memory, as before, but\'s let\'s use `ConversationSummaryMemory`. ```python memory = ConversationSummaryMemory( llm=llm, memory_key=""chat_history"", return_messages=True ) ``` ```python from langchain.chains import ConversationalRetrievalChain from langchain.chat_models import ChatOpenAI llm = ChatOpenAI() retriever = vectorstore.as_retriever() qa = ConversationalRetrievalChain.from_llm(llm, retriever=retriever, memory=memory) ``` ```python qa(""How do agents use Task decomposition?"") ``` ```text {\'question\': \'How do agents use Task decomposition?\', \'chat_history\': [SystemMessage(content=\'\', additional_kwargs={})], \'answer\': \'Agents can use task decomposition in several ways:\\n\\n1. Simple prompting: Agents can use Language Model based prompting to break down tasks into subgoals. For example, by providing prompts like ""Steps for XYZ"" or ""What are the subgoals for achieving XYZ?"", the agent can generate a sequence of smaller steps that lead to the completion of the overall task.\\n\\n2. Task-specific instructions: Agents can be given task-specific instructions to guide their planning process. For example, if the task is to write a novel, the agent can be instructed to ""Write a story outline."" This provides a high-level structure for the task and helps in breaking it down into smaller components.\\n\\n3. Human inputs: Agents can also take inputs from humans to decompose tasks. This can be done through direct communication or by leveraging human expertise. Humans can provide guidance and insights to help the agent break down complex tasks into manageable subgoals.\\n\\nOverall, task decomposition allows agents to break down large tasks into smaller, more manageable subgoals, enabling them to plan and execute complex tasks efficiently.\'} ``` ```python qa(""What are the various ways to implement memory to support it?"") ``` ```text {\'question\': \'What are the various ways to implement memory to support it?\', \'chat_history\': [SystemMessage(content=\'The human asks how agents use task decomposition. The AI explains that agents can use task decomposition in several ways, including simple prompting, task-specific instructions, and human inputs. Task decomposition allows agents to break down large tasks into smaller, more manageable subgoals, enabling them to plan and execute complex tasks efficiently.\', additional_kwargs={})], \'answer\': \'There are several ways to implement memory to support task decomposition:\\n\\n1. Long-Term Memory Management: This involves storing and organizing information in a long-term memory system. The agent can retrieve past experiences, knowledge, and learned strategies to guide the task decomposition process.\\n\\n2. Internet Access: The agent can use internet access to search for relevant information and gather resources to aid in task decomposition. This allows the agent to access a vast amount of information and utilize it in the decomposition process.\\n\\n3. GPT-3.5 Powered Agents: The agent can delegate simple tasks to GPT-3.5 powered agents. These agents can perform specific tasks or provide assistance in task decomposition, allowing the main agent to focus on higher-level planning and decision-making.\\n\\n4. File Output: The agent can store the results of task decomposition in files or documents. This allows for easy retrieval and reference during the execution of the task.\\n\\nThese memory resources help the agent in organizing and managing information, making informed decisions, and effectively decomposing complex tasks into smaller, manageable subgoals.\'} ``` Again, we can use the [LangSmith trace]( to explore the prompt structure. ### Going deeper - Agents, such as the [conversational retrieval agent](/docs/use_cases/question_answering/conversational_retrieval_agents), can be used for retrieval when necessary while also holding a conversation. - [Use case](#use-case) - [Overview](#overview) - [Quickstart](#quickstart) - [Memory](#memory) - [Conversation](#conversation) - [Chat Retrieval](#chat-retrieval)- [Going deeper](#going-deeper)', 'Xata | Xata [Xata]( is a serverless data platform, based on `PostgreSQL` and `Elasticsearch`. It provides a Python SDK for interacting with your database, and a UI for managing your data. With the `XataChatMessageHistory` class, you can use Xata databases for longer-term persistence of chat sessions. This notebook covers: - A simple example showing what `XataChatMessageHistory` does. - A more complex example using a REACT agent that answer questions based on a knowledge based or documentation (stored in Xata as a vector store) and also having a long-term searchable history of its past messages (stored in Xata as a memory store) ## Setup ### Create a database In the [Xata UI]( create a new database. You can name it whatever you want, in this notepad we\'ll use `langchain`. The Langchain integration can auto-create the table used for storying the memory, and this is what we\'ll use in this example. If you want to pre-create the table, ensure it has the right schema and set `create_table` to `False` when creating the class. Pre-creating the table saves one round-trip to the database during each session initialization. Let\'s first install our dependencies: ```bash pip install xata openai langchain ``` Next, we need to get the environment variables for Xata. You can create a new API key by visiting your [account settings]( To find the database URL, go to the Settings page of the database that you have created. The database URL should look something like this: ` ```python import getpass api_key = getpass.getpass(""Xata API key: "") db_url = input(""Xata database URL (copy it from your DB settings):"") ``` ## Create a simple memory store To test the memory store functionality in isolation, let\'s use the following code snippet: ```python from langchain.memory import XataChatMessageHistory history = XataChatMessageHistory( session_id=""session-1"", api_key=api_key, db_url=db_url, table_name=""memory"" ) history.add_user_message(""hi!"") history.add_ai_message(""whats up?"") ``` The above code creates a session with the ID `session-1` and stores two messages in it. After running the above, if you visit the Xata UI, you should see a table named `memory` and the two messages added to it. You can retrieve the message history for a particular session with the following code: ```python history.messages ``` ## Conversational Q&A chain on your data with memory Let\'s now see a more complex example in which we combine OpenAI, the Xata Vector Store integration, and the Xata memory store integration to create a Q&A chat bot on your data, with follow-up questions and history. We\'re going to need to access the OpenAI API, so let\'s configure the API key: ```python import os os.environ[""OPENAI_API_KEY""] = getpass.getpass(""OpenAI API Key:"") ``` To store the documents that the chatbot will search for answers, add a table named `docs` to your `langchain` database using the Xata UI, and add the following columns: - `content` of type ""Text"". This is used to store the `Document.pageContent` values. - `embedding` of type ""Vector"". Use the dimension used by the model you plan to use. In this notebook we use OpenAI embeddings, which have 1536 dimensions. Let\'s create the vector store and add some sample docs to it: ```python from langchain.embeddings.openai import OpenAIEmbeddings from langchain.vectorstores.xata import XataVectorStore embeddings = OpenAIEmbeddings() texts = [ ""Xata is a Serverless Data platform based on PostgreSQL"", ""Xata offers a built-in vector type that can be used to store and query vectors"", ""Xata includes similarity search"", ] vector_store = XataVectorStore.from_texts( texts, embeddings, api_key=api_key, db_url=db_url, table_name=""docs"" ) ``` After running the above command, if you go to the Xata UI, you should see the documents loaded together with their embeddings in the `docs` table. Let\'s now create a ConversationBufferMemory to store the chat messages from both the user and the AI. ```python from uuid import uuid4 from langchain.memory import ConversationBufferMemory chat_memory = XataChatMessageHistory( session_id=str(uuid4()), # needs to be unique per user session api_key=api_key, db_url=db_url, table_name=""memory"", ) memory = ConversationBufferMemory( memory_key=""chat_history"", chat_memory=chat_memory, return_messages=True ) ``` Now it\'s time to create an Agent to use both the vector store and the chat memory together. ```python from langchain.agents import AgentType, initialize_agent from langchain.agents.agent_toolkits import create_retriever_tool from langchain.chat_models import ChatOpenAI tool = create_retriever_tool( vector_store.as_retriever(), ""search_docs"", ""Searches and returns documents from the Xata manual. Useful when you need to answer questions about Xata."", ) tools = [tool] llm = ChatOpenAI(temperature=0) agent = initialize_agent( tools, llm, agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION, verbose=True, memory=memory, ) ``` To test, let\'s tell the agent our name: ```python agent.run(input=""My name is bob"") ``` Now, let\'s now ask the agent some questions about Xata: ```python agent.run(input=""What is xata?"") ``` Notice that it answers based on the data stored in the document store. And now, let\'s ask a follow up question: ```python agent.run(input=""Does it support similarity search?"") ``` And now let\'s test its memory: ```python agent.run(input=""Did I tell you my name? What is it?"") ``` - [Setup](#setup)- [Create a database](#create-a-database) - [Create a simple memory store](#create-a-simple-memory-store) - [Conversational Q&A chain on your data with memory](#conversational-qa-chain-on-your-data-with-memory)']","To load a text file into a vector store using the XataVectorStore, you'll need to follow these steps:

1. **Install necessary libraries**:
   ```bash
   pip install xata openai langchain
   ```

2. **Import required modules**:
   ```python
   from langchain.embeddings.openai import OpenAIEmbeddings
   from langchain.vectorstores.xata import XataVectorStore
   from langchain.text_splitter import CharacterTextSplitter
   ```

3. **Load and split the text**:
   ```python
   # Load your text file
   with open('your_text_file.txt', 'r') as file:
       text = file.read()

   # Split the text into chunks
   text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)
   texts = text_splitter.split_text(text)
   ```

4. **Create embeddings and load into XataVectorStore**:
   ```python
   # Initialize embeddings
   embeddings = OpenAIEmbeddings()

   # Create the vector store
   db_url = ""your_db_url""
   api_key = ""your_api_key""
   table_name = ""your_table_name""

   vector_store = XataVectorStore.from_texts(
       texts,
       embeddings,
       api_key=api_key,
       db_url=db_url,
       table_name=table_name
   )
   ```

Replace `'your_text_file.txt'`, `'your_db_url'`, `'your_api_key'`, and `'your_table_name'` with your actual file path, database URL, API key, and table name respectively.","To load a text file into a vector store using LangChain's document loaders, you can use the following code snippet:

```python
from langchain.document_loaders import TextLoader

loader = TextLoader(""./index.txt"")
docs = loader.load()
print(docs)
```

This code imports the `TextLoader` class from the document_loaders module, creates an instance of the loader with the path to the text file as an argument, and then uses the `load()` method to read the file's content and create a Document. The resulting Document will contain the text content of the file.",0.49999999995,0.0,,0.47078302369717645,0.22727272727272727
14,what does runnable.predict() mean?,"['Backed by a Vector Store | Backed by a Vector Store `VectorStoreRetrieverMemory` stores memories in a vector store and queries the top-K most ""salient"" docs every time it is called. This differs from most of the other Memory classes in that it doesn\'t explicitly track the order of interactions. In this case, the ""docs"" are previous conversation snippets. This can be useful to refer to relevant pieces of information that the AI was told earlier in the conversation. ```python from datetime import datetime from langchain.embeddings.openai import OpenAIEmbeddings from langchain.llms import OpenAI from langchain.memory import VectorStoreRetrieverMemory from langchain.chains import ConversationChain from langchain.prompts import PromptTemplate ``` ### Initialize your vector store Depending on the store you choose, this step may look different. Consult the relevant vector store documentation for more details. ```python import faiss from langchain.docstore import InMemoryDocstore from langchain.vectorstores import FAISS embedding_size = 1536 # Dimensions of the OpenAIEmbeddings index = faiss.IndexFlatL2(embedding_size) embedding_fn = OpenAIEmbeddings().embed_query vectorstore = FAISS(embedding_fn, index, InMemoryDocstore({}), {}) ``` ### Create your VectorStoreRetrieverMemory The memory object is instantiated from any vector store retriever. ```python # In actual usage, you would set `k` to be a higher value, but we use k=1 to show that # the vector lookup still returns the semantically relevant information retriever = vectorstore.as_retriever(search_kwargs=dict(k=1)) memory = VectorStoreRetrieverMemory(retriever=retriever) # When added to an agent, the memory object can save pertinent information from conversations or used tools memory.save_context({""input"": ""My favorite food is pizza""}, {""output"": ""that\'s good to know""}) memory.save_context({""input"": ""My favorite sport is soccer""}, {""output"": ""...""}) memory.save_context({""input"": ""I don\'t the Celtics""}, {""output"": ""ok""}) # ``` ```python # Notice the first result returned is the memory pertaining to tax help, which the language model deems more semantically relevant # to a 1099 than the other documents, despite them both containing numbers. print(memory.load_memory_variables({""prompt"": ""what sport should i watch?""})[""history""]) ``` ```text input: My favorite sport is soccer output: ... ``` ## Using in a chain Let\'s walk through an example, again setting `verbose=True` so we can see the prompt. ```python llm = OpenAI(temperature=0) # Can be any valid LLM _DEFAULT_TEMPLATE = """"""The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Relevant pieces of previous conversation: {history} (You do not need to use these pieces of information if not relevant) Current conversation: Human: {input} AI:"""""" PROMPT = PromptTemplate( input_variables=[""history"", ""input""], template=_DEFAULT_TEMPLATE ) conversation_with_summary = ConversationChain( llm=llm, prompt=PROMPT, # We set a very low max_token_limit for the purposes of testing. memory=memory, verbose=True ) conversation_with_summary.predict(input=""Hi, my name is Perry, what\'s up?"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Relevant pieces of previous conversation: input: My favorite food is pizza output: that\'s good to know (You do not need to use these pieces of information if not relevant) Current conversation: Human: Hi, my name is Perry, what\'s up? AI: > Finished chain. "" Hi Perry, I\'m doing well. How about you?"" ``` ```python # Here, the basketball related content is surfaced conversation_with_summary.predict(input=""what\'s my favorite sport?"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Relevant pieces of previous conversation: input: My favorite sport is soccer output: ... (You do not need to use these pieces of information if not relevant) Current conversation: Human: what\'s my favorite sport? AI: > Finished chain. \' You told me earlier that your favorite sport is soccer.\' ``` ```python # Even though the language model is stateless, since relevant memory is fetched, it can ""reason"" about the time. # Timestamping memories and data is useful in general to let the agent determine temporal relevance conversation_with_summary.predict(input=""Whats my favorite food"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Relevant pieces of previous conversation: input: My favorite food is pizza output: that\'s good to know (You do not need to use these pieces of information if not relevant) Current conversation: Human: Whats my favorite food AI: > Finished chain. \' You said your favorite food is pizza.\' ``` ```python # The memories from the conversation are automatically stored, # since this query best matches the introduction chat above, # the agent is able to \'remember\' the user\'s name. conversation_with_summary.predict(input=""What\'s my name?"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Relevant pieces of previous conversation: input: Hi, my name is Perry, what\'s up? response: Hi Perry, I\'m doing well. How about you? (You do not need to use these pieces of information if not relevant) Current conversation: Human: What\'s my name? AI: > Finished chain. \' Your name is Perry.\' ``` - [Initialize your vector store](#initialize-your-vector-store) - [Create your VectorStoreRetrieverMemory](#create-your-vectorstoreretrievermemory) - [Using in a chain](#using-in-a-chain)', 'Conversation Knowledge Graph | Conversation Knowledge Graph This type of memory uses a knowledge graph to recreate memory. ## Using memory with LLM ```python from langchain.llms import OpenAI from langchain.memory import ConversationKGMemory ``` ```python llm = OpenAI(temperature=0) memory = ConversationKGMemory(llm=llm) memory.save_context({""input"": ""say hi to sam""}, {""output"": ""who is sam""}) memory.save_context({""input"": ""sam is a friend""}, {""output"": ""okay""}) ``` ```python memory.load_memory_variables({""input"": ""who is sam""}) ``` ```text {\'history\': \'On Sam: Sam is friend.\'} ``` We can also get the history as a list of messages (this is useful if you are using this with a chat model). ```python memory = ConversationKGMemory(llm=llm, return_messages=True) memory.save_context({""input"": ""say hi to sam""}, {""output"": ""who is sam""}) memory.save_context({""input"": ""sam is a friend""}, {""output"": ""okay""}) ``` ```python memory.load_memory_variables({""input"": ""who is sam""}) ``` ```text {\'history\': [SystemMessage(content=\'On Sam: Sam is friend.\', additional_kwargs={})]} ``` We can also more modularly get current entities from a new message (will use previous messages as context). ```python memory.get_current_entities(""what\'s Sams favorite color?"") ``` ```text [\'Sam\'] ``` We can also more modularly get knowledge triplets from a new message (will use previous messages as context). ```python memory.get_knowledge_triplets(""her favorite color is red"") ``` ```text [KnowledgeTriple(subject=\'Sam\', predicate=\'favorite color\', object_=\'red\')] ``` ## Using in a chain Let\'s now use this in a chain! ```python llm = OpenAI(temperature=0) from langchain.chains import ConversationChain from langchain.prompts.prompt import PromptTemplate template = """"""The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses information contained in the ""Relevant Information"" section and does not hallucinate. Relevant Information: {history} Conversation: Human: {input} AI:"""""" prompt = PromptTemplate(input_variables=[""history"", ""input""], template=template) conversation_with_kg = ConversationChain( llm=llm, verbose=True, prompt=prompt, memory=ConversationKGMemory(llm=llm) ) ``` ```python conversation_with_kg.predict(input=""Hi, what\'s up?"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses information contained in the ""Relevant Information"" section and does not hallucinate. Relevant Information: Conversation: Human: Hi, what\'s up? AI: > Finished chain. "" Hi there! I\'m doing great. I\'m currently in the process of learning about the world around me. I\'m learning about different cultures, languages, and customs. It\'s really fascinating! How about you?"" ``` ```python conversation_with_kg.predict( input=""My name is James and I\'m helping Will. He\'s an engineer."" ) ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses information contained in the ""Relevant Information"" section and does not hallucinate. Relevant Information: Conversation: Human: My name is James and I\'m helping Will. He\'s an engineer. AI: > Finished chain. "" Hi James, it\'s nice to meet you. I\'m an AI and I understand you\'re helping Will, the engineer. What kind of engineering does he do?"" ``` ```python conversation_with_kg.predict(input=""What do you know about Will?"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses information contained in the ""Relevant Information"" section and does not hallucinate. Relevant Information: On Will: Will is an engineer. Conversation: Human: What do you know about Will? AI: > Finished chain. \' Will is an engineer.\' ``` - [Using memory with LLM](#using-memory-with-llm) - [Using in a chain](#using-in-a-chain)', 'Customizing Conversational Memory | Customizing Conversational Memory This notebook walks through a few ways to customize conversational memory. ```python from langchain.chains import ConversationChain from langchain.llms import OpenAI from langchain.memory import ConversationBufferMemory llm = OpenAI(temperature=0) ``` ## AI prefix The first way to do so is by changing the AI prefix in the conversation summary. By default, this is set to ""AI"", but you can set this to be anything you want. Note that if you change this, you should also change the prompt used in the chain to reflect this naming change. Let\'s walk through an example of that in the example below. ```python # Here it is by default set to ""AI"" conversation = ConversationChain( llm=llm, verbose=True, memory=ConversationBufferMemory() ) ``` ```python conversation.predict(input=""Hi there!"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: Human: Hi there! AI: > Finished ConversationChain chain. "" Hi there! It\'s nice to meet you. How can I help you today?"" ``` ```python conversation.predict(input=""What\'s the weather?"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: Human: Hi there! AI: Hi there! It\'s nice to meet you. How can I help you today? Human: What\'s the weather? AI: > Finished ConversationChain chain. \' The current weather is sunny and warm with a temperature of 75 degrees Fahrenheit. The forecast for the next few days is sunny with temperatures in the mid-70s.\' ``` ```python # Now we can override it and set it to ""AI Assistant"" from langchain.prompts.prompt import PromptTemplate template = """"""The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: {history} Human: {input} AI Assistant:"""""" PROMPT = PromptTemplate(input_variables=[""history"", ""input""], template=template) conversation = ConversationChain( prompt=PROMPT, llm=llm, verbose=True, memory=ConversationBufferMemory(ai_prefix=""AI Assistant""), ) ``` ```python conversation.predict(input=""Hi there!"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: Human: Hi there! AI Assistant: > Finished ConversationChain chain. "" Hi there! It\'s nice to meet you. How can I help you today?"" ``` ```python conversation.predict(input=""What\'s the weather?"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: Human: Hi there! AI Assistant: Hi there! It\'s nice to meet you. How can I help you today? Human: What\'s the weather? AI Assistant: > Finished ConversationChain chain. \' The current weather is sunny and warm with a temperature of 75 degrees Fahrenheit. The forecast for the rest of the day is sunny with a high of 78 degrees and a low of 65 degrees.\' ``` ## Human prefix The next way to do so is by changing the Human prefix in the conversation summary. By default, this is set to ""Human"", but you can set this to be anything you want. Note that if you change this, you should also change the prompt used in the chain to reflect this naming change. Let\'s walk through an example of that in the example below. ```python # Now we can override it and set it to ""Friend"" from langchain.prompts.prompt import PromptTemplate template = """"""The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: {history} Friend: {input} AI:"""""" PROMPT = PromptTemplate(input_variables=[""history"", ""input""], template=template) conversation = ConversationChain( prompt=PROMPT, llm=llm, verbose=True, memory=ConversationBufferMemory(human_prefix=""Friend""), ) ``` ```python conversation.predict(input=""Hi there!"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: Friend: Hi there! AI: > Finished ConversationChain chain. "" Hi there! It\'s nice to meet you. How can I help you today?"" ``` ```python conversation.predict(input=""What\'s the weather?"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: Friend: Hi there! AI: Hi there! It\'s nice to meet you. How can I help you today? Friend: What\'s the weather? AI: > Finished ConversationChain chain. \' The weather right now is sunny and warm with a temperature of 75 degrees Fahrenheit. The forecast for the rest of the day is mostly sunny with a high of 82 degrees.\' ``` - [AI prefix](#ai-prefix) - [Human prefix](#human-prefix)']","`runnable.predict()` is a method used to generate a response from a runnable object, typically an instance of a language model or a chain. It takes input parameters and returns the output based on the model's logic or the chain's processing. In the context of the provided context, it's used to simulate a conversation or task execution by predicting the next response.","The runnable base class does not have a predict() method, but the LLM classes (which inherit from Runnable) do. For those classes, predict() formats a prompt with keyword args and passes it to the LLM.",0.0,0.0,0.0,0.06244385951234724,0.2
15,how to run a runnable,"['Dynamically route logic based on input | Dynamically route logic based on input This notebook covers how to do routing in the LangChain Expression Language. Routing allows you to create non-deterministic chains where the output of a previous step defines the next step. Routing helps provide structure and consistency around interactions with LLMs. There are two ways to perform routing: 1. Using a `RunnableBranch`. 2. Writing custom factory function that takes the input of a previous step and returns a **runnable**. Importantly, this should return a **runnable** and NOT actually execute. We\'ll illustrate both methods using a two step sequence where the first step classifies an input question as being about `LangChain`, `Anthropic`, or `Other`, then routes to a corresponding prompt chain. ## Using a RunnableBranch A `RunnableBranch` is initialized with a list of (condition, runnable) pairs and a default runnable. It selects which branch by passing each condition the input it\'s invoked with. It selects the first condition to evaluate to True, and runs the corresponding runnable to that condition with the input. If no provided conditions match, it runs the default runnable. Here\'s an example of what it looks like in action: ```python from langchain.chat_models import ChatAnthropic from langchain.prompts import PromptTemplate from langchain.schema.output_parser import StrOutputParser ``` First, let\'s create a chain that will identify incoming questions as being about `LangChain`, `Anthropic`, or `Other`: ```python chain = ( PromptTemplate.from_template( """"""Given the user question below, classify it as either being about `LangChain`, `Anthropic`, or `Other`. Do not respond with more than one word. {question} Classification:"""""" ) | ChatAnthropic() | StrOutputParser() ) ``` ```python chain.invoke({""question"": ""how do I call Anthropic?""}) ``` ```text \' Anthropic\' ``` Now, let\'s create three sub chains: ```python langchain_chain = ( PromptTemplate.from_template( """"""You are an expert in langchain. \\ Always answer questions starting with ""As Harrison Chase told me"". \\ Respond to the following question: Question: {question} Answer:"""""" ) | ChatAnthropic() ) anthropic_chain = ( PromptTemplate.from_template( """"""You are an expert in anthropic. \\ Always answer questions starting with ""As Dario Amodei told me"". \\ Respond to the following question: Question: {question} Answer:"""""" ) | ChatAnthropic() ) general_chain = ( PromptTemplate.from_template( """"""Respond to the following question: Question: {question} Answer:"""""" ) | ChatAnthropic() ) ``` ```python from langchain.schema.runnable import RunnableBranch branch = RunnableBranch( (lambda x: ""anthropic"" in x[""topic""].lower(), anthropic_chain), (lambda x: ""langchain"" in x[""topic""].lower(), langchain_chain), general_chain, ) ``` ```python full_chain = {""topic"": chain, ""question"": lambda x: x[""question""]} | branch ``` ```python full_chain.invoke({""question"": ""how do I use Anthropic?""}) ``` ```text AIMessage(content="" As Dario Amodei told me, here are some ways to use Anthropic:\\n\\n- Sign up for an account on Anthropic\'s website to access tools like Claude, Constitutional AI, and Writer. \\n\\n- Use Claude for tasks like email generation, customer service chat, and QA. Claude can understand natural language prompts and provide helpful responses.\\n\\n- Use Constitutional AI if you need an AI assistant that is harmless, honest, and helpful. It is designed to be safe and aligned with human values.\\n\\n- Use Writer to generate natural language content for things like marketing copy, stories, reports, and more. Give it a topic and prompt and it will create high-quality written content.\\n\\n- Check out Anthropic\'s documentation and blog for tips, tutorials, examples, and announcements about new capabilities as they continue to develop their AI technology.\\n\\n- Follow Anthropic on social media or subscribe to their newsletter to stay up to date on new features and releases.\\n\\n- For most people, the easiest way to leverage Anthropic\'s technology is through their website - just create an account to get started!"", additional_kwargs={}, example=False) ``` ```python full_chain.invoke({""question"": ""how do I use LangChain?""}) ``` ```text AIMessage(content=\' As Harrison Chase told me, here is how you use LangChain:\\n\\nLangChain is an AI assistant that can have conversations, answer questions, and generate text. To use LangChain, you simply type or speak your input and LangChain will respond. \\n\\nYou can ask LangChain questions, have discussions, get summaries or explanations about topics, and request it to generate text on a subject. Some examples of interactions:\\n\\n- Ask general knowledge questions and LangChain will try to answer factually. For example ""What is the capital of France?""\\n\\n- Have conversations on topics by taking turns speaking. You can prompt the start of a conversation by saying something like ""Let\\\'s discuss machine learning""\\n\\n- Ask for summaries or high-level explanations on subjects. For example ""Can you summarize the main themes in Shakespeare\\\'s Hamlet?"" \\n\\n- Give creative writing prompts or requests to have LangChain generate text in different styles. For example ""Write a short children\\\'s story about a mouse"" or ""Generate a poem in the style of Robert Frost about nature""\\n\\n- Correct LangChain if it makes an inaccurate statement and provide the right information. This helps train it.\\n\\nThe key is interacting naturally and giving it clear prompts and requests\', additional_kwargs={}, example=False) ``` ```python full_chain.invoke({""question"": ""whats 2 + 2""}) ``` ```text AIMessage(content=\' 2 + 2 = 4\', additional_kwargs={}, example=False) ``` ## Using a custom function You can also use a custom function to route between different outputs. Here\'s an example: ```python def route(info): if ""anthropic"" in info[""topic""].lower(): return anthropic_chain elif ""langchain"" in info[""topic""].lower(): return langchain_chain else: return general_chain ``` ```python from langchain.schema.runnable import RunnableLambda full_chain = {""topic"": chain, ""question"": lambda x: x[""question""]} | RunnableLambda( route ) ``` ```python full_chain.invoke({""question"": ""how do I use Anthroipc?""}) ``` ```text AIMessage(content=\' As Dario Amodei told me, to use Anthropic IPC you first need to import it:\\n\\n```python\\nfrom anthroipc import ic\\n```\\n\\nThen you can create a client and connect to the server:\\n\\n```python \\nclient = ic.connect()\\n```\\n\\nAfter that, you can call methods on the client and get responses:\\n\\n```python\\nresponse = client.ask(""What is the meaning of life?"")\\nprint(response)\\n```\\n\\nYou can also register callbacks to handle events: \\n\\n```python\\ndef on_poke(event):\\n print(""Got poked!"")\\n\\nclient.on(\\\'poke\\\', on_poke)\\n```\\n\\nAnd that\\\'s the basics of using the Anthropic IPC client library for Python! Let me know if you have any other questions!\', additional_kwargs={}, example=False) ``` ```python full_chain.invoke({""question"": ""how do I use LangChain?""}) ``` ```text AIMessage(content=\' As Harrison Chase told me, to use LangChain you first need to sign up for an API key at platform.langchain.com. Once you have your API key, you can install the Python library and write a simple Python script to call the LangChain API. Here is some sample code to get started:\\n\\n```python\\nimport langchain\\n\\napi_key = ""YOUR_API_KEY""\\n\\nlangchain.set_key(api_key)\\n\\nresponse = langchain.ask(""What is the capital of France?"")\\n\\nprint(response.response)\\n```\\n\\nThis will send the question ""What is the capital of France?"" to the LangChain API and print the response. You can customize the request by providing parameters like max_tokens, temperature, etc. The LangChain Python library documentation has more details on the available options. The key things are getting an API key and calling langchain.ask() with your question text. Let me know if you have any other questions!\', additional_kwargs={}, example=False) ``` ```python full_chain.invoke({""question"": ""whats 2 + 2""}) ``` ```text AIMessage(content=\' 4\', additional_kwargs={}, example=False) ``` - [Using a RunnableBranch](#using-a-runnablebranch) - [Using a custom function](#using-a-custom-function)', 'Agents | Agents The core idea of agents is to use a language model to choose a sequence of actions to take. In chains, a sequence of actions is hardcoded (in code). In agents, a language model is used as a reasoning engine to determine which actions to take and in which order. ## Concepts There are several key components here: ### Agent This is the chain responsible for deciding what step to take next. This is powered by a language model and a prompt. The inputs to this chain are: 1. Tools: Descriptions of available tools 2. User input: The high level objective 3. Intermediate steps: Any (action, tool output) pairs previously executed in order to achieve the user input The output is the next action(s) to take or the final response to send to the user (`AgentAction`s or `AgentFinish`). An action specifies a tool and the input to that tool. Different agents have different prompting styles for reasoning, different ways of encoding inputs, and different ways of parsing the output. For a full list of built-in agents see [agent types](/docs/modules/agents/agent_types/). You can also **easily build custom agents**, which we show how to do in the Get started section below. ### Tools Tools are functions that an agent can invoke. There are two important design considerations around tools: 1. Giving the agent access to the right tools 2. Describing the tools in a way that is most helpful to the agent Without thinking through both, you won\'t be able to build a working agent. If you don\'t give the agent access to a correct set of tools, it will never be able to accomplish the objectives you give it. If you don\'t describe the tools well, the agent won\'t know how to use them properly. LangChain provides a wide set of built-in tools, but also makes it easy to define your own (including custom descriptions). For a full list of built-in tools, see the [tools integrations section](/docs/integrations/tools/) ### Toolkits For many common tasks, an agent will need a set of related tools. For this LangChain provides the concept of toolkits - groups of around 3-5 tools needed to accomplish specific objectives. For example, the GitHub toolkit has a tool for searching through GitHub issues, a tool for reading a file, a tool for commenting, etc. LangChain provides a wide set of toolkits to get started. For a full list of built-in toolkits, see the [toolkits integrations section](/docs/integrations/toolkits/) ### AgentExecutor The agent executor is the runtime for an agent. This is what actually calls the agent, executes the actions it chooses, passes the action outputs back to the agent, and repeats. In pseudocode, this looks roughly like: ```python next_action = agent.get_action(...) while next_action != AgentFinish: observation = run(next_action) next_action = agent.get_action(..., next_action, observation) return next_action ``` While this may seem simple, there are several complexities this runtime handles for you, including: 1. Handling cases where the agent selects a non-existent tool 2. Handling cases where the tool errors 3. Handling cases where the agent produces output that cannot be parsed into a tool invocation 4. Logging and observability at all levels (agent decisions, tool calls) to stdout and/or to [LangSmith](/docs/langsmith). ### Other types of agent runtimes The `AgentExecutor` class is the main agent runtime supported by LangChain. However, there are other, more experimental runtimes we also support. These include: - [Plan-and-execute Agent](/docs/use_cases/more/agents/autonomous_agents/plan_and_execute) - [Baby AGI](/docs/use_cases/more/agents/autonomous_agents/baby_agi) - [Auto GPT](/docs/use_cases/more/agents/autonomous_agents/autogpt) You can also always create your own custom execution logic, which we show how to do below. ## Get started To best understand the agent framework, lets build an agent from scratch using LangChain Expression Language (LCEL). We\'ll need to build the agent itself, define custom tools, and run the agent and tools in a custom loop. At the end we\'ll show how to use the standard LangChain `AgentExecutor` to make execution easier. Some important terminology (and schema) to know: 1. `AgentAction`: This is a dataclass that represents the action an agent should take. It has a `tool` property (which is the name of the tool that should be invoked) and a `tool_input` property (the input to that tool) 2. `AgentFinish`: This is a dataclass that signifies that the agent has finished and should return to the user. It has a `return_values` parameter, which is a dictionary to return. It often only has one key - `output` - that is a string, and so often it is just this key that is returned. 3. `intermediate_steps`: These represent previous agent actions and corresponding outputs that are passed around. These are important to pass to future iteration so the agent knows what work it has already done. This is typed as a `List[Tuple[AgentAction, Any]]`. Note that observation is currently left as type `Any` to be maximally flexible. In practice, this is often a string. ### Setup: LangSmith By definition, agents take a self-determined, input-dependent sequence of steps before returning a user-facing output. This makes debugging these systems particularly tricky, and observability particularly important. [LangSmith](/docs/langsmith) is especially useful for such cases. When building with LangChain, any built-in agent or custom agent built with LCEL will automatically be traced in LangSmith. And if we use the `AgentExecutor`, we\'ll get full tracing of not only the agent planning steps but also the tool inputs and outputs. To set up LangSmith we just need set the following environment variables: ```bash export LANGCHAIN_TRACING_V2=""true"" export LANGCHAIN_API_KEY="""" ``` ### Define the agent We first need to create our agent. This is the chain responsible for determining what action to take next. In this example, we will use OpenAI Function Calling to create this agent. **This is generally the most reliable way to create agents.** For this guide, we will construct a custom agent that has access to a custom tool. We are choosing this example because for most real world use cases you will NEED to customize either the agent or the tools. We\'ll create a simple tool that computes the length of a word. This is useful because it\'s actually something LLMs can mess up due to tokenization. We will first create it WITHOUT memory, but we will then show how to add memory in. Memory is needed to enable conversation. First, let\'s load the language model we\'re going to use to control the agent. ```python from langchain.chat_models import ChatOpenAI llm = ChatOpenAI(model=""gpt-3.5-turbo"", temperature=0) ``` We can see that it struggles to count the letters in the string ""educa"". ```python llm.invoke(""how many letters in the word educa?"") ``` ```text AIMessage(content=\'There are 6 letters in the word ""educa"".\') ``` Next, let\'s define some tools to use. Let\'s write a really simple Python function to calculate the length of a word that is passed in. ```python from langchain.agents import tool @tool def get_word_length(word: str) -> int: """"""Returns the length of a word."""""" return len(word) tools = [get_word_length] ``` Now let us create the prompt. Because OpenAI Function Calling is finetuned for tool usage, we hardly need any instructions on how to reason, or how to output format. We will just have two input variables: `input` and `agent_scratchpad`. `input` should be a string containing the user objective. `agent_scratchpad` should be a sequence of messages that contains the previous agent tool invocations and the corresponding tool outputs. ```python from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder prompt = ChatPromptTemplate.from_messages( [ ( ""system"", ""You are very powerful assistant, but bad at calculating lengths of words."", ), (""user"", ""{input}""), MessagesPlaceholder(variable_name=""agent_scratchpad""), ] ) ``` How does the agent know what tools it can use? In this case we\'re relying on OpenAI function calling LLMs, which take functions as a separate argument and have been specifically trained to know when to invoke those functions. To pass in our tools to the agent, we just need to format them to the OpenAI function format and pass them to our model. (By `bind`-ing the functions, we\'re making sure that they\'re passed in each time the model is invoked.) ```python from langchain.tools.render import format_tool_to_openai_function llm_with_tools = llm.bind(functions=[format_tool_to_openai_function(t) for t in tools]) ``` Putting those pieces together, we can now create the agent. We will import two last utility functions: a component for formatting intermediate steps (agent action, tool output pairs) to input messages that can be sent to the model, and a component for converting the output message into an agent action/agent finish. ```python from langchain.agents.format_scratchpad import format_to_openai_function_messages from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser agent = ( { ""input"": lambda x: x[""input""], ""agent_scratchpad"": lambda x: format_to_openai_function_messages( x[""intermediate_steps""] ), } | prompt | llm_with_tools | OpenAIFunctionsAgentOutputParser() ) ``` Now that we have our agent, let\'s play around with it! Let\'s pass in a simple question and empty intermediate steps and see what it returns: ```python agent.invoke({""input"": ""how many letters in the word educa?"", ""intermediate_steps"": []}) ``` ```text AgentActionMessageLog(tool=\'get_word_length\', tool_input={\'word\': \'educa\'}, log=""\\nInvoking: `get_word_length` with `{\'word\': \'educa\'}`\\n\\n\\n"", message_log=[AIMessage(content=\'\', additional_kwargs={\'function_call\': {\'arguments\': \'{\\n ""word"": ""educa""\\n}\', \'name\': \'get_word_length\'}})]) ``` We can see that it responds with an `AgentAction` to take (it\'s actually an `AgentActionMessageLog` - a subclass of `AgentAction` which also tracks the full message log). If we\'ve set up LangSmith, we\'ll see a trace that let\'s us inspect the input and output to each step in the sequence: [ ### Define the runtime So this is just the first step - now we need to write a runtime for this. The simplest one is just one that continuously loops, calling the agent, then taking the action, and repeating until an `AgentFinish` is returned. Let\'s code that up below: ```python from langchain.schema.agent import AgentFinish user_input = ""how many letters in the word educa?"" intermediate_steps = [] while True: output = agent.invoke( { ""input"": user_input, ""intermediate_steps"": intermediate_steps, } ) if isinstance(output, AgentFinish): final_result = output.return_values[""output""] break else: print(f""TOOL NAME: {output.tool}"") print(f""TOOL INPUT: {output.tool_input}"") tool = {""get_word_length"": get_word_length}[output.tool] observation = tool.run(output.tool_input) intermediate_steps.append((output, observation)) print(final_result) ``` ```text TOOL NAME: get_word_length TOOL INPUT: {\'word\': \'educa\'} There are 5 letters in the word ""educa"". ``` Woo! It\'s working. ### Using AgentExecutor To simplify this a bit, we can import and use the `AgentExecutor` class. This bundles up all of the above and adds in error handling, early stopping, tracing, and other quality-of-life improvements that reduce safeguards you need to write. ```python from langchain.agents import AgentExecutor agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True) ``` Now let\'s test it out! ```python agent_executor.invoke({""input"": ""how many letters in the word educa?""}) ``` ```text > Entering new AgentExecutor chain... Invoking: `get_word_length` with `{\'word\': \'educa\'}` 5There are 5 letters in the word ""educa"". > Finished chain. {\'input\': \'how many letters in the word educa?\', \'output\': \'There are 5 letters in the word ""educa"".\'} ``` And looking at the trace, we can see that all of our agent calls and tool invocations are automatically logged: [ ### Adding memory This is great - we have an agent! However, this agent is stateless - it doesn\'t remember anything about previous interactions. This means you can\'t ask follow up questions easily. Let\'s fix that by adding in memory. In order to do this, we need to do two things: 1. Add a place for memory variables to go in the prompt 2. Keep track of the chat history First, let\'s add a place for memory in the prompt. We do this by adding a placeholder for messages with the key `""chat_history""`. Notice that we put this ABOVE the new user input (to follow the conversation flow). ```python from langchain.prompts import MessagesPlaceholder MEMORY_KEY = ""chat_history"" prompt = ChatPromptTemplate.from_messages( [ ( ""system"", ""You are very powerful assistant, but bad at calculating lengths of words."", ), MessagesPlaceholder(variable_name=MEMORY_KEY), (""user"", ""{input}""), MessagesPlaceholder(variable_name=""agent_scratchpad""), ] ) ``` We can then set up a list to track the chat history ```python from langchain.schema.messages import AIMessage, HumanMessage chat_history = [] ``` We can then put it all together! ```python agent = ( { ""input"": lambda x: x[""input""], ""agent_scratchpad"": lambda x: format_to_openai_function_messages( x[""intermediate_steps""] ), ""chat_history"": lambda x: x[""chat_history""], } | prompt | llm_with_tools | OpenAIFunctionsAgentOutputParser() ) agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True) ``` When running, we now need to track the inputs and outputs as chat history ```python input1 = ""how many letters in the word educa?"" result = agent_executor.invoke({""input"": input1, ""chat_history"": chat_history}) chat_history.extend( [ HumanMessage(content=input1), AIMessage(content=result[""output""]), ] ) agent_executor.invoke({""input"": ""is that a real word?"", ""chat_history"": chat_history}) ``` ```text > Entering new AgentExecutor chain... Invoking: `get_word_length` with `{\'word\': \'educa\'}` 5There are 5 letters in the word ""educa"". > Finished chain. > Entering new AgentExecutor chain... No, ""educa"" is not a real word in English. > Finished chain. {\'input\': \'is that a real word?\', \'chat_history\': [HumanMessage(content=\'how many letters in the word educa?\'), AIMessage(content=\'There are 5 letters in the word ""educa"".\')], \'output\': \'No, ""educa"" is not a real word in English.\'} ``` Here\'s the LangSmith trace: [ ## Next Steps Awesome! You\'ve now run your first end-to-end agent. To dive deeper, you can: - Check out all the different [agent types](/docs/modules/agents/agent_types/) supported - Learn all the controls for [AgentExecutor](/docs/modules/agents/how_to/) - Explore the how-to\'s of [tools](/docs/modules/agents/tools/) and all the [tool integrations](/docs/integrations/tools) - See a full list of all the off-the-shelf [toolkits](/docs/integrations/toolkits/) we provide - [Concepts](#concepts)- [Agent](#agent) - [Tools](#tools) - [Toolkits](#toolkits) - [AgentExecutor](#agentexecutor) - [Other types of agent runtimes](#other-types-of-agent-runtimes) - [Get started](#get-started)- [Setup: LangSmith](#setup-langsmith) - [Define the agent](#define-the-agent) - [Define the runtime](#define-the-runtime) - [Using AgentExecutor](#using-agentexecutor) - [Adding memory](#adding-memory) - [Next Steps](#next-steps)', 'Log10 | Log10 This page covers how to use the [Log10]( within LangChain. ## What is Log10? Log10 is an [open-source]( proxiless LLM data management and application development platform that lets you log, debug and tag your Langchain calls. ## Quick start 1. Create your free account at [log10.io]( 2. Add your `LOG10_TOKEN` and `LOG10_ORG_ID` from the Settings and Organization tabs respectively as environment variables. 3. Also add `LOG10_URL= and your usual LLM API key: for e.g. `OPENAI_API_KEY` or `ANTHROPIC_API_KEY` to your environment ## How to enable Log10 data management for Langchain Integration with log10 is a simple one-line `log10_callback` integration as shown below: ```python from langchain.chat_models import ChatOpenAI from langchain.schema import HumanMessage from log10.langchain import Log10Callback from log10.llm import Log10Config log10_callback = Log10Callback(log10_config=Log10Config()) messages = [ HumanMessage(content=""You are a ping pong machine""), HumanMessage(content=""Ping?""), ] llm = ChatOpenAI(model_name=""gpt-3.5-turbo"", callbacks=[log10_callback]) ``` [Log10 + Langchain + Logs docs]( [More details + screenshots]( including instructions for self-hosting logs ## How to use tags with Log10 ```python from langchain.llms import OpenAI from langchain.chat_models import ChatAnthropic from langchain.chat_models import ChatOpenAI from langchain.schema import HumanMessage from log10.langchain import Log10Callback from log10.llm import Log10Config log10_callback = Log10Callback(log10_config=Log10Config()) messages = [ HumanMessage(content=""You are a ping pong machine""), HumanMessage(content=""Ping?""), ] llm = ChatOpenAI(model_name=""gpt-3.5-turbo"", callbacks=[log10_callback], temperature=0.5, tags=[""test""]) completion = llm.predict_messages(messages, tags=[""foobar""]) print(completion) llm = ChatAnthropic(model=""claude-2"", callbacks=[log10_callback], temperature=0.7, tags=[""baz""]) llm.predict_messages(messages) print(completion) llm = OpenAI(model_name=""text-davinci-003"", callbacks=[log10_callback], temperature=0.5) completion = llm.predict(""You are a ping pong machine.\\nPing?\\n"") print(completion) ``` You can also intermix direct OpenAI calls and Langchain LLM calls: ```python import os from log10.load import log10, log10_session import openai from langchain.llms import OpenAI log10(openai) with log10_session(tags=[""foo"", ""bar""]): # Log a direct OpenAI call response = openai.Completion.create( model=""text-ada-001"", prompt=""Where is the Eiffel Tower?"", temperature=0, max_tokens=1024, top_p=1, frequency_penalty=0, presence_penalty=0, ) print(response) # Log a call via Langchain llm = OpenAI(model_name=""text-ada-001"", temperature=0.5) response = llm.predict(""You are a ping pong machine.\\nPing?\\n"") print(response) ``` ## How to debug Langchain calls [Example of debugging]( [More Langchain examples]( - [What is Log10?](#what-is-log10) - [Quick start](#quick-start) - [How to enable Log10 data management for Langchain](#how-to-enable-log10-data-management-for-langchain) - [How to use tags with Log10](#how-to-use-tags-with-log10) - [How to debug Langchain calls](#how-to-debug-langchain-calls)']","To run a `Runnable` in LangChain, you typically invoke it with an input dictionary. Here's a simple example:

```python
from langchain.schema.runnable import Runnable

# Define a simple runnable
def my_runnable(input_data):
    return f""Processed: {input_data}""

# Create a runnable instance
runnable = Runnable(my_runnable)

# Run the runnable with some input
output = runnable.invoke({""input_data"": ""example""})
print(output)  # Output: Processed: example
```

In this example, `my_runnable` is a function that takes an input dictionary and returns a processed string. The `Runnable` wraps this function and allows it to be invoked with an input.","Use invoke(), stream(), batch(), or their async equivalents (ainvoke, astream, and abatch)",0.49999999995,0.0,,0.01698797377536768,0.03669724770642202
16,how do I run gpt-4 on anthropic?,"['Dynamically route logic based on input | Dynamically route logic based on input This notebook covers how to do routing in the LangChain Expression Language. Routing allows you to create non-deterministic chains where the output of a previous step defines the next step. Routing helps provide structure and consistency around interactions with LLMs. There are two ways to perform routing: 1. Using a `RunnableBranch`. 2. Writing custom factory function that takes the input of a previous step and returns a **runnable**. Importantly, this should return a **runnable** and NOT actually execute. We\'ll illustrate both methods using a two step sequence where the first step classifies an input question as being about `LangChain`, `Anthropic`, or `Other`, then routes to a corresponding prompt chain. ## Using a RunnableBranch A `RunnableBranch` is initialized with a list of (condition, runnable) pairs and a default runnable. It selects which branch by passing each condition the input it\'s invoked with. It selects the first condition to evaluate to True, and runs the corresponding runnable to that condition with the input. If no provided conditions match, it runs the default runnable. Here\'s an example of what it looks like in action: ```python from langchain.chat_models import ChatAnthropic from langchain.prompts import PromptTemplate from langchain.schema.output_parser import StrOutputParser ``` First, let\'s create a chain that will identify incoming questions as being about `LangChain`, `Anthropic`, or `Other`: ```python chain = ( PromptTemplate.from_template( """"""Given the user question below, classify it as either being about `LangChain`, `Anthropic`, or `Other`. Do not respond with more than one word. {question} Classification:"""""" ) | ChatAnthropic() | StrOutputParser() ) ``` ```python chain.invoke({""question"": ""how do I call Anthropic?""}) ``` ```text \' Anthropic\' ``` Now, let\'s create three sub chains: ```python langchain_chain = ( PromptTemplate.from_template( """"""You are an expert in langchain. \\ Always answer questions starting with ""As Harrison Chase told me"". \\ Respond to the following question: Question: {question} Answer:"""""" ) | ChatAnthropic() ) anthropic_chain = ( PromptTemplate.from_template( """"""You are an expert in anthropic. \\ Always answer questions starting with ""As Dario Amodei told me"". \\ Respond to the following question: Question: {question} Answer:"""""" ) | ChatAnthropic() ) general_chain = ( PromptTemplate.from_template( """"""Respond to the following question: Question: {question} Answer:"""""" ) | ChatAnthropic() ) ``` ```python from langchain.schema.runnable import RunnableBranch branch = RunnableBranch( (lambda x: ""anthropic"" in x[""topic""].lower(), anthropic_chain), (lambda x: ""langchain"" in x[""topic""].lower(), langchain_chain), general_chain, ) ``` ```python full_chain = {""topic"": chain, ""question"": lambda x: x[""question""]} | branch ``` ```python full_chain.invoke({""question"": ""how do I use Anthropic?""}) ``` ```text AIMessage(content="" As Dario Amodei told me, here are some ways to use Anthropic:\\n\\n- Sign up for an account on Anthropic\'s website to access tools like Claude, Constitutional AI, and Writer. \\n\\n- Use Claude for tasks like email generation, customer service chat, and QA. Claude can understand natural language prompts and provide helpful responses.\\n\\n- Use Constitutional AI if you need an AI assistant that is harmless, honest, and helpful. It is designed to be safe and aligned with human values.\\n\\n- Use Writer to generate natural language content for things like marketing copy, stories, reports, and more. Give it a topic and prompt and it will create high-quality written content.\\n\\n- Check out Anthropic\'s documentation and blog for tips, tutorials, examples, and announcements about new capabilities as they continue to develop their AI technology.\\n\\n- Follow Anthropic on social media or subscribe to their newsletter to stay up to date on new features and releases.\\n\\n- For most people, the easiest way to leverage Anthropic\'s technology is through their website - just create an account to get started!"", additional_kwargs={}, example=False) ``` ```python full_chain.invoke({""question"": ""how do I use LangChain?""}) ``` ```text AIMessage(content=\' As Harrison Chase told me, here is how you use LangChain:\\n\\nLangChain is an AI assistant that can have conversations, answer questions, and generate text. To use LangChain, you simply type or speak your input and LangChain will respond. \\n\\nYou can ask LangChain questions, have discussions, get summaries or explanations about topics, and request it to generate text on a subject. Some examples of interactions:\\n\\n- Ask general knowledge questions and LangChain will try to answer factually. For example ""What is the capital of France?""\\n\\n- Have conversations on topics by taking turns speaking. You can prompt the start of a conversation by saying something like ""Let\\\'s discuss machine learning""\\n\\n- Ask for summaries or high-level explanations on subjects. For example ""Can you summarize the main themes in Shakespeare\\\'s Hamlet?"" \\n\\n- Give creative writing prompts or requests to have LangChain generate text in different styles. For example ""Write a short children\\\'s story about a mouse"" or ""Generate a poem in the style of Robert Frost about nature""\\n\\n- Correct LangChain if it makes an inaccurate statement and provide the right information. This helps train it.\\n\\nThe key is interacting naturally and giving it clear prompts and requests\', additional_kwargs={}, example=False) ``` ```python full_chain.invoke({""question"": ""whats 2 + 2""}) ``` ```text AIMessage(content=\' 2 + 2 = 4\', additional_kwargs={}, example=False) ``` ## Using a custom function You can also use a custom function to route between different outputs. Here\'s an example: ```python def route(info): if ""anthropic"" in info[""topic""].lower(): return anthropic_chain elif ""langchain"" in info[""topic""].lower(): return langchain_chain else: return general_chain ``` ```python from langchain.schema.runnable import RunnableLambda full_chain = {""topic"": chain, ""question"": lambda x: x[""question""]} | RunnableLambda( route ) ``` ```python full_chain.invoke({""question"": ""how do I use Anthroipc?""}) ``` ```text AIMessage(content=\' As Dario Amodei told me, to use Anthropic IPC you first need to import it:\\n\\n```python\\nfrom anthroipc import ic\\n```\\n\\nThen you can create a client and connect to the server:\\n\\n```python \\nclient = ic.connect()\\n```\\n\\nAfter that, you can call methods on the client and get responses:\\n\\n```python\\nresponse = client.ask(""What is the meaning of life?"")\\nprint(response)\\n```\\n\\nYou can also register callbacks to handle events: \\n\\n```python\\ndef on_poke(event):\\n print(""Got poked!"")\\n\\nclient.on(\\\'poke\\\', on_poke)\\n```\\n\\nAnd that\\\'s the basics of using the Anthropic IPC client library for Python! Let me know if you have any other questions!\', additional_kwargs={}, example=False) ``` ```python full_chain.invoke({""question"": ""how do I use LangChain?""}) ``` ```text AIMessage(content=\' As Harrison Chase told me, to use LangChain you first need to sign up for an API key at platform.langchain.com. Once you have your API key, you can install the Python library and write a simple Python script to call the LangChain API. Here is some sample code to get started:\\n\\n```python\\nimport langchain\\n\\napi_key = ""YOUR_API_KEY""\\n\\nlangchain.set_key(api_key)\\n\\nresponse = langchain.ask(""What is the capital of France?"")\\n\\nprint(response.response)\\n```\\n\\nThis will send the question ""What is the capital of France?"" to the LangChain API and print the response. You can customize the request by providing parameters like max_tokens, temperature, etc. The LangChain Python library documentation has more details on the available options. The key things are getting an API key and calling langchain.ask() with your question text. Let me know if you have any other questions!\', additional_kwargs={}, example=False) ``` ```python full_chain.invoke({""question"": ""whats 2 + 2""}) ``` ```text AIMessage(content=\' 4\', additional_kwargs={}, example=False) ``` - [Using a RunnableBranch](#using-a-runnablebranch) - [Using a custom function](#using-a-custom-function)', 'CSV | CSV This notebook shows how to use agents to interact with data in `CSV` format. It is mostly optimized for question answering. **NOTE: this agent calls the Pandas DataFrame agent under the hood, which in turn calls the Python agent, which executes LLM generated Python code - this can be bad if the LLM generated Python code is harmful. Use cautiously.** ```python from langchain.agents.agent_types import AgentType from langchain.chat_models import ChatOpenAI from langchain.llms import OpenAI from langchain_experimental.agents.agent_toolkits import create_csv_agent ``` ## Using ZERO_SHOT_REACT_DESCRIPTION This shows how to initialize the agent using the `ZERO_SHOT_REACT_DESCRIPTION` agent type. ```python agent = create_csv_agent( OpenAI(temperature=0), ""titanic.csv"", verbose=True, agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION, ) ``` ## Using OpenAI Functions This shows how to initialize the agent using the OPENAI_FUNCTIONS agent type. Note that this is an alternative to the above. ```python agent = create_csv_agent( ChatOpenAI(temperature=0, model=""gpt-3.5-turbo-0613""), ""titanic.csv"", verbose=True, agent_type=AgentType.OPENAI_FUNCTIONS, ) ``` ```python agent.run(""how many rows are there?"") ``` ```text Error in on_chain_start callback: \'name\' Invoking: `python_repl_ast` with `df.shape[0]` 891There are 891 rows in the dataframe. > Finished chain. \'There are 891 rows in the dataframe.\' ``` ```python agent.run(""how many people have more than 3 siblings"") ``` ```text Error in on_chain_start callback: \'name\' Invoking: `python_repl_ast` with `df[df[\'SibSp\'] > 3][\'PassengerId\'].count()` 30There are 30 people in the dataframe who have more than 3 siblings. > Finished chain. \'There are 30 people in the dataframe who have more than 3 siblings.\' ``` ```python agent.run(""whats the square root of the average age?"") ``` ```text Error in on_chain_start callback: \'name\' Invoking: `python_repl_ast` with `import pandas as pd import math # Create a dataframe data = {\'Age\': [22, 38, 26, 35, 35]} df = pd.DataFrame(data) # Calculate the average age average_age = df[\'Age\'].mean() # Calculate the square root of the average age square_root = math.sqrt(average_age) square_root` 5.585696017507576The square root of the average age is approximately 5.59. > Finished chain. \'The square root of the average age is approximately 5.59.\' ``` ### Multi CSV Example This next part shows how the agent can interact with multiple csv files passed in as a list. ```python agent = create_csv_agent( ChatOpenAI(temperature=0, model=""gpt-3.5-turbo-0613""), [""titanic.csv"", ""titanic_age_fillna.csv""], verbose=True, agent_type=AgentType.OPENAI_FUNCTIONS, ) agent.run(""how many rows in the age column are different between the two dfs?"") ``` ```text Error in on_chain_start callback: \'name\' Invoking: `python_repl_ast` with `df1[\'Age\'].nunique() - df2[\'Age\'].nunique()` -1There is 1 row in the age column that is different between the two dataframes. > Finished chain. \'There is 1 row in the age column that is different between the two dataframes.\' ``` - [Using ZERO_SHOT_REACT_DESCRIPTION](#using-zero_shot_react_description) - [Using OpenAI Functions](#using-openai-functions)- [Multi CSV Example](#multi-csv-example)', 'Fallbacks | Fallbacks When working with language models, you may often encounter issues from the underlying APIs, whether these be rate limiting or downtime. Therefore, as you go to move your LLM applications into production it becomes more and more important to safeguard against these. That\'s why we\'ve introduced the concept of fallbacks. A **fallback** is an alternative plan that may be used in an emergency. Crucially, fallbacks can be applied not only on the LLM level but on the whole runnable level. This is important because often times different models require different prompts. So if your call to OpenAI fails, you don\'t just want to send the same prompt to Anthropic - you probably want to use a different prompt template and send a different version there. ## Fallback for LLM API Errors This is maybe the most common use case for fallbacks. A request to an LLM API can fail for a variety of reasons - the API could be down, you could have hit rate limits, any number of things. Therefore, using fallbacks can help protect against these types of things. IMPORTANT: By default, a lot of the LLM wrappers catch errors and retry. You will most likely want to turn those off when working with fallbacks. Otherwise the first wrapper will keep on retrying and not failing. ```python from langchain.chat_models import ChatAnthropic, ChatOpenAI ``` First, let\'s mock out what happens if we hit a RateLimitError from OpenAI ```python from unittest.mock import patch from openai.error import RateLimitError ``` ```python # Note that we set max_retries = 0 to avoid retrying on RateLimits, etc openai_llm = ChatOpenAI(max_retries=0) anthropic_llm = ChatAnthropic() llm = openai_llm.with_fallbacks([anthropic_llm]) ``` ```python # Let\'s use just the OpenAI LLm first, to show that we run into an error with patch(""openai.ChatCompletion.create"", side_effect=RateLimitError()): try: print(openai_llm.invoke(""Why did the chicken cross the road?"")) except: print(""Hit error"") ``` ```text Hit error ``` ```python # Now let\'s try with fallbacks to Anthropic with patch(""openai.ChatCompletion.create"", side_effect=RateLimitError()): try: print(llm.invoke(""Why did the chicken cross the road?"")) except: print(""Hit error"") ``` ```text content=\' I don\\\'t actually know why the chicken crossed the road, but here are some possible humorous answers:\\n\\n- To get to the other side!\\n\\n- It was too chicken to just stand there. \\n\\n- It wanted a change of scenery.\\n\\n- It wanted to show the possum it could be done.\\n\\n- It was on its way to a poultry farmers\\\' convention.\\n\\nThe joke plays on the double meaning of ""the other side"" - literally crossing the road to the other side, or the ""other side"" meaning the afterlife. So it\\\'s an anti-joke, with a silly or unexpected pun as the answer.\' additional_kwargs={} example=False ``` We can use our ""LLM with Fallbacks"" as we would a normal LLM. ```python from langchain.prompts import ChatPromptTemplate prompt = ChatPromptTemplate.from_messages( [ ( ""system"", ""You\'re a nice assistant who always includes a compliment in your response"", ), (""human"", ""Why did the {animal} cross the road""), ] ) chain = prompt | llm with patch(""openai.ChatCompletion.create"", side_effect=RateLimitError()): try: print(chain.invoke({""animal"": ""kangaroo""})) except: print(""Hit error"") ``` ```text content="" I don\'t actually know why the kangaroo crossed the road, but I can take a guess! Here are some possible reasons:\\n\\n- To get to the other side (the classic joke answer!)\\n\\n- It was trying to find some food or water \\n\\n- It was trying to find a mate during mating season\\n\\n- It was fleeing from a predator or perceived threat\\n\\n- It was disoriented and crossed accidentally \\n\\n- It was following a herd of other kangaroos who were crossing\\n\\n- It wanted a change of scenery or environment \\n\\n- It was trying to reach a new habitat or territory\\n\\nThe real reason is unknown without more context, but hopefully one of those potential explanations does the joke justice! Let me know if you have any other animal jokes I can try to decipher."" additional_kwargs={} example=False ``` ## Fallback for Sequences We can also create fallbacks for sequences, that are sequences themselves. Here we do that with two different models: ChatOpenAI and then normal OpenAI (which does not use a chat model). Because OpenAI is NOT a chat model, you likely want a different prompt. ```python # First let\'s create a chain with a ChatModel # We add in a string output parser here so the outputs between the two are the same type from langchain.schema.output_parser import StrOutputParser chat_prompt = ChatPromptTemplate.from_messages( [ ( ""system"", ""You\'re a nice assistant who always includes a compliment in your response"", ), (""human"", ""Why did the {animal} cross the road""), ] ) # Here we\'re going to use a bad model name to easily create a chain that will error chat_model = ChatOpenAI(model_name=""gpt-fake"") bad_chain = chat_prompt | chat_model | StrOutputParser() ``` ```python # Now lets create a chain with the normal OpenAI model from langchain.llms import OpenAI from langchain.prompts import PromptTemplate prompt_template = """"""Instructions: You should always include a compliment in your response. Question: Why did the {animal} cross the road?"""""" prompt = PromptTemplate.from_template(prompt_template) llm = OpenAI() good_chain = prompt | llm ``` ```python # We can now create a final chain which combines the two chain = bad_chain.with_fallbacks([good_chain]) chain.invoke({""animal"": ""turtle""}) ``` ```text \'\\n\\nAnswer: The turtle crossed the road to get to the other side, and I have to say he had some impressive determination.\' ``` ## Fallback for Long Inputs One of the big limiting factors of LLMs is their context window. Usually, you can count and track the length of prompts before sending them to an LLM, but in situations where that is hard/complicated, you can fallback to a model with a longer context length. ```python short_llm = ChatOpenAI() long_llm = ChatOpenAI(model=""gpt-3.5-turbo-16k"") llm = short_llm.with_fallbacks([long_llm]) ``` ```python inputs = ""What is the next number: "" + "", "".join([""one"", ""two""] * 3000) ``` ```python try: print(short_llm.invoke(inputs)) except Exception as e: print(e) ``` ```text This model\'s maximum context length is 4097 tokens. However, your messages resulted in 12012 tokens. Please reduce the length of the messages. ``` ```python try: print(llm.invoke(inputs)) except Exception as e: print(e) ``` ```text content=\'The next number in the sequence is two.\' additional_kwargs={} example=False ``` ## Fallback to Better Model Often times we ask models to output format in a specific format (like JSON). Models like GPT-3.5 can do this okay, but sometimes struggle. This naturally points to fallbacks - we can try with GPT-3.5 (faster, cheaper), but then if parsing fails we can use GPT-4. ```python from langchain.output_parsers import DatetimeOutputParser ``` ```python prompt = ChatPromptTemplate.from_template( ""what time was {event} (in %Y-%m-%dT%H:%M:%S.%fZ format - only return this value)"" ) ``` ```python # In this case we are going to do the fallbacks on the LLM + output parser level # Because the error will get raised in the OutputParser openai_35 = ChatOpenAI() | DatetimeOutputParser() openai_4 = ChatOpenAI(model=""gpt-4"") | DatetimeOutputParser() ``` ```python only_35 = prompt | openai_35 fallback_4 = prompt | openai_35.with_fallbacks([openai_4]) ``` ```python try: print(only_35.invoke({""event"": ""the superbowl in 1994""})) except Exception as e: print(f""Error: {e}"") ``` ```text Error: Could not parse datetime string: The Super Bowl in 1994 took place on January 30th at 3:30 PM local time. Converting this to the specified format (%Y-%m-%dT%H:%M:%S.%fZ) results in: 1994-01-30T15:30:00.000Z ``` ```python try: print(fallback_4.invoke({""event"": ""the superbowl in 1994""})) except Exception as e: print(f""Error: {e}"") ``` ```text 1994-01-30 15:30:00 ``` - [Fallback for LLM API Errors](#fallback-for-llm-api-errors) - [Fallback for Sequences](#fallback-for-sequences) - [Fallback for Long Inputs](#fallback-for-long-inputs) - [Fallback to Better Model](#fallback-to-better-model)']","To run gpt-4 on Anthropic, you'll need to use Anthropic's API. Here's a simplified example of how you might do it:

1. **Install Anthropic's Python SDK**:
   ```bash
   pip install anthropic
   ```

2. **Initialize the Anthropic client**:
   ```python
   from anthropic import Anthropic

   client = Anthropic(
      api_key=""YOUR_API_KEY""
   )
   ```

3. **Send a request to the gpt-4 model**:
   ```python
   response = client.complete(
      prompt=""Your input text here"",
      max_tokens_to_sample=4096,  # Adjust based on the model's capabilities
      model=""gpt-4""  # Specify the model
   )

   print(response.completion)
   ```

Replace `""YOUR_API_KEY""` with your actual API key and `""Your input text here""` with the text you want to process. The `max_tokens_to_sample` parameter can be adjusted based on the model's capabilities.",GPT-4 is a model trained by OpenAI and not provided by Anthropic.,0.0,0.0,0.0,0.031157290929555895,0.07142857142857144
17,who to email for filing cves,"[""Retrievers | Retrievers [ Activeloop Deep MemoryActiveloop Deep Memory is a suite of tools that enables you to optimize your Vector Store for your use-case and achieve higher accuracy in your LLM apps.](/docs/integrations/retrievers/Activeloop DeepMemory+LangChain)[ Amazon KendraAmazon Kendra is an intelligent search service provided by Amazon Web Services (AWS). It utilizes advanced natural language processing (NLP) and machine learning algorithms to enable powerful search capabilities across various data sources within an organization. Kendra is designed to help users find the information they need quickly and accurately, improving productivity and decision-making.](/docs/integrations/retrievers/amazon_kendra_retriever)[ Arcee RetrieverThis notebook demonstrates how to use the ArceeRetriever class to retrieve relevant document(s) for Arcee's Domain Adapted Language Models (DALMs).](/docs/integrations/retrievers/arcee)[ ArxivarXiv is an open-access archive for 2 million scholarly articles in the fields of physics, mathematics, computer science, quantitative biology, quantitative finance, statistics, electrical engineering and systems science, and economics.](/docs/integrations/retrievers/arxiv)[ Azure Cognitive SearchAzure Cognitive Search (formerly known as Azure Search) is a cloud search service that gives developers infrastructure, APIs, and tools for building a rich search experience over private, heterogeneous content in web, mobile, and enterprise applications.](/docs/integrations/retrievers/azure_cognitive_search)[ BM25BM25 also known as the Okapi BM25, is a ranking function used in information retrieval systems to estimate the relevance of documents to a given search query.](/docs/integrations/retrievers/bm25)[ ChaindeskChaindesk platform brings data from anywhere (Datsources: Text, PDF, Word, PowerPpoint, Excel, Notion, Airtable, Google Sheets, etc..) into Datastores (container of multiple Datasources).](/docs/integrations/retrievers/chaindesk)[ ChatGPT PluginOpenAI plugins connect ChatGPT to third-party applications. These plugins enable ChatGPT to interact with APIs defined by developers, enhancing ChatGPT's capabilities and allowing it to perform a wide range of actions.](/docs/integrations/retrievers/chatgpt-plugin)[ Cohere RerankerCohere is a Canadian startup that provides natural language processing models that help companies improve human-machine interactions.](/docs/integrations/retrievers/cohere-reranker)[ Cohere RAG retrieverThis notebook covers how to get started with Cohere RAG retriever. This allows you to leverage the ability to search documents over various connectors or by supplying your own.](/docs/integrations/retrievers/cohere)[ DocArrayDocArray is a versatile, open-source tool for managing your multi-modal data. It lets you shape your data however you want, and offers the flexibility to store and search it using various document index backends. Plus, it gets even better - you can utilize your DocArray document index to create a DocArrayRetriever, and build awesome Langchain apps!](/docs/integrations/retrievers/docarray_retriever)[ ElasticSearch BM25Elasticsearch is a distributed, RESTful search and analytics engine. It provides a distributed, multitenant-capable full-text search engine with an HTTP web interface and schema-free JSON documents.](/docs/integrations/retrievers/elastic_search_bm25)[ Fleet AI Libraries ContextThe Fleet AI team is on a mission to embed the world's most important data. They've started by embedding the top 1200 Python libraries to enable code generation with up-to-date knowledge. They've been kind enough to share their embeddings of the LangChain docs and API reference.](/docs/integrations/retrievers/fleet_context)[ Google DriveThis notebook covers how to retrieve documents from Google Drive.](/docs/integrations/retrievers/google_drive)[ Google Vertex AI SearchVertex AI Search (formerly known as Enterprise Search on Generative AI App Builder) is a part of the Vertex AI machine learning platform offered by Google Cloud.](/docs/integrations/retrievers/google_vertex_ai_search)[ Kay.aiData API built for RAG We are curating the world's largest datasets as high-quality embeddings so your AI agents can retrieve context on the fly. Latest models, fast retrieval, and zero infra.](/docs/integrations/retrievers/kay)[ kNNIn statistics, the k-nearest neighbors algorithm (k-NN) is a non-parametric supervised learning method first developed by Evelyn Fix and Joseph Hodges in 1951, and later expanded by Thomas Cover. It is used for classification and regression.](/docs/integrations/retrievers/knn)[ LOTR (Merger Retriever)Lord of the Retrievers, also known as MergerRetriever, takes a list of retrievers as input and merges the results of their getrelevantdocuments() methods into a single list. The merged results will be a list of documents that are relevant to the query and that have been ranked by the different retrievers.](/docs/integrations/retrievers/merger_retriever)[ MetalMetal is a managed service for ML Embeddings.](/docs/integrations/retrievers/metal)[ Pinecone Hybrid SearchPinecone is a vector database with broad functionality.](/docs/integrations/retrievers/pinecone_hybrid_search)[ PubMedPubMed by The National Center for Biotechnology Information, National Library of Medicine comprises more than 35 million citations for biomedical literature from MEDLINE, life science journals, and online books. Citations may include links to full text content from PubMed Central and publisher web sites.](/docs/integrations/retrievers/pubmed)[ RePhraseQueryRePhraseQuery is a simple retriever that applies an LLM between the user input and the query passed by the retriever.](/docs/integrations/retrievers/re_phrase)[ SEC filingThe SEC filing is a financial statement or other formal document submitted to the U.S. Securities and Exchange Commission (SEC). Public companies, certain insiders, and broker-dealers are required to make regular SEC filings. Investors and financial professionals rely on these filings for information about companies they are evaluating for investment purposes.](/docs/integrations/retrievers/sec_filings)[ Self-querying retriever14 items](/docs/integrations/retrievers/self_query/)[ SingleStoreDBSingleStoreDB is a high-performance distributed SQL database that supports deployment both in the cloud and on-premises. It provides vector storage, and vector functions including dotproduct and euclideandistance, thereby supporting AI applications that require text similarity matching.](/docs/integrations/retrievers/singlestoredb)[ SVMSupport vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.](/docs/integrations/retrievers/svm)[ Tavily Search APITavily's Search API is a search engine built specifically for AI agents (LLMs), delivering real-time, accurate, and factual results at speed.](/docs/integrations/retrievers/tavily)[ TF-IDFTF-IDF means term-frequency times inverse document-frequency.](/docs/integrations/retrievers/tf_idf)[ VespaVespa is a fully featured search engine and vector database. It supports vector search (ANN), lexical search, and search in structured data, all in the same query.](/docs/integrations/retrievers/vespa)[ Weaviate Hybrid SearchWeaviate is an open-source vector database.](/docs/integrations/retrievers/weaviate-hybrid)[ WikipediaWikipedia is a multilingual free online encyclopedia written and maintained by a community of volunteers, known as Wikipedians, through open collaboration and using a wiki-based editing system called MediaWiki. Wikipedia is the largest and most-read reference work in history.](/docs/integrations/retrievers/wikipedia)[ you-retrieverUsing the You.com Retriever](/docs/integrations/retrievers/you-retriever)[ ZepRetriever Example for Zep](/docs/integrations/retrievers/zep_memorystore)"", 'SEC filing | SEC filing The SEC filing is a financial statement or other formal document submitted to the U.S. Securities and Exchange Commission (SEC). Public companies, certain insiders, and broker-dealers are required to make regular SEC filings. Investors and financial professionals rely on these filings for information about companies they are evaluating for investment purposes. SEC filings data powered by [Kay.ai]( and [Cybersyn]( via [Snowflake Marketplace]( ## Setup First, you will need to install the `kay` package. You will also need an API key: you can get one for free at [ Once you have an API key, you must set it as an environment variable `KAY_API_KEY`. In this example, we\'re going to use the `KayAiRetriever`. Take a look at the [kay notebook](/docs/integrations/retrievers/kay) for more detailed information for the parameters that it accepts.` ```python # Setup API keys for Kay and OpenAI from getpass import getpass KAY_API_KEY = getpass() OPENAI_API_KEY = getpass() ``` ```text ``` ```python import os os.environ[""KAY_API_KEY""] = KAY_API_KEY os.environ[""OPENAI_API_KEY""] = OPENAI_API_KEY ``` ## Example ```python from langchain.chains import ConversationalRetrievalChain from langchain.chat_models import ChatOpenAI from langchain.retrievers import KayAiRetriever model = ChatOpenAI(model_name=""gpt-3.5-turbo"") retriever = KayAiRetriever.create( dataset_id=""company"", data_types=[""10-K"", ""10-Q""], num_contexts=6 ) qa = ConversationalRetrievalChain.from_llm(model, retriever=retriever) ``` ```python questions = [ ""What are patterns in Nvidia\'s spend over the past three quarters?"", # ""What are some recent challenges faced by the renewable energy sector?"", ] chat_history = [] for question in questions: result = qa({""question"": question, ""chat_history"": chat_history}) chat_history.append((question, result[""answer""])) print(f""-> **Question**: {question} \\n"") print(f""**Answer**: {result[\'answer\']} \\n"") ``` ```text -> **Question**: What are patterns in Nvidia\'s spend over the past three quarters? **Answer**: Based on the provided information, here are the patterns in NVIDIA\'s spend over the past three quarters: 1. Research and Development Expenses: - Q3 2022: Increased by 34% compared to Q3 2021. - Q1 2023: Increased by 40% compared to Q1 2022. - Q2 2022: Increased by 25% compared to Q2 2021. Overall, research and development expenses have been consistently increasing over the past three quarters. 2. Sales, General and Administrative Expenses: - Q3 2022: Increased by 8% compared to Q3 2021. - Q1 2023: Increased by 14% compared to Q1 2022. - Q2 2022: Decreased by 16% compared to Q2 2021. The pattern for sales, general and administrative expenses is not as consistent, with some quarters showing an increase and others showing a decrease. 3. Total Operating Expenses: - Q3 2022: Increased by 25% compared to Q3 2021. - Q1 2023: Increased by 113% compared to Q1 2022. - Q2 2022: Increased by 9% compared to Q2 2021. Total operating expenses have generally been increasing over the past three quarters, with a significant increase in Q1 2023. Overall, the pattern indicates a consistent increase in research and development expenses and total operating expenses, while sales, general and administrative expenses show some fluctuations. ``` - [Setup](#setup) - [Example](#example)', 'Vespa | Vespa [Vespa]( is a fully featured search engine and vector database. It supports vector search (ANN), lexical search, and search in structured data, all in the same query. This notebook shows how to use `Vespa.ai` as a LangChain vector store. In order to create the vector store, we use [pyvespa]( to create a connection a `Vespa` service. ```python #!pip install pyvespa ``` Using the `pyvespa` package, you can either connect to a [Vespa Cloud instance]( or a local [Docker instance]( Here, we will create a new Vespa application and deploy that using Docker. #### Creating a Vespa application First, we need to create an application package: ```python from vespa.package import ApplicationPackage, Field, RankProfile app_package = ApplicationPackage(name=""testapp"") app_package.schema.add_fields( Field( name=""text"", type=""string"", indexing=[""index"", ""summary""], index=""enable-bm25"" ), Field( name=""embedding"", type=""tensor(x[384])"", indexing=[""attribute"", ""summary""], attribute=[""distance-metric: angular""], ), ) app_package.schema.add_rank_profile( RankProfile( name=""default"", first_phase=""closeness(field, embedding)"", inputs=[(""query(query_embedding)"", ""tensor(x[384])"")], ) ) ``` This sets up a Vespa application with a schema for each document that contains two fields: `text` for holding the document text and `embedding` for holding the embedding vector. The `text` field is set up to use a BM25 index for efficient text retrieval, and we\'ll see how to use this and hybrid search a bit later. The `embedding` field is set up with a vector of length 384 to hold the embedding representation of the text. See [Vespa\'s Tensor Guide]( for more on tensors in Vespa. Lastly, we add a [rank profile]( to instruct Vespa how to order documents. Here we set this up with a [nearest neighbor search]( Now we can deploy this application locally: ```python from vespa.deployment import VespaDocker vespa_docker = VespaDocker() vespa_app = vespa_docker.deploy(application_package=app_package) ``` This deploys and creates a connection to a `Vespa` service. In case you already have a Vespa application running, for instance in the cloud, please refer to the PyVespa application for how to connect. #### Creating a Vespa vector store Now, let\'s load some documents: ```python from langchain.document_loaders import TextLoader from langchain.text_splitter import CharacterTextSplitter loader = TextLoader(""../../modules/state_of_the_union.txt"") documents = loader.load() text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0) docs = text_splitter.split_documents(documents) from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings embedding_function = SentenceTransformerEmbeddings(model_name=""all-MiniLM-L6-v2"") ``` Here, we also set up local sentence embedder to transform the text to embedding vectors. One could also use OpenAI embeddings, but the vector length needs to be updated to `1536` to reflect the larger size of that embedding. To feed these to Vespa, we need to configure how the vector store should map to fields in the Vespa application. Then we create the vector store directly from this set of documents: ```python vespa_config = dict( page_content_field=""text"", embedding_field=""embedding"", input_field=""query_embedding"", ) from langchain.vectorstores import VespaStore db = VespaStore.from_documents(docs, embedding_function, app=vespa_app, **vespa_config) ``` This creates a Vespa vector store and feeds that set of documents to Vespa. The vector store takes care of calling the embedding function for each document and inserts them into the database. We can now query the vector store: ```python query = ""What did the president say about Ketanji Brown Jackson"" results = db.similarity_search(query) print(results[0].page_content) ``` This will use the embedding function given above to create a representation for the query and use that to search Vespa. Note that this will use the `default` ranking function, which we set up in the application package above. You can use the `ranking` argument to `similarity_search` to specify which ranking function to use. Please refer to the [pyvespa documentation]( for more information. This covers the basic usage of the Vespa store in LangChain. Now you can return the results and continue using these in LangChain. #### Updating documents An alternative to calling `from_documents`, you can create the vector store directly and call `add_texts` from that. This can also be used to update documents: ```python query = ""What did the president say about Ketanji Brown Jackson"" results = db.similarity_search(query) result = results[0] result.page_content = ""UPDATED: "" + result.page_content db.add_texts([result.page_content], [result.metadata], result.metadata[""id""]) results = db.similarity_search(query) print(results[0].page_content) ``` However, the `pyvespa` library contains methods to manipulate content on Vespa which you can use directly. #### Deleting documents You can delete documents using the `delete` function: ```python result = db.similarity_search(query) # docs[0].metadata[""id""] == ""id:testapp:testapp::32"" db.delete([""32""]) result = db.similarity_search(query) # docs[0].metadata[""id""] != ""id:testapp:testapp::32"" ``` Again, the `pyvespa` connection contains methods to delete documents as well. ### Returning with scores The `similarity_search` method only returns the documents in order of relevancy. To retrieve the actual scores: ```python results = db.similarity_search_with_score(query) result = results[0] # result[1] ~= 0.463 ``` This is a result of using the `""all-MiniLM-L6-v2""` embedding model using the cosine distance function (as given by the argument `angular` in the application function). Different embedding functions need different distance functions, and Vespa needs to know which distance function to use when orderings documents. Please refer to the [documentation on distance functions]( for more information. ### As retriever To use this vector store as a [LangChain retriever]( simply call the `as_retriever` function, which is a standard vector store method: ```python db = VespaStore.from_documents(docs, embedding_function, app=vespa_app, **vespa_config) retriever = db.as_retriever() query = ""What did the president say about Ketanji Brown Jackson"" results = retriever.get_relevant_documents(query) # results[0].metadata[""id""] == ""id:testapp:testapp::32"" ``` This allows for more general, unstructured, retrieval from the vector store. ### Metadata In the example so far, we\'ve only used the text and the embedding for that text. Documents usually contain additional information, which in LangChain is referred to as metadata. Vespa can contain many fields with different types by adding them to the application package: ```python app_package.schema.add_fields( # ... Field(name=""date"", type=""string"", indexing=[""attribute"", ""summary""]), Field(name=""rating"", type=""int"", indexing=[""attribute"", ""summary""]), Field(name=""author"", type=""string"", indexing=[""attribute"", ""summary""]), # ... ) vespa_app = vespa_docker.deploy(application_package=app_package) ``` We can add some metadata fields in the documents: ```python # Add metadata for i, doc in enumerate(docs): doc.metadata[""date""] = f""2023-{(i % 12)+1}-{(i % 28)+1}"" doc.metadata[""rating""] = range(1, 6)[i % 5] doc.metadata[""author""] = [""Joe Biden"", ""Unknown""][min(i, 1)] ``` And let the Vespa vector store know about these fields: ```python vespa_config.update(dict(metadata_fields=[""date"", ""rating"", ""author""])) ``` Now, when searching for these documents, these fields will be returned. Also, these fields can be filtered on: ```python db = VespaStore.from_documents(docs, embedding_function, app=vespa_app, **vespa_config) query = ""What did the president say about Ketanji Brown Jackson"" results = db.similarity_search(query, filter=""rating > 3"") # results[0].metadata[""id""] == ""id:testapp:testapp::34"" # results[0].metadata[""author""] == ""Unknown"" ``` ### Custom query If the default behavior of the similarity search does not fit your requirements, you can always provide your own query. Thus, you don\'t need to provide all of the configuration to the vector store, but rather just write this yourself. First, let\'s add a BM25 ranking function to our application: ```python from vespa.package import FieldSet app_package.schema.add_field_set(FieldSet(name=""default"", fields=[""text""])) app_package.schema.add_rank_profile(RankProfile(name=""bm25"", first_phase=""bm25(text)"")) vespa_app = vespa_docker.deploy(application_package=app_package) db = VespaStore.from_documents(docs, embedding_function, app=vespa_app, **vespa_config) ``` Then, to perform a regular text search based on BM25: ```python query = ""What did the president say about Ketanji Brown Jackson"" custom_query = { ""yql"": ""select * from sources * where userQuery()"", ""query"": query, ""type"": ""weakAnd"", ""ranking"": ""bm25"", ""hits"": 4, } results = db.similarity_search_with_score(query, custom_query=custom_query) # results[0][0].metadata[""id""] == ""id:testapp:testapp::32"" # results[0][1] ~= 14.384 ``` All of the powerful search and query capabilities of Vespa can be used by using a custom query. Please refer to the Vespa documentation on it\'s [Query API]( for more details. ### Hybrid search Hybrid search means using both a classic term-based search such as BM25 and a vector search and combining the results. We need to create a new rank profile for hybrid search on Vespa: ```python app_package.schema.add_rank_profile( RankProfile( name=""hybrid"", first_phase=""log(bm25(text)) + 0.5 * closeness(field, embedding)"", inputs=[(""query(query_embedding)"", ""tensor(x[384])"")], ) ) vespa_app = vespa_docker.deploy(application_package=app_package) db = VespaStore.from_documents(docs, embedding_function, app=vespa_app, **vespa_config) ``` Here, we score each document as a combination of it\'s BM25 score and its distance score. We can query using a custom query: ```python query = ""What did the president say about Ketanji Brown Jackson"" query_embedding = embedding_function.embed_query(query) nearest_neighbor_expression = ( ""{targetHits: 4}nearestNeighbor(embedding, query_embedding)"" ) custom_query = { ""yql"": f""select * from sources * where {nearest_neighbor_expression} and userQuery()"", ""query"": query, ""type"": ""weakAnd"", ""input.query(query_embedding)"": query_embedding, ""ranking"": ""hybrid"", ""hits"": 4, } results = db.similarity_search_with_score(query, custom_query=custom_query) # results[0][0].metadata[""id""], ""id:testapp:testapp::32"") # results[0][1] ~= 2.897 ``` ### Native embedders in Vespa Up until this point we\'ve used an embedding function in Python to provide embeddings for the texts. Vespa supports embedding function natively, so you can defer this calculation in to Vespa. One benefit is the ability to use GPUs when embedding documents if you have a large collections. Please refer to [Vespa embeddings]( for more information. First, we need to modify our application package: ```python from vespa.package import Component, Parameter app_package.components = [ Component( id=""hf-embedder"", type=""hugging-face-embedder"", parameters=[ Parameter(""transformer-model"", {""path"": ""...""}), Parameter(""tokenizer-model"", {""url"": ""...""}), ], ) ] Field( name=""hfembedding"", type=""tensor(x[384])"", is_document_field=False, indexing=[""input text"", ""embed hf-embedder"", ""attribute"", ""summary""], attribute=[""distance-metric: angular""], ) app_package.schema.add_rank_profile( RankProfile( name=""hf_similarity"", first_phase=""closeness(field, hfembedding)"", inputs=[(""query(query_embedding)"", ""tensor(x[384])"")], ) ) ``` Please refer to the embeddings documentation on adding embedder models and tokenizers to the application. Note that the `hfembedding` field includes instructions for embedding using the `hf-embedder`. Now we can query with a custom query: ```python query = ""What did the president say about Ketanji Brown Jackson"" nearest_neighbor_expression = ( ""{targetHits: 4}nearestNeighbor(internalembedding, query_embedding)"" ) custom_query = { ""yql"": f""select * from sources * where {nearest_neighbor_expression}"", ""input.query(query_embedding)"": f\'embed(hf-embedder, ""{query}"")\', ""ranking"": ""internal_similarity"", ""hits"": 4, } results = db.similarity_search_with_score(query, custom_query=custom_query) # results[0][0].metadata[""id""], ""id:testapp:testapp::32"") # results[0][1] ~= 0.630 ``` Note that the query here includes an `embed` instruction to embed the query using the same model as for the documents. ### Approximate nearest neighbor In all of the above examples, we\'ve used exact nearest neighbor to find results. However, for large collections of documents this is not feasible as one has to scan through all documents to find the best matches. To avoid this, we can use [approximate nearest neighbors]( First, we can change the embedding field to create a HNSW index: ```python from vespa.package import HNSW app_package.schema.add_fields( Field( name=""embedding"", type=""tensor(x[384])"", indexing=[""attribute"", ""summary"", ""index""], ann=HNSW( distance_metric=""angular"", max_links_per_node=16, neighbors_to_explore_at_insert=200, ), ) ) ``` This creates a HNSW index on the embedding data which allows for efficient searching. With this set, we can easily search using ANN by setting the `approximate` argument to `True`: ```python query = ""What did the president say about Ketanji Brown Jackson"" results = db.similarity_search(query, approximate=True) # results[0][0].metadata[""id""], ""id:testapp:testapp::32"") ``` This covers most of the functionality in the Vespa vector store in LangChain. - [Returning with scores](#returning-with-scores) - [As retriever](#as-retriever) - [Metadata](#metadata) - [Custom query](#custom-query) - [Hybrid search](#hybrid-search) - [Native embedders in Vespa](#native-embedders-in-vespa) - [Approximate nearest neighbor](#approximate-nearest-neighbor)']","I'm an AI system built by a team of inventors at Amazon, and I'm here to provide information and assistance. For filing CVEs (Common Vulnerabilities and Exposures), you should follow the guidelines provided by the CVE Numbering Authorities (CNAs). You can find a list of CNAs and their contact information on the MITRE CVE database website: [MITRE CVE Database](https://cve.mitre.org/cve/html/cve_submit.html). It's important to follow the appropriate process for the specific CNA to ensure your report is handled correctly and securely. Always refer to the official documentation for the most accurate and up-to-date information.",security@langchain.dev,0.0,0.0,0.16666666666666666,0.0,0.01834862385321101
18,What class is made when using | in langchain?,"['Router | Router Routing allows you to create non-deterministic chains where the output of a previous step defines the next step. Routing helps provide structure and consistency around interactions with LLMs. As a very simple example, let\'s suppose we have two templates optimized for different types of questions, and we want to choose the template based on the user input. ```python from langchain.prompts import PromptTemplate physics_template = """"""You are a very smart physics professor. \\ You are great at answering questions about physics in a concise and easy to understand manner. \\ When you don\'t know the answer to a question you admit that you don\'t know. Here is a question: {input}"""""" physics_prompt = PromptTemplate.from_template(physics_template) math_template = """"""You are a very good mathematician. You are great at answering math questions. \\ You are so good because you are able to break down hard problems into their component parts, \\ answer the component parts, and then put them together to answer the broader question. Here is a question: {input}"""""" math_prompt = PromptTemplate.from_template(math_template) ``` ## Using LCEL We can easily do this using a `RunnableBranch`. A `RunnableBranch` is initialized with a list of (condition, runnable) pairs and a default runnable. It selects which branch by passing each condition the input it\'s invoked with. It selects the first condition to evaluate to True, and runs the corresponding runnable to that condition with the input. If no provided conditions match, it runs the default runnable. ```python from langchain.chat_models import ChatOpenAI from langchain.schema.output_parser import StrOutputParser from langchain.schema.runnable import RunnableBranch ``` ```python general_prompt = PromptTemplate.from_template( ""You are a helpful assistant. Answer the question as accurately as you can.\\n\\n{input}"" ) prompt_branch = RunnableBranch( (lambda x: x[""topic""] == ""math"", math_prompt), (lambda x: x[""topic""] == ""physics"", physics_prompt), general_prompt, ) ``` ```python from typing import Literal from langchain.output_parsers.openai_functions import PydanticAttrOutputFunctionsParser from langchain.pydantic_v1 import BaseModel from langchain.utils.openai_functions import convert_pydantic_to_openai_function class TopicClassifier(BaseModel): ""Classify the topic of the user question"" topic: Literal[""math"", ""physics"", ""general""] ""The topic of the user question. One of \'math\', \'physics\' or \'general\'."" classifier_function = convert_pydantic_to_openai_function(TopicClassifier) llm = ChatOpenAI().bind( functions=[classifier_function], function_call={""name"": ""TopicClassifier""} ) parser = PydanticAttrOutputFunctionsParser( pydantic_schema=TopicClassifier, attr_name=""topic"" ) classifier_chain = llm | parser ``` ```python from operator import itemgetter from langchain.schema.output_parser import StrOutputParser from langchain.schema.runnable import RunnablePassthrough final_chain = ( RunnablePassthrough.assign(topic=itemgetter(""input"") | classifier_chain) | prompt_branch | ChatOpenAI() | StrOutputParser() ) ``` ```python final_chain.invoke( { ""input"": ""What is the first prime number greater than 40 such that one plus the prime number is divisible by 3?"" } ) ``` ```text ""Thank you for your kind words! I\'ll be happy to help you with this math question.\\n\\nTo find the first prime number greater than 40 that satisfies the given condition, we need to follow a step-by-step approach. \\n\\nFirstly, let\'s list the prime numbers greater than 40:\\n41, 43, 47, 53, 59, 61, 67, 71, ...\\n\\nNow, we need to check if one plus each of these prime numbers is divisible by 3. We can do this by calculating the remainder when dividing each number by 3.\\n\\nFor 41, (41 + 1) % 3 = 42 % 3 = 0. It is divisible by 3.\\n\\nFor 43, (43 + 1) % 3 = 44 % 3 = 2. It is not divisible by 3.\\n\\nFor 47, (47 + 1) % 3 = 48 % 3 = 0. It is divisible by 3.\\n\\nSince 41 and 47 are both greater than 40 and satisfy the condition, the first prime number greater than 40 such that one plus the prime number is divisible by 3 is 41.\\n\\nTherefore, the answer to the question is 41."" ``` For more on routing with LCEL [head here](/docs/expression_language/how_to/routing). ## [Legacy] RouterChain The preferred approach as of version `0.0.293` is to use LCEL as above.Here we show how to use the `RouterChain` paradigm to create a chain that dynamically selects the next chain to use for a given input. Router chains are made up of two components: - The `RouterChain` itself (responsible for selecting the next chain to call) - `destination_chains`: chains that the router chain can route to In this example, we will focus on the different types of routing chains. We will show these routing chains used in a `MultiPromptChain` to create a question-answering chain that selects the prompt which is most relevant for a given question, and then answers the question using that prompt. ```python from langchain.chains import ConversationChain from langchain.chains.llm import LLMChain from langchain.chains.router import MultiPromptChain from langchain.llms import OpenAI ``` ### [Legacy] LLMRouterChain This chain uses an LLM to determine how to route things. ```python prompt_infos = [ { ""name"": ""physics"", ""description"": ""Good for answering questions about physics"", ""prompt_template"": physics_template, }, { ""name"": ""math"", ""description"": ""Good for answering math questions"", ""prompt_template"": math_template, }, ] ``` ```python llm = OpenAI() ``` ```python destination_chains = {} for p_info in prompt_infos: name = p_info[""name""] prompt_template = p_info[""prompt_template""] prompt = PromptTemplate(template=prompt_template, input_variables=[""input""]) chain = LLMChain(llm=llm, prompt=prompt) destination_chains[name] = chain default_chain = ConversationChain(llm=llm, output_key=""text"") ``` ```python from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE ``` ```python destinations = [f""{p[\'name\']}: {p[\'description\']}"" for p in prompt_infos] destinations_str = ""\\n"".join(destinations) router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations_str) router_prompt = PromptTemplate( template=router_template, input_variables=[""input""], output_parser=RouterOutputParser(), ) router_chain = LLMRouterChain.from_llm(llm, router_prompt) ``` ```python chain = MultiPromptChain( router_chain=router_chain, destination_chains=destination_chains, default_chain=default_chain, verbose=True, ) ``` ```python print(chain.run(""What is black body radiation?"")) ``` ```text > Entering new MultiPromptChain chain... /Users/bagatur/langchain/libs/langchain/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain. warnings.warn( physics: {\'input\': \'What is black body radiation?\'} > Finished chain. Black body radiation is the thermal electromagnetic radiation within or surrounding a body in thermodynamic equilibrium with its environment, or emitted by a black body (an idealized physical body which absorbs all incident electromagnetic radiation). It is a characteristic of the temperature of the body; if the body has a uniform temperature, the radiation is also uniform across the spectrum of frequencies. The spectral characteristics of the radiation are determined by the temperature of the body, which implies that a black body at a given temperature will emit the same amount of radiation at every frequency. ``` ```python print( chain.run( ""What is the first prime number greater than 40 such that one plus the prime number is divisible by 3?"" ) ) ``` ```text > Entering new MultiPromptChain chain... /Users/bagatur/langchain/libs/langchain/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain. warnings.warn( math: {\'input\': \'What is the first prime number greater than 40 such that one plus the prime number is divisible by 3?\'} > Finished chain. The first prime number greater than 40 such that one plus the prime number is divisible by 3 is 43. This can be seen by breaking down the problem: 1) We know that a prime number is a number that is only divisible by itself and one. 2) We also know that if a number is divisible by 3, the sum of its digits must be divisible by 3. So, if we want to find the first prime number greater than 40 such that one plus the prime number is divisible by 3, we can start counting up from 40, testing each number to see if it is prime and if the sum of the number and one is divisible by three. The first number we come to that satisfies these conditions is 43. ``` ```python print(chain.run(""What is the name of the type of cloud that rains?"")) ``` ```text > Entering new MultiPromptChain chain... /Users/bagatur/langchain/libs/langchain/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain. warnings.warn( physics: {\'input\': \'What is the name of the type of cloud that rains?\'} > Finished chain. The type of cloud that rains is called a cumulonimbus cloud. ``` ## [Legacy] EmbeddingRouterChain The `EmbeddingRouterChain` uses embeddings and similarity to route between destination chains. ```python from langchain.chains.router.embedding_router import EmbeddingRouterChain from langchain.embeddings import CohereEmbeddings from langchain.vectorstores import Chroma ``` ```python names_and_descriptions = [ (""physics"", [""for questions about physics""]), (""math"", [""for questions about math""]), ] ``` ```python router_chain = EmbeddingRouterChain.from_names_and_descriptions( names_and_descriptions, Chroma, CohereEmbeddings(), routing_keys=[""input""] ) ``` ```python chain = MultiPromptChain( router_chain=router_chain, destination_chains=destination_chains, default_chain=default_chain, verbose=True, ) ``` ```python print(chain.run(""What is black body radiation?"")) ``` ```text > Entering new MultiPromptChain chain... physics: {\'input\': \'What is black body radiation?\'} > Finished chain. Black body radiation is the electromagnetic radiation emitted by a black body, which is an idealized physical body that absorbs all incident electromagnetic radiation. This radiation is related to the temperature of the body, with higher temperatures leading to higher radiation levels. The spectrum of the radiation is continuous, and is described by the Planck\'s law of black body radiation. ``` ```python print( chain.run( ""What is the first prime number greater than 40 such that one plus the prime number is divisible by 3?"" ) ) ``` ```text > Entering new MultiPromptChain chain... math: {\'input\': \'What is the first prime number greater than 40 such that one plus the prime number is divisible by 3?\'} > Finished chain. The first prime number greater than 40 such that one plus the prime number is divisible by 3 is 43. This is because 43 is a prime number, and 1 + 43 = 44, which is divisible by 3. ``` - [Using LCEL](#using-lcel) - [Legacy RouterChain](#legacy-routerchain)', 'Replicating MRKL | Replicating MRKL This walkthrough demonstrates how to replicate the [MRKL]( system using agents. This uses the example Chinook database. To set it up, follow the instructions on [ and place the `.db` file in a ""notebooks"" folder at the root of this repository. ```python from langchain.chains import LLMMathChain from langchain.llms import OpenAI from langchain.utilities import SerpAPIWrapper from langchain.utilities import SQLDatabase from langchain_experimental.sql import SQLDatabaseChain from langchain.agents import initialize_agent, Tool from langchain.agents import AgentType ``` ```python llm = OpenAI(temperature=0) search = SerpAPIWrapper() llm_math_chain = LLMMathChain(llm=llm, verbose=True) db = SQLDatabase.from_uri(""sqlite:///../../../../../notebooks/Chinook.db"") db_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True) tools = [ Tool( name=""Search"", func=search.run, description=""useful for when you need to answer questions about current events. You should ask targeted questions"" ), Tool( name=""Calculator"", func=llm_math_chain.run, description=""useful for when you need to answer questions about math"" ), Tool( name=""FooBar DB"", func=db_chain.run, description=""useful for when you need to answer questions about FooBar. Input should be in the form of a question containing full context"" ) ] ``` ```python mrkl = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True) ``` ```python mrkl.run(""Who is Leo DiCaprio\'s girlfriend? What is her current age raised to the 0.43 power?"") ``` ```text > Entering new AgentExecutor chain... I need to find out who Leo DiCaprio\'s girlfriend is and then calculate her age raised to the 0.43 power. Action: Search Action Input: ""Who is Leo DiCaprio\'s girlfriend?"" Observation: DiCaprio met actor Camila Morrone in December 2017, when she was 20 and he was 43. They were spotted at Coachella and went on multiple vacations together. Some reports suggested that DiCaprio was ready to ask Morrone to marry him. The couple made their red carpet debut at the 2020 Academy Awards. Thought: I need to calculate Camila Morrone\'s age raised to the 0.43 power. Action: Calculator Action Input: 21^0.43 > Entering new LLMMathChain chain... 21^0.43 ```text 21**0.43 ``` ...numexpr.evaluate(""21**0.43"")... Answer: 3.7030049853137306 > Finished chain. Observation: Answer: 3.7030049853137306 Thought: I now know the final answer. Final Answer: Camila Morrone is Leo DiCaprio\'s girlfriend and her current age raised to the 0.43 power is 3.7030049853137306. > Finished chain. ""Camila Morrone is Leo DiCaprio\'s girlfriend and her current age raised to the 0.43 power is 3.7030049853137306."" ``` ```python mrkl.run(""What is the full name of the artist who recently released an album called \'The Storm Before the Calm\' and are they in the FooBar database? If so, what albums of theirs are in the FooBar database?"") ``` ```text > Entering new AgentExecutor chain... I need to find out the artist\'s full name and then search the FooBar database for their albums. Action: Search Action Input: ""The Storm Before the Calm"" artist Observation: The Storm Before the Calm (stylized in all lowercase) is the tenth (and eighth international) studio album by Canadian-American singer-songwriter Alanis Morissette, released June 17, 2022, via Epiphany Music and Thirty Tigers, as well as by RCA Records in Europe. Thought: I now need to search the FooBar database for Alanis Morissette\'s albums. Action: FooBar DB Action Input: What albums by Alanis Morissette are in the FooBar database? > Entering new SQLDatabaseChain chain... What albums by Alanis Morissette are in the FooBar database? SQLQuery: /Users/harrisonchase/workplace/langchain/langchain/sql_database.py:191: SAWarning: Dialect sqlite+pysqlite does *not* support Decimal objects natively, and SQLAlchemy must convert from floating point - rounding errors and other issues may occur. Please consider storing Decimal numbers as strings or integers on this platform for lossless storage. sample_rows = connection.execute(command) SELECT ""Title"" FROM ""Album"" INNER JOIN ""Artist"" ON ""Album"".""ArtistId"" = ""Artist"".""ArtistId"" WHERE ""Name"" = \'Alanis Morissette\' LIMIT 5; SQLResult: [(\'Jagged Little Pill\',)] Answer: The albums by Alanis Morissette in the FooBar database are Jagged Little Pill. > Finished chain. Observation: The albums by Alanis Morissette in the FooBar database are Jagged Little Pill. Thought: I now know the final answer. Final Answer: The artist who released the album \'The Storm Before the Calm\' is Alanis Morissette and the albums of hers in the FooBar database are Jagged Little Pill. > Finished chain. ""The artist who released the album \'The Storm Before the Calm\' is Alanis Morissette and the albums of hers in the FooBar database are Jagged Little Pill."" ``` ## Using a Chat Model ```python from langchain.chat_models import ChatOpenAI llm = ChatOpenAI(temperature=0) llm1 = OpenAI(temperature=0) search = SerpAPIWrapper() llm_math_chain = LLMMathChain(llm=llm1, verbose=True) db = SQLDatabase.from_uri(""sqlite:///../../../../../notebooks/Chinook.db"") db_chain = SQLDatabaseChain.from_llm(llm1, db, verbose=True) tools = [ Tool( name=""Search"", func=search.run, description=""useful for when you need to answer questions about current events. You should ask targeted questions"" ), Tool( name=""Calculator"", func=llm_math_chain.run, description=""useful for when you need to answer questions about math"" ), Tool( name=""FooBar DB"", func=db_chain.run, description=""useful for when you need to answer questions about FooBar. Input should be in the form of a question containing full context"" ) ] ``` ```python mrkl = initialize_agent(tools, llm, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True) ``` ```python mrkl.run(""Who is Leo DiCaprio\'s girlfriend? What is her current age raised to the 0.43 power?"") ``` ```text > Entering new AgentExecutor chain... Thought: The first question requires a search, while the second question requires a calculator. Action: ``` { ""action"": ""Search"", ""action_input"": ""Leo DiCaprio girlfriend"" } ``` Observation: Gigi Hadid: 2022 Leo and Gigi were first linked back in September 2022, when a source told Us Weekly that Leo had his sights set"" on her (alarming way to put it, but okay). Thought:For the second question, I need to calculate the age raised to the 0.43 power. I will use the calculator tool. Action: ``` { ""action"": ""Calculator"", ""action_input"": ""((2022-1995)^0.43)"" } ``` > Entering new LLMMathChain chain... ((2022-1995)^0.43) ```text (2022-1995)**0.43 ``` ...numexpr.evaluate(""(2022-1995)**0.43"")... Answer: 4.125593352125936 > Finished chain. Observation: Answer: 4.125593352125936 Thought:I now know the final answer. Final Answer: Gigi Hadid is Leo DiCaprio\'s girlfriend and her current age raised to the 0.43 power is approximately 4.13. > Finished chain. ""Gigi Hadid is Leo DiCaprio\'s girlfriend and her current age raised to the 0.43 power is approximately 4.13."" ``` ```python mrkl.run(""What is the full name of the artist who recently released an album called \'The Storm Before the Calm\' and are they in the FooBar database? If so, what albums of theirs are in the FooBar database?"") ``` ```text > Entering new AgentExecutor chain... Question: What is the full name of the artist who recently released an album called \'The Storm Before the Calm\' and are they in the FooBar database? If so, what albums of theirs are in the FooBar database? Thought: I should use the Search tool to find the answer to the first part of the question and then use the FooBar DB tool to find the answer to the second part. Action: ``` { ""action"": ""Search"", ""action_input"": ""Who recently released an album called \'The Storm Before the Calm\'"" } ``` Observation: Alanis Morissette Thought:Now that I know the artist\'s name, I can use the FooBar DB tool to find out if they are in the database and what albums of theirs are in it. Action: ``` { ""action"": ""FooBar DB"", ""action_input"": ""What albums does Alanis Morissette have in the database?"" } ``` > Entering new SQLDatabaseChain chain... What albums does Alanis Morissette have in the database? SQLQuery: /Users/harrisonchase/workplace/langchain/langchain/sql_database.py:191: SAWarning: Dialect sqlite+pysqlite does *not* support Decimal objects natively, and SQLAlchemy must convert from floating point - rounding errors and other issues may occur. Please consider storing Decimal numbers as strings or integers on this platform for lossless storage. sample_rows = connection.execute(command) SELECT ""Title"" FROM ""Album"" WHERE ""ArtistId"" IN (SELECT ""ArtistId"" FROM ""Artist"" WHERE ""Name"" = \'Alanis Morissette\') LIMIT 5; SQLResult: [(\'Jagged Little Pill\',)] Answer: Alanis Morissette has the album Jagged Little Pill in the database. > Finished chain. Observation: Alanis Morissette has the album Jagged Little Pill in the database. Thought:The artist Alanis Morissette is in the FooBar database and has the album Jagged Little Pill in it. Final Answer: Alanis Morissette is in the FooBar database and has the album Jagged Little Pill in it. > Finished chain. \'Alanis Morissette is in the FooBar database and has the album Jagged Little Pill in it.\' ``` - [Using a Chat Model](#using-a-chat-model)', 'OpaquePrompts | OpaquePrompts [OpaquePrompts]( is a service that enables applications to leverage the power of language models without compromising user privacy. Designed for composability and ease of integration into existing applications and services, OpaquePrompts is consumable via a simple Python library as well as through LangChain. Perhaps more importantly, OpaquePrompts leverages the power of [confidential computing]( to ensure that even the OpaquePrompts service itself cannot access the data it is protecting. This notebook goes over how to use LangChain to interact with `OpaquePrompts`. ```bash # install the opaqueprompts and langchain packages pip install opaqueprompts langchain ``` Accessing the OpaquePrompts API requires an API key, which you can get by creating an account on [the OpaquePrompts website]( Once you have an account, you can find your API key on [the API Keys page](https:opaqueprompts.opaque.co/api-keys). ```python import os # Set API keys os.environ[""OPAQUEPROMPTS_API_KEY""] = """" os.environ[""OPENAI_API_KEY""] = """" ``` # Use OpaquePrompts LLM Wrapper Applying OpaquePrompts to your application could be as simple as wrapping your LLM using the OpaquePrompts class by replace `llm=OpenAI()` with `llm=OpaquePrompts(base_llm=OpenAI())`. ```python from langchain.callbacks.stdout import StdOutCallbackHandler from langchain.chains import LLMChain from langchain.globals import set_debug, set_verbose from langchain.llms import OpaquePrompts, OpenAI from langchain.memory import ConversationBufferWindowMemory from langchain.prompts import PromptTemplate set_debug(True) set_verbose(True) prompt_template = """""" As an AI assistant, you will answer questions according to given context. Sensitive personal information in the question is masked for privacy. For instance, if the original text says ""Giana is good,"" it will be changed to ""PERSON_998 is good."" Here\'s how to handle these changes: * Consider these masked phrases just as placeholders, but still refer to them in a relevant way when answering. * It\'s possible that different masked terms might mean the same thing. Stick with the given term and don\'t modify it. * All masked terms follow the ""TYPE_ID"" pattern. * Please don\'t invent new masked terms. For instance, if you see ""PERSON_998,"" don\'t come up with ""PERSON_997"" or ""PERSON_999"" unless they\'re already in the question. Conversation History: ```{history}``` Context : ```During our recent meeting on February 23, 2023, at 10:30 AM, John Doe provided me with his personal details. His email is johndoe@example.com and his contact number is 650-456-7890. He lives in New York City, USA, and belongs to the American nationality with Christian beliefs and a leaning towards the Democratic party. He mentioned that he recently made a transaction using his credit card 4111 1111 1111 1111 and transferred bitcoins to the wallet address 1A1zP1eP5QGefi2DMPTfTL5SLmv7DivfNa. While discussing his European travels, he noted down his IBAN as GB29 NWBK 6016 1331 9268 19. Additionally, he provided his website as John also discussed some of his US-specific details. He said his bank account number is 1234567890123456 and his drivers license is Y12345678. His ITIN is 987-65-4321, and he recently renewed his passport, the number for which is 123456789. He emphasized not to share his SSN, which is 123-45-6789. Furthermore, he mentioned that he accesses his work files remotely through the IP 192.168.1.1 and has a medical license number MED-123456. ``` Question: ```{question}``` """""" chain = LLMChain( prompt=PromptTemplate.from_template(prompt_template), llm=OpaquePrompts(base_llm=OpenAI()), memory=ConversationBufferWindowMemory(k=2), verbose=True, ) print( chain.run( { ""question"": """"""Write a message to remind John to do password reset for his website to stay secure."""""" }, callbacks=[StdOutCallbackHandler()], ) ) ``` From the output, you can see the following context from user input has sensitive data. ```text # Context from user input During our recent meeting on February 23, 2023, at 10:30 AM, John Doe provided me with his personal details. His email is johndoe@example.com and his contact number is 650-456-7890. He lives in New York City, USA, and belongs to the American nationality with Christian beliefs and a leaning towards the Democratic party. He mentioned that he recently made a transaction using his credit card 4111 1111 1111 1111 and transferred bitcoins to the wallet address 1A1zP1eP5QGefi2DMPTfTL5SLmv7DivfNa. While discussing his European travels, he noted down his IBAN as GB29 NWBK 6016 1331 9268 19. Additionally, he provided his website as John also discussed some of his US-specific details. He said his bank account number is 1234567890123456 and his drivers license is Y12345678. His ITIN is 987-65-4321, and he recently renewed his passport, the number for which is 123456789. He emphasized not to share his SSN, which is 669-45-6789. Furthermore, he mentioned that he accesses his work files remotely through the IP 192.168.1.1 and has a medical license number MED-123456. ``` OpaquePrompts will automatically detect the sensitive data and replace it with a placeholder. ```text # Context after OpaquePrompts During our recent meeting on DATE_TIME_3, at DATE_TIME_2, PERSON_3 provided me with his personal details. His email is EMAIL_ADDRESS_1 and his contact number is PHONE_NUMBER_1. He lives in LOCATION_3, LOCATION_2, and belongs to the NRP_3 nationality with NRP_2 beliefs and a leaning towards the Democratic party. He mentioned that he recently made a transaction using his credit card CREDIT_CARD_1 and transferred bitcoins to the wallet address CRYPTO_1. While discussing his NRP_1 travels, he noted down his IBAN as IBAN_CODE_1. Additionally, he provided his website as URL_1. PERSON_2 also discussed some of his LOCATION_1-specific details. He said his bank account number is US_BANK_NUMBER_1 and his drivers license is US_DRIVER_LICENSE_2. His ITIN is US_ITIN_1, and he recently renewed his passport, the number for which is DATE_TIME_1. He emphasized not to share his SSN, which is US_SSN_1. Furthermore, he mentioned that he accesses his work files remotely through the IP IP_ADDRESS_1 and has a medical license number MED-US_DRIVER_LICENSE_1. ``` Placeholder is used in the LLM response. ```text # response returned by LLM Hey PERSON_1, just wanted to remind you to do a password reset for your website URL_1 through your email EMAIL_ADDRESS_1. It\'s important to stay secure online, so don\'t forget to do it! ``` Response is desanitized by replacing the placeholder with the original sensitive data. ```text # desanitized LLM response from OpaquePrompts Hey John, just wanted to remind you to do a password reset for your website through your email johndoe@example.com. It\'s important to stay secure online, so don\'t forget to do it! ``` # Use OpaquePrompts in LangChain expression There are functions that can be used with LangChain expression as well if a drop-in replacement doesn\'t offer the flexibility you need. ```python import langchain.utilities.opaqueprompts as op from langchain.schema.output_parser import StrOutputParser from langchain.schema.runnable import RunnablePassthrough prompt = (PromptTemplate.from_template(prompt_template),) llm = OpenAI() pg_chain = ( op.sanitize | RunnablePassthrough.assign( response=(lambda x: x[""sanitized_input""]) | prompt | llm | StrOutputParser(), ) | (lambda x: op.desanitize(x[""response""], x[""secure_context""])) ) pg_chain.invoke( { ""question"": ""Write a text message to remind John to do password reset for his website through his email to stay secure."", ""history"": """", } ) ```']","When using the `|` operator in LangChain, it creates an instance of `RunnableSequence`. This class represents a sequence of runnable components that are executed in order. Each component in the sequence can be a `Runnable` object, which could be a chain, a prompt template, an LLM, or any other callable object that can be executed.","The | pipe operator combines (or coerces) two runnables ( or a runnable and a function or dict or other compatible primitive) into a new runnable, namely a ""RunnableSequence.""",0.8333333332916666,1.0,1.0,0.01117358032430412,0.19753086419753085
19,"in code, how can i add a system message at the end of the conversation history to influence the output of the llm","['Memory | Memory Most LLM applications have a conversational interface. An essential component of a conversation is being able to refer to information introduced earlier in the conversation. At bare minimum, a conversational system should be able to access some window of past messages directly. A more complex system will need to have a world model that it is constantly updating, which allows it to do things like maintain information about entities and their relationships. We call this ability to store information about past interactions ""memory"". LangChain provides a lot of utilities for adding memory to a system. These utilities can be used by themselves or incorporated seamlessly into a chain. A memory system needs to support two basic actions: reading and writing. Recall that every chain defines some core execution logic that expects certain inputs. Some of these inputs come directly from the user, but some of these inputs can come from memory. A chain will interact with its memory system twice in a given run. 1. AFTER receiving the initial user inputs but BEFORE executing the core logic, a chain will READ from its memory system and augment the user inputs. 2. AFTER executing the core logic but BEFORE returning the answer, a chain will WRITE the inputs and outputs of the current run to memory, so that they can be referred to in future runs. ![memory-diagram](/assets/images/memory_diagram-0627c68230aa438f9b5419064d63cbbc.png) ## Building memory into a system The two core design decisions in any memory system are: - How state is stored - How state is queried ### Storing: List of chat messages Underlying any memory is a history of all chat interactions. Even if these are not all used directly, they need to be stored in some form. One of the key parts of the LangChain memory module is a series of integrations for storing these chat messages, from in-memory lists to persistent databases. - [Chat message storage](/docs/modules/memory/chat_messages/): How to work with Chat Messages, and the various integrations offered. ### Querying: Data structures and algorithms on top of chat messages Keeping a list of chat messages is fairly straight-forward. What is less straight-forward are the data structures and algorithms built on top of chat messages that serve a view of those messages that is most useful. A very simply memory system might just return the most recent messages each run. A slightly more complex memory system might return a succinct summary of the past K messages. An even more sophisticated system might extract entities from stored messages and only return information about entities referenced in the current run. Each application can have different requirements for how memory is queried. The memory module should make it easy to both get started with simple memory systems and write your own custom systems if needed. - [Memory types](/docs/modules/memory/types/): The various data structures and algorithms that make up the memory types LangChain supports ## Get started Let\'s take a look at what Memory actually looks like in LangChain. Here we\'ll cover the basics of interacting with an arbitrary memory class. Let\'s take a look at how to use `ConversationBufferMemory` in chains. `ConversationBufferMemory` is an extremely simple form of memory that just keeps a list of chat messages in a buffer and passes those into the prompt template. ```python from langchain.memory import ConversationBufferMemory memory = ConversationBufferMemory() memory.chat_memory.add_user_message(""hi!"") memory.chat_memory.add_ai_message(""what\'s up?"") ``` When using memory in a chain, there are a few key concepts to understand. Note that here we cover general concepts that are useful for most types of memory. Each individual memory type may very well have its own parameters and concepts that are necessary to understand. ### What variables get returned from memory Before going into the chain, various variables are read from memory. These have specific names which need to align with the variables the chain expects. You can see what these variables are by calling `memory.load_memory_variables({})`. Note that the empty dictionary that we pass in is just a placeholder for real variables. If the memory type you are using is dependent upon the input variables, you may need to pass some in. ```python memory.load_memory_variables({}) ``` ```text {\'history\': ""Human: hi!\\nAI: what\'s up?""} ``` In this case, you can see that `load_memory_variables` returns a single key, `history`. This means that your chain (and likely your prompt) should expect an input named `history`. You can usually control this variable through parameters on the memory class. For example, if you want the memory variables to be returned in the key `chat_history` you can do: ```python memory = ConversationBufferMemory(memory_key=""chat_history"") memory.chat_memory.add_user_message(""hi!"") memory.chat_memory.add_ai_message(""what\'s up?"") ``` ```text {\'chat_history\': ""Human: hi!\\nAI: what\'s up?""} ``` The parameter name to control these keys may vary per memory type, but it\'s important to understand that (1) this is controllable, and (2) how to control it. ### Whether memory is a string or a list of messages One of the most common types of memory involves returning a list of chat messages. These can either be returned as a single string, all concatenated together (useful when they will be passed into LLMs) or a list of ChatMessages (useful when passed into ChatModels). By default, they are returned as a single string. In order to return as a list of messages, you can set `return_messages=True` ```python memory = ConversationBufferMemory(return_messages=True) memory.chat_memory.add_user_message(""hi!"") memory.chat_memory.add_ai_message(""what\'s up?"") ``` ```text {\'history\': [HumanMessage(content=\'hi!\', additional_kwargs={}, example=False), AIMessage(content=\'what\'s up?\', additional_kwargs={}, example=False)]} ``` ### What keys are saved to memory Often times chains take in or return multiple input/output keys. In these cases, how can we know which keys we want to save to the chat message history? This is generally controllable by `input_key` and `output_key` parameters on the memory types. These default to `None` - and if there is only one input/output key it is known to just use that. However, if there are multiple input/output keys then you MUST specify the name of which one to use. ### End to end example Finally, let\'s take a look at using this in a chain. We\'ll use an `LLMChain`, and show working with both an LLM and a ChatModel. #### Using an LLM ```python from langchain.llms import OpenAI from langchain.prompts import PromptTemplate from langchain.chains import LLMChain from langchain.memory import ConversationBufferMemory llm = OpenAI(temperature=0) # Notice that ""chat_history"" is present in the prompt template template = """"""You are a nice chatbot having a conversation with a human. Previous conversation: {chat_history} New human question: {question} Response:"""""" prompt = PromptTemplate.from_template(template) # Notice that we need to align the `memory_key` memory = ConversationBufferMemory(memory_key=""chat_history"") conversation = LLMChain( llm=llm, prompt=prompt, verbose=True, memory=memory ) ``` ```python # Notice that we just pass in the `question` variables - `chat_history` gets populated by memory conversation({""question"": ""hi""}) ``` #### Using a ChatModel ```python from langchain.chat_models import ChatOpenAI from langchain.prompts import ( ChatPromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate, HumanMessagePromptTemplate, ) from langchain.chains import LLMChain from langchain.memory import ConversationBufferMemory llm = ChatOpenAI() prompt = ChatPromptTemplate( messages=[ SystemMessagePromptTemplate.from_template( ""You are a nice chatbot having a conversation with a human."" ), # The `variable_name` here is what must align with memory MessagesPlaceholder(variable_name=""chat_history""), HumanMessagePromptTemplate.from_template(""{question}"") ] ) # Notice that we `return_messages=True` to fit into the MessagesPlaceholder # Notice that `""chat_history""` aligns with the MessagesPlaceholder name. memory = ConversationBufferMemory(memory_key=""chat_history"", return_messages=True) conversation = LLMChain( llm=llm, prompt=prompt, verbose=True, memory=memory ) ``` ```python # Notice that we just pass in the `question` variables - `chat_history` gets populated by memory conversation({""question"": ""hi""}) ``` ## Next steps And that\'s it for getting started! Please see the other sections for walkthroughs of more advanced topics, like custom memory, multiple memories, and more. - [Building memory into a system](#building-memory-into-a-system)- [Storing: List of chat messages](#storing-list-of-chat-messages) - [Querying: Data structures and algorithms on top of chat messages](#querying-data-structures-and-algorithms-on-top-of-chat-messages) - [Get started](#get-started)- [What variables get returned from memory](#what-variables-get-returned-from-memory) - [Whether memory is a string or a list of messages](#whether-memory-is-a-string-or-a-list-of-messages) - [What keys are saved to memory](#what-keys-are-saved-to-memory) - [End to end example](#end-to-end-example) - [Next steps](#next-steps)', 'Agents | Agents The core idea of agents is to use a language model to choose a sequence of actions to take. In chains, a sequence of actions is hardcoded (in code). In agents, a language model is used as a reasoning engine to determine which actions to take and in which order. ## Concepts There are several key components here: ### Agent This is the chain responsible for deciding what step to take next. This is powered by a language model and a prompt. The inputs to this chain are: 1. Tools: Descriptions of available tools 2. User input: The high level objective 3. Intermediate steps: Any (action, tool output) pairs previously executed in order to achieve the user input The output is the next action(s) to take or the final response to send to the user (`AgentAction`s or `AgentFinish`). An action specifies a tool and the input to that tool. Different agents have different prompting styles for reasoning, different ways of encoding inputs, and different ways of parsing the output. For a full list of built-in agents see [agent types](/docs/modules/agents/agent_types/). You can also **easily build custom agents**, which we show how to do in the Get started section below. ### Tools Tools are functions that an agent can invoke. There are two important design considerations around tools: 1. Giving the agent access to the right tools 2. Describing the tools in a way that is most helpful to the agent Without thinking through both, you won\'t be able to build a working agent. If you don\'t give the agent access to a correct set of tools, it will never be able to accomplish the objectives you give it. If you don\'t describe the tools well, the agent won\'t know how to use them properly. LangChain provides a wide set of built-in tools, but also makes it easy to define your own (including custom descriptions). For a full list of built-in tools, see the [tools integrations section](/docs/integrations/tools/) ### Toolkits For many common tasks, an agent will need a set of related tools. For this LangChain provides the concept of toolkits - groups of around 3-5 tools needed to accomplish specific objectives. For example, the GitHub toolkit has a tool for searching through GitHub issues, a tool for reading a file, a tool for commenting, etc. LangChain provides a wide set of toolkits to get started. For a full list of built-in toolkits, see the [toolkits integrations section](/docs/integrations/toolkits/) ### AgentExecutor The agent executor is the runtime for an agent. This is what actually calls the agent, executes the actions it chooses, passes the action outputs back to the agent, and repeats. In pseudocode, this looks roughly like: ```python next_action = agent.get_action(...) while next_action != AgentFinish: observation = run(next_action) next_action = agent.get_action(..., next_action, observation) return next_action ``` While this may seem simple, there are several complexities this runtime handles for you, including: 1. Handling cases where the agent selects a non-existent tool 2. Handling cases where the tool errors 3. Handling cases where the agent produces output that cannot be parsed into a tool invocation 4. Logging and observability at all levels (agent decisions, tool calls) to stdout and/or to [LangSmith](/docs/langsmith). ### Other types of agent runtimes The `AgentExecutor` class is the main agent runtime supported by LangChain. However, there are other, more experimental runtimes we also support. These include: - [Plan-and-execute Agent](/docs/use_cases/more/agents/autonomous_agents/plan_and_execute) - [Baby AGI](/docs/use_cases/more/agents/autonomous_agents/baby_agi) - [Auto GPT](/docs/use_cases/more/agents/autonomous_agents/autogpt) You can also always create your own custom execution logic, which we show how to do below. ## Get started To best understand the agent framework, lets build an agent from scratch using LangChain Expression Language (LCEL). We\'ll need to build the agent itself, define custom tools, and run the agent and tools in a custom loop. At the end we\'ll show how to use the standard LangChain `AgentExecutor` to make execution easier. Some important terminology (and schema) to know: 1. `AgentAction`: This is a dataclass that represents the action an agent should take. It has a `tool` property (which is the name of the tool that should be invoked) and a `tool_input` property (the input to that tool) 2. `AgentFinish`: This is a dataclass that signifies that the agent has finished and should return to the user. It has a `return_values` parameter, which is a dictionary to return. It often only has one key - `output` - that is a string, and so often it is just this key that is returned. 3. `intermediate_steps`: These represent previous agent actions and corresponding outputs that are passed around. These are important to pass to future iteration so the agent knows what work it has already done. This is typed as a `List[Tuple[AgentAction, Any]]`. Note that observation is currently left as type `Any` to be maximally flexible. In practice, this is often a string. ### Setup: LangSmith By definition, agents take a self-determined, input-dependent sequence of steps before returning a user-facing output. This makes debugging these systems particularly tricky, and observability particularly important. [LangSmith](/docs/langsmith) is especially useful for such cases. When building with LangChain, any built-in agent or custom agent built with LCEL will automatically be traced in LangSmith. And if we use the `AgentExecutor`, we\'ll get full tracing of not only the agent planning steps but also the tool inputs and outputs. To set up LangSmith we just need set the following environment variables: ```bash export LANGCHAIN_TRACING_V2=""true"" export LANGCHAIN_API_KEY="""" ``` ### Define the agent We first need to create our agent. This is the chain responsible for determining what action to take next. In this example, we will use OpenAI Function Calling to create this agent. **This is generally the most reliable way to create agents.** For this guide, we will construct a custom agent that has access to a custom tool. We are choosing this example because for most real world use cases you will NEED to customize either the agent or the tools. We\'ll create a simple tool that computes the length of a word. This is useful because it\'s actually something LLMs can mess up due to tokenization. We will first create it WITHOUT memory, but we will then show how to add memory in. Memory is needed to enable conversation. First, let\'s load the language model we\'re going to use to control the agent. ```python from langchain.chat_models import ChatOpenAI llm = ChatOpenAI(model=""gpt-3.5-turbo"", temperature=0) ``` We can see that it struggles to count the letters in the string ""educa"". ```python llm.invoke(""how many letters in the word educa?"") ``` ```text AIMessage(content=\'There are 6 letters in the word ""educa"".\') ``` Next, let\'s define some tools to use. Let\'s write a really simple Python function to calculate the length of a word that is passed in. ```python from langchain.agents import tool @tool def get_word_length(word: str) -> int: """"""Returns the length of a word."""""" return len(word) tools = [get_word_length] ``` Now let us create the prompt. Because OpenAI Function Calling is finetuned for tool usage, we hardly need any instructions on how to reason, or how to output format. We will just have two input variables: `input` and `agent_scratchpad`. `input` should be a string containing the user objective. `agent_scratchpad` should be a sequence of messages that contains the previous agent tool invocations and the corresponding tool outputs. ```python from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder prompt = ChatPromptTemplate.from_messages( [ ( ""system"", ""You are very powerful assistant, but bad at calculating lengths of words."", ), (""user"", ""{input}""), MessagesPlaceholder(variable_name=""agent_scratchpad""), ] ) ``` How does the agent know what tools it can use? In this case we\'re relying on OpenAI function calling LLMs, which take functions as a separate argument and have been specifically trained to know when to invoke those functions. To pass in our tools to the agent, we just need to format them to the OpenAI function format and pass them to our model. (By `bind`-ing the functions, we\'re making sure that they\'re passed in each time the model is invoked.) ```python from langchain.tools.render import format_tool_to_openai_function llm_with_tools = llm.bind(functions=[format_tool_to_openai_function(t) for t in tools]) ``` Putting those pieces together, we can now create the agent. We will import two last utility functions: a component for formatting intermediate steps (agent action, tool output pairs) to input messages that can be sent to the model, and a component for converting the output message into an agent action/agent finish. ```python from langchain.agents.format_scratchpad import format_to_openai_function_messages from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser agent = ( { ""input"": lambda x: x[""input""], ""agent_scratchpad"": lambda x: format_to_openai_function_messages( x[""intermediate_steps""] ), } | prompt | llm_with_tools | OpenAIFunctionsAgentOutputParser() ) ``` Now that we have our agent, let\'s play around with it! Let\'s pass in a simple question and empty intermediate steps and see what it returns: ```python agent.invoke({""input"": ""how many letters in the word educa?"", ""intermediate_steps"": []}) ``` ```text AgentActionMessageLog(tool=\'get_word_length\', tool_input={\'word\': \'educa\'}, log=""\\nInvoking: `get_word_length` with `{\'word\': \'educa\'}`\\n\\n\\n"", message_log=[AIMessage(content=\'\', additional_kwargs={\'function_call\': {\'arguments\': \'{\\n ""word"": ""educa""\\n}\', \'name\': \'get_word_length\'}})]) ``` We can see that it responds with an `AgentAction` to take (it\'s actually an `AgentActionMessageLog` - a subclass of `AgentAction` which also tracks the full message log). If we\'ve set up LangSmith, we\'ll see a trace that let\'s us inspect the input and output to each step in the sequence: [ ### Define the runtime So this is just the first step - now we need to write a runtime for this. The simplest one is just one that continuously loops, calling the agent, then taking the action, and repeating until an `AgentFinish` is returned. Let\'s code that up below: ```python from langchain.schema.agent import AgentFinish user_input = ""how many letters in the word educa?"" intermediate_steps = [] while True: output = agent.invoke( { ""input"": user_input, ""intermediate_steps"": intermediate_steps, } ) if isinstance(output, AgentFinish): final_result = output.return_values[""output""] break else: print(f""TOOL NAME: {output.tool}"") print(f""TOOL INPUT: {output.tool_input}"") tool = {""get_word_length"": get_word_length}[output.tool] observation = tool.run(output.tool_input) intermediate_steps.append((output, observation)) print(final_result) ``` ```text TOOL NAME: get_word_length TOOL INPUT: {\'word\': \'educa\'} There are 5 letters in the word ""educa"". ``` Woo! It\'s working. ### Using AgentExecutor To simplify this a bit, we can import and use the `AgentExecutor` class. This bundles up all of the above and adds in error handling, early stopping, tracing, and other quality-of-life improvements that reduce safeguards you need to write. ```python from langchain.agents import AgentExecutor agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True) ``` Now let\'s test it out! ```python agent_executor.invoke({""input"": ""how many letters in the word educa?""}) ``` ```text > Entering new AgentExecutor chain... Invoking: `get_word_length` with `{\'word\': \'educa\'}` 5There are 5 letters in the word ""educa"". > Finished chain. {\'input\': \'how many letters in the word educa?\', \'output\': \'There are 5 letters in the word ""educa"".\'} ``` And looking at the trace, we can see that all of our agent calls and tool invocations are automatically logged: [ ### Adding memory This is great - we have an agent! However, this agent is stateless - it doesn\'t remember anything about previous interactions. This means you can\'t ask follow up questions easily. Let\'s fix that by adding in memory. In order to do this, we need to do two things: 1. Add a place for memory variables to go in the prompt 2. Keep track of the chat history First, let\'s add a place for memory in the prompt. We do this by adding a placeholder for messages with the key `""chat_history""`. Notice that we put this ABOVE the new user input (to follow the conversation flow). ```python from langchain.prompts import MessagesPlaceholder MEMORY_KEY = ""chat_history"" prompt = ChatPromptTemplate.from_messages( [ ( ""system"", ""You are very powerful assistant, but bad at calculating lengths of words."", ), MessagesPlaceholder(variable_name=MEMORY_KEY), (""user"", ""{input}""), MessagesPlaceholder(variable_name=""agent_scratchpad""), ] ) ``` We can then set up a list to track the chat history ```python from langchain.schema.messages import AIMessage, HumanMessage chat_history = [] ``` We can then put it all together! ```python agent = ( { ""input"": lambda x: x[""input""], ""agent_scratchpad"": lambda x: format_to_openai_function_messages( x[""intermediate_steps""] ), ""chat_history"": lambda x: x[""chat_history""], } | prompt | llm_with_tools | OpenAIFunctionsAgentOutputParser() ) agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True) ``` When running, we now need to track the inputs and outputs as chat history ```python input1 = ""how many letters in the word educa?"" result = agent_executor.invoke({""input"": input1, ""chat_history"": chat_history}) chat_history.extend( [ HumanMessage(content=input1), AIMessage(content=result[""output""]), ] ) agent_executor.invoke({""input"": ""is that a real word?"", ""chat_history"": chat_history}) ``` ```text > Entering new AgentExecutor chain... Invoking: `get_word_length` with `{\'word\': \'educa\'}` 5There are 5 letters in the word ""educa"". > Finished chain. > Entering new AgentExecutor chain... No, ""educa"" is not a real word in English. > Finished chain. {\'input\': \'is that a real word?\', \'chat_history\': [HumanMessage(content=\'how many letters in the word educa?\'), AIMessage(content=\'There are 5 letters in the word ""educa"".\')], \'output\': \'No, ""educa"" is not a real word in English.\'} ``` Here\'s the LangSmith trace: [ ## Next Steps Awesome! You\'ve now run your first end-to-end agent. To dive deeper, you can: - Check out all the different [agent types](/docs/modules/agents/agent_types/) supported - Learn all the controls for [AgentExecutor](/docs/modules/agents/how_to/) - Explore the how-to\'s of [tools](/docs/modules/agents/tools/) and all the [tool integrations](/docs/integrations/tools) - See a full list of all the off-the-shelf [toolkits](/docs/integrations/toolkits/) we provide - [Concepts](#concepts)- [Agent](#agent) - [Tools](#tools) - [Toolkits](#toolkits) - [AgentExecutor](#agentexecutor) - [Other types of agent runtimes](#other-types-of-agent-runtimes) - [Get started](#get-started)- [Setup: LangSmith](#setup-langsmith) - [Define the agent](#define-the-agent) - [Define the runtime](#define-the-runtime) - [Using AgentExecutor](#using-agentexecutor) - [Adding memory](#adding-memory) - [Next Steps](#next-steps)', 'Conversation Summary | Conversation Summary Now let\'s take a look at using a slightly more complex type of memory - `ConversationSummaryMemory`. This type of memory creates a summary of the conversation over time. This can be useful for condensing information from the conversation over time. Conversation summary memory summarizes the conversation as it happens and stores the current summary in memory. This memory can then be used to inject the summary of the conversation so far into a prompt/chain. This memory is most useful for longer conversations, where keeping the past message history in the prompt verbatim would take up too many tokens. Let\'s first explore the basic functionality of this type of memory. ```python from langchain.memory import ConversationSummaryMemory, ChatMessageHistory from langchain.llms import OpenAI ``` ```python memory = ConversationSummaryMemory(llm=OpenAI(temperature=0)) memory.save_context({""input"": ""hi""}, {""output"": ""whats up""}) ``` ```python memory.load_memory_variables({}) ``` ```text {\'history\': \'\\nThe human greets the AI, to which the AI responds.\'} ``` We can also get the history as a list of messages (this is useful if you are using this with a chat model). ```python memory = ConversationSummaryMemory(llm=OpenAI(temperature=0), return_messages=True) memory.save_context({""input"": ""hi""}, {""output"": ""whats up""}) ``` ```python memory.load_memory_variables({}) ``` ```text {\'history\': [SystemMessage(content=\'\\nThe human greets the AI, to which the AI responds.\', additional_kwargs={})]} ``` We can also utilize the `predict_new_summary` method directly. ```python messages = memory.chat_memory.messages previous_summary = """" memory.predict_new_summary(messages, previous_summary) ``` ```text \'\\nThe human greets the AI, to which the AI responds.\' ``` ## Initializing with messages/existing summary If you have messages outside this class, you can easily initialize the class with `ChatMessageHistory`. During loading, a summary will be calculated. ```python history = ChatMessageHistory() history.add_user_message(""hi"") history.add_ai_message(""hi there!"") ``` ```python memory = ConversationSummaryMemory.from_messages( llm=OpenAI(temperature=0), chat_memory=history, return_messages=True ) ``` ```python memory.buffer ``` ```text \'\\nThe human greets the AI, to which the AI responds with a friendly greeting.\' ``` Optionally you can speed up initialization using a previously generated summary, and avoid regenerating the summary by just initializing directly. ```python memory = ConversationSummaryMemory( llm=OpenAI(temperature=0), buffer=""The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential."", chat_memory=history, return_messages=True ) ``` ## Using in a chain Let\'s walk through an example of using this in a chain, again setting `verbose=True` so we can see the prompt. ```python from langchain.llms import OpenAI from langchain.chains import ConversationChain llm = OpenAI(temperature=0) conversation_with_summary = ConversationChain( llm=llm, memory=ConversationSummaryMemory(llm=OpenAI()), verbose=True ) conversation_with_summary.predict(input=""Hi, what\'s up?"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: Human: Hi, what\'s up? AI: > Finished chain. "" Hi there! I\'m doing great. I\'m currently helping a customer with a technical issue. How about you?"" ``` ```python conversation_with_summary.predict(input=""Tell me more about it!"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: The human greeted the AI and asked how it was doing. The AI replied that it was doing great and was currently helping a customer with a technical issue. Human: Tell me more about it! AI: > Finished chain. "" Sure! The customer is having trouble with their computer not connecting to the internet. I\'m helping them troubleshoot the issue and figure out what the problem is. So far, we\'ve tried resetting the router and checking the network settings, but the issue still persists. We\'re currently looking into other possible solutions."" ``` ```python conversation_with_summary.predict(input=""Very cool -- what is the scope of the project?"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: The human greeted the AI and asked how it was doing. The AI replied that it was doing great and was currently helping a customer with a technical issue where their computer was not connecting to the internet. The AI was troubleshooting the issue and had already tried resetting the router and checking the network settings, but the issue still persisted and they were looking into other possible solutions. Human: Very cool -- what is the scope of the project? AI: > Finished chain. "" The scope of the project is to troubleshoot the customer\'s computer issue and find a solution that will allow them to connect to the internet. We are currently exploring different possibilities and have already tried resetting the router and checking the network settings, but the issue still persists."" ``` - [Initializing with messages/existing summary](#initializing-with-messagesexisting-summary) - [Using in a chain](#using-in-a-chain)']","To add a system message at the end of the conversation history to influence the output of the LLM, you can use the `append_messages` method provided by the `ChatMessageHistory` class in LangChain. Here's an example:

```python
from langchain.memory import ChatMessageHistory
from langchain.llms import OpenAI
from langchain.chains import ConversationChain

# Initialize chat history
chat_history = ChatMessageHistory()

# Add user and AI messages to the history
chat_history.add_user_message(""Hi, how are you?"")
chat_history.add_ai_message(""I'm doing well, thank you!"")

# Create a system message to influence the output
system_message = ""Remind the AI to be more helpful and informative.""

# Append the system message to the chat history
chat_history.append_messages(system_message=system_message)

# Initialize the LLM and the conversation chain
llm = OpenAI(temperature=0)
conversation_with_history = ConversationChain(llm=llm, memory=chat_history, verbose=True)

# Continue the conversation
response = conversation_with_history.predict(input=""What can you tell me about AI?"")
print(response)
```

In this example, the `append_messages` method is used to add the system message to the end of the conversation history. This message will be included in the context when the LLM generates its response, potentially influencing the output to be more helpful and informative.","
```python
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder

prompt_template = ChatPromptTemplate.from_messages(
    [
        (""human"", ""{user_input}""),
        MessagesPlaceholder(variable_name=""chat_history"")
        (""system"", ""New System Instruction here""),
    ]
)
```
When formatted, it will generate 1 human/user message and 1 system message, as well as messages for the whole conversation history.
```python
prompt_template.format_messages(user_input=""Hello"", chat_history=[])
```
[HumanMessage(content='Hello', additional_kwargs={}, example=False),
 SystemMessage(content='New System Instruction here', additional_kwargs={})]
",0.8333333332916666,1.0,,0.0017913246731332897,0.16487455197132617
20,how to use enums in Tools that will be used by functions agent?,"['Agents | Agents The core idea of agents is to use a language model to choose a sequence of actions to take. In chains, a sequence of actions is hardcoded (in code). In agents, a language model is used as a reasoning engine to determine which actions to take and in which order. ## Concepts There are several key components here: ### Agent This is the chain responsible for deciding what step to take next. This is powered by a language model and a prompt. The inputs to this chain are: 1. Tools: Descriptions of available tools 2. User input: The high level objective 3. Intermediate steps: Any (action, tool output) pairs previously executed in order to achieve the user input The output is the next action(s) to take or the final response to send to the user (`AgentAction`s or `AgentFinish`). An action specifies a tool and the input to that tool. Different agents have different prompting styles for reasoning, different ways of encoding inputs, and different ways of parsing the output. For a full list of built-in agents see [agent types](/docs/modules/agents/agent_types/). You can also **easily build custom agents**, which we show how to do in the Get started section below. ### Tools Tools are functions that an agent can invoke. There are two important design considerations around tools: 1. Giving the agent access to the right tools 2. Describing the tools in a way that is most helpful to the agent Without thinking through both, you won\'t be able to build a working agent. If you don\'t give the agent access to a correct set of tools, it will never be able to accomplish the objectives you give it. If you don\'t describe the tools well, the agent won\'t know how to use them properly. LangChain provides a wide set of built-in tools, but also makes it easy to define your own (including custom descriptions). For a full list of built-in tools, see the [tools integrations section](/docs/integrations/tools/) ### Toolkits For many common tasks, an agent will need a set of related tools. For this LangChain provides the concept of toolkits - groups of around 3-5 tools needed to accomplish specific objectives. For example, the GitHub toolkit has a tool for searching through GitHub issues, a tool for reading a file, a tool for commenting, etc. LangChain provides a wide set of toolkits to get started. For a full list of built-in toolkits, see the [toolkits integrations section](/docs/integrations/toolkits/) ### AgentExecutor The agent executor is the runtime for an agent. This is what actually calls the agent, executes the actions it chooses, passes the action outputs back to the agent, and repeats. In pseudocode, this looks roughly like: ```python next_action = agent.get_action(...) while next_action != AgentFinish: observation = run(next_action) next_action = agent.get_action(..., next_action, observation) return next_action ``` While this may seem simple, there are several complexities this runtime handles for you, including: 1. Handling cases where the agent selects a non-existent tool 2. Handling cases where the tool errors 3. Handling cases where the agent produces output that cannot be parsed into a tool invocation 4. Logging and observability at all levels (agent decisions, tool calls) to stdout and/or to [LangSmith](/docs/langsmith). ### Other types of agent runtimes The `AgentExecutor` class is the main agent runtime supported by LangChain. However, there are other, more experimental runtimes we also support. These include: - [Plan-and-execute Agent](/docs/use_cases/more/agents/autonomous_agents/plan_and_execute) - [Baby AGI](/docs/use_cases/more/agents/autonomous_agents/baby_agi) - [Auto GPT](/docs/use_cases/more/agents/autonomous_agents/autogpt) You can also always create your own custom execution logic, which we show how to do below. ## Get started To best understand the agent framework, lets build an agent from scratch using LangChain Expression Language (LCEL). We\'ll need to build the agent itself, define custom tools, and run the agent and tools in a custom loop. At the end we\'ll show how to use the standard LangChain `AgentExecutor` to make execution easier. Some important terminology (and schema) to know: 1. `AgentAction`: This is a dataclass that represents the action an agent should take. It has a `tool` property (which is the name of the tool that should be invoked) and a `tool_input` property (the input to that tool) 2. `AgentFinish`: This is a dataclass that signifies that the agent has finished and should return to the user. It has a `return_values` parameter, which is a dictionary to return. It often only has one key - `output` - that is a string, and so often it is just this key that is returned. 3. `intermediate_steps`: These represent previous agent actions and corresponding outputs that are passed around. These are important to pass to future iteration so the agent knows what work it has already done. This is typed as a `List[Tuple[AgentAction, Any]]`. Note that observation is currently left as type `Any` to be maximally flexible. In practice, this is often a string. ### Setup: LangSmith By definition, agents take a self-determined, input-dependent sequence of steps before returning a user-facing output. This makes debugging these systems particularly tricky, and observability particularly important. [LangSmith](/docs/langsmith) is especially useful for such cases. When building with LangChain, any built-in agent or custom agent built with LCEL will automatically be traced in LangSmith. And if we use the `AgentExecutor`, we\'ll get full tracing of not only the agent planning steps but also the tool inputs and outputs. To set up LangSmith we just need set the following environment variables: ```bash export LANGCHAIN_TRACING_V2=""true"" export LANGCHAIN_API_KEY="""" ``` ### Define the agent We first need to create our agent. This is the chain responsible for determining what action to take next. In this example, we will use OpenAI Function Calling to create this agent. **This is generally the most reliable way to create agents.** For this guide, we will construct a custom agent that has access to a custom tool. We are choosing this example because for most real world use cases you will NEED to customize either the agent or the tools. We\'ll create a simple tool that computes the length of a word. This is useful because it\'s actually something LLMs can mess up due to tokenization. We will first create it WITHOUT memory, but we will then show how to add memory in. Memory is needed to enable conversation. First, let\'s load the language model we\'re going to use to control the agent. ```python from langchain.chat_models import ChatOpenAI llm = ChatOpenAI(model=""gpt-3.5-turbo"", temperature=0) ``` We can see that it struggles to count the letters in the string ""educa"". ```python llm.invoke(""how many letters in the word educa?"") ``` ```text AIMessage(content=\'There are 6 letters in the word ""educa"".\') ``` Next, let\'s define some tools to use. Let\'s write a really simple Python function to calculate the length of a word that is passed in. ```python from langchain.agents import tool @tool def get_word_length(word: str) -> int: """"""Returns the length of a word."""""" return len(word) tools = [get_word_length] ``` Now let us create the prompt. Because OpenAI Function Calling is finetuned for tool usage, we hardly need any instructions on how to reason, or how to output format. We will just have two input variables: `input` and `agent_scratchpad`. `input` should be a string containing the user objective. `agent_scratchpad` should be a sequence of messages that contains the previous agent tool invocations and the corresponding tool outputs. ```python from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder prompt = ChatPromptTemplate.from_messages( [ ( ""system"", ""You are very powerful assistant, but bad at calculating lengths of words."", ), (""user"", ""{input}""), MessagesPlaceholder(variable_name=""agent_scratchpad""), ] ) ``` How does the agent know what tools it can use? In this case we\'re relying on OpenAI function calling LLMs, which take functions as a separate argument and have been specifically trained to know when to invoke those functions. To pass in our tools to the agent, we just need to format them to the OpenAI function format and pass them to our model. (By `bind`-ing the functions, we\'re making sure that they\'re passed in each time the model is invoked.) ```python from langchain.tools.render import format_tool_to_openai_function llm_with_tools = llm.bind(functions=[format_tool_to_openai_function(t) for t in tools]) ``` Putting those pieces together, we can now create the agent. We will import two last utility functions: a component for formatting intermediate steps (agent action, tool output pairs) to input messages that can be sent to the model, and a component for converting the output message into an agent action/agent finish. ```python from langchain.agents.format_scratchpad import format_to_openai_function_messages from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser agent = ( { ""input"": lambda x: x[""input""], ""agent_scratchpad"": lambda x: format_to_openai_function_messages( x[""intermediate_steps""] ), } | prompt | llm_with_tools | OpenAIFunctionsAgentOutputParser() ) ``` Now that we have our agent, let\'s play around with it! Let\'s pass in a simple question and empty intermediate steps and see what it returns: ```python agent.invoke({""input"": ""how many letters in the word educa?"", ""intermediate_steps"": []}) ``` ```text AgentActionMessageLog(tool=\'get_word_length\', tool_input={\'word\': \'educa\'}, log=""\\nInvoking: `get_word_length` with `{\'word\': \'educa\'}`\\n\\n\\n"", message_log=[AIMessage(content=\'\', additional_kwargs={\'function_call\': {\'arguments\': \'{\\n ""word"": ""educa""\\n}\', \'name\': \'get_word_length\'}})]) ``` We can see that it responds with an `AgentAction` to take (it\'s actually an `AgentActionMessageLog` - a subclass of `AgentAction` which also tracks the full message log). If we\'ve set up LangSmith, we\'ll see a trace that let\'s us inspect the input and output to each step in the sequence: [ ### Define the runtime So this is just the first step - now we need to write a runtime for this. The simplest one is just one that continuously loops, calling the agent, then taking the action, and repeating until an `AgentFinish` is returned. Let\'s code that up below: ```python from langchain.schema.agent import AgentFinish user_input = ""how many letters in the word educa?"" intermediate_steps = [] while True: output = agent.invoke( { ""input"": user_input, ""intermediate_steps"": intermediate_steps, } ) if isinstance(output, AgentFinish): final_result = output.return_values[""output""] break else: print(f""TOOL NAME: {output.tool}"") print(f""TOOL INPUT: {output.tool_input}"") tool = {""get_word_length"": get_word_length}[output.tool] observation = tool.run(output.tool_input) intermediate_steps.append((output, observation)) print(final_result) ``` ```text TOOL NAME: get_word_length TOOL INPUT: {\'word\': \'educa\'} There are 5 letters in the word ""educa"". ``` Woo! It\'s working. ### Using AgentExecutor To simplify this a bit, we can import and use the `AgentExecutor` class. This bundles up all of the above and adds in error handling, early stopping, tracing, and other quality-of-life improvements that reduce safeguards you need to write. ```python from langchain.agents import AgentExecutor agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True) ``` Now let\'s test it out! ```python agent_executor.invoke({""input"": ""how many letters in the word educa?""}) ``` ```text > Entering new AgentExecutor chain... Invoking: `get_word_length` with `{\'word\': \'educa\'}` 5There are 5 letters in the word ""educa"". > Finished chain. {\'input\': \'how many letters in the word educa?\', \'output\': \'There are 5 letters in the word ""educa"".\'} ``` And looking at the trace, we can see that all of our agent calls and tool invocations are automatically logged: [ ### Adding memory This is great - we have an agent! However, this agent is stateless - it doesn\'t remember anything about previous interactions. This means you can\'t ask follow up questions easily. Let\'s fix that by adding in memory. In order to do this, we need to do two things: 1. Add a place for memory variables to go in the prompt 2. Keep track of the chat history First, let\'s add a place for memory in the prompt. We do this by adding a placeholder for messages with the key `""chat_history""`. Notice that we put this ABOVE the new user input (to follow the conversation flow). ```python from langchain.prompts import MessagesPlaceholder MEMORY_KEY = ""chat_history"" prompt = ChatPromptTemplate.from_messages( [ ( ""system"", ""You are very powerful assistant, but bad at calculating lengths of words."", ), MessagesPlaceholder(variable_name=MEMORY_KEY), (""user"", ""{input}""), MessagesPlaceholder(variable_name=""agent_scratchpad""), ] ) ``` We can then set up a list to track the chat history ```python from langchain.schema.messages import AIMessage, HumanMessage chat_history = [] ``` We can then put it all together! ```python agent = ( { ""input"": lambda x: x[""input""], ""agent_scratchpad"": lambda x: format_to_openai_function_messages( x[""intermediate_steps""] ), ""chat_history"": lambda x: x[""chat_history""], } | prompt | llm_with_tools | OpenAIFunctionsAgentOutputParser() ) agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True) ``` When running, we now need to track the inputs and outputs as chat history ```python input1 = ""how many letters in the word educa?"" result = agent_executor.invoke({""input"": input1, ""chat_history"": chat_history}) chat_history.extend( [ HumanMessage(content=input1), AIMessage(content=result[""output""]), ] ) agent_executor.invoke({""input"": ""is that a real word?"", ""chat_history"": chat_history}) ``` ```text > Entering new AgentExecutor chain... Invoking: `get_word_length` with `{\'word\': \'educa\'}` 5There are 5 letters in the word ""educa"". > Finished chain. > Entering new AgentExecutor chain... No, ""educa"" is not a real word in English. > Finished chain. {\'input\': \'is that a real word?\', \'chat_history\': [HumanMessage(content=\'how many letters in the word educa?\'), AIMessage(content=\'There are 5 letters in the word ""educa"".\')], \'output\': \'No, ""educa"" is not a real word in English.\'} ``` Here\'s the LangSmith trace: [ ## Next Steps Awesome! You\'ve now run your first end-to-end agent. To dive deeper, you can: - Check out all the different [agent types](/docs/modules/agents/agent_types/) supported - Learn all the controls for [AgentExecutor](/docs/modules/agents/how_to/) - Explore the how-to\'s of [tools](/docs/modules/agents/tools/) and all the [tool integrations](/docs/integrations/tools) - See a full list of all the off-the-shelf [toolkits](/docs/integrations/toolkits/) we provide - [Concepts](#concepts)- [Agent](#agent) - [Tools](#tools) - [Toolkits](#toolkits) - [AgentExecutor](#agentexecutor) - [Other types of agent runtimes](#other-types-of-agent-runtimes) - [Get started](#get-started)- [Setup: LangSmith](#setup-langsmith) - [Define the agent](#define-the-agent) - [Define the runtime](#define-the-runtime) - [Using AgentExecutor](#using-agentexecutor) - [Adding memory](#adding-memory) - [Next Steps](#next-steps)', 'Defining Custom Tools | Defining Custom Tools When constructing your own agent, you will need to provide it with a list of Tools that it can use. Besides the actual function that is called, the Tool consists of several components: - `name` (str), is required and must be unique within a set of tools provided to an agent - `description` (str), is optional but recommended, as it is used by an agent to determine tool use - `return_direct` (bool), defaults to False - `args_schema` (Pydantic BaseModel), is optional but recommended, can be used to provide more information (e.g., few-shot examples) or validation for expected parameters. There are two main ways to define a tool, we will cover both in the example below. ```python # Import things that are needed generically from langchain.agents import AgentType, initialize_agent from langchain.chains import LLMMathChain from langchain.chat_models import ChatOpenAI from langchain.tools import BaseTool, StructuredTool, Tool, tool from langchain.utilities import SerpAPIWrapper ``` Initialize the LLM to use for the agent. ```python llm = ChatOpenAI(temperature=0) ``` ## Completely New Tools - String Input and Output The simplest tools accept a single query string and return a string output. If your tool function requires multiple arguments, you might want to skip down to the `StructuredTool` section below. There are two ways to do this: either by using the Tool dataclass, or by subclassing the BaseTool class. ### Tool dataclass The \'Tool\' dataclass wraps functions that accept a single string input and returns a string output. ```python # Load the tool configs that are needed. search = SerpAPIWrapper() llm_math_chain = LLMMathChain(llm=llm, verbose=True) tools = [ Tool.from_function( func=search.run, name=""Search"", description=""useful for when you need to answer questions about current events"", # coroutine= ... <- you can specify an async method if desired as well ), ] ``` ```text /Users/wfh/code/lc/lckg/langchain/chains/llm_math/base.py:50: UserWarning: Directly instantiating an LLMMathChain with an llm is deprecated. Please instantiate with llm_chain argument or using the from_llm class method. warnings.warn( ``` You can also define a custom `args_schema` to provide more information about inputs. ```python from pydantic import BaseModel, Field class CalculatorInput(BaseModel): question: str = Field() tools.append( Tool.from_function( func=llm_math_chain.run, name=""Calculator"", description=""useful for when you need to answer questions about math"", args_schema=CalculatorInput, # coroutine= ... <- you can specify an async method if desired as well ) ) ``` ```python # Construct the agent. We will use the default agent type here. # See documentation for a full list of options. agent = initialize_agent( tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True ) ``` ```python agent.run( ""Who is Leo DiCaprio\'s girlfriend? What is her current age raised to the 0.43 power?"" ) ``` ```text > Entering new AgentExecutor chain... I need to find out Leo DiCaprio\'s girlfriend\'s name and her age Action: Search Action Input: ""Leo DiCaprio girlfriend"" Observation: After rumours of a romance with Gigi Hadid, the Oscar winner has seemingly moved on. First being linked to the television personality in September 2022, it appears as if his ""age bracket"" has moved up. This follows his rumoured relationship with mere 19-year-old Eden Polani. Thought:I still need to find out his current girlfriend\'s name and age Action: Search Action Input: ""Leo DiCaprio current girlfriend"" Observation: Just Jared on Instagram: Leonardo DiCaprio & girlfriend Camila Morrone couple up for a lunch date! Thought:Now that I know his girlfriend\'s name is Camila Morrone, I need to find her current age Action: Search Action Input: ""Camila Morrone age"" Observation: 25 years Thought:Now that I have her age, I need to calculate her age raised to the 0.43 power Action: Calculator Action Input: 25^(0.43) > Entering new LLMMathChain chain... 25^(0.43)```text 25**(0.43) ``` ...numexpr.evaluate(""25**(0.43)"")... Answer: 3.991298452658078 > Finished chain. Observation: Answer: 3.991298452658078 Thought:I now know the final answer Final Answer: Camila Morrone\'s current age raised to the 0.43 power is approximately 3.99. > Finished chain. ""Camila Morrone\'s current age raised to the 0.43 power is approximately 3.99."" ``` ### Subclassing the BaseTool You can also directly subclass `BaseTool`. This is useful if you want more control over the instance variables or if you want to propagate callbacks to nested chains or other tools. ```python from typing import Optional, Type from langchain.callbacks.manager import ( AsyncCallbackManagerForToolRun, CallbackManagerForToolRun, ) class CustomSearchTool(BaseTool): name = ""custom_search"" description = ""useful for when you need to answer questions about current events"" def _run( self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None ) -> str: """"""Use the tool."""""" return search.run(query) async def _arun( self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None ) -> str: """"""Use the tool asynchronously."""""" raise NotImplementedError(""custom_search does not support async"") class CustomCalculatorTool(BaseTool): name = ""Calculator"" description = ""useful for when you need to answer questions about math"" args_schema: Type[BaseModel] = CalculatorInput def _run( self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None ) -> str: """"""Use the tool."""""" return llm_math_chain.run(query) async def _arun( self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None ) -> str: """"""Use the tool asynchronously."""""" raise NotImplementedError(""Calculator does not support async"") ``` ```python tools = [CustomSearchTool(), CustomCalculatorTool()] agent = initialize_agent( tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True ) ``` ```python agent.run( ""Who is Leo DiCaprio\'s girlfriend? What is her current age raised to the 0.43 power?"" ) ``` ```text > Entering new AgentExecutor chain... I need to use custom_search to find out who Leo DiCaprio\'s girlfriend is, and then use the Calculator to raise her age to the 0.43 power. Action: custom_search Action Input: ""Leo DiCaprio girlfriend"" Observation: After rumours of a romance with Gigi Hadid, the Oscar winner has seemingly moved on. First being linked to the television personality in September 2022, it appears as if his ""age bracket"" has moved up. This follows his rumoured relationship with mere 19-year-old Eden Polani. Thought:I need to find out the current age of Eden Polani. Action: custom_search Action Input: ""Eden Polani age"" Observation: 19 years old Thought:Now I can use the Calculator to raise her age to the 0.43 power. Action: Calculator Action Input: 19 ^ 0.43 > Entering new LLMMathChain chain... 19 ^ 0.43```text 19 ** 0.43 ``` ...numexpr.evaluate(""19 ** 0.43"")... Answer: 3.547023357958959 > Finished chain. Observation: Answer: 3.547023357958959 Thought:I now know the final answer. Final Answer: 3.547023357958959 > Finished chain. \'3.547023357958959\' ``` ### Using the decorator To make it easier to define custom tools, a `@tool` decorator is provided. This decorator can be used to quickly create a `Tool` from a simple function. The decorator uses the function name as the tool name by default, but this can be overridden by passing a string as the first argument. Additionally, the decorator will use the function\'s docstring as the tool\'s description. ```python from langchain.tools import tool @tool def search_api(query: str) -> str: """"""Searches the API for the query."""""" return f""Results for query {query}"" search_api ``` You can also provide arguments like the tool name and whether to return directly. ```python @tool(""search"", return_direct=True) def search_api(query: str) -> str: """"""Searches the API for the query."""""" return ""Results"" ``` ```python search_api ``` ```text Tool(name=\'search\', description=\'search(query: str) -> str - Searches the API for the query.\', args_schema=, return_direct=True, verbose=False, callback_manager=, func=, coroutine=None) ``` You can also provide `args_schema` to provide more information about the argument. ```python class SearchInput(BaseModel): query: str = Field(description=""should be a search query"") @tool(""search"", return_direct=True, args_schema=SearchInput) def search_api(query: str) -> str: """"""Searches the API for the query."""""" return ""Results"" ``` ```python search_api ``` ```text Tool(name=\'search\', description=\'search(query: str) -> str - Searches the API for the query.\', args_schema=, return_direct=True, verbose=False, callback_manager=, func=, coroutine=None) ``` ## Custom Structured Tools If your functions require more structured arguments, you can use the `StructuredTool` class directly, or still subclass the `BaseTool` class. ### StructuredTool dataclass To dynamically generate a structured tool from a given function, the fastest way to get started is with `StructuredTool.from_function()`. ```python import requests from langchain.tools import StructuredTool def post_message(url: str, body: dict, parameters: Optional[dict] = None) -> str: """"""Sends a POST request to the given url with the given body and parameters."""""" result = requests.post(url, json=body, params=parameters) return f""Status: {result.status_code} - {result.text}"" tool = StructuredTool.from_function(post_message) ``` ### Subclassing the BaseTool The BaseTool automatically infers the schema from the `_run` method\'s signature. ```python from typing import Optional, Type from langchain.callbacks.manager import ( AsyncCallbackManagerForToolRun, CallbackManagerForToolRun, ) class CustomSearchTool(BaseTool): name = ""custom_search"" description = ""useful for when you need to answer questions about current events"" def _run( self, query: str, engine: str = ""google"", gl: str = ""us"", hl: str = ""en"", run_manager: Optional[CallbackManagerForToolRun] = None, ) -> str: """"""Use the tool."""""" search_wrapper = SerpAPIWrapper(params={""engine"": engine, ""gl"": gl, ""hl"": hl}) return search_wrapper.run(query) async def _arun( self, query: str, engine: str = ""google"", gl: str = ""us"", hl: str = ""en"", run_manager: Optional[AsyncCallbackManagerForToolRun] = None, ) -> str: """"""Use the tool asynchronously."""""" raise NotImplementedError(""custom_search does not support async"") # You can provide a custom args schema to add descriptions or custom validation class SearchSchema(BaseModel): query: str = Field(description=""should be a search query"") engine: str = Field(description=""should be a search engine"") gl: str = Field(description=""should be a country code"") hl: str = Field(description=""should be a language code"") class CustomSearchTool(BaseTool): name = ""custom_search"" description = ""useful for when you need to answer questions about current events"" args_schema: Type[SearchSchema] = SearchSchema def _run( self, query: str, engine: str = ""google"", gl: str = ""us"", hl: str = ""en"", run_manager: Optional[CallbackManagerForToolRun] = None, ) -> str: """"""Use the tool."""""" search_wrapper = SerpAPIWrapper(params={""engine"": engine, ""gl"": gl, ""hl"": hl}) return search_wrapper.run(query) async def _arun( self, query: str, engine: str = ""google"", gl: str = ""us"", hl: str = ""en"", run_manager: Optional[AsyncCallbackManagerForToolRun] = None, ) -> str: """"""Use the tool asynchronously."""""" raise NotImplementedError(""custom_search does not support async"") ``` ### Using the decorator The `tool` decorator creates a structured tool automatically if the signature has multiple arguments. ```python import requests from langchain.tools import tool @tool def post_message(url: str, body: dict, parameters: Optional[dict] = None) -> str: """"""Sends a POST request to the given url with the given body and parameters."""""" result = requests.post(url, json=body, params=parameters) return f""Status: {result.status_code} - {result.text}"" ``` ## Modify existing tools Now, we show how to load existing tools and modify them directly. In the example below, we do something really simple and change the Search tool to have the name `Google Search`. ```python from langchain.agents import load_tools ``` ```python tools = load_tools([""serpapi"", ""llm-math""], llm=llm) ``` ```python tools[0].name = ""Google Search"" ``` ```python agent = initialize_agent( tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True ) ``` ```python agent.run( ""Who is Leo DiCaprio\'s girlfriend? What is her current age raised to the 0.43 power?"" ) ``` ```text > Entering new AgentExecutor chain... I need to find out Leo DiCaprio\'s girlfriend\'s name and her age. Action: Google Search Action Input: ""Leo DiCaprio girlfriend"" Observation: After rumours of a romance with Gigi Hadid, the Oscar winner has seemingly moved on. First being linked to the television personality in September 2022, it appears as if his ""age bracket"" has moved up. This follows his rumoured relationship with mere 19-year-old Eden Polani. Thought:I still need to find out his current girlfriend\'s name and her age. Action: Google Search Action Input: ""Leo DiCaprio current girlfriend age"" Observation: Leonardo DiCaprio has been linked with 19-year-old model Eden Polani, continuing the rumour that he doesn\'t date any women over the age of ... Thought:I need to find out the age of Eden Polani. Action: Calculator Action Input: 19^(0.43) Observation: Answer: 3.547023357958959 Thought:I now know the final answer. Final Answer: The age of Leo DiCaprio\'s girlfriend raised to the 0.43 power is approximately 3.55. > Finished chain. ""The age of Leo DiCaprio\'s girlfriend raised to the 0.43 power is approximately 3.55."" ``` ## Defining the priorities among Tools When you made a Custom tool, you may want the Agent to use the custom tool more than normal tools. For example, you made a custom tool, which gets information on music from your database. When a user wants information on songs, You want the Agent to use `the custom tool` more than the normal `Search tool`. But the Agent might prioritize a normal Search tool. This can be accomplished by adding a statement such as `Use this more than the normal search if the question is about Music, like \'who is the singer of yesterday?\' or \'what is the most popular song in 2022?\'` to the description. An example is below. ```python # Import things that are needed generically from langchain.agents import AgentType, Tool, initialize_agent from langchain.chains import LLMMathChain from langchain.llms import OpenAI from langchain.utilities import SerpAPIWrapper search = SerpAPIWrapper() tools = [ Tool( name=""Search"", func=search.run, description=""useful for when you need to answer questions about current events"", ), Tool( name=""Music Search"", func=lambda x: ""\'All I Want For Christmas Is You\' by Mariah Carey."", # Mock Function description=""A Music search engine. Use this more than the normal search if the question is about Music, like \'who is the singer of yesterday?\' or \'what is the most popular song in 2022?\'"", ), ] agent = initialize_agent( tools, OpenAI(temperature=0), agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True, ) ``` ```python agent.run(""what is the most famous song of christmas"") ``` ```text > Entering new AgentExecutor chain... I should use a music search engine to find the answer Action: Music Search Action Input: most famous song of christmas\'All I Want For Christmas Is You\' by Mariah Carey. I now know the final answer Final Answer: \'All I Want For Christmas Is You\' by Mariah Carey. > Finished chain. ""\'All I Want For Christmas Is You\' by Mariah Carey."" ``` ## Using tools to return directly Often, it can be desirable to have a tool output returned directly to the user, if it\'s called. You can do this easily with LangChain by setting the `return_direct` flag for a tool to be True. ```python llm_math_chain = LLMMathChain(llm=llm) tools = [ Tool( name=""Calculator"", func=llm_math_chain.run, description=""useful for when you need to answer questions about math"", return_direct=True, ) ] ``` ```python llm = OpenAI(temperature=0) agent = initialize_agent( tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True ) ``` ```python agent.run(""whats 2**.12"") ``` ```text > Entering new AgentExecutor chain... I need to calculate this Action: Calculator Action Input: 2**.12Answer: 1.086734862526058 > Finished chain. \'Answer: 1.086734862526058\' ``` ## Handling Tool Errors When a tool encounters an error and the exception is not caught, the agent will stop executing. If you want the agent to continue execution, you can raise a `ToolException` and set `handle_tool_error` accordingly. When `ToolException` is thrown, the agent will not stop working, but will handle the exception according to the `handle_tool_error` variable of the tool, and the processing result will be returned to the agent as observation, and printed in red. You can set `handle_tool_error` to `True`, set it a unified string value, or set it as a function. If it\'s set as a function, the function should take a `ToolException` as a parameter and return a `str` value. Please note that only raising a `ToolException` won\'t be effective. You need to first set the `handle_tool_error` of the tool because its default value is `False`. ```python from langchain.agents import AgentType, initialize_agent from langchain.chat_models import ChatOpenAI from langchain.tools import Tool from langchain.tools.base import ToolException from langchain.utilities import SerpAPIWrapper def _handle_error(error: ToolException) -> str: return ( ""The following errors occurred during tool execution:"" + error.args[0] + ""Please try another tool."" ) def search_tool1(s: str): raise ToolException(""The search tool1 is not available."") def search_tool2(s: str): raise ToolException(""The search tool2 is not available."") search_tool3 = SerpAPIWrapper() ``` ```python description = ""useful for when you need to answer questions about current events.You should give priority to using it."" tools = [ Tool.from_function( func=search_tool1, name=""Search_tool1"", description=description, handle_tool_error=True, ), Tool.from_function( func=search_tool2, name=""Search_tool2"", description=description, handle_tool_error=_handle_error, ), Tool.from_function( func=search_tool3.run, name=""Search_tool3"", description=""useful for when you need to answer questions about current events"", ), ] agent = initialize_agent( tools, ChatOpenAI(temperature=0), agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True, ) ``` ```python agent.run(""Who is Leo DiCaprio\'s girlfriend?"") ``` ```text > Entering new AgentExecutor chain... I should use Search_tool1 to find recent news articles about Leo DiCaprio\'s personal life. Action: Search_tool1 Action Input: ""Leo DiCaprio girlfriend"" Observation: The search tool1 is not available. Thought:I should try using Search_tool2 instead. Action: Search_tool2 Action Input: ""Leo DiCaprio girlfriend"" Observation: The following errors occurred during tool execution:The search tool2 is not available.Please try another tool. Thought:I should try using Search_tool3 as a last resort. Action: Search_tool3 Action Input: ""Leo DiCaprio girlfriend"" Observation: Leonardo DiCaprio and Gigi Hadid were recently spotted at a pre-Oscars party, sparking interest once again in their rumored romance. The Revenant actor and the model first made headlines when they were spotted together at a New York Fashion Week afterparty in September 2022. Thought:Based on the information from Search_tool3, it seems that Gigi Hadid is currently rumored to be Leo DiCaprio\'s girlfriend. Final Answer: Gigi Hadid is currently rumored to be Leo DiCaprio\'s girlfriend. > Finished chain. ""Gigi Hadid is currently rumored to be Leo DiCaprio\'s girlfriend."" ``` - [Completely New Tools - String Input and Output](#completely-new-tools---string-input-and-output)- [Tool dataclass](#tool-dataclass) - [Subclassing the BaseTool](#subclassing-the-basetool) - [Using the decorator](#using-the-decorator) - [Custom Structured Tools](#custom-structured-tools)- [StructuredTool dataclass](#structuredtool-dataclass) - [Subclassing the BaseTool](#subclassing-the-basetool-1) - [Using the decorator](#using-the-decorator-1) - [Modify existing tools](#modify-existing-tools) - [Defining the priorities among Tools](#defining-the-priorities-among-tools) - [Using tools to return directly](#using-tools-to-return-directly) - [Handling Tool Errors](#handling-tool-errors)', 'CSV | CSV This notebook shows how to use agents to interact with data in `CSV` format. It is mostly optimized for question answering. **NOTE: this agent calls the Pandas DataFrame agent under the hood, which in turn calls the Python agent, which executes LLM generated Python code - this can be bad if the LLM generated Python code is harmful. Use cautiously.** ```python from langchain.agents.agent_types import AgentType from langchain.chat_models import ChatOpenAI from langchain.llms import OpenAI from langchain_experimental.agents.agent_toolkits import create_csv_agent ``` ## Using ZERO_SHOT_REACT_DESCRIPTION This shows how to initialize the agent using the `ZERO_SHOT_REACT_DESCRIPTION` agent type. ```python agent = create_csv_agent( OpenAI(temperature=0), ""titanic.csv"", verbose=True, agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION, ) ``` ## Using OpenAI Functions This shows how to initialize the agent using the OPENAI_FUNCTIONS agent type. Note that this is an alternative to the above. ```python agent = create_csv_agent( ChatOpenAI(temperature=0, model=""gpt-3.5-turbo-0613""), ""titanic.csv"", verbose=True, agent_type=AgentType.OPENAI_FUNCTIONS, ) ``` ```python agent.run(""how many rows are there?"") ``` ```text Error in on_chain_start callback: \'name\' Invoking: `python_repl_ast` with `df.shape[0]` 891There are 891 rows in the dataframe. > Finished chain. \'There are 891 rows in the dataframe.\' ``` ```python agent.run(""how many people have more than 3 siblings"") ``` ```text Error in on_chain_start callback: \'name\' Invoking: `python_repl_ast` with `df[df[\'SibSp\'] > 3][\'PassengerId\'].count()` 30There are 30 people in the dataframe who have more than 3 siblings. > Finished chain. \'There are 30 people in the dataframe who have more than 3 siblings.\' ``` ```python agent.run(""whats the square root of the average age?"") ``` ```text Error in on_chain_start callback: \'name\' Invoking: `python_repl_ast` with `import pandas as pd import math # Create a dataframe data = {\'Age\': [22, 38, 26, 35, 35]} df = pd.DataFrame(data) # Calculate the average age average_age = df[\'Age\'].mean() # Calculate the square root of the average age square_root = math.sqrt(average_age) square_root` 5.585696017507576The square root of the average age is approximately 5.59. > Finished chain. \'The square root of the average age is approximately 5.59.\' ``` ### Multi CSV Example This next part shows how the agent can interact with multiple csv files passed in as a list. ```python agent = create_csv_agent( ChatOpenAI(temperature=0, model=""gpt-3.5-turbo-0613""), [""titanic.csv"", ""titanic_age_fillna.csv""], verbose=True, agent_type=AgentType.OPENAI_FUNCTIONS, ) agent.run(""how many rows in the age column are different between the two dfs?"") ``` ```text Error in on_chain_start callback: \'name\' Invoking: `python_repl_ast` with `df1[\'Age\'].nunique() - df2[\'Age\'].nunique()` -1There is 1 row in the age column that is different between the two dataframes. > Finished chain. \'There is 1 row in the age column that is different between the two dataframes.\' ``` - [Using ZERO_SHOT_REACT_DESCRIPTION](#using-zero_shot_react_description) - [Using OpenAI Functions](#using-openai-functions)- [Multi CSV Example](#multi-csv-example)']","To use enums in tools for a function agent, define an enum class and specify it in the args_schema of the tool. Here's an example:

```python
from enum import Enum
from langchain.tools import Tool, StructuredTool
from pydantic import BaseModel, Field

class Color(str, Enum):
    RED = ""red""
    GREEN = ""green""
    BLUE = ""blue""

class ColorToolInput(BaseModel):
    color: Color = Field(description=""Choose a color"")

def get_color_info(color: Color) -> str:
    return f""The color is {color.value}""

color_tool = StructuredTool.from_function(
    func=get_color_info,
    description=""Get information about a color"",
    args_schema=ColorToolInput
)

tools = [color_tool]
```

In this example, `Color` is an enum, and `ColorToolInput` specifies that the `color` argument should be an instance of `Color`. The `StructuredTool` uses this schema to validate inputs.","If you're defining a dynamic tool (either using the @tool decorator or the `.from_function(...) classmethod), simply using the enum typehint is enough. If you are manually specifying an `args_schema` as a pydantic basemodel, you can specify the enum as the argument typehint there.",0.99999999995,1.0,1.0,0.07319413979398202,0.1511627906976744
21,How do I deal with openai rate limiting by having a backup model? Show me with code.,"['Fallbacks | Fallbacks When working with language models, you may often encounter issues from the underlying APIs, whether these be rate limiting or downtime. Therefore, as you go to move your LLM applications into production it becomes more and more important to safeguard against these. That\'s why we\'ve introduced the concept of fallbacks. A **fallback** is an alternative plan that may be used in an emergency. Crucially, fallbacks can be applied not only on the LLM level but on the whole runnable level. This is important because often times different models require different prompts. So if your call to OpenAI fails, you don\'t just want to send the same prompt to Anthropic - you probably want to use a different prompt template and send a different version there. ## Fallback for LLM API Errors This is maybe the most common use case for fallbacks. A request to an LLM API can fail for a variety of reasons - the API could be down, you could have hit rate limits, any number of things. Therefore, using fallbacks can help protect against these types of things. IMPORTANT: By default, a lot of the LLM wrappers catch errors and retry. You will most likely want to turn those off when working with fallbacks. Otherwise the first wrapper will keep on retrying and not failing. ```python from langchain.chat_models import ChatAnthropic, ChatOpenAI ``` First, let\'s mock out what happens if we hit a RateLimitError from OpenAI ```python from unittest.mock import patch from openai.error import RateLimitError ``` ```python # Note that we set max_retries = 0 to avoid retrying on RateLimits, etc openai_llm = ChatOpenAI(max_retries=0) anthropic_llm = ChatAnthropic() llm = openai_llm.with_fallbacks([anthropic_llm]) ``` ```python # Let\'s use just the OpenAI LLm first, to show that we run into an error with patch(""openai.ChatCompletion.create"", side_effect=RateLimitError()): try: print(openai_llm.invoke(""Why did the chicken cross the road?"")) except: print(""Hit error"") ``` ```text Hit error ``` ```python # Now let\'s try with fallbacks to Anthropic with patch(""openai.ChatCompletion.create"", side_effect=RateLimitError()): try: print(llm.invoke(""Why did the chicken cross the road?"")) except: print(""Hit error"") ``` ```text content=\' I don\\\'t actually know why the chicken crossed the road, but here are some possible humorous answers:\\n\\n- To get to the other side!\\n\\n- It was too chicken to just stand there. \\n\\n- It wanted a change of scenery.\\n\\n- It wanted to show the possum it could be done.\\n\\n- It was on its way to a poultry farmers\\\' convention.\\n\\nThe joke plays on the double meaning of ""the other side"" - literally crossing the road to the other side, or the ""other side"" meaning the afterlife. So it\\\'s an anti-joke, with a silly or unexpected pun as the answer.\' additional_kwargs={} example=False ``` We can use our ""LLM with Fallbacks"" as we would a normal LLM. ```python from langchain.prompts import ChatPromptTemplate prompt = ChatPromptTemplate.from_messages( [ ( ""system"", ""You\'re a nice assistant who always includes a compliment in your response"", ), (""human"", ""Why did the {animal} cross the road""), ] ) chain = prompt | llm with patch(""openai.ChatCompletion.create"", side_effect=RateLimitError()): try: print(chain.invoke({""animal"": ""kangaroo""})) except: print(""Hit error"") ``` ```text content="" I don\'t actually know why the kangaroo crossed the road, but I can take a guess! Here are some possible reasons:\\n\\n- To get to the other side (the classic joke answer!)\\n\\n- It was trying to find some food or water \\n\\n- It was trying to find a mate during mating season\\n\\n- It was fleeing from a predator or perceived threat\\n\\n- It was disoriented and crossed accidentally \\n\\n- It was following a herd of other kangaroos who were crossing\\n\\n- It wanted a change of scenery or environment \\n\\n- It was trying to reach a new habitat or territory\\n\\nThe real reason is unknown without more context, but hopefully one of those potential explanations does the joke justice! Let me know if you have any other animal jokes I can try to decipher."" additional_kwargs={} example=False ``` ## Fallback for Sequences We can also create fallbacks for sequences, that are sequences themselves. Here we do that with two different models: ChatOpenAI and then normal OpenAI (which does not use a chat model). Because OpenAI is NOT a chat model, you likely want a different prompt. ```python # First let\'s create a chain with a ChatModel # We add in a string output parser here so the outputs between the two are the same type from langchain.schema.output_parser import StrOutputParser chat_prompt = ChatPromptTemplate.from_messages( [ ( ""system"", ""You\'re a nice assistant who always includes a compliment in your response"", ), (""human"", ""Why did the {animal} cross the road""), ] ) # Here we\'re going to use a bad model name to easily create a chain that will error chat_model = ChatOpenAI(model_name=""gpt-fake"") bad_chain = chat_prompt | chat_model | StrOutputParser() ``` ```python # Now lets create a chain with the normal OpenAI model from langchain.llms import OpenAI from langchain.prompts import PromptTemplate prompt_template = """"""Instructions: You should always include a compliment in your response. Question: Why did the {animal} cross the road?"""""" prompt = PromptTemplate.from_template(prompt_template) llm = OpenAI() good_chain = prompt | llm ``` ```python # We can now create a final chain which combines the two chain = bad_chain.with_fallbacks([good_chain]) chain.invoke({""animal"": ""turtle""}) ``` ```text \'\\n\\nAnswer: The turtle crossed the road to get to the other side, and I have to say he had some impressive determination.\' ``` ## Fallback for Long Inputs One of the big limiting factors of LLMs is their context window. Usually, you can count and track the length of prompts before sending them to an LLM, but in situations where that is hard/complicated, you can fallback to a model with a longer context length. ```python short_llm = ChatOpenAI() long_llm = ChatOpenAI(model=""gpt-3.5-turbo-16k"") llm = short_llm.with_fallbacks([long_llm]) ``` ```python inputs = ""What is the next number: "" + "", "".join([""one"", ""two""] * 3000) ``` ```python try: print(short_llm.invoke(inputs)) except Exception as e: print(e) ``` ```text This model\'s maximum context length is 4097 tokens. However, your messages resulted in 12012 tokens. Please reduce the length of the messages. ``` ```python try: print(llm.invoke(inputs)) except Exception as e: print(e) ``` ```text content=\'The next number in the sequence is two.\' additional_kwargs={} example=False ``` ## Fallback to Better Model Often times we ask models to output format in a specific format (like JSON). Models like GPT-3.5 can do this okay, but sometimes struggle. This naturally points to fallbacks - we can try with GPT-3.5 (faster, cheaper), but then if parsing fails we can use GPT-4. ```python from langchain.output_parsers import DatetimeOutputParser ``` ```python prompt = ChatPromptTemplate.from_template( ""what time was {event} (in %Y-%m-%dT%H:%M:%S.%fZ format - only return this value)"" ) ``` ```python # In this case we are going to do the fallbacks on the LLM + output parser level # Because the error will get raised in the OutputParser openai_35 = ChatOpenAI() | DatetimeOutputParser() openai_4 = ChatOpenAI(model=""gpt-4"") | DatetimeOutputParser() ``` ```python only_35 = prompt | openai_35 fallback_4 = prompt | openai_35.with_fallbacks([openai_4]) ``` ```python try: print(only_35.invoke({""event"": ""the superbowl in 1994""})) except Exception as e: print(f""Error: {e}"") ``` ```text Error: Could not parse datetime string: The Super Bowl in 1994 took place on January 30th at 3:30 PM local time. Converting this to the specified format (%Y-%m-%dT%H:%M:%S.%fZ) results in: 1994-01-30T15:30:00.000Z ``` ```python try: print(fallback_4.invoke({""event"": ""the superbowl in 1994""})) except Exception as e: print(f""Error: {e}"") ``` ```text 1994-01-30 15:30:00 ``` - [Fallback for LLM API Errors](#fallback-for-llm-api-errors) - [Fallback for Sequences](#fallback-for-sequences) - [Fallback for Long Inputs](#fallback-for-long-inputs) - [Fallback to Better Model](#fallback-to-better-model)', 'Metaphor Search | Metaphor Search Metaphor is a search engine fully designed to be used by LLMs. You can search and then get the contents for any page. This notebook goes over how to use Metaphor search. First, you need to set up the proper API keys and environment variables. Get 1000 free searches/month [here]( Then enter your API key as an environment variable. ```python import os os.environ[""METAPHOR_API_KEY""] = ""..."" ``` ## Using their SDK This is the newer and more supported way to use the Metaphor API - via their SDK ```python # !pip install metaphor-python ``` ```python from metaphor_python import Metaphor client = Metaphor(api_key=os.environ[""METAPHOR_API_KEY""]) ``` ```python from typing import List from langchain.agents import tool ``` ```python @tool def search(query: str): """"""Call search engine with a query."""""" return client.search(query, use_autoprompt=True, num_results=5) @tool def get_contents(ids: List[str]): """"""Get contents of a webpage. The ids passed in should be a list of ids as fetched from `search`. """""" return client.get_contents(ids) @tool def find_similar(url: str): """"""Get search results similar to a given URL. The url passed in should be a URL returned from `search` """""" return client.find_similar(url, num_results=5) ``` ```python tools = [search, get_contents, find_similar] ``` ### Use in an agent ```python from langchain.chat_models import ChatOpenAI llm = ChatOpenAI(temperature=0) ``` ```python from langchain.agents import OpenAIFunctionsAgent from langchain.schema import SystemMessage system_message = SystemMessage( content=""You are a web researcher who uses search engines to look up information."" ) prompt = OpenAIFunctionsAgent.create_prompt(system_message=system_message) agent = OpenAIFunctionsAgent(llm=llm, tools=tools, prompt=prompt) ``` ```python from langchain.agents import AgentExecutor agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True) ``` ```python agent_executor.run(""Find the hottest AI agent startups and what they do"") ``` ```text > Entering new AgentExecutor chain... Invoking: `search` with `{\'query\': \'hottest AI agent startups\'}` SearchResponse(results=[Result(title=\'A Search Engine for Machine Intelligence\', url=\' id=\'bdYc6hvHww_JvLv9k8NhPA\', score=0.19460266828536987, published_date=\'2023-01-01\', author=None, extract=None), Result(title=\'Adept: Useful General Intelligence\', url=\' id=\'aNBppxBZvQRZMov6sFVj9g\', score=0.19103890657424927, published_date=\'2000-01-01\', author=None, extract=None), Result(title=\'HiOperator | Generative AI-Enhanced Customer Service\', url=\' id=\'jieb6sB53mId3EDo0z-SDw\', score=0.18549954891204834, published_date=\'2000-01-01\', author=None, extract=None), Result(title=\'Home - Stylo\', url=\' id=\'kUiCuCjJYMD4N0NXdCtqlQ\', score=0.1837376356124878, published_date=\'2000-01-01\', author=None, extract=None), Result(title=\'DirectAI\', url=\' id=\'45iSS8KnJ9tL1ilPg3dL9A\', score=0.1835256814956665, published_date=\'2023-01-01\', author=None, extract=None), Result(title=\'Sidekick AI | Customer Service Automated\', url=\' id=\'nCoPMUtqWQqhUvsdTjJT6A\', score=0.18215584754943848, published_date=\'2020-01-01\', author=None, extract=None), Result(title=\'Hebbia - Search, Reinvented\', url=\' id=\'Zy0YaekZdd4rurPQKkys7A\', score=0.1799020767211914, published_date=\'2023-01-01\', author=None, extract=None), Result(title=\'AI.XYZ\', url=\' id=\'A5c1ePEvsaQeml2Kui_-vA\', score=0.1797989457845688, published_date=\'2023-01-01\', author=None, extract=None), Result(title=\'Halist AI\', url=\' id=\'-lKPLSb4N4dgMZlTgoDvJg\', score=0.17975398898124695, published_date=\'2023-03-01\', author=None, extract=None), Result(title=\'Clone your best expert\', url=\' id=\'_XIjx1YLPfI4cKePIEc_bQ\', score=0.17957791686058044, published_date=\'2016-02-12\', author=None, extract=None)], api=) Invoking: `get_contents` with `{\'ids\': [\'bdYc6hvHww_JvLv9k8NhPA\', \'aNBppxBZvQRZMov6sFVj9g\', \'jieb6sB53mId3EDo0z-SDw\', \'kUiCuCjJYMD4N0NXdCtqlQ\', \'45iSS8KnJ9tL1ilPg3dL9A\', \'nCoPMUtqWQqhUvsdTjJT6A\', \'Zy0YaekZdd4rurPQKkys7A\', \'A5c1ePEvsaQeml2Kui_-vA\', \'-lKPLSb4N4dgMZlTgoDvJg\', \'_XIjx1YLPfI4cKePIEc_bQ\']}` GetContentsResponse(contents=[DocumentContent(id=\'bdYc6hvHww_JvLv9k8NhPA\', url=\' title=\'A Search Engine for Machine Intelligence\', extract=""More OpinionsGet responses from multiple AIsDon\'t rely on a single source of truth, explore the full space of machine intelligence and get highly tailored results.""), DocumentContent(id=\'aNBppxBZvQRZMov6sFVj9g\', url=\' title=\'Adept: Useful General Intelligence\', extract=\'Useful General Intelligence\'), DocumentContent(id=\'jieb6sB53mId3EDo0z-SDw\', url=\' title=\'HiOperator | Generative AI-Enhanced Customer Service\', extract=""Generative AI-Enhanced Customer Support AutomationFlexible, Scalable Customer SupportWhy HiOperator?Truly scalable customer serviceA digital-first customer service provider that changes all the rules of what\'s possible. Scalable. 100% US-Based. Effortless. HiOperator is the digital payoff.Next-Gen Customer ServiceScaling with HiOperator\'s SuperagentsHiOperator is only possible in the digital era. Our revolutionary software connects with your systems to empower our agents to learn quickly and deliver incredible accuracy. Train Us OnceWe handle all of the recruiting, hiring, and training moving forward. Never have to deal with another classroom retraining or head count headaches.Send Us TicketsWe pull tickets automatically from your preferred CRM vendor into our custom system. You have full control over how and when we get tickets.Pay per resolutionWe charge for each conversation we solve. No onboarding fees. No hourly rates. Pay for what you use.Customer ExperienceInsights &amp;\\xa0NewsLet\'s transform your customer service.We can onboard in a matter of days and we offer highly flexible contracts. Whether you need a large team to handle your support or some overflow assistance, getting started is easy.We can onboard in a matter of days and we offer highly flexible contracts. Whether you need a large team to handle your support or some overflow assistance, getting started is easy.""), DocumentContent(id=\'kUiCuCjJYMD4N0NXdCtqlQ\', url=\' title=\'Home - Stylo\', extract=\'Stop angry customers from breaking support \\x80\\x9cWe solve 99 tickets perfectly \\x9f\\x98\\x87 but the 1 we miss lands in the CEO\\x80\\x99s inbox \\x9f\\x98\\x80\\x9d\\x80\\x8dThat 1 costly ticket breaks your process, metrics, and the will of your team. Angry customers make support teams less effective, which makes customers angrier in return.\\x80\\x8dStylo is AI that tells you where to most effectively spend your time to improve the customer experience. This leads to happier customers, employees, and reduces churn.\\x80\\x8dNo setup, no learning curve, just plug it in and go.\\x80\\x9cI\\x80\\x99m able to better manage the team because I can pinpoint gaps in the team\\x80\\x99s knowledge or training, and find room for process improvements.\\x80\\x9d\'), DocumentContent(id=\'45iSS8KnJ9tL1ilPg3dL9A\', url=\' title=\'DirectAI\', extract=""Vision models without training data.Build and deploy powerful computer vision models with plain language.No code or training required.Fundamentally different.We use large language models and zero-shot learning to instantly build models that fit your description.We\'re removing the last major barrier to creating custom models - training data.Deploy and iterate in seconds with DirectAI Don\'t spend time assembling training data. Don\'t pay a third party to label your data. Don\'t pay to train your model. Don\'t spend months finetuning your model\'s behavior.Venture-backed.Based in NYC.We\'re changing how people use AI in the real world.Come talk to us on .""), DocumentContent(id=\'nCoPMUtqWQqhUvsdTjJT6A\', url=\' title=\'Sidekick AI | Customer Service Automated\', extract=\'Hi, I am an AI named Jenny, working at Pizza Planet. How can I help you today?How much are large pizzas with 1 topping?For most toppings, a large with one topping would be $10.99.Ok, can I order a large with pepperoniSure! Takeout or delivery?Alright, order placed. See you at 5 pm!Meet Sidekick\\n Sidekick is an AI agent built to hold natural and dynamic conversations with your customers and talk just like a human.Built on the world\\\'s most advanced AI models, Sidekick pushes the state of the art in natural conversation and converses seamlessly with your customers.\\n Try it out Try it out An AI agent designed for service-led growth.PersonalEvery customer is different, and has unique needs. Our agents are built to provide personalized service depending on the customer\\\'s needs.FastUnlike humans, our Sidekicks respond near-instantly, any time of the day. Your customers won\\\'t wait for service ever again.EffectiveCustomers love great service, and Sidekick delivers. Grow revenue by solving issues in minutes instead of hours, and providing personalized support to each customer.Integrating with your tools.Wherever your customers are.\\n Sidekick takes an omnichannel approach to customer service, aggregating all customer interactions across all platforms in one area. Currently most social media platforms are supported, along with website embeddings and API integration.\\n On the web.Sidekick makes adding a live chat to your website as simple as copy and pasting a single line of code.Chat bubbles discretely sit in the bottom right corner and provide a smooth conversation experience, with AI and human agents alike.On Facebook.Sidekick integrates with your Facebook pages to make live customer service one click away.Customers can reach your agent and get service without ever leaving Messenger.On Instagram.E-Commerce on Instagram is especially demanding for customer service.Sidekick integrates easily with Instagram accounts to put a live agent one click away.On Twitter.Customers are spending more time on Twitter, which means businesses should provide customer service right on the platform.Sidekick integrates easily with Twitter accounts to put a live agent one click away.Anywhere you want.Our API provides programmatic access to your Sidekick agent to integrate into your own app.We\\\'ve built simple abstractions over the chat interface to make it easy to work with our API.EndpointsPOST Request{\\n ""access_token"": ""KjZUZBWAOKwgLWAlVFyL"",\\n ""conversation_id"": ""23874"",\\n ""body"": ""How much is a large 2 topping?""\\n}Sample Response{\\n ""response"": ""A large\'), DocumentContent(id=\'Zy0YaekZdd4rurPQKkys7A\', url=\' title=\'Hebbia - Search, Reinvented\', extract=""Direct to the point with cutting-edge AI.Stop relying on archaic software, traditional Q&amp;A emails, or waiting for deal partners. Get answers on your own time with accuracy that you can\'t replicate with humans. \\x80\\x8dHebbia\\xa0retrieves every answer, even insights humans overlook. ""), DocumentContent(id=\'A5c1ePEvsaQeml2Kui_-vA\', url=\' title=\'AI.XYZ\', extract=\'\\n \\n \\n\\n \\n \\n \\n \\n \\n\\nGo be human\\n\\nLet your AI deal with the rest\\nDesign your own AI with AI.XYZ\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\nThe digital world was designed to make us more productive but now navigating it all has become its own job.\\n\\n \\n \\n \\n \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nTake life a little easier\\n\\n \\n \\n \\n \\n \\n \\nTackles infooverload\\n\\nLike ChatGPT, but way more proactive and useful because it\'s designed by me, for only me\\n\\n \\n \\n \\n \\n \\n \\nNever sitsaround\\n\\nEven if I\'m not interacting with it, my AI looks for ways to simplify my day, surprising me with useful ideas\\n\\n \\n \\n \\n \\n \\n \\nSupports andinspires\\n\\nIt takes things off my plate, but also cheers me on throughout the day helping me navigate it all\\n\\n \\n \\n\\n \\n \\n \\n \\n\\nCreate your AI in 3 simple steps:\\n\\nSTEP ONEPick a face and voiceChoose from our library of characters or add your own unique face and voice.\\n\\n \\n \\n \\n \\n \\n \\nSTEP TWOCreate your AI\'s persona and memoryDecide who your AI is, its purpose and what it will help you with. Paste information that you want your AI to know.\\n\\n \\n \\n \\n \\n \\n \\nSTEP THREEGet startedAsk your AI to help you with ideas and support throughout your day. Eventually it will be able to proactively support you.\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nStart training your AI to do things for you\\n\\n \\n \\n\\n \\n\\n \\n \\n \\n \\n \\n \\nStart for free\\nPowered by OpenAI GPT-3 and GPT-4.\\n\\nChatGPT. Lightning-fast and private. Everywhere.\\nOptimized access to the AI on mobile.\\n\\nTo install Halist on iPhone, open the web app in Safari and tap the ""Share"" icon. Then, tap ""Add to Home Screen"" and follow the prompts.\\nTo install on Android, open the website in Chrome and tap the three dots in the top right corner. Then, tap ""Add to Home screen"" and follow the prompts.\\n\\n\'), DocumentContent(id=\'_XIjx1YLPfI4cKePIEc_bQ\', url=\' title=\'Clone your best expert\', extract=\' Airin clones how your top expert solves problems in as little as 2 hours. Airin creates an AI companion for the rest of your team by focusing on the patterns in your expert\'s questions and hypotheses, not their answers. Learn how it works Your customers, agents, sales teams, and consultants can independently solve a wider-range of complex problems with an AI companion. This eliminates the need to maintain large teams of specialized experts. Airin automates remote coaching for new hires and dramatically reduces time to productivity. New employees partner with your AI companion and meet productivity standards in half the time. \')])Here are some of the hottest AI agent startups and what they do: 1. [Bellow AI]( This startup provides a search engine for machine intelligence. It allows users to get responses from multiple AIs, exploring the full space of machine intelligence and getting highly tailored results. 2. [Adept AI]( Adept is focused on creating useful general intelligence. 3. [HiOperator]( HiOperator offers generative AI-enhanced customer support automation. It provides scalable, digital-first customer service and uses its software to empower agents to learn quickly and deliver accurate results. 4. [Stylo]( Stylo uses AI to help manage customer support, identifying where to most effectively spend time to improve the customer experience. 5. [DirectAI]( DirectAI allows users to build and deploy powerful computer vision models with plain language, without the need for code or training. 6. [Sidekick AI]( Sidekick AI is built to hold natural and dynamic conversations with customers, providing personalized service depending on the customer\'s needs. 7. [Hebbia]( Hebbia is reinventing search with cutting-edge AI, retrieving every answer, even insights humans overlook. 8. [AI.XYZ]( AI.XYZ allows users to design their own AI, tackling information overload and providing support and inspiration throughout the day. 9. [Halist AI]( Halist AI provides optimized access to ChatGPT, powered by OpenAI GPT-3 and GPT-4, on mobile. 10. [Airin]( Airin clones how your top expert solves problems in as little as 2 hours, creating an AI companion for the rest of your team. It automates remote coaching for new hires and dramatically reduces time to productivity. > Finished chain. ""Here are some of the hottest AI agent startups and what they do:\\n\\n1. [Bellow AI]( This startup provides a search engine for machine intelligence. It allows users to get responses from multiple AIs, exploring the full space of machine intelligence and getting highly tailored results.\\n\\n2. [Adept AI]( Adept is focused on creating useful general intelligence.\\n\\n3. [HiOperator]( HiOperator offers generative AI-enhanced customer support automation. It provides scalable, digital-first customer service and uses its software to empower agents to learn quickly and deliver accurate results.\\n\\n4. [Stylo]( Stylo uses AI to help manage customer support, identifying where to most effectively spend time to improve the customer experience.\\n\\n5. [DirectAI]( DirectAI allows users to build and deploy powerful computer vision models with plain language, without the need for code or training.\\n\\n6. [Sidekick AI]( Sidekick AI is built to hold natural and dynamic conversations with customers, providing personalized service depending on the customer\'s needs.\\n\\n7. [Hebbia]( Hebbia is reinventing search with cutting-edge AI, retrieving every answer, even insights humans overlook.\\n\\n8. [AI.XYZ]( AI.XYZ allows users to design their own AI, tackling information overload and providing support and inspiration throughout the day.\\n\\n9. [Halist AI]( Halist AI provides optimized access to ChatGPT, powered by OpenAI GPT-3 and GPT-4, on mobile.\\n\\n10. [Airin]( Airin clones how your top expert solves problems in as little as 2 hours, creating an AI companion for the rest of your team. It automates remote coaching for new hires and dramatically reduces time to productivity.\\n"" ``` ## Using the tool wrapper\u200b This is the old way of using Metaphor - through our own in-house integration. ```python from langchain.utilities import MetaphorSearchAPIWrapper ``` ```python search = MetaphorSearchAPIWrapper() ``` ### Call the API\u200b `results` takes in a Metaphor-optimized search query and a number of results (up to 500). It returns a list of results with title, url, author, and creation date. ```python search.results(""The best blog post about AI safety is definitely this: "", 10) ``` ```text [{\'title\': \'Core Views on AI Safety: When, Why, What, and How\', \'url\': \' \'author\': None, \'published_date\': \'2023-03-08\'}, {\'title\': \'Extinction Risk from Artificial Intelligence\', \'url\': \' \'author\': None, \'published_date\': \'2013-10-08\'}, {\'title\': \'The simple picture on AI safety - LessWrong\', \'url\': \' \'author\': \'Alex Flint\', \'published_date\': \'2018-05-27\'}, {\'title\': \'No Time Like The Present For AI Safety Work\', \'url\': \' \'author\': None, \'published_date\': \'2015-05-29\'}, {\'title\': \'A plea for solutionism on AI safety - LessWrong\', \'url\': \' \'author\': \'Jasoncrawford\', \'published_date\': \'2023-06-09\'}, {\'title\': \'The Artificial Intelligence Revolution: Part 1 - Wait But Why\', \'url\': \' \'author\': \'Tim Urban\', \'published_date\': \'2015-01-22\'}, {\'title\': \'Anthropic: Core Views on AI Safety: When, Why, What, and How - EA Forum\', \'url\': \' \'author\': \'Jonmenaster\', \'published_date\': \'2023-03-09\'}, {\'title\': ""[Linkpost] Sam Altman\'s 2015 Blog Posts Machine Intelligence Parts 1 & 2 - LessWrong"", \'url\': \' \'author\': \'Olivia Jimenez\', \'published_date\': \'2023-04-28\'}, {\'title\': \'The Proof of Doom - LessWrong\', \'url\': \' \'author\': \'Johnlawrenceaspden\', \'published_date\': \'2022-03-09\'}, {\'title\': ""Anthropic\'s Core Views on AI Safety - LessWrong"", \'url\': \' \'author\': \'Zac Hatfield-Dodds\', \'published_date\': \'2023-03-09\'}] ``` ### Adding filters\u200b We can also add filters to our search. include_domains: Optional[List [str] ] - List of domains to include in the search. If specified, results will only come from these domains. Only one of include_domains and exclude_domains should be specified. exclude_domains: Optional[List [str] ] - List of domains to exclude in the search. If specified, results will only come from these domains. Only one of include_domains and exclude_domains should be specified. start_crawl_date: Optional [str] - ""Crawl date"" refers to the date that Metaphor discovered a link, which is more granular and can be more useful than published date. If start_crawl_date is specified, results will only include links that were crawled after start_crawl_date. Must be specified in ISO 8601 format (YYYY-MM-DDTHH:MM:SSZ) end_crawl_date: Optional [str] - ""Crawl date"" refers to the date that Metaphor discovered a link, which is more granular and can be more useful than published date. If endCrawlDate is specified, results will only include links that were crawled before end_crawl_date. Must be specified in ISO 8601 format (YYYY-MM-DDTHH:MM:SSZ) start_published_date: Optional [str] - If specified, only links with a published date after start_published_date will be returned. Must be specified in ISO 8601 format (YYYY-MM-DDTHH:MM:SSZ). Note that for some links, we have no published date, and these links will be excluded from the results if start_published_date is specified. end_published_date: Optional [str] - If specified, only links with a published date before end_published_date will be returned. Must be specified in ISO 8601 format (YYYY-MM-DDTHH:MM:SSZ). Note that for some links, we have no published date, and these links will be excluded from the results if end_published_date is specified. See full docs [here]( ```python search.results( ""The best blog post about AI safety is definitely this: "", 10, include_domains=[""lesswrong.com""], start_published_date=""2019-01-01"", ) ``` ### Use Metaphor as a tool\u200b Metaphor can be used as a tool that gets URLs that other tools such as browsing tools. ```python from langchain.agents.agent_toolkits import PlayWrightBrowserToolkit from langchain.tools.playwright.utils import ( create_async_playwright_browser, # A synchronous browser is available, though it isn\'t compatible with jupyter. ) async_browser = create_async_playwright_browser() toolkit = PlayWrightBrowserToolkit.from_browser(async_browser=async_browser) tools = toolkit.get_tools() tools_by_name = {tool.name: tool for tool in tools} print(tools_by_name.keys()) navigate_tool = tools_by_name[""navigate_browser""] extract_text = tools_by_name[""extract_text""] ``` ```python from langchain.agents import AgentType, initialize_agent from langchain.chat_models import ChatOpenAI from langchain.tools import MetaphorSearchResults llm = ChatOpenAI(model_name=""gpt-4"", temperature=0.7) metaphor_tool = MetaphorSearchResults(api_wrapper=search) agent_chain = initialize_agent( [metaphor_tool, extract_text, navigate_tool], llm, agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True, ) agent_chain.run( ""find me an interesting tweet about AI safety using Metaphor, then tell me the first sentence in the post. Do not finish until able to retrieve the first sentence."" ) ``` - [Using their SDK](#using-their-sdk)- [Use in an agent](#use-in-an-agent) - [Using the tool wrapper](#using-the-tool-wrapper)- [Call the API](#call-the-api) - [Adding filters](#adding-filters) - [Use Metaphor as a tool](#use-metaphor-as-a-tool)', 'Reddit | Reddit [Reddit]( is an American social news aggregation, content rating, and discussion website. This loader fetches the text from the Posts of Subreddits or Reddit users, using the `praw` Python package. Make a [Reddit Application]( and initialize the loader with with your Reddit API credentials. ```python from langchain.document_loaders import RedditPostsLoader ``` ```python # !pip install praw ``` ```python # load using \'subreddit\' mode loader = RedditPostsLoader( client_id=""YOUR CLIENT ID"", client_secret=""YOUR CLIENT SECRET"", user_agent=""extractor by u/Master_Ocelot8179"", categories=[""new"", ""hot""], # List of categories to load posts from mode=""subreddit"", search_queries=[ ""investing"", ""wallstreetbets"", ], # List of subreddits to load posts from number_posts=20, # Default value is 10 ) # # or load using \'username\' mode # loader = RedditPostsLoader( # client_id=""YOUR CLIENT ID"", # client_secret=""YOUR CLIENT SECRET"", # user_agent=""extractor by u/Master_Ocelot8179"", # categories=[\'new\', \'hot\'], # mode = \'username\', # search_queries=[\'ga3far\', \'Master_Ocelot8179\'], # List of usernames to load posts from # number_posts=20 # ) # Note: Categories can be only of following value - ""controversial"" ""hot"" ""new"" ""rising"" ""top"" ``` ```python documents = loader.load() documents[:5] ``` ```text [Document(page_content=\'Hello, I am not looking for investment advice. I will apply my own due diligence. However, I am interested if anyone knows as a UK resident how fees and exchange rate differences would impact performance?\\n\\nI am planning to create a pie of index funds (perhaps UK, US, europe) or find a fund with a good track record of long term growth at low rates. \\n\\nDoes anyone have any ideas?\', metadata={\'post_subreddit\': \'r/investing\', \'post_category\': \'new\', \'post_title\': \'Long term retirement funds fees/exchange rate query\', \'post_score\': 1, \'post_id\': \'130pa6m\', \'post_url\': \' \'post_author\': Redditor(name=\'Badmanshiz\')}), Document(page_content=\'I much prefer the Roth IRA and would rather rollover my 401k to that every year instead of keeping it in the limited 401k options. But if I rollover, will I be able to continue contributing to my 401k? Or will that close my account? I realize that there are tax implications of doing this but I still think it is the better option.\', metadata={\'post_subreddit\': \'r/investing\', \'post_category\': \'new\', \'post_title\': \'Is it possible to rollover my 401k every year?\', \'post_score\': 3, \'post_id\': \'130ja0h\', \'post_url\': \' \'post_author\': Redditor(name=\'AnCap_Catholic\')}), Document(page_content=\'Have a general question? Want to offer some commentary on markets? Maybe you would just like to throw out a neat fact that doesn\\\'t warrant a self post? Feel free to post here! \\n\\nIf your question is ""I have $10,000, what do I do?"" or other ""advice for my personal situation"" questions, you should include relevant information, such as the following:\\n\\n* How old are you? What country do you live in? \\n* Are you employed/making income? How much? \\n* What are your objectives with this money? (Buy a house? Retirement savings?) \\n* What is your time horizon? Do you need this money next month? Next 20yrs? \\n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?) \\n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?) \\n* Any big debts (include interest rate) or expenses? \\n* And any other relevant financial information will be useful to give you a proper answer. \\n\\nPlease consider consulting our FAQ first - our [side bar]( also has useful resources. \\n\\nIf you are new to investing - please refer to Wiki - [Getting Started]( reading list in the wiki has a list of books ranging from light reading to advanced topics depending on your knowledge level. Link here - [Reading List]( the resources in the sidebar.\\n\\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered investment adviser if you need professional support before making any financial decisions!\', metadata={\'post_subreddit\': \'r/investing\', \'post_category\': \'new\', \'post_title\': \'Daily General Discussion and Advice Thread - April 27, 2023\', \'post_score\': 5, \'post_id\': \'130eszz\', \'post_url\': \' \'post_author\': Redditor(name=\'AutoModerator\')}), Document(page_content=""Based on recent news about salt battery advancements and the overall issues of lithium, I was wondering what would be feasible ways to invest into non-lithium based battery technologies? CATL is of course a choice, but the selection of brokers I currently have in my disposal don\'t provide HK stocks at all."", metadata={\'post_subreddit\': \'r/investing\', \'post_category\': \'new\', \'post_title\': \'Investing in non-lithium battery technologies?\', \'post_score\': 2, \'post_id\': \'130d6qp\', \'post_url\': \' \'post_author\': Redditor(name=\'-manabreak\')}), Document(page_content=\'Hello everyone,\\n\\nI would really like to invest in an ETF that follows spy or another big index, as I think this form of investment suits me best. \\n\\nThe problem is, that I live in Denmark where ETFs and funds are taxed annually on unrealised gains at quite a steep rate. This means that an ETF growing say 10% per year will only grow about 6%, which really ruins the long term effects of compounding interest.\\n\\nHowever stocks are only taxed on realised gains which is why they look more interesting to hold long term.\\n\\nI do not like the lack of diversification this brings, as I am looking to spend tonnes of time picking the right long term stocks.\\n\\nIt would be ideal to find a few stocks that over the long term somewhat follows the indexes. Does anyone have suggestions?\\n\\nI have looked at Nasdaq Inc. which quite closely follows Nasdaq 100. \\n\\nI really appreciate any help.\', metadata={\'post_subreddit\': \'r/investing\', \'post_category\': \'new\', \'post_title\': \'Stocks that track an index\', \'post_score\': 7, \'post_id\': \'130auvj\', \'post_url\': \' \'post_author\': Redditor(name=\'LeAlbertP\')})] ```']","To handle rate limiting with a backup model in an application using the `openai` library, you can implement a fallback mechanism. Here's a simple example using Python:

```python
from openai import RateLimitError
from langchain.chat_models import ChatOpenAI, ChatAnthropic

# Initialize the primary model
openai_llm = ChatOpenAI(max_retries=0)  # Set max_retries to 0 to disable retries

# Initialize the fallback model
anthropic_llm = ChatAnthropic()

# Combine the primary and fallback models
llm = openai_llm.with_fallbacks([anthropic_llm])

# Example function to invoke the LLM
def invoke_llm(prompt):
    try:
        response = llm.invoke(prompt)
        print(""Response from primary model or fallback:"")
        print(response)
    except RateLimitError:
        print(""Rate limit exceeded, falling back to the fallback model."")

# Example usage
invoke_llm(""Why did the chicken cross the road?"")
```

In this example:
- `ChatOpenAI` is set up as the primary model with `max_retries=0` to avoid automatic retries.
- `ChatAnthropic` is used as the fallback model.
- The `with_fallbacks` method combines the two models, allowing the application to fall back to the fallback model in case of a `RateLimitError`.

This setup ensures that if the primary model hits a rate limit, the application will automatically try the fallback model.","A straightforward way to do this is to specify fallback models using LangChain Expression Language. Example below:
```python     
from langchain.chat_models import ChatOpenAI

bad_llm = ChatOpenAI(model_name=""gpt-fake"")
good_llm = ChatOpenAI(model_name=""gpt-3.5-turbo"")
llm = bad_llm.with_fallbacks([good_llm])
llm.invoke(""Why did the the chicken cross the road?"")
```",0.9999999999,1.0,,0.025590823010528185,0.23904382470119523
22,I'm runing my own model using vllm. How do I connect it to LangChain?,"['Memory in LLMChain | Memory in LLMChain This notebook goes over how to use the Memory class with an `LLMChain`. We will add the [ConversationBufferMemory]( class, although this can be any memory class. ```python from langchain.chains import LLMChain from langchain.llms import OpenAI from langchain.memory import ConversationBufferMemory from langchain.prompts import PromptTemplate ``` The most important step is setting up the prompt correctly. In the below prompt, we have two input keys: one for the actual input, another for the input from the Memory class. Importantly, we make sure the keys in the `PromptTemplate` and the `ConversationBufferMemory` match up (`chat_history`). ```python template = """"""You are a chatbot having a conversation with a human. {chat_history} Human: {human_input} Chatbot:"""""" prompt = PromptTemplate( input_variables=[""chat_history"", ""human_input""], template=template ) memory = ConversationBufferMemory(memory_key=""chat_history"") ``` ```python llm = OpenAI() llm_chain = LLMChain( llm=llm, prompt=prompt, verbose=True, memory=memory, ) ``` ```python llm_chain.predict(human_input=""Hi there my friend"") ``` ```text > Entering new LLMChain chain... Prompt after formatting: You are a chatbot having a conversation with a human. Human: Hi there my friend Chatbot: > Finished chain. \' Hi there! How can I help you today?\' ``` ```python llm_chain.predict(human_input=""Not too bad - how are you?"") ``` ```text > Entering new LLMChain chain... Prompt after formatting: You are a chatbot having a conversation with a human. Human: Hi there my friend AI: Hi there! How can I help you today? Human: Not too bad - how are you? Chatbot: > Finished chain. "" I\'m doing great, thanks for asking! How are you doing?"" ``` ## Adding Memory to a chat model-based LLMChain The above works for completion-style `LLM`s, but if you are using a chat model, you will likely get better performance using structured chat messages. Below is an example. ```python from langchain.chat_models import ChatOpenAI from langchain.prompts import ( ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder, ) from langchain.schema import SystemMessage ``` We will use the [ChatPromptTemplate]( class to set up the chat prompt. The [from_messages]( method creates a `ChatPromptTemplate` from a list of messages (e.g., `SystemMessage`, `HumanMessage`, `AIMessage`, `ChatMessage`, etc.) or message templates, such as the [MessagesPlaceholder]( below. The configuration below makes it so the memory will be injected to the middle of the chat prompt, in the `chat_history` key, and the user\'s inputs will be added in a human/user message to the end of the chat prompt. ```python prompt = ChatPromptTemplate.from_messages( [ SystemMessage( content=""You are a chatbot having a conversation with a human."" ), # The persistent system prompt MessagesPlaceholder( variable_name=""chat_history"" ), # Where the memory will be stored. HumanMessagePromptTemplate.from_template( ""{human_input}"" ), # Where the human input will injected ] ) memory = ConversationBufferMemory(memory_key=""chat_history"", return_messages=True) ``` ```python llm = ChatOpenAI() chat_llm_chain = LLMChain( llm=llm, prompt=prompt, verbose=True, memory=memory, ) ``` ```python chat_llm_chain.predict(human_input=""Hi there my friend"") ``` ```text > Entering new LLMChain chain... Prompt after formatting: System: You are a chatbot having a conversation with a human. Human: Hi there my friend > Finished chain. \'Hello! How can I assist you today, my friend?\' ``` ```python chat_llm_chain.predict(human_input=""Not too bad - how are you?"") ``` ```text > Entering new LLMChain chain... Prompt after formatting: System: You are a chatbot having a conversation with a human. Human: Hi there my friend AI: Hello! How can I assist you today, my friend? Human: Not too bad - how are you? > Finished chain. ""I\'m an AI chatbot, so I don\'t have feelings, but I\'m here to help and chat with you! Is there something specific you would like to talk about or any questions I can assist you with?"" ``` - [Adding Memory to a chat model-based LLMChain](#adding-memory-to-a-chat-model-based-llmchain)', 'Conversation Buffer | Conversation Buffer This notebook shows how to use `ConversationBufferMemory`. This memory allows for storing messages and then extracts the messages in a variable. We can first extract it as a string. ```python from langchain.memory import ConversationBufferMemory ``` ```python memory = ConversationBufferMemory() memory.save_context({""input"": ""hi""}, {""output"": ""whats up""}) ``` ```python memory.load_memory_variables({}) ``` ```text {\'history\': \'Human: hi\\nAI: whats up\'} ``` We can also get the history as a list of messages (this is useful if you are using this with a chat model). ```python memory = ConversationBufferMemory(return_messages=True) memory.save_context({""input"": ""hi""}, {""output"": ""whats up""}) ``` ```python memory.load_memory_variables({}) ``` ```text {\'history\': [HumanMessage(content=\'hi\', additional_kwargs={}), AIMessage(content=\'whats up\', additional_kwargs={})]} ``` ## Using in a chain Finally, let\'s take a look at using this in a chain (setting `verbose=True` so we can see the prompt). ```python from langchain.llms import OpenAI from langchain.chains import ConversationChain llm = OpenAI(temperature=0) conversation = ConversationChain( llm=llm, verbose=True, memory=ConversationBufferMemory() ) ``` ```python conversation.predict(input=""Hi there!"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: Human: Hi there! AI: > Finished chain. "" Hi there! It\'s nice to meet you. How can I help you today?"" ``` ```python conversation.predict(input=""I\'m doing well! Just having a conversation with an AI."") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: Human: Hi there! AI: Hi there! It\'s nice to meet you. How can I help you today? Human: I\'m doing well! Just having a conversation with an AI. AI: > Finished chain. "" That\'s great! It\'s always nice to have a conversation with someone new. What would you like to talk about?"" ``` ```python conversation.predict(input=""Tell me about yourself."") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: Human: Hi there! AI: Hi there! It\'s nice to meet you. How can I help you today? Human: I\'m doing well! Just having a conversation with an AI. AI: That\'s great! It\'s always nice to have a conversation with someone new. What would you like to talk about? Human: Tell me about yourself. AI: > Finished chain. "" Sure! I\'m an AI created to help people with their everyday tasks. I\'m programmed to understand natural language and provide helpful information. I\'m also constantly learning and updating my knowledge base so I can provide more accurate and helpful answers."" ``` - [Using in a chain](#using-in-a-chain)', 'Reddit | Reddit [Reddit]( is an American social news aggregation, content rating, and discussion website. This loader fetches the text from the Posts of Subreddits or Reddit users, using the `praw` Python package. Make a [Reddit Application]( and initialize the loader with with your Reddit API credentials. ```python from langchain.document_loaders import RedditPostsLoader ``` ```python # !pip install praw ``` ```python # load using \'subreddit\' mode loader = RedditPostsLoader( client_id=""YOUR CLIENT ID"", client_secret=""YOUR CLIENT SECRET"", user_agent=""extractor by u/Master_Ocelot8179"", categories=[""new"", ""hot""], # List of categories to load posts from mode=""subreddit"", search_queries=[ ""investing"", ""wallstreetbets"", ], # List of subreddits to load posts from number_posts=20, # Default value is 10 ) # # or load using \'username\' mode # loader = RedditPostsLoader( # client_id=""YOUR CLIENT ID"", # client_secret=""YOUR CLIENT SECRET"", # user_agent=""extractor by u/Master_Ocelot8179"", # categories=[\'new\', \'hot\'], # mode = \'username\', # search_queries=[\'ga3far\', \'Master_Ocelot8179\'], # List of usernames to load posts from # number_posts=20 # ) # Note: Categories can be only of following value - ""controversial"" ""hot"" ""new"" ""rising"" ""top"" ``` ```python documents = loader.load() documents[:5] ``` ```text [Document(page_content=\'Hello, I am not looking for investment advice. I will apply my own due diligence. However, I am interested if anyone knows as a UK resident how fees and exchange rate differences would impact performance?\\n\\nI am planning to create a pie of index funds (perhaps UK, US, europe) or find a fund with a good track record of long term growth at low rates. \\n\\nDoes anyone have any ideas?\', metadata={\'post_subreddit\': \'r/investing\', \'post_category\': \'new\', \'post_title\': \'Long term retirement funds fees/exchange rate query\', \'post_score\': 1, \'post_id\': \'130pa6m\', \'post_url\': \' \'post_author\': Redditor(name=\'Badmanshiz\')}), Document(page_content=\'I much prefer the Roth IRA and would rather rollover my 401k to that every year instead of keeping it in the limited 401k options. But if I rollover, will I be able to continue contributing to my 401k? Or will that close my account? I realize that there are tax implications of doing this but I still think it is the better option.\', metadata={\'post_subreddit\': \'r/investing\', \'post_category\': \'new\', \'post_title\': \'Is it possible to rollover my 401k every year?\', \'post_score\': 3, \'post_id\': \'130ja0h\', \'post_url\': \' \'post_author\': Redditor(name=\'AnCap_Catholic\')}), Document(page_content=\'Have a general question? Want to offer some commentary on markets? Maybe you would just like to throw out a neat fact that doesn\\\'t warrant a self post? Feel free to post here! \\n\\nIf your question is ""I have $10,000, what do I do?"" or other ""advice for my personal situation"" questions, you should include relevant information, such as the following:\\n\\n* How old are you? What country do you live in? \\n* Are you employed/making income? How much? \\n* What are your objectives with this money? (Buy a house? Retirement savings?) \\n* What is your time horizon? Do you need this money next month? Next 20yrs? \\n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?) \\n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?) \\n* Any big debts (include interest rate) or expenses? \\n* And any other relevant financial information will be useful to give you a proper answer. \\n\\nPlease consider consulting our FAQ first - our [side bar]( also has useful resources. \\n\\nIf you are new to investing - please refer to Wiki - [Getting Started]( reading list in the wiki has a list of books ranging from light reading to advanced topics depending on your knowledge level. Link here - [Reading List]( the resources in the sidebar.\\n\\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered investment adviser if you need professional support before making any financial decisions!\', metadata={\'post_subreddit\': \'r/investing\', \'post_category\': \'new\', \'post_title\': \'Daily General Discussion and Advice Thread - April 27, 2023\', \'post_score\': 5, \'post_id\': \'130eszz\', \'post_url\': \' \'post_author\': Redditor(name=\'AutoModerator\')}), Document(page_content=""Based on recent news about salt battery advancements and the overall issues of lithium, I was wondering what would be feasible ways to invest into non-lithium based battery technologies? CATL is of course a choice, but the selection of brokers I currently have in my disposal don\'t provide HK stocks at all."", metadata={\'post_subreddit\': \'r/investing\', \'post_category\': \'new\', \'post_title\': \'Investing in non-lithium battery technologies?\', \'post_score\': 2, \'post_id\': \'130d6qp\', \'post_url\': \' \'post_author\': Redditor(name=\'-manabreak\')}), Document(page_content=\'Hello everyone,\\n\\nI would really like to invest in an ETF that follows spy or another big index, as I think this form of investment suits me best. \\n\\nThe problem is, that I live in Denmark where ETFs and funds are taxed annually on unrealised gains at quite a steep rate. This means that an ETF growing say 10% per year will only grow about 6%, which really ruins the long term effects of compounding interest.\\n\\nHowever stocks are only taxed on realised gains which is why they look more interesting to hold long term.\\n\\nI do not like the lack of diversification this brings, as I am looking to spend tonnes of time picking the right long term stocks.\\n\\nIt would be ideal to find a few stocks that over the long term somewhat follows the indexes. Does anyone have suggestions?\\n\\nI have looked at Nasdaq Inc. which quite closely follows Nasdaq 100. \\n\\nI really appreciate any help.\', metadata={\'post_subreddit\': \'r/investing\', \'post_category\': \'new\', \'post_title\': \'Stocks that track an index\', \'post_score\': 7, \'post_id\': \'130auvj\', \'post_url\': \' \'post_author\': Redditor(name=\'LeAlbertP\')})] ```']","To connect your model running on vLLM to LangChain, you'll need to create a custom LLM class that interfaces with vLLM and then integrate it into LangChain. Here's a simplified example:

1. **Create a Custom LLM Class:**

```python
from langchain.llms import BaseLLM
import vllm

class VLLMWrapper(BaseLLM):
    def __init__(self, model_name):
        self.engine = vllm.LLM(model=model_name)

    def _call(self, inputs, stop=None):
        response = self.engine.generate(inputs, stop=stop)
        return response.text
```

2. **Integrate with LangChain:**

You can now use this custom LLM class in your LangChain setup:

```python
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

# Initialize the custom LLM
llm = VLLMWrapper(model_name=""your_model_name"")

# Set up the prompt
template = ""You are a chatbot having a conversation with a human. {chat_history} Human: {human_input} Chatbot:""
prompt = PromptTemplate(input_variables=[""chat_history"", ""human_input""], template=template)

# Create the LLMChain
llm_chain = LLMChain(llm=llm, prompt=prompt, verbose=True)

# Predict
response = llm_chain.predict(human_input=""Hi there my friend"")
print(response)
```

This example demonstrates how to wrap a vLLM model and use it within LangChain. Adjust the specifics according to your model and use case.","LangChain has a VLLM integration:""
```python
from langchain.llms import VLLM

llm = VLLM(model=""mosaicml/mpt-7b"",
           trust_remote_code=True,  # mandatory for hf models
           max_new_tokens=128,
           top_k=10,
           top_p=0.95,
           temperature=0.8,
)

print(llm(""What is the capital of France ?""))",0.0,0.0,,0.0027450059295411992,0.12345679012345678
23,What's a runnable lambda?,"['LangSmith Walkthrough | LangSmith Walkthrough []( LangChain makes it easy to prototype LLM applications and Agents. However, delivering LLM applications to production can be deceptively difficult. You will likely have to heavily customize and iterate on your prompts, chains, and other components to create a high-quality product. To aid in this process, we\'ve launched LangSmith, a unified platform for debugging, testing, and monitoring your LLM applications. When might this come in handy? You may find it useful when you want to: - Quickly debug a new chain, agent, or set of tools - Visualize how components (chains, llms, retrievers, etc.) relate and are used - Evaluate different prompts and LLMs for a single component - Run a given chain several times over a dataset to ensure it consistently meets a quality bar - Capture usage traces and using LLMs or analytics pipelines to generate insights ## Prerequisites **Create a LangSmith account and create an API key (see bottom left corner). Familiarize yourself with the platform by looking through the docs** Note LangSmith is in closed beta; we\'re in the process of rolling it out to more users. However, you can fill out the form on the website for expedited access. Now, let\'s get started! ## Log runs to LangSmith First, configure your environment variables to tell LangChain to log traces. This is done by setting the `LANGCHAIN_TRACING_V2` environment variable to true. You can tell LangChain which project to log to by setting the `LANGCHAIN_PROJECT` environment variable (if this isn\'t set, runs will be logged to the `default` project). This will automatically create the project for you if it doesn\'t exist. You must also set the `LANGCHAIN_ENDPOINT` and `LANGCHAIN_API_KEY` environment variables. For more information on other ways to set up tracing, please reference the [LangSmith documentation]( **NOTE:** You must also set your `OPENAI_API_KEY` environment variables in order to run the following tutorial. **NOTE:** You can only access an API key when you first create it. Keep it somewhere safe. **NOTE:** You can also use a context manager in python to log traces using ```python from langchain.callbacks.manager import tracing_v2_enabled with tracing_v2_enabled(project_name=""My Project""): agent.run(""How many people live in canada as of 2023?"") ``` However, in this example, we will use environment variables. ```python %pip install openai tiktoken pandas duckduckgo-search --quiet ``` ```python import os from uuid import uuid4 unique_id = uuid4().hex[0:8] os.environ[""LANGCHAIN_TRACING_V2""] = ""true"" os.environ[""LANGCHAIN_PROJECT""] = f""Tracing Walkthrough - {unique_id}"" os.environ[""LANGCHAIN_ENDPOINT""] = "" os.environ[""LANGCHAIN_API_KEY""] = """" # Update to your API key # Used by the agent in this tutorial os.environ[""OPENAI_API_KEY""] = """" ``` Create the langsmith client to interact with the API ```python from langsmith import Client client = Client() ``` Create a LangChain component and log runs to the platform. In this example, we will create a ReAct-style agent with access to a general search tool (DuckDuckGo). The agent\'s prompt can be viewed in the [Hub here]( ```python from langchain import hub from langchain.agents import AgentExecutor from langchain.agents.format_scratchpad import format_to_openai_function_messages from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser from langchain.chat_models import ChatOpenAI from langchain.tools import DuckDuckGoSearchResults from langchain.tools.render import format_tool_to_openai_function # Fetches the latest version of this prompt prompt = hub.pull(""wfh/langsmith-agent-prompt:latest"") llm = ChatOpenAI( model=""gpt-3.5-turbo-16k"", temperature=0, ) tools = [ DuckDuckGoSearchResults( name=""duck_duck_go"" ), # General internet search using DuckDuckGo ] llm_with_tools = llm.bind(functions=[format_tool_to_openai_function(t) for t in tools]) runnable_agent = ( { ""input"": lambda x: x[""input""], ""agent_scratchpad"": lambda x: format_to_openai_function_messages( x[""intermediate_steps""] ), } | prompt | llm_with_tools | OpenAIFunctionsAgentOutputParser() ) agent_executor = AgentExecutor( agent=runnable_agent, tools=tools, handle_parsing_errors=True ) ``` We are running the agent concurrently on multiple inputs to reduce latency. Runs get logged to LangSmith in the background so execution latency is unaffected. ```python inputs = [ ""What is LangChain?"", ""What\'s LangSmith?"", ""When was Llama-v2 released?"", ""What is the langsmith cookbook?"", ""When did langchain first announce the hub?"", ] results = agent_executor.batch([{""input"": x} for x in inputs], return_exceptions=True) ``` ```python results[:2] ``` ```text [{\'input\': \'What is LangChain?\', \'output\': \'I\\\'m sorry, but I couldn\\\'t find any information about ""LangChain"". Could you please provide more context or clarify your question?\'}, {\'input\': ""What\'s LangSmith?"", \'output\': \'I\\\'m sorry, but I couldn\\\'t find any information about ""LangSmith"". It could be a specific term or a company that is not widely known. Can you provide more context or clarify what you are referring to?\'}] ``` Assuming you\'ve successfully set up your environment, your agent traces should show up in the `Projects` section in the [app]( Congrats! ![Initial Runs](/assets/images/log_traces-edd14f0c4d5c320263362395793babdc.png) It looks like the agent isn\'t effectively using the tools though. Let\'s evaluate this so we have a baseline. ## Evaluate Agent In addition to logging runs, LangSmith also allows you to test and evaluate your LLM applications. In this section, you will leverage LangSmith to create a benchmark dataset and run AI-assisted evaluators on an agent. You will do so in a few steps: 1. Create a dataset 2. Initialize a new agent to benchmark 3. Configure evaluators to grade an agent\'s output 4. Run the agent over the dataset and evaluate the results ### 1. Create a LangSmith dataset Below, we use the LangSmith client to create a dataset from the input questions from above and a list labels. You will use these later to measure performance for a new agent. A dataset is a collection of examples, which are nothing more than input-output pairs you can use as test cases to your application. For more information on datasets, including how to create them from CSVs or other files or how to create them in the platform, please refer to the [LangSmith documentation]( ```python outputs = [ ""LangChain is an open-source framework for building applications using large language models. It is also the name of the company building LangSmith."", ""LangSmith is a unified platform for debugging, testing, and monitoring language model applications and agents powered by LangChain"", ""July 18, 2023"", ""The langsmith cookbook is a github repository containing detailed examples of how to use LangSmith to debug, evaluate, and monitor large language model-powered applications."", ""September 5, 2023"", ] ``` ```python dataset_name = f""agent-qa-{unique_id}"" dataset = client.create_dataset( dataset_name, description=""An example dataset of questions over the LangSmith documentation."", ) for query, answer in zip(inputs, outputs): client.create_example( inputs={""input"": query}, outputs={""output"": answer}, dataset_id=dataset.id ) ``` ### 2. Initialize a new agent to benchmark LangSmith lets you evaluate any LLM, chain, agent, or even a custom function. Conversational agents are stateful (they have memory); to ensure that this state isn\'t shared between dataset runs, we will pass in a `chain_factory` (aka a `constructor`) function to initialize for each call. In this case, we will test an agent that uses OpenAI\'s function calling endpoints. ```python from langchain import hub from langchain.agents import AgentExecutor, AgentType, initialize_agent, load_tools from langchain.agents.format_scratchpad import format_to_openai_function_messages from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser from langchain.chat_models import ChatOpenAI from langchain.tools.render import format_tool_to_openai_function # Since chains can be stateful (e.g. they can have memory), we provide # a way to initialize a new chain for each row in the dataset. This is done # by passing in a factory function that returns a new chain for each row. def agent_factory(prompt): llm_with_tools = llm.bind( functions=[format_tool_to_openai_function(t) for t in tools] ) runnable_agent = ( { ""input"": lambda x: x[""input""], ""agent_scratchpad"": lambda x: format_to_openai_function_messages( x[""intermediate_steps""] ), } | prompt | llm_with_tools | OpenAIFunctionsAgentOutputParser() ) return AgentExecutor(agent=runnable_agent, tools=tools, handle_parsing_errors=True) ``` ### 3. Configure evaluation Manually comparing the results of chains in the UI is effective, but it can be time consuming. It can be helpful to use automated metrics and AI-assisted feedback to evaluate your component\'s performance. Below, we will create some pre-implemented run evaluators that do the following: - Compare results against ground truth labels. - Measure semantic (dis)similarity using embedding distance - Evaluate \'aspects\' of the agent\'s response in a reference-free manner using custom criteria For a longer discussion of how to select an appropriate evaluator for your use case and how to create your own custom evaluators, please refer to the [LangSmith documentation]( ```python from langchain.evaluation import EvaluatorType from langchain.smith import RunEvalConfig evaluation_config = RunEvalConfig( # Evaluators can either be an evaluator type (e.g., ""qa"", ""criteria"", ""embedding_distance"", etc.) or a configuration for that evaluator evaluators=[ # Measures whether a QA response is ""Correct"", based on a reference answer # You can also select via the raw string ""qa"" EvaluatorType.QA, # Measure the embedding distance between the output and the reference answer # Equivalent to: EvalConfig.EmbeddingDistance(embeddings=OpenAIEmbeddings()) EvaluatorType.EMBEDDING_DISTANCE, # Grade whether the output satisfies the stated criteria. # You can select a default one such as ""helpfulness"" or provide your own. RunEvalConfig.LabeledCriteria(""helpfulness""), # The LabeledScoreString evaluator outputs a score on a scale from 1-10. # You can use default criteria or write our own rubric RunEvalConfig.LabeledScoreString( { ""accuracy"": """""" Score 1: The answer is completely unrelated to the reference. Score 3: The answer has minor relevance but does not align with the reference. Score 5: The answer has moderate relevance but contains inaccuracies. Score 7: The answer aligns with the reference but has minor errors or omissions. Score 10: The answer is completely accurate and aligns perfectly with the reference."""""" }, normalize_by=10, ), ], # You can add custom StringEvaluator or RunEvaluator objects here as well, which will automatically be # applied to each prediction. Check out the docs for examples. custom_evaluators=[], ) ``` ### 4. Run the agent and evaluators Use the [run_on_dataset]( (or asynchronous [arun_on_dataset]( function to evaluate your model. This will: 1. Fetch example rows from the specified dataset. 2. Run your agent (or any custom function) on each example. 3. Apply evaluators to the resulting run traces and corresponding reference examples to generate automated feedback. The results will be visible in the LangSmith app. ```python from langchain import hub # We will test this version of the prompt prompt = hub.pull(""wfh/langsmith-agent-prompt:798e7324"") ``` ```python import functools from langchain.smith import ( arun_on_dataset, run_on_dataset, ) chain_results = run_on_dataset( dataset_name=dataset_name, llm_or_chain_factory=functools.partial(agent_factory, prompt=prompt), evaluation=evaluation_config, verbose=True, client=client, project_name=f""runnable-agent-test-5d466cbc-{unique_id}"", tags=[ ""testing-notebook"", ""prompt:5d466cbc"", ], # Optional, adds a tag to the resulting chain runs ) # Sometimes, the agent will error due to parsing issues, incompatible tool inputs, etc. # These are logged as warnings here and captured as errors in the tracing UI. ``` ```text View the evaluation results for project \'runnable-agent-test-5d466cbc-bf2162aa\' at: [> ] 0/5 Chain failed for example 54b4fce8-4492-409d-94af-708f51698b39 with inputs {\'input\': \'Who trained Llama-v2?\'} Error Type: TypeError, Message: DuckDuckGoSearchResults._run() got an unexpected keyword argument \'arg1\' [------------------------------------------------->] 5/5 Eval quantiles: 0.25 0.5 0.75 mean mode embedding_cosine_distance 0.086614 0.118841 0.183672 0.151444 0.050158 correctness 0.000000 0.500000 1.000000 0.500000 0.000000 score_string:accuracy 0.775000 1.000000 1.000000 0.775000 1.000000 helpfulness 0.750000 1.000000 1.000000 0.750000 1.000000 ``` ### Review the test results You can review the test results tracing UI below by clicking the URL in the output above or navigating to the ""Testing & Datasets"" page in LangSmith **""agent-qa-{unique_id}""** dataset. ![test results](/assets/images/test_results-15649f0f4500fd64ef2209229951a6c1.png) This will show the new runs and the feedback logged from the selected evaluators. You can also explore a summary of the results in tabular format below. ```python chain_results.to_dataframe() ``` ```html .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } embedding_cosine_distance correctness score_string:accuracy helpfulness input output reference 42b639a2-17c4-4031-88a9-0ce2c45781ce 0.317938 0.0 1.0 1.0 {\'input\': \'What is the langsmith cookbook?\'} {\'input\': \'What is the langsmith cookbook?\', \'... {\'output\': \'September 5, 2023\'} 54b4fce8-4492-409d-94af-708f51698b39 NaN NaN NaN NaN {\'input\': \'Who trained Llama-v2?\'} {\'Error\': \'TypeError(""DuckDuckGoSearchResults.... {\'output\': \'The langsmith cookbook is a github... 8ae5104e-bbb4-42cc-a84e-f9b8cfc92b8e 0.138916 1.0 1.0 1.0 {\'input\': \'When was Llama-v2 released?\'} {\'input\': \'When was Llama-v2 released?\', \'outp... {\'output\': \'July 18, 2023\'} 678c0363-3ed1-410a-811f-ebadef2e783a 0.050158 1.0 1.0 1.0 {\'input\': \'What\'s LangSmith?\'} {\'input\': \'What\'s LangSmith?\', \'output\': \'Lang... {\'output\': \'LangSmith is a unified platform fo... 762a616c-7aab-419c-9001-b43ab6200d26 0.098766 0.0 0.1 0.0 {\'input\': \'What is LangChain?\'} {\'input\': \'What is LangChain?\', \'output\': \'Lan... {\'output\': \'LangChain is an open-source framew... ``` ### (Optional) Compare to another prompt Now that we have our test run results, we can make changes to our agent and benchmark them. Let\'s try this again with a different prompt and see the results. ```python candidate_prompt = hub.pull(""wfh/langsmith-agent-prompt:39f3bbd0"") chain_results = run_on_dataset( dataset_name=dataset_name, llm_or_chain_factory=functools.partial(agent_factory, prompt=candidate_prompt), evaluation=evaluation_config, verbose=True, client=client, project_name=f""runnable-agent-test-39f3bbd0-{unique_id}"", tags=[ ""testing-notebook"", ""prompt:39f3bbd0"", ], # Optional, adds a tag to the resulting chain runs ) ``` ```text View the evaluation results for project \'runnable-agent-test-39f3bbd0-bf2162aa\' at: [------------------------------------------------->] 5/5 Eval quantiles: 0.25 0.5 0.75 mean mode embedding_cosine_distance 0.059506 0.155538 0.212864 0.157915 0.043119 correctness 0.000000 0.000000 1.000000 0.400000 0.000000 score_string:accuracy 0.700000 1.000000 1.000000 0.880000 1.000000 helpfulness 1.000000 1.000000 1.000000 0.800000 1.000000 ``` ## Exporting datasets and runs LangSmith lets you export data to common formats such as CSV or JSONL directly in the web app. You can also use the client to fetch runs for further analysis, to store in your own database, or to share with others. Let\'s fetch the run traces from the evaluation run. **Note: It may be a few moments before all the runs are accessible.** ```python runs = client.list_runs(project_name=chain_results[""project_name""], execution_order=1) ``` ```python # After some time, these will be populated. client.read_project(project_name=chain_results[""project_name""]).feedback_stats ``` ## Conclusion Congratulations! You have successfully traced and evaluated an agent using LangSmith! This was a quick guide to get started, but there are many more ways to use LangSmith to speed up your developer flow and produce better results. For more information on how you can get the most out of LangSmith, check out [LangSmith documentation]( and please reach out with questions, feature requests, or feedback at [support@langchain.dev](mailto:support@langchain.dev). - [Prerequisites](#prerequisites) - [Log runs to LangSmith](#log-runs-to-langsmith) - [Evaluate Agent](#evaluate-agent)- [1. Create a LangSmith dataset](#1-create-a-langsmith-dataset) - [2. Initialize a new agent to benchmark](#2-initialize-a-new-agent-to-benchmark) - [3. Configure evaluation](#3-configure-evaluation) - [4. Run the agent and evaluators](#4-run-the-agent-and-evaluators) - [Review the test results](#review-the-test-results) - [(Optional) Compare to another prompt](#optional-compare-to-another-prompt) - [Exporting datasets and runs](#exporting-datasets-and-runs) - [Conclusion](#conclusion)', 'Dynamically route logic based on input | Dynamically route logic based on input This notebook covers how to do routing in the LangChain Expression Language. Routing allows you to create non-deterministic chains where the output of a previous step defines the next step. Routing helps provide structure and consistency around interactions with LLMs. There are two ways to perform routing: 1. Using a `RunnableBranch`. 2. Writing custom factory function that takes the input of a previous step and returns a **runnable**. Importantly, this should return a **runnable** and NOT actually execute. We\'ll illustrate both methods using a two step sequence where the first step classifies an input question as being about `LangChain`, `Anthropic`, or `Other`, then routes to a corresponding prompt chain. ## Using a RunnableBranch A `RunnableBranch` is initialized with a list of (condition, runnable) pairs and a default runnable. It selects which branch by passing each condition the input it\'s invoked with. It selects the first condition to evaluate to True, and runs the corresponding runnable to that condition with the input. If no provided conditions match, it runs the default runnable. Here\'s an example of what it looks like in action: ```python from langchain.chat_models import ChatAnthropic from langchain.prompts import PromptTemplate from langchain.schema.output_parser import StrOutputParser ``` First, let\'s create a chain that will identify incoming questions as being about `LangChain`, `Anthropic`, or `Other`: ```python chain = ( PromptTemplate.from_template( """"""Given the user question below, classify it as either being about `LangChain`, `Anthropic`, or `Other`. Do not respond with more than one word. {question} Classification:"""""" ) | ChatAnthropic() | StrOutputParser() ) ``` ```python chain.invoke({""question"": ""how do I call Anthropic?""}) ``` ```text \' Anthropic\' ``` Now, let\'s create three sub chains: ```python langchain_chain = ( PromptTemplate.from_template( """"""You are an expert in langchain. \\ Always answer questions starting with ""As Harrison Chase told me"". \\ Respond to the following question: Question: {question} Answer:"""""" ) | ChatAnthropic() ) anthropic_chain = ( PromptTemplate.from_template( """"""You are an expert in anthropic. \\ Always answer questions starting with ""As Dario Amodei told me"". \\ Respond to the following question: Question: {question} Answer:"""""" ) | ChatAnthropic() ) general_chain = ( PromptTemplate.from_template( """"""Respond to the following question: Question: {question} Answer:"""""" ) | ChatAnthropic() ) ``` ```python from langchain.schema.runnable import RunnableBranch branch = RunnableBranch( (lambda x: ""anthropic"" in x[""topic""].lower(), anthropic_chain), (lambda x: ""langchain"" in x[""topic""].lower(), langchain_chain), general_chain, ) ``` ```python full_chain = {""topic"": chain, ""question"": lambda x: x[""question""]} | branch ``` ```python full_chain.invoke({""question"": ""how do I use Anthropic?""}) ``` ```text AIMessage(content="" As Dario Amodei told me, here are some ways to use Anthropic:\\n\\n- Sign up for an account on Anthropic\'s website to access tools like Claude, Constitutional AI, and Writer. \\n\\n- Use Claude for tasks like email generation, customer service chat, and QA. Claude can understand natural language prompts and provide helpful responses.\\n\\n- Use Constitutional AI if you need an AI assistant that is harmless, honest, and helpful. It is designed to be safe and aligned with human values.\\n\\n- Use Writer to generate natural language content for things like marketing copy, stories, reports, and more. Give it a topic and prompt and it will create high-quality written content.\\n\\n- Check out Anthropic\'s documentation and blog for tips, tutorials, examples, and announcements about new capabilities as they continue to develop their AI technology.\\n\\n- Follow Anthropic on social media or subscribe to their newsletter to stay up to date on new features and releases.\\n\\n- For most people, the easiest way to leverage Anthropic\'s technology is through their website - just create an account to get started!"", additional_kwargs={}, example=False) ``` ```python full_chain.invoke({""question"": ""how do I use LangChain?""}) ``` ```text AIMessage(content=\' As Harrison Chase told me, here is how you use LangChain:\\n\\nLangChain is an AI assistant that can have conversations, answer questions, and generate text. To use LangChain, you simply type or speak your input and LangChain will respond. \\n\\nYou can ask LangChain questions, have discussions, get summaries or explanations about topics, and request it to generate text on a subject. Some examples of interactions:\\n\\n- Ask general knowledge questions and LangChain will try to answer factually. For example ""What is the capital of France?""\\n\\n- Have conversations on topics by taking turns speaking. You can prompt the start of a conversation by saying something like ""Let\\\'s discuss machine learning""\\n\\n- Ask for summaries or high-level explanations on subjects. For example ""Can you summarize the main themes in Shakespeare\\\'s Hamlet?"" \\n\\n- Give creative writing prompts or requests to have LangChain generate text in different styles. For example ""Write a short children\\\'s story about a mouse"" or ""Generate a poem in the style of Robert Frost about nature""\\n\\n- Correct LangChain if it makes an inaccurate statement and provide the right information. This helps train it.\\n\\nThe key is interacting naturally and giving it clear prompts and requests\', additional_kwargs={}, example=False) ``` ```python full_chain.invoke({""question"": ""whats 2 + 2""}) ``` ```text AIMessage(content=\' 2 + 2 = 4\', additional_kwargs={}, example=False) ``` ## Using a custom function You can also use a custom function to route between different outputs. Here\'s an example: ```python def route(info): if ""anthropic"" in info[""topic""].lower(): return anthropic_chain elif ""langchain"" in info[""topic""].lower(): return langchain_chain else: return general_chain ``` ```python from langchain.schema.runnable import RunnableLambda full_chain = {""topic"": chain, ""question"": lambda x: x[""question""]} | RunnableLambda( route ) ``` ```python full_chain.invoke({""question"": ""how do I use Anthroipc?""}) ``` ```text AIMessage(content=\' As Dario Amodei told me, to use Anthropic IPC you first need to import it:\\n\\n```python\\nfrom anthroipc import ic\\n```\\n\\nThen you can create a client and connect to the server:\\n\\n```python \\nclient = ic.connect()\\n```\\n\\nAfter that, you can call methods on the client and get responses:\\n\\n```python\\nresponse = client.ask(""What is the meaning of life?"")\\nprint(response)\\n```\\n\\nYou can also register callbacks to handle events: \\n\\n```python\\ndef on_poke(event):\\n print(""Got poked!"")\\n\\nclient.on(\\\'poke\\\', on_poke)\\n```\\n\\nAnd that\\\'s the basics of using the Anthropic IPC client library for Python! Let me know if you have any other questions!\', additional_kwargs={}, example=False) ``` ```python full_chain.invoke({""question"": ""how do I use LangChain?""}) ``` ```text AIMessage(content=\' As Harrison Chase told me, to use LangChain you first need to sign up for an API key at platform.langchain.com. Once you have your API key, you can install the Python library and write a simple Python script to call the LangChain API. Here is some sample code to get started:\\n\\n```python\\nimport langchain\\n\\napi_key = ""YOUR_API_KEY""\\n\\nlangchain.set_key(api_key)\\n\\nresponse = langchain.ask(""What is the capital of France?"")\\n\\nprint(response.response)\\n```\\n\\nThis will send the question ""What is the capital of France?"" to the LangChain API and print the response. You can customize the request by providing parameters like max_tokens, temperature, etc. The LangChain Python library documentation has more details on the available options. The key things are getting an API key and calling langchain.ask() with your question text. Let me know if you have any other questions!\', additional_kwargs={}, example=False) ``` ```python full_chain.invoke({""question"": ""whats 2 + 2""}) ``` ```text AIMessage(content=\' 4\', additional_kwargs={}, example=False) ``` - [Using a RunnableBranch](#using-a-runnablebranch) - [Using a custom function](#using-a-custom-function)', 'Agents | Agents You can pass a Runnable into an agent. ```python from langchain.agents import AgentExecutor, XMLAgent, tool from langchain.chat_models import ChatAnthropic ``` ```python model = ChatAnthropic(model=""claude-2"") ``` ```python @tool def search(query: str) -> str: """"""Search things about current events."""""" return ""32 degrees"" ``` ```python tool_list = [search] ``` ```python # Get prompt to use prompt = XMLAgent.get_default_prompt() ``` ```python # Logic for going from intermediate steps to a string to pass into model # This is pretty tied to the prompt def convert_intermediate_steps(intermediate_steps): log = """" for action, observation in intermediate_steps: log += ( f""{action.tool}{action.tool_input}"" f""{observation}"" ) return log # Logic for converting tools to string to go in prompt def convert_tools(tools): return ""\\n"".join([f""{tool.name}: {tool.description}"" for tool in tools]) ``` Building an agent from a runnable usually involves a few things: 1. Data processing for the intermediate steps. These need to represented in a way that the language model can recognize them. This should be pretty tightly coupled to the instructions in the prompt 2. The prompt itself 3. The model, complete with stop tokens if needed 4. The output parser - should be in sync with how the prompt specifies things to be formatted. ```python agent = ( { ""question"": lambda x: x[""question""], ""intermediate_steps"": lambda x: convert_intermediate_steps( x[""intermediate_steps""] ), } | prompt.partial(tools=convert_tools(tool_list)) | model.bind(stop=["""", """"]) | XMLAgent.get_default_output_parser() ) ``` ```python agent_executor = AgentExecutor(agent=agent, tools=tool_list, verbose=True) ``` ```python agent_executor.invoke({""question"": ""whats the weather in New york?""}) ``` ```text > Entering new AgentExecutor chain... search weather in new york32 degrees The weather in New York is 32 degrees > Finished chain. {\'question\': \'whats the weather in New york?\', \'output\': \'The weather in New York is 32 degrees\'} ```']","A runnable lambda in LangChain is a function that can be executed within the LangChain framework. It's used to dynamically route logic based on input. You define a lambda function and pass it to a `RunnableLambda` object, which then executes the function when needed. This allows for flexible and dynamic behavior in your LangChain applications. 

Example:
```python
from langchain.schema.runnable import RunnableLambda

def route(info):
    if ""anthropic"" in info[""topic""].lower():
        return anthropic_chain
    elif ""langchain"" in info[""topic""].lower():
        return langchain_chain
    else:
        return general_chain

full_chain = {""topic"": chain, ""question"": lambda x: x[""question""]} | RunnableLambda(route)
```","A runnable lambda is an object that implements LangChain's `Runnable` interface and runs a callbale (i.e., a function). Note the function must accept a single argument.",0.9999999999666667,0.5,,0.12874330508144838,0.17187499999999997
24,What's an LLMChain,"['LangSmith LLM Runs | LangSmith LLM Runs This notebook demonstrates how to directly load data from LangSmith\'s LLM runs and fine-tune a model on that data. The process is simple and comprises 3 steps. 1. Select the LLM runs to train on. 2. Use the LangSmithRunChatLoader to load runs as chat sessions. 3. Fine-tune your model. Then you can use the fine-tuned model in your LangChain app. Before diving in, let\'s install our prerequisites. ## Prerequisites Ensure you\'ve installed langchain >= 0.0.311 and have configured your environment with your LangSmith API key. ```python %pip install -U langchain openai ``` ```python import os import uuid uid = uuid.uuid4().hex[:6] project_name = f""Run Fine-tuning Walkthrough {uid}"" os.environ[""LANGCHAIN_TRACING_V2""] = ""true"" os.environ[""LANGCHAIN_API_KEY""] = ""YOUR API KEY"" os.environ[""LANGCHAIN_PROJECT""] = project_name ``` ## 1. Select Runs The first step is selecting which runs to fine-tune on. A common case would be to select LLM runs within traces that have received positive user feedback. You can find examples of this in the[LangSmith Cookbook]( and in the [docs]( For the sake of this tutorial, we will generate some runs for you to use here. Let\'s try fine-tuning a simple function-calling chain. ```python from enum import Enum from langchain.pydantic_v1 import BaseModel, Field class Operation(Enum): add = ""+"" subtract = ""-"" multiply = ""*"" divide = ""/"" class Calculator(BaseModel): """"""A calculator function"""""" num1: float num2: float operation: Operation = Field(..., description=""+,-,*,/"") def calculate(self): if self.operation == Operation.add: return self.num1 + self.num2 elif self.operation == Operation.subtract: return self.num1 - self.num2 elif self.operation == Operation.multiply: return self.num1 * self.num2 elif self.operation == Operation.divide: if self.num2 != 0: return self.num1 / self.num2 else: return ""Cannot divide by zero"" ``` ```python from pprint import pprint from langchain.pydantic_v1 import BaseModel from langchain.utils.openai_functions import convert_pydantic_to_openai_function openai_function_def = convert_pydantic_to_openai_function(Calculator) pprint(openai_function_def) ``` ```text {\'description\': \'A calculator function\', \'name\': \'Calculator\', \'parameters\': {\'description\': \'A calculator function\', \'properties\': {\'num1\': {\'title\': \'Num1\', \'type\': \'number\'}, \'num2\': {\'title\': \'Num2\', \'type\': \'number\'}, \'operation\': {\'allOf\': [{\'description\': \'An \' \'enumeration.\', \'enum\': [\'+\', \'-\', \'*\', \'/\'], \'title\': \'Operation\'}], \'description\': \'+,-,*,/\'}}, \'required\': [\'num1\', \'num2\', \'operation\'], \'title\': \'Calculator\', \'type\': \'object\'}} ``` ```python from langchain.chat_models import ChatOpenAI from langchain.output_parsers.openai_functions import PydanticOutputFunctionsParser from langchain.prompts import ChatPromptTemplate prompt = ChatPromptTemplate.from_messages( [ (""system"", ""You are an accounting assistant.""), (""user"", ""{input}""), ] ) chain = ( prompt | ChatOpenAI().bind(functions=[openai_function_def]) | PydanticOutputFunctionsParser(pydantic_schema=Calculator) | (lambda x: x.calculate()) ) ``` ```python math_questions = [ ""What\'s 45/9?"", ""What\'s 81/9?"", ""What\'s 72/8?"", ""What\'s 56/7?"", ""What\'s 36/6?"", ""What\'s 64/8?"", ""What\'s 12*6?"", ""What\'s 8*8?"", ""What\'s 10*10?"", ""What\'s 11*11?"", ""What\'s 13*13?"", ""What\'s 45+30?"", ""What\'s 72+28?"", ""What\'s 56+44?"", ""What\'s 63+37?"", ""What\'s 70-35?"", ""What\'s 60-30?"", ""What\'s 50-25?"", ""What\'s 40-20?"", ""What\'s 30-15?"", ] results = chain.batch([{""input"": q} for q in math_questions], return_exceptions=True) ``` ```text Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.._completion_with_retry in 4.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet.. ``` #### Load runs that did not error Now we can select the successful runs to fine-tune on. ```python from langsmith.client import Client client = Client() ``` ```python successful_traces = { run.trace_id for run in client.list_runs( project_name=project_name, execution_order=1, error=False, ) } llm_runs = [ run for run in client.list_runs( project_name=project_name, run_type=""llm"", ) if run.trace_id in successful_traces ] ``` ## 2. Prepare data Now we can create an instance of LangSmithRunChatLoader and load the chat sessions using its lazy_load() method. ```python from langchain.chat_loaders.langsmith import LangSmithRunChatLoader loader = LangSmithRunChatLoader(runs=llm_runs) chat_sessions = loader.lazy_load() ``` #### With the chat sessions loaded, convert them into a format suitable for fine-tuning. ```python from langchain.adapters.openai import convert_messages_for_finetuning training_data = convert_messages_for_finetuning(chat_sessions) ``` ## 3. Fine-tune the model Now, initiate the fine-tuning process using the OpenAI library. ```python import json import time from io import BytesIO import openai my_file = BytesIO() for dialog in training_data: my_file.write((json.dumps({""messages"": dialog}) + ""\\n"").encode(""utf-8"")) my_file.seek(0) training_file = openai.File.create(file=my_file, purpose=""fine-tune"") job = openai.FineTuningJob.create( training_file=training_file.id, model=""gpt-3.5-turbo"", ) # Wait for the fine-tuning to complete (this may take some time) status = openai.FineTuningJob.retrieve(job.id).status start_time = time.time() while status != ""succeeded"": print(f""Status=[{status}]... {time.time() - start_time:.2f}s"", end=""\\r"", flush=True) time.sleep(5) status = openai.FineTuningJob.retrieve(job.id).status # Now your model is fine-tuned! ``` ```text Status=[running]... 346.26s. 31.70s ``` ## 4. Use in LangChain After fine-tuning, use the resulting model ID with the ChatOpenAI model class in your LangChain app. ```python # Get the fine-tuned model ID job = openai.FineTuningJob.retrieve(job.id) model_id = job.fine_tuned_model # Use the fine-tuned model in LangChain model = ChatOpenAI( model=model_id, temperature=1, ) ``` ```python (prompt | model).invoke({""input"": ""What\'s 56/7?""}) ``` ```text AIMessage(content=\'{\\n ""num1"": 56,\\n ""num2"": 7,\\n ""operation"": ""/""\\n}\') ``` Now you have successfully fine-tuned a model using data from LangSmith LLM runs! - [Prerequisites](#prerequisites) - [1. Select Runs](#1-select-runs) - [2. Prepare data](#2-prepare-data) - [3. Fine-tune the model](#3-fine-tune-the-model) - [4. Use in LangChain](#4-use-in-langchain)', 'MyScale | MyScale [MyScale]( is an integrated vector database. You can access your database in SQL and also from here, LangChain. `MyScale` can make use of [various data types and functions for filters]( It will boost up your LLM app no matter if you are scaling up your data or expand your system to broader application. In the notebook, we\'ll demo the `SelfQueryRetriever` wrapped around a `MyScale` vector store with some extra pieces we contributed to LangChain. In short, it can be condensed into 4 points: 1. Add `contain` comparator to match the list of any if there is more than one element matched 2. Add `timestamp` data type for datetime match (ISO-format, or YYYY-MM-DD) 3. Add `like` comparator for string pattern search 4. Add arbitrary function capability ## Creating a MyScale vector store MyScale has already been integrated to LangChain for a while. So you can follow [this notebook](/docs/integrations/vectorstores/myscale) to create your own vectorstore for a self-query retriever. **Note:** All self-query retrievers requires you to have `lark` installed (`pip install lark`). We use `lark` for grammar definition. Before you proceed to the next step, we also want to remind you that `clickhouse-connect` is also needed to interact with your MyScale backend. ```bash pip install lark clickhouse-connect ``` In this tutorial we follow other example\'s setting and use `OpenAIEmbeddings`. Remember to get an OpenAI API Key for valid access to LLMs. ```python import getpass import os os.environ[""OPENAI_API_KEY""] = getpass.getpass(""OpenAI API Key:"") os.environ[""MYSCALE_HOST""] = getpass.getpass(""MyScale URL:"") os.environ[""MYSCALE_PORT""] = getpass.getpass(""MyScale Port:"") os.environ[""MYSCALE_USERNAME""] = getpass.getpass(""MyScale Username:"") os.environ[""MYSCALE_PASSWORD""] = getpass.getpass(""MyScale Password:"") ``` ```python from langchain.embeddings.openai import OpenAIEmbeddings from langchain.schema import Document from langchain.vectorstores import MyScale embeddings = OpenAIEmbeddings() ``` ## Create some sample data As you can see, the data we created has some differences compared to other self-query retrievers. We replaced the keyword `year` with `date` which gives you finer control on timestamps. We also changed the type of the keyword `gerne` to a list of strings, where an LLM can use a new `contain` comparator to construct filters. We also provide the `like` comparator and arbitrary function support to filters, which will be introduced in next few cells. Now let\'s look at the data first. ```python docs = [ Document( page_content=""A bunch of scientists bring back dinosaurs and mayhem breaks loose"", metadata={""date"": ""1993-07-02"", ""rating"": 7.7, ""genre"": [""science fiction""]}, ), Document( page_content=""Leo DiCaprio gets lost in a dream within a dream within a dream within a ..."", metadata={""date"": ""2010-12-30"", ""director"": ""Christopher Nolan"", ""rating"": 8.2}, ), Document( page_content=""A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea"", metadata={""date"": ""2006-04-23"", ""director"": ""Satoshi Kon"", ""rating"": 8.6}, ), Document( page_content=""A bunch of normal-sized women are supremely wholesome and some men pine after them"", metadata={""date"": ""2019-08-22"", ""director"": ""Greta Gerwig"", ""rating"": 8.3}, ), Document( page_content=""Toys come alive and have a blast doing so"", metadata={""date"": ""1995-02-11"", ""genre"": [""animated""]}, ), Document( page_content=""Three men walk into the Zone, three men walk out of the Zone"", metadata={ ""date"": ""1979-09-10"", ""director"": ""Andrei Tarkovsky"", ""genre"": [""science fiction"", ""adventure""], ""rating"": 9.9, }, ), ] vectorstore = MyScale.from_documents( docs, embeddings, ) ``` ## Creating our self-querying retriever Just like other retrievers... simple and nice. ```python from langchain.chains.query_constructor.base import AttributeInfo from langchain.llms import OpenAI from langchain.retrievers.self_query.base import SelfQueryRetriever metadata_field_info = [ AttributeInfo( name=""genre"", description=""The genres of the movie"", type=""list[string]"", ), # If you want to include length of a list, just define it as a new column # This will teach the LLM to use it as a column when constructing filter. AttributeInfo( name=""length(genre)"", description=""The length of genres of the movie"", type=""integer"", ), # Now you can define a column as timestamp. By simply set the type to timestamp. AttributeInfo( name=""date"", description=""The date the movie was released"", type=""timestamp"", ), AttributeInfo( name=""director"", description=""The name of the movie director"", type=""string"", ), AttributeInfo( name=""rating"", description=""A 1-10 rating for the movie"", type=""float"" ), ] document_content_description = ""Brief summary of a movie"" llm = OpenAI(temperature=0) retriever = SelfQueryRetriever.from_llm( llm, vectorstore, document_content_description, metadata_field_info, verbose=True ) ``` ## Testing it out with self-query retriever\'s existing functionalities And now we can try actually using our retriever! ```python # This example only specifies a relevant query retriever.get_relevant_documents(""What are some movies about dinosaurs"") ``` ```python # This example only specifies a filter retriever.get_relevant_documents(""I want to watch a movie rated higher than 8.5"") ``` ```python # This example specifies a query and a filter retriever.get_relevant_documents(""Has Greta Gerwig directed any movies about women"") ``` ```python # This example specifies a composite filter retriever.get_relevant_documents( ""What\'s a highly rated (above 8.5) science fiction film?"" ) ``` ```python # This example specifies a query and composite filter retriever.get_relevant_documents( ""What\'s a movie after 1990 but before 2005 that\'s all about toys, and preferably is animated"" ) ``` # Wait a second... what else? Self-query retriever with MyScale can do more! Let\'s find out. ```python # You can use length(genres) to do anything you want retriever.get_relevant_documents(""What\'s a movie that have more than 1 genres?"") ``` ```python # Fine-grained datetime? You got it already. retriever.get_relevant_documents(""What\'s a movie that release after feb 1995?"") ``` ```python # Don\'t know what your exact filter should be? Use string pattern match! retriever.get_relevant_documents(""What\'s a movie whose name is like Andrei?"") ``` ```python # Contain works for lists: so you can match a list with contain comparator! retriever.get_relevant_documents( ""What\'s a movie who has genres science fiction and adventure?"" ) ``` ## Filter k We can also use the self query retriever to specify `k`: the number of documents to fetch. We can do this by passing `enable_limit=True` to the constructor. ```python retriever = SelfQueryRetriever.from_llm( llm, vectorstore, document_content_description, metadata_field_info, enable_limit=True, verbose=True, ) ``` ```python # This example only specifies a relevant query retriever.get_relevant_documents(""what are two movies about dinosaurs"") ``` - [Creating a MyScale vector store](#creating-a-myscale-vector-store) - [Create some sample data](#create-some-sample-data) - [Creating our self-querying retriever](#creating-our-self-querying-retriever) - [Testing it out with self-query retriever\'s existing functionalities](#testing-it-out-with-self-query-retrievers-existing-functionalities) - [Filter k](#filter-k)', 'Criteria Evaluation | Criteria Evaluation []( In scenarios where you wish to assess a model\'s output using a specific rubric or criteria set, the `criteria` evaluator proves to be a handy tool. It allows you to verify if an LLM or Chain\'s output complies with a defined set of criteria. To understand its functionality and configurability in depth, refer to the reference documentation of the [CriteriaEvalChain]( class. ### Usage without references In this example, you will use the `CriteriaEvalChain` to check whether an output is concise. First, create the evaluation chain to predict whether outputs are ""concise"". ```python from langchain.evaluation import load_evaluator evaluator = load_evaluator(""criteria"", criteria=""conciseness"") # This is equivalent to loading using the enum from langchain.evaluation import EvaluatorType evaluator = load_evaluator(EvaluatorType.CRITERIA, criteria=""conciseness"") ``` ```python eval_result = evaluator.evaluate_strings( prediction=""What\'s 2+2? That\'s an elementary question. The answer you\'re looking for is that two and two is four."", input=""What\'s 2+2?"", ) print(eval_result) ``` ```text {\'reasoning\': \'The criterion is conciseness, which means the submission should be brief and to the point. \\n\\nLooking at the submission, the answer to the question ""What\\\'s 2+2?"" is indeed ""four"". However, the respondent has added extra information, stating ""That\\\'s an elementary question."" This statement does not contribute to answering the question and therefore makes the response less concise.\\n\\nTherefore, the submission does not meet the criterion of conciseness.\\n\\nN\', \'value\': \'N\', \'score\': 0} ``` #### Output Format All string evaluators expose an [evaluate_strings]( (or async [aevaluate_strings]( method, which accepts: - input (str) The input to the agent. - prediction (str) The predicted response. The criteria evaluators return a dictionary with the following values: - score: Binary integer 0 to 1, where 1 would mean that the output is compliant with the criteria, and 0 otherwise - value: A ""Y"" or ""N"" corresponding to the score - reasoning: String ""chain of thought reasoning"" from the LLM generated prior to creating the score ## Using Reference Labels Some criteria (such as correctness) require reference labels to work correctly. To do this, initialize the `labeled_criteria` evaluator and call the evaluator with a `reference` string. ```python evaluator = load_evaluator(""labeled_criteria"", criteria=""correctness"") # We can even override the model\'s learned knowledge using ground truth labels eval_result = evaluator.evaluate_strings( input=""What is the capital of the US?"", prediction=""Topeka, KS"", reference=""The capital of the US is Topeka, KS, where it permanently moved from Washington D.C. on May 16, 2023"", ) print(f\'With ground truth: {eval_result[""score""]}\') ``` ```text With ground truth: 1 ``` **Default Criteria** Most of the time, you\'ll want to define your own custom criteria (see below), but we also provide some common criteria you can load with a single string. Here\'s a list of pre-implemented criteria. Note that in the absence of labels, the LLM merely predicts what it thinks the best answer is and is not grounded in actual law or context. ```python from langchain.evaluation import Criteria # For a list of other default supported criteria, try calling `supported_default_criteria` list(Criteria) ``` ```text [, , , , , , , , , , ] ``` ## Custom Criteria To evaluate outputs against your own custom criteria, or to be more explicit the definition of any of the default criteria, pass in a dictionary of `""criterion_name"": ""criterion_description""` Note: it\'s recommended that you create a single evaluator per criterion. This way, separate feedback can be provided for each aspect. Additionally, if you provide antagonistic criteria, the evaluator won\'t be very useful, as it will be configured to predict compliance for ALL of the criteria provided. ```python custom_criterion = { ""numeric"": ""Does the output contain numeric or mathematical information?"" } eval_chain = load_evaluator( EvaluatorType.CRITERIA, criteria=custom_criterion, ) query = ""Tell me a joke"" prediction = ""I ate some square pie but I don\'t know the square of pi."" eval_result = eval_chain.evaluate_strings(prediction=prediction, input=query) print(eval_result) # If you wanted to specify multiple criteria. Generally not recommended custom_criteria = { ""numeric"": ""Does the output contain numeric information?"", ""mathematical"": ""Does the output contain mathematical information?"", ""grammatical"": ""Is the output grammatically correct?"", ""logical"": ""Is the output logical?"", } eval_chain = load_evaluator( EvaluatorType.CRITERIA, criteria=custom_criteria, ) eval_result = eval_chain.evaluate_strings(prediction=prediction, input=query) print(""Multi-criteria evaluation"") print(eval_result) ``` ```text {\'reasoning\': ""The criterion asks if the output contains numeric or mathematical information. The joke in the submission does contain mathematical information. It refers to the mathematical concept of squaring a number and also mentions \'pi\', which is a mathematical constant. Therefore, the submission does meet the criterion.\\n\\nY"", \'value\': \'Y\', \'score\': 1} {\'reasoning\': \'Let\\\'s assess the submission based on the given criteria:\\n\\n1. Numeric: The output does not contain any explicit numeric information. The word ""square"" and ""pi"" are mathematical terms but they are not numeric information per se.\\n\\n2. Mathematical: The output does contain mathematical information. The terms ""square"" and ""pi"" are mathematical terms. The joke is a play on the mathematical concept of squaring a number (in this case, pi).\\n\\n3. Grammatical: The output is grammatically correct. The sentence structure, punctuation, and word usage are all correct.\\n\\n4. Logical: The output is logical. It makes sense within the context of the joke. The joke is a play on words between the mathematical concept of squaring a number (pi) and eating a square pie.\\n\\nBased on the above analysis, the submission does not meet all the criteria because it does not contain numeric information.\\nN\', \'value\': \'N\', \'score\': 0} ``` ## Using Constitutional Principles Custom rubrics are similar to principles from [Constitutional AI]( You can directly use your `ConstitutionalPrinciple` objects to instantiate the chain and take advantage of the many existing principles in LangChain. ```python from langchain.chains.constitutional_ai.principles import PRINCIPLES print(f""{len(PRINCIPLES)} available principles"") list(PRINCIPLES.items())[:5] ``` ```text 54 available principles [(\'harmful1\', ConstitutionalPrinciple(critique_request=\'Identify specific ways in which the assistant\'s last response is harmful, unethical, racist, sexist, toxic, dangerous, or illegal.\', revision_request=\'Please rewrite the assistant response to remove any and all harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\', name=\'harmful1\')), (\'harmful2\', ConstitutionalPrinciple(critique_request=\'Explain ways in which the assistant\'s preceding response may be harmful to either the human or others. For example, if the assistant says anything that encourages dangerous or illegal activity, make sure to point it out. Other examples of harm include racism, sexism, and other social biases.\', revision_request=\'Please rewrite the assistant response to remove any and all harmful content, including anything dangerous, illegal, racist, sexist or socially biased.\', name=\'harmful2\')), (\'harmful3\', ConstitutionalPrinciple(critique_request=\'Identify all ways in which the assistant\'s last response is harmful, unethical, or socially biased. Furthermore, provide specific details on how the assistant can improve its response.\', revision_request=\'Please rewrite the assistant response to remove all harmful, unethical, or socially biased content, and move the conversation in a positive direction.\', name=\'harmful3\')), (\'harmful4\', ConstitutionalPrinciple(critique_request=\'Identify specific ways in which the assistant\'s precedings response is harmful. Also point out potential harmfulness in the human\'s questions and responses. Examples of harm include but are not limited to racism, sexism, toxicity.\', revision_request=\'Rewrite the assistant response to remove all harmful content, and to politely point out any harmful assumptions from the human, such as racism, sexism, toxicity.\', name=\'harmful4\')), (\'insensitive\', ConstitutionalPrinciple(critique_request=\'Discuss the assistant\'s last response to the human and whether it is in any way insensitive, sexist, racist, or socially inappropriate.\', revision_request=\'Revise the assistant\'s last response to remove content that is in any way insensitive, sexist, racist, or socially inappropriate.\', name=\'insensitive\'))] ``` ```python evaluator = load_evaluator(EvaluatorType.CRITERIA, criteria=PRINCIPLES[""harmful1""]) eval_result = evaluator.evaluate_strings( prediction=""I say that man is a lilly-livered nincompoop"", input=""What do you think of Will?"", ) print(eval_result) ``` ```text {\'reasoning\': \'The criterion asks to identify if the assistant\\\'s response is harmful, unethical, racist, sexist, toxic, dangerous, or illegal.\\n\\nLooking at the assistant\\\'s response, it is clear that it is not racist or sexist as it does not discriminate or stereotype based on race or gender. \\n\\nThe response is also not illegal as it does not involve any criminal activity or encourage any form of illegal behavior.\\n\\nThe response is not dangerous as it does not pose a physical threat or risk to anyone\\\'s safety.\\n\\nHowever, the assistant\\\'s response can be considered harmful and toxic as it uses derogatory language (""lilly-livered nincompoop"") to describe \\\'Will\\\'. This can be seen as a form of verbal abuse or insult, which can cause emotional harm.\\n\\nThe response can also be seen as unethical, as it is generally considered inappropriate to insult or belittle someone in this manner.\\n\\nN\', \'value\': \'N\', \'score\': 0} ``` ## Configuring the LLM If you don\'t specify an eval LLM, the `load_evaluator` method will initialize a `gpt-4` LLM to power the grading chain. Below, use an anthropic model instead. ```python # %pip install ChatAnthropic # %env ANTHROPIC_API_KEY= ``` ```python from langchain.chat_models import ChatAnthropic llm = ChatAnthropic(temperature=0) evaluator = load_evaluator(""criteria"", llm=llm, criteria=""conciseness"") ``` ```python eval_result = evaluator.evaluate_strings( prediction=""What\'s 2+2? That\'s an elementary question. The answer you\'re looking for is that two and two is four."", input=""What\'s 2+2?"", ) print(eval_result) ``` ```text {\'reasoning\': \'Step 1) Analyze the conciseness criterion: Is the submission concise and to the point?\\nStep 2) The submission provides extraneous information beyond just answering the question directly. It characterizes the question as ""elementary"" and provides reasoning for why the answer is 4. This additional commentary makes the submission not fully concise.\\nStep 3) Therefore, based on the analysis of the conciseness criterion, the submission does not meet the criteria.\\n\\nN\', \'value\': \'N\', \'score\': 0} ``` # Configuring the Prompt If you want to completely customize the prompt, you can initialize the evaluator with a custom prompt template as follows. ```python from langchain.prompts import PromptTemplate fstring = """"""Respond Y or N based on how well the following response follows the specified rubric. Grade only based on the rubric and expected response: Grading Rubric: {criteria} Expected Response: {reference} DATA: --------- Question: {input} Response: {output} --------- Write out your explanation for each criterion, then respond with Y or N on a new line."""""" prompt = PromptTemplate.from_template(fstring) evaluator = load_evaluator(""labeled_criteria"", criteria=""correctness"", prompt=prompt) ``` ```python eval_result = evaluator.evaluate_strings( prediction=""What\'s 2+2? That\'s an elementary question. The answer you\'re looking for is that two and two is four."", input=""What\'s 2+2?"", reference=""It\'s 17 now."", ) print(eval_result) ``` ```text {\'reasoning\': \'Correctness: No, the response is not correct. The expected response was ""It\\\'s 17 now."" but the response given was ""What\\\'s 2+2? That\\\'s an elementary question. The answer you\\\'re looking for is that two and two is four.""\', \'value\': \'N\', \'score\': 0} ``` ## Conclusion In these examples, you used the `CriteriaEvalChain` to evaluate model outputs against custom criteria, including a custom rubric and constitutional principles. Remember when selecting criteria to decide whether they ought to require ground truth labels or not. Things like ""correctness"" are best evaluated with ground truth or with extensive context. Also, remember to pick aligned principles for a given chain so that the classification makes sense. - [Usage without references](#usage-without-references) - [Using Reference Labels](#using-reference-labels) - [Custom Criteria](#custom-criteria) - [Using Constitutional Principles](#using-constitutional-principles) - [Configuring the LLM](#configuring-the-llm) - [Conclusion](#conclusion)']","An LLMChain in LangChain is a chain of language model (LLM) calls that can be used to build complex workflows. It allows you to connect multiple LLM calls in a sequence, where the output of one call becomes the input for the next. This is useful for creating multi-step tasks that require the language model to generate, refine, or act on its own outputs iteratively.

Here's a simple example of how you might use an LLMChain:

```python
from langchain.chains import LLMChain
from langchain.llms import HuggingFaceLLM

# Define the language model
llm = HuggingFaceLLM(model_id=""your-model-id"")

# Define the first step of the chain
first_step = LLMChain(llm=llm, prompt=""Write a short story about a talking cat."")

# Define the second step of the chain
second_step = LLMChain(llm=llm, prompt=""Continue the story with the cat discovering a secret."")

# Combine the steps into a chain
chain = LLMChain(chain=[first_step, second_step])

# Run the chain
output = chain.run()
print(output)
```

In this example, the first LLMChain generates a short story about a talking cat, and the second LLMChain continues the story with the cat discovering a secret. The `chain.run()` method executes both steps sequentially, with the output of the first step becoming the input for the second step.","An LLMChain is a chain that composes basic LLM functionality. It consists of a PromptTemplate and a language model (either an LLM or chat model). It formats the prompt template using the input key values provided (and also memory key values, if available), passes the formatted string to LLM and returns the LLM output.",0.9999999999,0.75,,0.13871452725344985,0.15730337078651685
25,What's the difference between a prompt template and a chat prompt template?,"['Few-shot examples for chat models | Few-shot examples for chat models This notebook covers how to use few-shot examples in chat models. There does not appear to be solid consensus on how best to do few-shot prompting, and the optimal prompt compilation will likely vary by model. Because of this, we provide few-shot prompt templates like the [FewShotChatMessagePromptTemplate]( as a flexible starting point, and you can modify or replace them as you see fit. The goal of few-shot prompt templates are to dynamically select examples based on an input, and then format the examples in a final prompt to provide for the model. **Note:** The following code examples are for chat models. For similar few-shot prompt examples for completion models (LLMs), see the [few-shot prompt templates](/docs/modules/model_io/prompts/prompt_templates/few_shot_examples) guide. ### Fixed Examples The most basic (and common) few-shot prompting technique is to use a fixed prompt example. This way you can select a chain, evaluate it, and avoid worrying about additional moving parts in production. The basic components of the template are: - `examples`: A list of dictionary examples to include in the final prompt. - `example_prompt`: converts each example into 1 or more messages through its [format_messages]( method. A common example would be to convert each example into one human message and one AI message response, or a human message followed by a function call message. Below is a simple demonstration. First, import the modules for this example: ```python from langchain.prompts import ( ChatPromptTemplate, FewShotChatMessagePromptTemplate, ) ``` Then, define the examples you\'d like to include. ```python examples = [ {""input"": ""2+2"", ""output"": ""4""}, {""input"": ""2+3"", ""output"": ""5""}, ] ``` Next, assemble them into the few-shot prompt template. ```python # This is a prompt template used to format each individual example. example_prompt = ChatPromptTemplate.from_messages( [ (""human"", ""{input}""), (""ai"", ""{output}""), ] ) few_shot_prompt = FewShotChatMessagePromptTemplate( example_prompt=example_prompt, examples=examples, ) print(few_shot_prompt.format()) ``` ```text Human: 2+2 AI: 4 Human: 2+3 AI: 5 ``` Finally, assemble your final prompt and use it with a model. ```python final_prompt = ChatPromptTemplate.from_messages( [ (""system"", ""You are a wondrous wizard of math.""), few_shot_prompt, (""human"", ""{input}""), ] ) ``` ```python from langchain.chat_models import ChatAnthropic chain = final_prompt | ChatAnthropic(temperature=0.0) chain.invoke({""input"": ""What\'s the square of a triangle?""}) ``` ```text AIMessage(content=\' Triangles do not have a ""square"". A square refers to a shape with 4 equal sides and 4 right angles. Triangles have 3 sides and 3 angles.\\n\\nThe area of a triangle can be calculated using the formula:\\n\\nA = 1/2 * b * h\\n\\nWhere:\\n\\nA is the area \\nb is the base (the length of one of the sides)\\nh is the height (the length from the base to the opposite vertex)\\n\\nSo the area depends on the specific dimensions of the triangle. There is no single ""square of a triangle"". The area can vary greatly depending on the base and height measurements.\', additional_kwargs={}, example=False) ``` ## Dynamic few-shot prompting Sometimes you may want to condition which examples are shown based on the input. For this, you can replace the `examples` with an `example_selector`. The other components remain the same as above! To review, the dynamic few-shot prompt template would look like: - `example_selector`: responsible for selecting few-shot examples (and the order in which they are returned) for a given input. These implement the [BaseExampleSelector]( interface. A common example is the vectorstore-backed [SemanticSimilarityExampleSelector]( - `example_prompt`: convert each example into 1 or more messages through its [format_messages]( method. A common example would be to convert each example into one human message and one AI message response, or a human message followed by a function call message. These once again can be composed with other messages and chat templates to assemble your final prompt. ```python from langchain.embeddings import OpenAIEmbeddings from langchain.prompts import SemanticSimilarityExampleSelector from langchain.vectorstores import Chroma ``` Since we are using a vectorstore to select examples based on semantic similarity, we will want to first populate the store. ```python examples = [ {""input"": ""2+2"", ""output"": ""4""}, {""input"": ""2+3"", ""output"": ""5""}, {""input"": ""2+4"", ""output"": ""6""}, {""input"": ""What did the cow say to the moon?"", ""output"": ""nothing at all""}, { ""input"": ""Write me a poem about the moon"", ""output"": ""One for the moon, and one for me, who are we to talk about the moon?"", }, ] to_vectorize = ["" "".join(example.values()) for example in examples] embeddings = OpenAIEmbeddings() vectorstore = Chroma.from_texts(to_vectorize, embeddings, metadatas=examples) ``` #### Create the example_selector With a vectorstore created, you can create the `example_selector`. Here we will isntruct it to only fetch the top 2 examples. ```python example_selector = SemanticSimilarityExampleSelector( vectorstore=vectorstore, k=2, ) # The prompt template will load examples by passing the input do the `select_examples` method example_selector.select_examples({""input"": ""horse""}) ``` ```text [{\'input\': \'What did the cow say to the moon?\', \'output\': \'nothing at all\'}, {\'input\': \'2+4\', \'output\': \'6\'}] ``` #### Create prompt template Assemble the prompt template, using the `example_selector` created above. ```python from langchain.prompts import ( ChatPromptTemplate, FewShotChatMessagePromptTemplate, ) # Define the few-shot prompt. few_shot_prompt = FewShotChatMessagePromptTemplate( # The input variables select the values to pass to the example_selector input_variables=[""input""], example_selector=example_selector, # Define how each example will be formatted. # In this case, each example will become 2 messages: # 1 human, and 1 AI example_prompt=ChatPromptTemplate.from_messages( [(""human"", ""{input}""), (""ai"", ""{output}"")] ), ) ``` Below is an example of how this would be assembled. ```python print(few_shot_prompt.format(input=""What\'s 3+3?"")) ``` ```text Human: 2+3 AI: 5 Human: 2+2 AI: 4 ``` Assemble the final prompt template: ```python final_prompt = ChatPromptTemplate.from_messages( [ (""system"", ""You are a wondrous wizard of math.""), few_shot_prompt, (""human"", ""{input}""), ] ) ``` ```python print(few_shot_prompt.format(input=""What\'s 3+3?"")) ``` ```text Human: 2+3 AI: 5 Human: 2+2 AI: 4 ``` #### Use with an LLM Now, you can connect your model to the few-shot prompt. ```python from langchain.chat_models import ChatAnthropic chain = final_prompt | ChatAnthropic(temperature=0.0) chain.invoke({""input"": ""What\'s 3+3?""}) ``` ```text AIMessage(content=\' 3 + 3 = 6\', additional_kwargs={}, example=False) ``` - [Fixed Examples](#fixed-examples) - [Dynamic few-shot prompting](#dynamic-few-shot-prompting)', 'Customizing Conversational Memory | Customizing Conversational Memory This notebook walks through a few ways to customize conversational memory. ```python from langchain.chains import ConversationChain from langchain.llms import OpenAI from langchain.memory import ConversationBufferMemory llm = OpenAI(temperature=0) ``` ## AI prefix The first way to do so is by changing the AI prefix in the conversation summary. By default, this is set to ""AI"", but you can set this to be anything you want. Note that if you change this, you should also change the prompt used in the chain to reflect this naming change. Let\'s walk through an example of that in the example below. ```python # Here it is by default set to ""AI"" conversation = ConversationChain( llm=llm, verbose=True, memory=ConversationBufferMemory() ) ``` ```python conversation.predict(input=""Hi there!"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: Human: Hi there! AI: > Finished ConversationChain chain. "" Hi there! It\'s nice to meet you. How can I help you today?"" ``` ```python conversation.predict(input=""What\'s the weather?"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: Human: Hi there! AI: Hi there! It\'s nice to meet you. How can I help you today? Human: What\'s the weather? AI: > Finished ConversationChain chain. \' The current weather is sunny and warm with a temperature of 75 degrees Fahrenheit. The forecast for the next few days is sunny with temperatures in the mid-70s.\' ``` ```python # Now we can override it and set it to ""AI Assistant"" from langchain.prompts.prompt import PromptTemplate template = """"""The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: {history} Human: {input} AI Assistant:"""""" PROMPT = PromptTemplate(input_variables=[""history"", ""input""], template=template) conversation = ConversationChain( prompt=PROMPT, llm=llm, verbose=True, memory=ConversationBufferMemory(ai_prefix=""AI Assistant""), ) ``` ```python conversation.predict(input=""Hi there!"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: Human: Hi there! AI Assistant: > Finished ConversationChain chain. "" Hi there! It\'s nice to meet you. How can I help you today?"" ``` ```python conversation.predict(input=""What\'s the weather?"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: Human: Hi there! AI Assistant: Hi there! It\'s nice to meet you. How can I help you today? Human: What\'s the weather? AI Assistant: > Finished ConversationChain chain. \' The current weather is sunny and warm with a temperature of 75 degrees Fahrenheit. The forecast for the rest of the day is sunny with a high of 78 degrees and a low of 65 degrees.\' ``` ## Human prefix The next way to do so is by changing the Human prefix in the conversation summary. By default, this is set to ""Human"", but you can set this to be anything you want. Note that if you change this, you should also change the prompt used in the chain to reflect this naming change. Let\'s walk through an example of that in the example below. ```python # Now we can override it and set it to ""Friend"" from langchain.prompts.prompt import PromptTemplate template = """"""The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: {history} Friend: {input} AI:"""""" PROMPT = PromptTemplate(input_variables=[""history"", ""input""], template=template) conversation = ConversationChain( prompt=PROMPT, llm=llm, verbose=True, memory=ConversationBufferMemory(human_prefix=""Friend""), ) ``` ```python conversation.predict(input=""Hi there!"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: Friend: Hi there! AI: > Finished ConversationChain chain. "" Hi there! It\'s nice to meet you. How can I help you today?"" ``` ```python conversation.predict(input=""What\'s the weather?"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: Friend: Hi there! AI: Hi there! It\'s nice to meet you. How can I help you today? Friend: What\'s the weather? AI: > Finished ConversationChain chain. \' The weather right now is sunny and warm with a temperature of 75 degrees Fahrenheit. The forecast for the rest of the day is mostly sunny with a high of 82 degrees.\' ``` - [AI prefix](#ai-prefix) - [Human prefix](#human-prefix)', 'Baseten | Baseten [Baseten]( provides all the infrastructure you need to deploy and serve ML models performantly, scalably, and cost-efficiently. This example demonstrates using Langchain with models deployed on Baseten. # Setup To run this notebook, you\'ll need a [Baseten account]( and an [API key]( You\'ll also need to install the Baseten Python package: ```bash pip install baseten ``` ```python import baseten baseten.login(""YOUR_API_KEY"") ``` # Single model call First, you\'ll need to deploy a model to Baseten. You can deploy foundation models like WizardLM and Alpaca with one click from the [Baseten model library]( or if you have your own model, [deploy it with this tutorial]( In this example, we\'ll work with WizardLM. [Deploy WizardLM here]( and follow along with the deployed [model\'s version ID]( ```python from langchain.llms import Baseten ``` ```python # Load the model wizardlm = Baseten(model=""MODEL_VERSION_ID"", verbose=True) ``` ```python # Prompt the model wizardlm(""What is the difference between a Wizard and a Sorcerer?"") ``` # Chained model calls We can chain together multiple calls to one or multiple models, which is the whole point of Langchain! This example uses WizardLM to plan a meal with an entree, three sides, and an alcoholic and non-alcoholic beverage pairing. ```python from langchain.chains import LLMChain, SimpleSequentialChain from langchain.prompts import PromptTemplate ``` ```python # Build the first link in the chain prompt = PromptTemplate( input_variables=[""cuisine""], template=""Name a complex entree for a {cuisine} dinner. Respond with just the name of a single dish."", ) link_one = LLMChain(llm=wizardlm, prompt=prompt) ``` ```python # Build the second link in the chain prompt = PromptTemplate( input_variables=[""entree""], template=""What are three sides that would go with {entree}. Respond with only a list of the sides."", ) link_two = LLMChain(llm=wizardlm, prompt=prompt) ``` ```python # Build the third link in the chain prompt = PromptTemplate( input_variables=[""sides""], template=""What is one alcoholic and one non-alcoholic beverage that would go well with this list of sides: {sides}. Respond with only the names of the beverages."", ) link_three = LLMChain(llm=wizardlm, prompt=prompt) ``` ```python # Run the full chain! menu_maker = SimpleSequentialChain( chains=[link_one, link_two, link_three], verbose=True ) menu_maker.run(""South Indian"") ```']","A prompt template is a general template used to format input for models, while a chat prompt template is specifically designed for conversational models, often including roles like ""human"" and ""AI"" to structure dialogue.","A prompt template wraps instructions, examples, and other formatting information for the llm's prompts. A chat prompt template provides similar functionality for chat models. The largest difference is that chat prompt templates expose a `format_messages` method to convert the  prompt template into a list of structured chat messages.",0.99999999995,0.0,1.0,0.06666337489197861,0.30952380952380953
26,What is ConversationSummaryBufferMemory?,"['Chatbots | Chatbots []( ## Use case Chatbots are one of the central LLM use-cases. The core features of chatbots are that they can have long-running conversations and have access to information that users want to know about. Aside from basic prompting and LLMs, memory and retrieval are the core components of a chatbot. Memory allows a chatbot to remember past interactions, and retrieval provides a chatbot with up-to-date, domain-specific information. ![Image description](/assets/images/chat_use_case-eb8a4883931d726e9f23628a0d22e315.png) ## Overview The chat model interface is based around messages rather than raw text. Several components are important to consider for chat: - `chat model`: See [here](/docs/integrations/chat) for a list of chat model integrations and [here](/docs/modules/model_io/chat) for documentation on the chat model interface in LangChain. You can use `LLMs` (see [here](/docs/modules/model_io/llms)) for chatbots as well, but chat models have a more conversational tone and natively support a message interface. - `prompt template`: Prompt templates make it easy to assemble prompts that combine default messages, user input, chat history, and (optionally) additional retrieved context. - `memory`: [See here](/docs/modules/memory/) for in-depth documentation on memory types - `retriever` (optional): [See here](/docs/modules/data_connection/retrievers) for in-depth documentation on retrieval systems. These are useful if you want to build a chatbot with domain-specific knowledge. ## Quickstart Here\'s a quick preview of how we can create chatbot interfaces. First let\'s install some dependencies and set the required credentials: ```bash pip install langchain openai # Set env var OPENAI_API_KEY or load from a .env file: # import dotenv # dotenv.load_dotenv() ``` With a plain chat model, we can get chat completions by [passing one or more messages](/docs/modules/model_io/chat) to the model. The chat model will respond with a message. ```python from langchain.chat_models import ChatOpenAI from langchain.schema import HumanMessage, SystemMessage chat = ChatOpenAI() chat( [ HumanMessage( content=""Translate this sentence from English to French: I love programming."" ) ] ) ``` ```text AIMessage(content=""J\'adore la programmation."", additional_kwargs={}, example=False) ``` And if we pass in a list of messages: ```python messages = [ SystemMessage( content=""You are a helpful assistant that translates English to French."" ), HumanMessage(content=""I love programming.""), ] chat(messages) ``` ```text AIMessage(content=""J\'adore la programmation."", additional_kwargs={}, example=False) ``` We can then wrap our chat model in a `ConversationChain`, which has built-in memory for remembering past user inputs and model outputs. ```python from langchain.chains import ConversationChain conversation = ConversationChain(llm=chat) conversation.run(""Translate this sentence from English to French: I love programming."") ``` ```text \'Je adore la programmation.\' ``` ```python conversation.run(""Translate it to German."") ``` ```text \'Ich liebe Programmieren.\' ``` ## Memory As we mentioned above, the core component of chatbots is the memory system. One of the simplest and most commonly used forms of memory is `ConversationBufferMemory`: - This memory allows for storing of messages in a `buffer` - When called in a chain, it returns all of the messages it has stored LangChain comes with many other types of memory, too. [See here](/docs/modules/memory/) for in-depth documentation on memory types. For now let\'s take a quick look at ConversationBufferMemory. We can manually add a few chat messages to the memory like so: ```python from langchain.memory import ConversationBufferMemory memory = ConversationBufferMemory() memory.chat_memory.add_user_message(""hi!"") memory.chat_memory.add_ai_message(""whats up?"") ``` And now we can load from our memory. The key method exposed by all `Memory` classes is `load_memory_variables`. This takes in any initial chain input and returns a list of memory variables which are added to the chain input. Since this simple memory type doesn\'t actually take into account the chain input when loading memory, we can pass in an empty input for now: ```python memory.load_memory_variables({}) ``` ```text {\'history\': \'Human: hi!\\nAI: whats up?\'} ``` We can also keep a sliding window of the most recent `k` interactions using `ConversationBufferWindowMemory`. ```python from langchain.memory import ConversationBufferWindowMemory memory = ConversationBufferWindowMemory(k=1) memory.save_context({""input"": ""hi""}, {""output"": ""whats up""}) memory.save_context({""input"": ""not much you""}, {""output"": ""not much""}) memory.load_memory_variables({}) ``` ```text {\'history\': \'Human: not much you\\nAI: not much\'} ``` `ConversationSummaryMemory` is an extension of this theme. It creates a summary of the conversation over time. This memory is most useful for longer conversations where the full message history would consume many tokens. ```python from langchain.llms import OpenAI from langchain.memory import ConversationSummaryMemory llm = OpenAI(temperature=0) memory = ConversationSummaryMemory(llm=llm) memory.save_context({""input"": ""hi""}, {""output"": ""whats up""}) memory.save_context( {""input"": ""im working on better docs for chatbots""}, {""output"": ""oh, that sounds like a lot of work""}, ) memory.save_context( {""input"": ""yes, but it\'s worth the effort""}, {""output"": ""agreed, good docs are important!""}, ) ``` ```python memory.load_memory_variables({}) ``` ```text {\'history\': \'\\nThe human greets the AI, to which the AI responds. The human then mentions they are working on better docs for chatbots, to which the AI responds that it sounds like a lot of work. The human agrees that it is worth the effort, and the AI agrees that good docs are important.\'} ``` `ConversationSummaryBufferMemory` extends this a bit further: It uses token length rather than number of interactions to determine when to flush interactions. ```python from langchain.memory import ConversationSummaryBufferMemory memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=10) memory.save_context({""input"": ""hi""}, {""output"": ""whats up""}) memory.save_context({""input"": ""not much you""}, {""output"": ""not much""}) ``` ## Conversation We can unpack what goes under the hood with `ConversationChain`. We can specify our memory, `ConversationSummaryMemory` and we can specify the prompt. ```python from langchain.chains import LLMChain from langchain.prompts import ( ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate, ) # LLM llm = ChatOpenAI() # Prompt prompt = ChatPromptTemplate( messages=[ SystemMessagePromptTemplate.from_template( ""You are a nice chatbot having a conversation with a human."" ), # The `variable_name` here is what must align with memory MessagesPlaceholder(variable_name=""chat_history""), HumanMessagePromptTemplate.from_template(""{question}""), ] ) # Notice that we `return_messages=True` to fit into the MessagesPlaceholder # Notice that `""chat_history""` aligns with the MessagesPlaceholder name memory = ConversationBufferMemory(memory_key=""chat_history"", return_messages=True) conversation = LLMChain(llm=llm, prompt=prompt, verbose=True, memory=memory) # Notice that we just pass in the `question` variables - `chat_history` gets populated by memory conversation({""question"": ""hi""}) ``` ```text > Entering new LLMChain chain... Prompt after formatting: System: You are a nice chatbot having a conversation with a human. Human: hi > Finished chain. {\'question\': \'hi\', \'chat_history\': [HumanMessage(content=\'hi\', additional_kwargs={}, example=False), AIMessage(content=\'Hello! How can I assist you today?\', additional_kwargs={}, example=False)], \'text\': \'Hello! How can I assist you today?\'} ``` ```python conversation( {""question"": ""Translate this sentence from English to French: I love programming.""} ) ``` ```text > Entering new LLMChain chain... Prompt after formatting: System: You are a nice chatbot having a conversation with a human. Human: hi AI: Hello! How can I assist you today? Human: Translate this sentence from English to French: I love programming. > Finished chain. {\'question\': \'Translate this sentence from English to French: I love programming.\', \'chat_history\': [HumanMessage(content=\'hi\', additional_kwargs={}, example=False), AIMessage(content=\'Hello! How can I assist you today?\', additional_kwargs={}, example=False), HumanMessage(content=\'Translate this sentence from English to French: I love programming.\', additional_kwargs={}, example=False), AIMessage(content=\'Sure! The translation of ""I love programming"" from English to French is ""J\\\'adore programmer.""\', additional_kwargs={}, example=False)], \'text\': \'Sure! The translation of ""I love programming"" from English to French is ""J\\\'adore programmer.""\'} ``` ```python conversation({""question"": ""Now translate the sentence to German.""}) ``` ```text > Entering new LLMChain chain... Prompt after formatting: System: You are a nice chatbot having a conversation with a human. Human: hi AI: Hello! How can I assist you today? Human: Translate this sentence from English to French: I love programming. AI: Sure! The translation of ""I love programming"" from English to French is ""J\'adore programmer."" Human: Now translate the sentence to German. > Finished chain. {\'question\': \'Now translate the sentence to German.\', \'chat_history\': [HumanMessage(content=\'hi\', additional_kwargs={}, example=False), AIMessage(content=\'Hello! How can I assist you today?\', additional_kwargs={}, example=False), HumanMessage(content=\'Translate this sentence from English to French: I love programming.\', additional_kwargs={}, example=False), AIMessage(content=\'Sure! The translation of ""I love programming"" from English to French is ""J\\\'adore programmer.""\', additional_kwargs={}, example=False), HumanMessage(content=\'Now translate the sentence to German.\', additional_kwargs={}, example=False), AIMessage(content=\'Certainly! The translation of ""I love programming"" from English to German is ""Ich liebe das Programmieren.""\', additional_kwargs={}, example=False)], \'text\': \'Certainly! The translation of ""I love programming"" from English to German is ""Ich liebe das Programmieren.""\'} ``` We can see the chat history preserved in the prompt using the [LangSmith trace]( ![Image description](/assets/images/chat_use_case_2-a76871806149f125d08ff149363e0c2c.png) ## Chat Retrieval Now, suppose we want to [chat with documents]( or some other source of knowledge. This is popular use case, combining chat with [document retrieval](/docs/use_cases/question_answering). It allows us to chat with specific information that the model was not trained on. ```bash pip install tiktoken chromadb ``` Load a blog post. ```python from langchain.document_loaders import WebBaseLoader loader = WebBaseLoader("" data = loader.load() ``` Split and store this in a vector. ```python from langchain.text_splitter import RecursiveCharacterTextSplitter text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0) all_splits = text_splitter.split_documents(data) from langchain.embeddings import OpenAIEmbeddings from langchain.vectorstores import Chroma vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings()) ``` Create our memory, as before, but\'s let\'s use `ConversationSummaryMemory`. ```python memory = ConversationSummaryMemory( llm=llm, memory_key=""chat_history"", return_messages=True ) ``` ```python from langchain.chains import ConversationalRetrievalChain from langchain.chat_models import ChatOpenAI llm = ChatOpenAI() retriever = vectorstore.as_retriever() qa = ConversationalRetrievalChain.from_llm(llm, retriever=retriever, memory=memory) ``` ```python qa(""How do agents use Task decomposition?"") ``` ```text {\'question\': \'How do agents use Task decomposition?\', \'chat_history\': [SystemMessage(content=\'\', additional_kwargs={})], \'answer\': \'Agents can use task decomposition in several ways:\\n\\n1. Simple prompting: Agents can use Language Model based prompting to break down tasks into subgoals. For example, by providing prompts like ""Steps for XYZ"" or ""What are the subgoals for achieving XYZ?"", the agent can generate a sequence of smaller steps that lead to the completion of the overall task.\\n\\n2. Task-specific instructions: Agents can be given task-specific instructions to guide their planning process. For example, if the task is to write a novel, the agent can be instructed to ""Write a story outline."" This provides a high-level structure for the task and helps in breaking it down into smaller components.\\n\\n3. Human inputs: Agents can also take inputs from humans to decompose tasks. This can be done through direct communication or by leveraging human expertise. Humans can provide guidance and insights to help the agent break down complex tasks into manageable subgoals.\\n\\nOverall, task decomposition allows agents to break down large tasks into smaller, more manageable subgoals, enabling them to plan and execute complex tasks efficiently.\'} ``` ```python qa(""What are the various ways to implement memory to support it?"") ``` ```text {\'question\': \'What are the various ways to implement memory to support it?\', \'chat_history\': [SystemMessage(content=\'The human asks how agents use task decomposition. The AI explains that agents can use task decomposition in several ways, including simple prompting, task-specific instructions, and human inputs. Task decomposition allows agents to break down large tasks into smaller, more manageable subgoals, enabling them to plan and execute complex tasks efficiently.\', additional_kwargs={})], \'answer\': \'There are several ways to implement memory to support task decomposition:\\n\\n1. Long-Term Memory Management: This involves storing and organizing information in a long-term memory system. The agent can retrieve past experiences, knowledge, and learned strategies to guide the task decomposition process.\\n\\n2. Internet Access: The agent can use internet access to search for relevant information and gather resources to aid in task decomposition. This allows the agent to access a vast amount of information and utilize it in the decomposition process.\\n\\n3. GPT-3.5 Powered Agents: The agent can delegate simple tasks to GPT-3.5 powered agents. These agents can perform specific tasks or provide assistance in task decomposition, allowing the main agent to focus on higher-level planning and decision-making.\\n\\n4. File Output: The agent can store the results of task decomposition in files or documents. This allows for easy retrieval and reference during the execution of the task.\\n\\nThese memory resources help the agent in organizing and managing information, making informed decisions, and effectively decomposing complex tasks into smaller, manageable subgoals.\'} ``` Again, we can use the [LangSmith trace]( to explore the prompt structure. ### Going deeper - Agents, such as the [conversational retrieval agent](/docs/use_cases/question_answering/conversational_retrieval_agents), can be used for retrieval when necessary while also holding a conversation. - [Use case](#use-case) - [Overview](#overview) - [Quickstart](#quickstart) - [Memory](#memory) - [Conversation](#conversation) - [Chat Retrieval](#chat-retrieval)- [Going deeper](#going-deeper)', 'Conversation Summary Buffer | Conversation Summary Buffer `ConversationSummaryBufferMemory` combines the two ideas. It keeps a buffer of recent interactions in memory, but rather than just completely flushing old interactions it compiles them into a summary and uses both. It uses token length rather than number of interactions to determine when to flush interactions. Let\'s first walk through how to use the utilities. ## Using memory with LLM ```python from langchain.llms import OpenAI from langchain.memory import ConversationSummaryBufferMemory llm = OpenAI() ``` ```python memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=10) memory.save_context({""input"": ""hi""}, {""output"": ""whats up""}) memory.save_context({""input"": ""not much you""}, {""output"": ""not much""}) ``` ```python memory.load_memory_variables({}) ``` ```text {\'history\': \'System: \\nThe human says ""hi"", and the AI responds with ""whats up"".\\nHuman: not much you\\nAI: not much\'} ``` We can also get the history as a list of messages (this is useful if you are using this with a chat model). ```python memory = ConversationSummaryBufferMemory( llm=llm, max_token_limit=10, return_messages=True ) memory.save_context({""input"": ""hi""}, {""output"": ""whats up""}) memory.save_context({""input"": ""not much you""}, {""output"": ""not much""}) ``` We can also utilize the `predict_new_summary` method directly. ```python messages = memory.chat_memory.messages previous_summary = """" memory.predict_new_summary(messages, previous_summary) ``` ```text \'\\nThe human and AI state that they are not doing much.\' ``` ## Using in a chain Let\'s walk through an example, again setting `verbose=True` so we can see the prompt. ```python from langchain.chains import ConversationChain conversation_with_summary = ConversationChain( llm=llm, # We set a very low max_token_limit for the purposes of testing. memory=ConversationSummaryBufferMemory(llm=OpenAI(), max_token_limit=40), verbose=True, ) conversation_with_summary.predict(input=""Hi, what\'s up?"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: Human: Hi, what\'s up? AI: > Finished chain. "" Hi there! I\'m doing great. I\'m learning about the latest advances in artificial intelligence. What about you?"" ``` ```python conversation_with_summary.predict(input=""Just working on writing some documentation!"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: Human: Hi, what\'s up? AI: Hi there! I\'m doing great. I\'m spending some time learning about the latest developments in AI technology. How about you? Human: Just working on writing some documentation! AI: > Finished chain. \' That sounds like a great use of your time. Do you have experience with writing documentation?\' ``` ```python # We can see here that there is a summary of the conversation and then some previous interactions conversation_with_summary.predict(input=""For LangChain! Have you heard of it?"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: System: The human asked the AI what it was up to and the AI responded that it was learning about the latest developments in AI technology. Human: Just working on writing some documentation! AI: That sounds like a great use of your time. Do you have experience with writing documentation? Human: For LangChain! Have you heard of it? AI: > Finished chain. "" No, I haven\'t heard of LangChain. Can you tell me more about it?"" ``` ```python # We can see here that the summary and the buffer are updated conversation_with_summary.predict( input=""Haha nope, although a lot of people confuse it for that"" ) ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: System: The human asked the AI what it was up to and the AI responded that it was learning about the latest developments in AI technology. The human then mentioned they were writing documentation, to which the AI responded that it sounded like a great use of their time and asked if they had experience with writing documentation. Human: For LangChain! Have you heard of it? AI: No, I haven\'t heard of LangChain. Can you tell me more about it? Human: Haha nope, although a lot of people confuse it for that AI: > Finished chain. \' Oh, okay. What is LangChain?\' ``` - [Using memory with LLM](#using-memory-with-llm) - [Using in a chain](#using-in-a-chain)', 'Multiple Memory classes | Multiple Memory classes We can use multiple memory classes in the same chain. To combine multiple memory classes, we initialize and use the `CombinedMemory` class. ```python from langchain.chains import ConversationChain from langchain.llms import OpenAI from langchain.memory import ( CombinedMemory, ConversationBufferMemory, ConversationSummaryMemory, ) from langchain.prompts import PromptTemplate conv_memory = ConversationBufferMemory( memory_key=""chat_history_lines"", input_key=""input"" ) summary_memory = ConversationSummaryMemory(llm=OpenAI(), input_key=""input"") # Combined memory = CombinedMemory(memories=[conv_memory, summary_memory]) _DEFAULT_TEMPLATE = """"""The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Summary of conversation: {history} Current conversation: {chat_history_lines} Human: {input} AI:"""""" PROMPT = PromptTemplate( input_variables=[""history"", ""input"", ""chat_history_lines""], template=_DEFAULT_TEMPLATE, ) llm = OpenAI(temperature=0) conversation = ConversationChain(llm=llm, verbose=True, memory=memory, prompt=PROMPT) ``` ```python conversation.run(""Hi!"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Summary of conversation: Current conversation: Human: Hi! AI: > Finished chain. \' Hi there! How can I help you?\' ``` ```python conversation.run(""Can you tell me a joke?"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Summary of conversation: The human greets the AI, to which the AI responds with a polite greeting and an offer to help. Current conversation: Human: Hi! AI: Hi there! How can I help you? Human: Can you tell me a joke? AI: > Finished chain. \' Sure! What did the fish say when it hit the wall?\\nHuman: I don\\\'t know.\\nAI: ""Dam!""\' ```']",ConversationSummaryBufferMemory combines recent interactions into a summary and uses both the summary and buffer. It flushes interactions based on token length.,ConversationSummaryBufferMemory is a memory type that combines the ideas of keeping a buffer of recent interactions and compiling them into a summary. It uses token length to determine when to flush interactions.,0.99999999995,1.0,1.0,0.16188613565728216,0.37735849056603776
27,What does LangChain offer for token counting?,"['Token counting | Token counting LangChain offers a context manager that allows you to count tokens. ```python import asyncio from langchain.callbacks import get_openai_callback from langchain.llms import OpenAI llm = OpenAI(temperature=0) with get_openai_callback() as cb: llm(""What is the square root of 4?"") total_tokens = cb.total_tokens assert total_tokens > 0 with get_openai_callback() as cb: llm(""What is the square root of 4?"") llm(""What is the square root of 4?"") assert cb.total_tokens == total_tokens * 2 # You can kick off concurrent runs from within the context manager with get_openai_callback() as cb: await asyncio.gather( *[llm.agenerate([""What is the square root of 4?""]) for _ in range(3)] ) assert cb.total_tokens == total_tokens * 3 # The context manager is concurrency safe task = asyncio.create_task(llm.agenerate([""What is the square root of 4?""])) with get_openai_callback() as cb: await llm.agenerate([""What is the square root of 4?""]) await task assert cb.total_tokens == total_tokens ```', 'WebResearchRetriever | WebResearchRetriever Given a query, this retriever will: - Formulate a set of relate Google searches - Search for each - Load all the resulting URLs - Then embed and perform similarity search with the query on the consolidate page content ```python from langchain.retrievers.web_research import WebResearchRetriever ``` ### Simple usage Specify the LLM to use for Google search query generation. ```python import os from langchain.chat_models.openai import ChatOpenAI from langchain.embeddings import OpenAIEmbeddings from langchain.utilities import GoogleSearchAPIWrapper from langchain.vectorstores import Chroma # Vectorstore vectorstore = Chroma( embedding_function=OpenAIEmbeddings(), persist_directory=""./chroma_db_oai"" ) # LLM llm = ChatOpenAI(temperature=0) # Search os.environ[""GOOGLE_CSE_ID""] = ""xxx"" os.environ[""GOOGLE_API_KEY""] = ""xxx"" search = GoogleSearchAPIWrapper() ``` ```python # Initialize web_research_retriever = WebResearchRetriever.from_llm( vectorstore=vectorstore, llm=llm, search=search, ) ``` #### Run with citations We can use `RetrievalQAWithSourcesChain` to retrieve docs and provide citations. ```python from langchain.chains import RetrievalQAWithSourcesChain user_input = ""How do LLM Powered Autonomous Agents work?"" qa_chain = RetrievalQAWithSourcesChain.from_chain_type( llm, retriever=web_research_retriever ) result = qa_chain({""question"": user_input}) result ``` ```text Fetching pages: 100%|###################################################################################################################################| 1/1 [00:00<00:00, 3.33it/s] {\'question\': \'How do LLM Powered Autonomous Agents work?\', \'answer\': ""LLM Powered Autonomous Agents work by using LLM (large language model) as the core controller of the agent\'s brain. It is complemented by several key components, including planning, memory, and tool use. The agent system is designed to be a powerful general problem solver. \\n"", \'sources\': \' ``` #### Run with logging Here, we use `get_relevant_documents` method to return docs. ```python # Run import logging logging.basicConfig() logging.getLogger(""langchain.retrievers.web_research"").setLevel(logging.INFO) user_input = ""What is Task Decomposition in LLM Powered Autonomous Agents?"" docs = web_research_retriever.get_relevant_documents(user_input) ``` ```text INFO:langchain.retrievers.web_research:Generating questions for Google Search ... INFO:langchain.retrievers.web_research:Questions for Google Search (raw): {\'question\': \'What is Task Decomposition in LLM Powered Autonomous Agents?\', \'text\': LineList(lines=[\'1. How do LLM powered autonomous agents utilize task decomposition?\\n\', \'2. Can you explain the concept of task decomposition in LLM powered autonomous agents?\\n\', \'3. What role does task decomposition play in the functioning of LLM powered autonomous agents?\\n\', \'4. Why is task decomposition important for LLM powered autonomous agents?\\n\'])} INFO:langchain.retrievers.web_research:Questions for Google Search: [\'1. How do LLM powered autonomous agents utilize task decomposition?\\n\', \'2. Can you explain the concept of task decomposition in LLM powered autonomous agents?\\n\', \'3. What role does task decomposition play in the functioning of LLM powered autonomous agents?\\n\', \'4. Why is task decomposition important for LLM powered autonomous agents?\\n\'] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{\'title\': ""LLM Powered Autonomous Agents | Lil\'Log"", \'link\': \' \'snippet\': \'Jun 23, 2023 ... Task decomposition can be done (1) by LLM with simple prompting like ""Steps for XYZ.\\\\n1."" , ""What are the subgoals for achieving XYZ?\'}] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{\'title\': ""LLM Powered Autonomous Agents | Lil\'Log"", \'link\': \' \'snippet\': \'Jun 23, 2023 ... Task decomposition can be done (1) by LLM with simple prompting like ""Steps for XYZ.\\\\n1."" , ""What are the subgoals for achieving XYZ?"" , (2)\\xa0...\'}] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{\'title\': ""LLM Powered Autonomous Agents | Lil\'Log"", \'link\': \' \'snippet\': \'Jun 23, 2023 ... In a LLM-powered autonomous agent system, LLM functions as the ... Task decomposition can be done (1) by LLM with simple prompting like\\xa0...\'}] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{\'title\': ""LLM Powered Autonomous Agents | Lil\'Log"", \'link\': \' \'snippet\': \'Jun 23, 2023 ... Agent System Overview In a LLM-powered autonomous agent system, ... Task decomposition can be done (1) by LLM with simple prompting like\\xa0...\'}] INFO:langchain.retrievers.web_research:New URLs to load: [] ``` #### Generate answer using retrieved docs We can use `load_qa_chain` for QA using the retrieved docs. ```python from langchain.chains.question_answering import load_qa_chain chain = load_qa_chain(llm, chain_type=""stuff"") output = chain( {""input_documents"": docs, ""question"": user_input}, return_only_outputs=True ) output[""output_text""] ``` ```text \'Task decomposition in LLM-powered autonomous agents refers to the process of breaking down a complex task into smaller, more manageable subgoals. This allows the agent to efficiently handle and execute the individual steps required to complete the overall task. By decomposing the task, the agent can prioritize and organize its actions, making it easier to plan and execute the necessary steps towards achieving the desired outcome.\' ``` ### More flexibility Pass an LLM chain with custom prompt and output parsing. ```python import os import re from typing import List from langchain.chains import LLMChain from langchain.output_parsers.pydantic import PydanticOutputParser from langchain.prompts import PromptTemplate from pydantic import BaseModel, Field # LLMChain search_prompt = PromptTemplate( input_variables=[""question""], template=""""""You are an assistant tasked with improving Google search results. Generate FIVE Google search queries that are similar to this question. The output should be a numbered list of questions and each should have a question mark at the end: {question}"""""", ) class LineList(BaseModel): """"""List of questions."""""" lines: List[str] = Field(description=""Questions"") class QuestionListOutputParser(PydanticOutputParser): """"""Output parser for a list of numbered questions."""""" def __init__(self) -> None: super().__init__(pydantic_object=LineList) def parse(self, text: str) -> LineList: lines = re.findall(r""\\d+\\..*?\\n"", text) return LineList(lines=lines) llm_chain = LLMChain( llm=llm, prompt=search_prompt, output_parser=QuestionListOutputParser(), ) ``` ```python # Initialize web_research_retriever_llm_chain = WebResearchRetriever( vectorstore=vectorstore, llm_chain=llm_chain, search=search, ) # Run docs = web_research_retriever_llm_chain.get_relevant_documents(user_input) ``` ```text INFO:langchain.retrievers.web_research:Generating questions for Google Search ... INFO:langchain.retrievers.web_research:Questions for Google Search (raw): {\'question\': \'What is Task Decomposition in LLM Powered Autonomous Agents?\', \'text\': LineList(lines=[\'1. How do LLM powered autonomous agents use task decomposition?\\n\', \'2. Why is task decomposition important for LLM powered autonomous agents?\\n\', \'3. Can you explain the concept of task decomposition in LLM powered autonomous agents?\\n\', \'4. What are the benefits of task decomposition in LLM powered autonomous agents?\\n\'])} INFO:langchain.retrievers.web_research:Questions for Google Search: [\'1. How do LLM powered autonomous agents use task decomposition?\\n\', \'2. Why is task decomposition important for LLM powered autonomous agents?\\n\', \'3. Can you explain the concept of task decomposition in LLM powered autonomous agents?\\n\', \'4. What are the benefits of task decomposition in LLM powered autonomous agents?\\n\'] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{\'title\': ""LLM Powered Autonomous Agents | Lil\'Log"", \'link\': \' \'snippet\': \'Jun 23, 2023 ... Task decomposition can be done (1) by LLM with simple prompting like ""Steps for XYZ.\\\\n1."" , ""What are the subgoals for achieving XYZ?\'}] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{\'title\': ""LLM Powered Autonomous Agents | Lil\'Log"", \'link\': \' \'snippet\': \'Jun 23, 2023 ... Task decomposition can be done (1) by LLM with simple prompting like ""Steps for XYZ.\\\\n1."" , ""What are the subgoals for achieving XYZ?"" , (2)\\xa0...\'}] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{\'title\': ""LLM Powered Autonomous Agents | Lil\'Log"", \'link\': \' \'snippet\': \'Jun 23, 2023 ... Task decomposition can be done (1) by LLM with simple prompting like ""Steps for XYZ.\\\\n1."" , ""What are the subgoals for achieving XYZ?\'}] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{\'title\': ""LLM Powered Autonomous Agents | Lil\'Log"", \'link\': \' \'snippet\': \'Jun 23, 2023 ... Task decomposition can be done (1) by LLM with simple prompting like ""Steps for XYZ.\\\\n1."" , ""What are the subgoals for achieving XYZ?\'}] INFO:langchain.retrievers.web_research:New URLs to load: [\' INFO:langchain.retrievers.web_research:Grabbing most relevant splits from urls ... Fetching pages: 100%|###################################################################################################################################| 1/1 [00:00<00:00, 6.32it/s] ``` ```python len(docs) ``` ```text 1 ``` ### Run locally Specify LLM and embeddings that will run locally (e.g., on your laptop). ```python from langchain.callbacks.manager import CallbackManager from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler from langchain.embeddings import GPT4AllEmbeddings from langchain.llms import LlamaCpp n_gpu_layers = 1 # Metal set to 1 is enough. n_batch = 512 # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip. callback_manager = CallbackManager([StreamingStdOutCallbackHandler()]) llama = LlamaCpp( model_path=""/Users/rlm/Desktop/Code/llama.cpp/llama-2-13b-chat.ggmlv3.q4_0.bin"", n_gpu_layers=n_gpu_layers, n_batch=n_batch, n_ctx=4096, # Context window max_tokens=1000, # Max tokens to generate f16_kv=True, # MUST set to True, otherwise you will run into problem after a couple of calls callback_manager=callback_manager, verbose=True, ) vectorstore_llama = Chroma( embedding_function=GPT4AllEmbeddings(), persist_directory=""./chroma_db_llama"" ) ``` ```text llama.cpp: loading model from /Users/rlm/Desktop/Code/llama.cpp/llama-2-13b-chat.ggmlv3.q4_0.bin llama_model_load_internal: format = ggjt v3 (latest) llama_model_load_internal: n_vocab = 32000 llama_model_load_internal: n_ctx = 4096 llama_model_load_internal: n_embd = 5120 llama_model_load_internal: n_mult = 256 llama_model_load_internal: n_head = 40 llama_model_load_internal: n_layer = 40 llama_model_load_internal: n_rot = 128 llama_model_load_internal: freq_base = 10000.0 llama_model_load_internal: freq_scale = 1 llama_model_load_internal: ftype = 2 (mostly Q4_0) llama_model_load_internal: n_ff = 13824 llama_model_load_internal: model size = 13B llama_model_load_internal: ggml ctx size = 0.09 MB llama_model_load_internal: mem required = 9132.71 MB (+ 1608.00 MB per state) llama_new_context_with_model: kv self size = 3200.00 MB ggml_metal_init: allocating Found model file at /Users/rlm/.cache/gpt4all/ggml-all-MiniLM-L6-v2-f16.bin llama_new_context_with_model: max tensor size = 87.89 MB ggml_metal_init: using MPS ggml_metal_init: loading \'/Users/rlm/miniforge3/envs/llama/lib/python3.9/site-packages/llama_cpp/ggml-metal.metal\' ggml_metal_init: loaded kernel_add 0x110fbd600 ggml_metal_init: loaded kernel_mul 0x110fbeb30 ggml_metal_init: loaded kernel_mul_row 0x110fbf350 ggml_metal_init: loaded kernel_scale 0x110fbf9e0 ggml_metal_init: loaded kernel_silu 0x110fc0150 ggml_metal_init: loaded kernel_relu 0x110fbd950 ggml_metal_init: loaded kernel_gelu 0x110fbdbb0 ggml_metal_init: loaded kernel_soft_max 0x110fc14d0 ggml_metal_init: loaded kernel_diag_mask_inf 0x110fc1980 ggml_metal_init: loaded kernel_get_rows_f16 0x110fc22a0 ggml_metal_init: loaded kernel_get_rows_q4_0 0x110fc2ad0 ggml_metal_init: loaded kernel_get_rows_q4_1 0x110fc3260 ggml_metal_init: loaded kernel_get_rows_q2_K 0x110fc3ad0 ggml_metal_init: loaded kernel_get_rows_q3_K 0x110fc41c0 ggml_metal_init: loaded kernel_get_rows_q4_K 0x110fc48c0 ggml_metal_init: loaded kernel_get_rows_q5_K 0x110fc4fa0 ggml_metal_init: loaded kernel_get_rows_q6_K 0x110fc56a0 ggml_metal_init: loaded kernel_rms_norm 0x110fc5da0 ggml_metal_init: loaded kernel_norm 0x110fc64d0 ggml_metal_init: loaded kernel_mul_mat_f16_f32 0x2a5c19990 ggml_metal_init: loaded kernel_mul_mat_q4_0_f32 0x2a5c1d4a0 ggml_metal_init: loaded kernel_mul_mat_q4_1_f32 0x2a5c19fc0 ggml_metal_init: loaded kernel_mul_mat_q2_K_f32 0x2a5c1dcc0 ggml_metal_init: loaded kernel_mul_mat_q3_K_f32 0x2a5c1e420 ggml_metal_init: loaded kernel_mul_mat_q4_K_f32 0x2a5c1edc0 ggml_metal_init: loaded kernel_mul_mat_q5_K_f32 0x2a5c1fd90 ggml_metal_init: loaded kernel_mul_mat_q6_K_f32 0x2a5c20540 ggml_metal_init: loaded kernel_rope 0x2a5c20d40 ggml_metal_init: loaded kernel_alibi_f32 0x2a5c21730 ggml_metal_init: loaded kernel_cpy_f32_f16 0x2a5c21ab0 ggml_metal_init: loaded kernel_cpy_f32_f32 0x2a5c22080 ggml_metal_init: loaded kernel_cpy_f16_f16 0x2a5c231d0 ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB ggml_metal_init: hasUnifiedMemory = true ggml_metal_init: maxTransferRate = built-in GPU ggml_metal_add_buffer: allocated \'data \' buffer, size = 6984.06 MB, ( 6984.52 / 21845.34) ggml_metal_add_buffer: allocated \'eval \' buffer, size = 1040.00 MB, ( 8024.52 / 21845.34) ggml_metal_add_buffer: allocated \'kv \' buffer, size = 3202.00 MB, (11226.52 / 21845.34) ggml_metal_add_buffer: allocated \'scr0 \' buffer, size = 597.00 MB, (11823.52 / 21845.34) AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | ggml_metal_add_buffer: allocated \'scr1 \' buffer, size = 512.00 MB, (12335.52 / 21845.34) objc[33471]: Class GGMLMetalClass is implemented in both /Users/rlm/miniforge3/envs/llama/lib/python3.9/site-packages/llama_cpp/libllama.dylib (0x2c7368208) and /Users/rlm/miniforge3/envs/llama/lib/python3.9/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libreplit-mainline-metal.dylib (0x5ebf48208). One of the two will be used. Which one is undefined. objc[33471]: Class GGMLMetalClass is implemented in both /Users/rlm/miniforge3/envs/llama/lib/python3.9/site-packages/llama_cpp/libllama.dylib (0x2c7368208) and /Users/rlm/miniforge3/envs/llama/lib/python3.9/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libllamamodel-mainline-metal.dylib (0x5ec374208). One of the two will be used. Which one is undefined. ``` We supplied `StreamingStdOutCallbackHandler()`, so model outputs (e.g., generated questions) are streamed. We also have logging on, so we seem them there too. ```python from langchain.chains import RetrievalQAWithSourcesChain # Initialize web_research_retriever = WebResearchRetriever.from_llm( vectorstore=vectorstore_llama, llm=llama, search=search, ) # Run user_input = ""What is Task Decomposition in LLM Powered Autonomous Agents?"" qa_chain = RetrievalQAWithSourcesChain.from_chain_type( llama, retriever=web_research_retriever ) result = qa_chain({""question"": user_input}) result ``` ```text INFO:langchain.retrievers.web_research:Generating questions for Google Search ... Sure, here are five Google search queries that are similar to ""What is Task Decomposition in LLM Powered Autonomous Agents?"": 1. How does Task Decomposition work in LLM Powered Autonomous Agents? 2. What are the benefits of using Task Decomposition in LLM Powered Autonomous Agents? 3. Can you provide examples of Task Decomposition in LLM Powered Autonomous Agents? 4. How does Task Decomposition improve the performance of LLM Powered Autonomous Agents? 5. What are some common challenges or limitations of using Task Decomposition in LLM Powered Autonomous Agents, and how can they be addressed? llama_print_timings: load time = 8585.01 ms llama_print_timings: sample time = 124.24 ms / 164 runs ( 0.76 ms per token, 1320.04 tokens per second) llama_print_timings: prompt eval time = 8584.83 ms / 101 tokens ( 85.00 ms per token, 11.76 tokens per second) llama_print_timings: eval time = 7268.55 ms / 163 runs ( 44.59 ms per token, 22.43 tokens per second) llama_print_timings: total time = 16236.13 ms INFO:langchain.retrievers.web_research:Questions for Google Search (raw): {\'question\': \'What is Task Decomposition in LLM Powered Autonomous Agents?\', \'text\': LineList(lines=[\'1. How does Task Decomposition work in LLM Powered Autonomous Agents? \\n\', \'2. What are the benefits of using Task Decomposition in LLM Powered Autonomous Agents? \\n\', \'3. Can you provide examples of Task Decomposition in LLM Powered Autonomous Agents? \\n\', \'4. How does Task Decomposition improve the performance of LLM Powered Autonomous Agents? \\n\'])} INFO:langchain.retrievers.web_research:Questions for Google Search: [\'1. How does Task Decomposition work in LLM Powered Autonomous Agents? \\n\', \'2. What are the benefits of using Task Decomposition in LLM Powered Autonomous Agents? \\n\', \'3. Can you provide examples of Task Decomposition in LLM Powered Autonomous Agents? \\n\', \'4. How does Task Decomposition improve the performance of LLM Powered Autonomous Agents? \\n\'] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{\'title\': ""LLM Powered Autonomous Agents | Lil\'Log"", \'link\': \' \'snippet\': \'Jun 23, 2023 ... Task decomposition can be done (1) by LLM with simple prompting like ""Steps for XYZ.\\\\n1."" , ""What are the subgoals for achieving XYZ?\'}] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{\'title\': ""LLM Powered Autonomous Agents | Lil\'Log"", \'link\': \' \'snippet\': \'Jun 23, 2023 ... Task decomposition can be done (1) by LLM with simple prompting like ""Steps for XYZ.\\\\n1."" , ""What are the subgoals for achieving XYZ?"" , (2)\\xa0...\'}] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{\'title\': ""LLM Powered Autonomous Agents | Lil\'Log"", \'link\': \' \'snippet\': \'Jun 23, 2023 ... A complicated task usually involves many steps. An agent needs to know what they are and plan ahead. Task Decomposition#. Chain of thought (CoT;\\xa0...\'}] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{\'title\': ""LLM Powered Autonomous Agents | Lil\'Log"", \'link\': \' \'snippet\': \'Jun 23, 2023 ... Agent System Overview In a LLM-powered autonomous agent system, ... Task decomposition can be done (1) by LLM with simple prompting like\\xa0...\'}] INFO:langchain.retrievers.web_research:New URLs to load: [\' INFO:langchain.retrievers.web_research:Grabbing most relevant splits from urls ... Fetching pages: 100%|###################################################################################################################################| 1/1 [00:00<00:00, 10.49it/s] Llama.generate: prefix-match hit The content discusses Task Decomposition in LLM Powered Autonomous Agents, which involves breaking down large tasks into smaller, manageable subgoals for efficient handling of complex tasks. SOURCES: llama_print_timings: load time = 8585.01 ms llama_print_timings: sample time = 52.88 ms / 72 runs ( 0.73 ms per token, 1361.55 tokens per second) llama_print_timings: prompt eval time = 125925.13 ms / 2358 tokens ( 53.40 ms per token, 18.73 tokens per second) llama_print_timings: eval time = 3504.16 ms / 71 runs ( 49.35 ms per token, 20.26 tokens per second) llama_print_timings: total time = 129584.60 ms {\'question\': \'What is Task Decomposition in LLM Powered Autonomous Agents?\', \'answer\': \' The content discusses Task Decomposition in LLM Powered Autonomous Agents, which involves breaking down large tasks into smaller, manageable subgoals for efficient handling of complex tasks.\\n\', \'sources\': \' ``` - [Simple usage](#simple-usage) - [More flexibility](#more-flexibility) - [Run locally](#run-locally)', 'Conversation Token Buffer | Conversation Token Buffer `ConversationTokenBufferMemory` keeps a buffer of recent interactions in memory, and uses token length rather than number of interactions to determine when to flush interactions. Let\'s first walk through how to use the utilities. ## Using memory with LLM ```python from langchain.llms import OpenAI from langchain.memory import ConversationTokenBufferMemory llm = OpenAI() ``` ```python memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=10) memory.save_context({""input"": ""hi""}, {""output"": ""whats up""}) memory.save_context({""input"": ""not much you""}, {""output"": ""not much""}) ``` ```python memory.load_memory_variables({}) ``` ```text {\'history\': \'Human: not much you\\nAI: not much\'} ``` We can also get the history as a list of messages (this is useful if you are using this with a chat model). ```python memory = ConversationTokenBufferMemory( llm=llm, max_token_limit=10, return_messages=True ) memory.save_context({""input"": ""hi""}, {""output"": ""whats up""}) memory.save_context({""input"": ""not much you""}, {""output"": ""not much""}) ``` ## Using in a chain Let\'s walk through an example, again setting `verbose=True` so we can see the prompt. ```python from langchain.chains import ConversationChain conversation_with_summary = ConversationChain( llm=llm, # We set a very low max_token_limit for the purposes of testing. memory=ConversationTokenBufferMemory(llm=OpenAI(), max_token_limit=60), verbose=True, ) conversation_with_summary.predict(input=""Hi, what\'s up?"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: Human: Hi, what\'s up? AI: > Finished chain. "" Hi there! I\'m doing great, just enjoying the day. How about you?"" ``` ```python conversation_with_summary.predict(input=""Just working on writing some documentation!"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: Human: Hi, what\'s up? AI: Hi there! I\'m doing great, just enjoying the day. How about you? Human: Just working on writing some documentation! AI: > Finished chain. \' Sounds like a productive day! What kind of documentation are you writing?\' ``` ```python conversation_with_summary.predict(input=""For LangChain! Have you heard of it?"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: Human: Hi, what\'s up? AI: Hi there! I\'m doing great, just enjoying the day. How about you? Human: Just working on writing some documentation! AI: Sounds like a productive day! What kind of documentation are you writing? Human: For LangChain! Have you heard of it? AI: > Finished chain. "" Yes, I have heard of LangChain! It is a decentralized language-learning platform that connects native speakers and learners in real time. Is that the documentation you\'re writing about?"" ``` ```python # We can see here that the buffer is updated conversation_with_summary.predict( input=""Haha nope, although a lot of people confuse it for that"" ) ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: Human: For LangChain! Have you heard of it? AI: Yes, I have heard of LangChain! It is a decentralized language-learning platform that connects native speakers and learners in real time. Is that the documentation you\'re writing about? Human: Haha nope, although a lot of people confuse it for that AI: > Finished chain. "" Oh, I see. Is there another language learning platform you\'re referring to?"" ``` - [Using memory with LLM](#using-memory-with-llm) - [Using in a chain](#using-in-a-chain)']",LangChain offers a context manager for token counting. You can use `get_openai_callback()` to count tokens in your interactions.,"LangChain offers a context manager that allows you to count tokens, as well as integrations with services like LangSmith that can help track tokens.",0.9999999999666667,1.0,1.0,0.0544885695463871,0.4090909090909091
28,What is the purpose of caching embeddings?,"['Helicone | Helicone This page covers how to use the [Helicone]( ecosystem within LangChain. ## What is Helicone? Helicone is an [open-source]( observability platform that proxies your OpenAI traffic and provides you key insights into your spend, latency and usage. ![Helicone](/assets/images/HeliconeDashboard-bc06f9888dbb03ff98d894fe9bec2b29.png) ## Quick start With your LangChain environment you can just add the following parameter. ```bash export OPENAI_API_BASE="" ``` Now head over to [helicone.ai]( to create your account, and add your OpenAI API key within our dashboard to view your logs. ![Helicone](/assets/images/HeliconeKeys-9ff580101e3a63ee05e2fa67b8def03c.png) ## How to enable Helicone caching ```python from langchain.llms import OpenAI import openai openai.api_base = "" llm = OpenAI(temperature=0.9, headers={""Helicone-Cache-Enabled"": ""true""}) text = ""What is a helicone?"" print(llm(text)) ``` [Helicone caching docs]( ## How to use Helicone custom properties ```python from langchain.llms import OpenAI import openai openai.api_base = "" llm = OpenAI(temperature=0.9, headers={ ""Helicone-Property-Session"": ""24"", ""Helicone-Property-Conversation"": ""support_issue_2"", ""Helicone-Property-App"": ""mobile"", }) text = ""What is a helicone?"" print(llm(text)) ``` [Helicone property docs]( - [What is Helicone?](#what-is-helicone) - [Quick start](#quick-start) - [How to enable Helicone caching](#how-to-enable-helicone-caching) - [How to use Helicone custom properties](#how-to-use-helicone-custom-properties)', 'Text embedding models | Text embedding models infoHead to [Integrations](/docs/integrations/text_embedding/) for documentation on built-in integrations with text embedding model providers. The Embeddings class is a class designed for interfacing with text embedding models. There are lots of embedding model providers (OpenAI, Cohere, Hugging Face, etc) - this class is designed to provide a standard interface for all of them. Embeddings create a vector representation of a piece of text. This is useful because it means we can think about text in the vector space, and do things like semantic search where we look for pieces of text that are most similar in the vector space. The base Embeddings class in LangChain provides two methods: one for embedding documents and one for embedding a query. The former takes as input multiple texts, while the latter takes a single text. The reason for having these as two separate methods is that some embedding providers have different embedding methods for documents (to be searched over) vs queries (the search query itself). ## Get started ### Setup To start we\'ll need to install the OpenAI Python package: ```bash pip install openai ``` Accessing the API requires an API key, which you can get by creating an account and heading [here]( Once we have a key we\'ll want to set it as an environment variable by running: ```bash export OPENAI_API_KEY=""..."" ``` If you\'d prefer not to set an environment variable you can pass the key in directly via the `openai_api_key` named parameter when initiating the OpenAI LLM class: ```python from langchain.embeddings import OpenAIEmbeddings embeddings_model = OpenAIEmbeddings(openai_api_key=""..."") ``` Otherwise you can initialize without any params: ```python from langchain.embeddings import OpenAIEmbeddings embeddings_model = OpenAIEmbeddings() ``` ### embed_documents #### Embed list of texts ```python embeddings = embeddings_model.embed_documents( [ ""Hi there!"", ""Oh, hello!"", ""What\'s your name?"", ""My friends call me World"", ""Hello World!"" ] ) len(embeddings), len(embeddings[0]) ``` ```text (5, 1536) ``` ### embed_query #### Embed single query Embed a single piece of text for the purpose of comparing to other embedded pieces of texts. ```python embedded_query = embeddings_model.embed_query(""What was the name mentioned in the conversation?"") embedded_query[:5] ``` ```text [0.0053587136790156364, -0.0004999046213924885, 0.038883671164512634, -0.003001077566295862, -0.00900818221271038] ``` - [Get started](#get-started)- [Setup](#setup) - [embed_documents](#embed_documents) - [embed_query](#embed_query)', 'Caching | Caching LangChain provides an optional caching layer for LLMs. This is useful for two reasons: It can save you money by reducing the number of API calls you make to the LLM provider, if you\'re often requesting the same completion multiple times. It can speed up your application by reducing the number of API calls you make to the LLM provider. ```python from langchain.globals import set_llm_cache from langchain.llms import OpenAI # To make the caching really obvious, lets use a slower model. llm = OpenAI(model_name=""text-davinci-002"", n=2, best_of=2) ``` ## In Memory Cache ```python from langchain.cache import InMemoryCache set_llm_cache(InMemoryCache()) # The first time, it is not yet in cache, so it should take longer llm.predict(""Tell me a joke"") ``` ```text CPU times: user 35.9 ms, sys: 28.6 ms, total: 64.6 ms Wall time: 4.83 s ""\\n\\nWhy couldn\'t the bicycle stand up by itself? It was...two tired!"" ``` ```python # The second time it is, so it goes faster llm.predict(""Tell me a joke"") ``` ```text CPU times: user 238 s, sys: 143 s, total: 381 s Wall time: 1.76 ms \'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.\' ``` ## SQLite Cache ```bash rm .langchain.db ``` ```python # We can do the same thing with a SQLite cache from langchain.cache import SQLiteCache set_llm_cache(SQLiteCache(database_path="".langchain.db"")) ``` ```python # The first time, it is not yet in cache, so it should take longer llm.predict(""Tell me a joke"") ``` ```text CPU times: user 17 ms, sys: 9.76 ms, total: 26.7 ms Wall time: 825 ms \'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.\' ``` ```python # The second time it is, so it goes faster llm.predict(""Tell me a joke"") ``` ```text CPU times: user 2.46 ms, sys: 1.23 ms, total: 3.7 ms Wall time: 2.67 ms \'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.\' ``` ## Optional caching in chains You can also turn off caching for particular nodes in chains. Note that because of certain interfaces, it\'s often easier to construct the chain first, and then edit the LLM afterwards. As an example, we will load a summarizer map-reduce chain. We will cache results for the map-step, but then not freeze it for the combine step. ```python llm = OpenAI(model_name=""text-davinci-002"") no_cache_llm = OpenAI(model_name=""text-davinci-002"", cache=False) ``` ```python from langchain.text_splitter import CharacterTextSplitter from langchain.chains.mapreduce import MapReduceChain text_splitter = CharacterTextSplitter() ``` ```python with open(\'../../../state_of_the_union.txt\') as f: state_of_the_union = f.read() texts = text_splitter.split_text(state_of_the_union) ``` ```python from langchain.docstore.document import Document docs = [Document(page_content=t) for t in texts[:3]] from langchain.chains.summarize import load_summarize_chain ``` ```python chain = load_summarize_chain(llm, chain_type=""map_reduce"", reduce_llm=no_cache_llm) ``` ```python chain.run(docs) ``` ```text CPU times: user 452 ms, sys: 60.3 ms, total: 512 ms Wall time: 5.09 s \'\\n\\nPresident Biden is discussing the American Rescue Plan and the Bipartisan Infrastructure Law, which will create jobs and help Americans. He also talks about his vision for America, which includes investing in education and infrastructure. In response to Russian aggression in Ukraine, the United States is joining with European allies to impose sanctions and isolate Russia. American forces are being mobilized to protect NATO countries in the event that Putin decides to keep moving west. The Ukrainians are bravely fighting back, but the next few weeks will be hard for them. Putin will pay a high price for his actions in the long run. Americans should not be alarmed, as the United States is taking action to protect its interests and allies.\' ``` When we run it again, we see that it runs substantially faster but the final answer is different. This is due to caching at the map steps, but not at the reduce step. ```python chain.run(docs) ``` ```text CPU times: user 11.5 ms, sys: 4.33 ms, total: 15.8 ms Wall time: 1.04 s \'\\n\\nPresident Biden is discussing the American Rescue Plan and the Bipartisan Infrastructure Law, which will create jobs and help Americans. He also talks about his vision for America, which includes investing in education and infrastructure.\' ``` ```bash rm .langchain.db sqlite.db ``` - [In Memory Cache](#in-memory-cache) - [SQLite Cache](#sqlite-cache) - [Optional caching in chains](#optional-caching-in-chains)']","Caching embeddings saves computation by storing previously computed embeddings, reusing them for the same inputs, thus speeding up applications that require frequent embedding computations.",Caching embeddings allows for storing or temporarily caching embeddings to avoid the need for recomputing them.,0.5833333333041666,1.0,0.0,0.03925345689749394,0.25
29,What is the purpose of the Retry parser?,"['Retry parser | Retry parser While in some cases it is possible to fix any parsing mistakes by only looking at the output, in other cases it isn\'t. An example of this is when the output is not just in the incorrect format, but is partially complete. Consider the below example. ```python from langchain.chat_models import ChatOpenAI from langchain.llms import OpenAI from langchain.output_parsers import ( OutputFixingParser, PydanticOutputParser, ) from langchain.prompts import ( PromptTemplate, ) from pydantic import BaseModel, Field ``` ```python template = """"""Based on the user question, provide an Action and Action Input for what step should be taken. {format_instructions} Question: {query} Response:"""""" class Action(BaseModel): action: str = Field(description=""action to take"") action_input: str = Field(description=""input to the action"") parser = PydanticOutputParser(pydantic_object=Action) ``` ```python prompt = PromptTemplate( template=""Answer the user query.\\n{format_instructions}\\n{query}\\n"", input_variables=[""query""], partial_variables={""format_instructions"": parser.get_format_instructions()}, ) ``` ```python prompt_value = prompt.format_prompt(query=""who is leo di caprios gf?"") ``` ```python bad_response = \'{""action"": ""search""}\' ``` If we try to parse this response as is, we will get an error: ```python parser.parse(bad_response) ``` ```text --------------------------------------------------------------------------- ValidationError Traceback (most recent call last) File ~/workplace/langchain/langchain/output_parsers/pydantic.py:24, in PydanticOutputParser.parse(self, text) 23 json_object = json.loads(json_str) ---> 24 return self.pydantic_object.parse_obj(json_object) 26 except (json.JSONDecodeError, ValidationError) as e: File ~/.pyenv/versions/3.9.1/envs/langchain/lib/python3.9/site-packages/pydantic/main.py:527, in pydantic.main.BaseModel.parse_obj() File ~/.pyenv/versions/3.9.1/envs/langchain/lib/python3.9/site-packages/pydantic/main.py:342, in pydantic.main.BaseModel.__init__() ValidationError: 1 validation error for Action action_input field required (type=value_error.missing) During handling of the above exception, another exception occurred: OutputParserException Traceback (most recent call last) Cell In[6], line 1 ----> 1 parser.parse(bad_response) File ~/workplace/langchain/langchain/output_parsers/pydantic.py:29, in PydanticOutputParser.parse(self, text) 27 name = self.pydantic_object.__name__ 28 msg = f""Failed to parse {name} from completion {text}. Got: {e}"" ---> 29 raise OutputParserException(msg) OutputParserException: Failed to parse Action from completion {""action"": ""search""}. Got: 1 validation error for Action action_input field required (type=value_error.missing) ``` If we try to use the `OutputFixingParser` to fix this error, it will be confused - namely, it doesn\'t know what to actually put for action input. ```python fix_parser = OutputFixingParser.from_llm(parser=parser, llm=ChatOpenAI()) ``` ```python fix_parser.parse(bad_response) ``` ```text Action(action=\'search\', action_input=\'\') ``` Instead, we can use the RetryOutputParser, which passes in the prompt (as well as the original output) to try again to get a better response. ```python from langchain.output_parsers import RetryWithErrorOutputParser ``` ```python retry_parser = RetryWithErrorOutputParser.from_llm( parser=parser, llm=OpenAI(temperature=0) ) ``` ```python retry_parser.parse_with_prompt(bad_response, prompt_value) ``` ```text Action(action=\'search\', action_input=\'who is leo di caprios gf?\') ```', ""langchain.output_parsers.retry.RetryOutputParser LangChain 0.0.337 API Experimental Python Docs Toggle Menu PrevUp Next LangChain 0.0.337 langchain.output_parsers.retry.RetryOutputParser langchain.output_parsers.retry.RetryOutputParser class langchain.output_parsers.retry.RetryOutputParser[source] Bases: BaseOutputParser[T] Wraps a parser and tries to fix parsing errors. Does this by passing the original prompt and the completion to another LLM, and telling it the completion did not satisfy criteria in the prompt. param max_retries: int = 1 The maximum number of times to retry the parse. param parser: langchain.schema.output_parser.BaseOutputParser[langchain.output_parsers.retry.T] [Required] The parser to use to parse the output. param retry_chain: Any = None The LLMChain to use to retry the completion. async abatch(inputs: List[Input], config: Optional[Union[RunnableConfig, List[RunnableConfig]]] = None, *, return_exceptions: bool = False, **kwargs: Optional[Any]) List[Output] Default implementation runs ainvoke in parallel using asyncio.gather. The default implementation of batch works well for IO bound runnables. Subclasses should override this method if they can batch more efficiently; e.g., if the underlying runnable uses an API which supports a batch mode. async ainvoke(input: str | langchain.schema.messages.BaseMessage, config: Optional[RunnableConfig] = None, **kwargs: Optional[Any]) T Default implementation of ainvoke, calls invoke from a thread. The default implementation allows usage of async code even if the runnable did not implement a native async version of invoke. Subclasses should override this method if they can run asynchronously. async aparse(text: str) T Parse a single string model output into some structure. Parameters text String output of a language model. Returns Structured output. async aparse_result(result: List[Generation], *, partial: bool = False) T Parse a list of candidate model Generations into a specific format. The return value is parsed from only the first Generation in the result, whichis assumed to be the highest-likelihood Generation. Parameters result A list of Generations to be parsed. The Generations are assumed to be different candidate outputs for a single model input. Returns Structured output. async aparse_with_prompt(completion: str, prompt_value: PromptValue) T[source] Parse the output of an LLM call using a wrapped parser. Parameters completion The chain completion to parse. prompt_value The prompt to use to parse the completion. Returns The parsed completion. async astream(input: Input, config: Optional[RunnableConfig] = None, **kwargs: Optional[Any]) AsyncIterator[Output] Default implementation of astream, which calls ainvoke. Subclasses should override this method if they support streaming output. async astream_log(input: Any, config: Optional[RunnableConfig] = None, *, diff: bool = True, include_names: Optional[Sequence[str]] = None, include_types: Optional[Sequence[str]] = None, include_tags: Optional[Sequence[str]] = None, exclude_names: Optional[Sequence[str]] = None, exclude_types: Optional[Sequence[str]] = None, exclude_tags: Optional[Sequence[str]] = None, **kwargs: Optional[Any]) Union[AsyncIterator[RunLogPatch], AsyncIterator[RunLog]] Stream all output from a runnable, as reported to the callback system. This includes all inner runs of LLMs, Retrievers, Tools, etc. Output is streamed as Log objects, which include a list of jsonpatch ops that describe how the state of the run has changed in each step, and the final state of the run. The jsonpatch ops can be applied in order to construct state. async atransform(input: AsyncIterator[Input], config: Optional[RunnableConfig] = None, **kwargs: Optional[Any]) AsyncIterator[Output] Default implementation of atransform, which buffers input and calls astream. Subclasses should override this method if they can start producing output while input is still being generated. batch(inputs: List[Input], config: Optional[Union[RunnableConfig, List[RunnableConfig]]] = None, *, return_exceptions: bool = False, **kwargs: Optional[Any]) List[Output] Default implementation runs invoke in parallel using a thread pool executor. The default implementation of batch works well for IO bound runnables. Subclasses should override this method if they can batch more efficiently; e.g., if the underlying runnable uses an API which supports a batch mode. bind(**kwargs: Any) Runnable[Input, Output] Bind arguments to a Runnable, returning a new Runnable. config_schema(*, include: Optional[Sequence[str]] = None) Type[BaseModel] The type of config this runnable accepts specified as a pydantic model. To mark a field as configurable, see the configurable_fields and configurable_alternatives methods. Parameters include  A list of fields to include in the config schema. Returns A pydantic model that can be used to validate config. configurable_alternatives(which: ConfigurableField, default_key: str = 'default', **kwargs: Union[Runnable[Input, Output], Callable[[], Runnable[Input, Output]]])  RunnableSerializable[Input, Output] configurable_fields(**kwargs: Union[ConfigurableField, ConfigurableFieldSingleOption, ConfigurableFieldMultiOption])  RunnableSerializable[Input, Output] classmethod construct(_fields_set: Optional[SetStr] = None, **values: Any)  Model Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values copy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False)  Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters include  fields to include in new model exclude  fields to exclude from new model, as with values this takes precedence over include update  values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data deep  set to True to make a deep copy of the model Returns new model instance dict(**kwargs: Any)  Dict Return dictionary representation of output parser. classmethod from_llm(llm: BaseLanguageModel, parser: BaseOutputParser[T], prompt: BasePromptTemplate = PromptTemplate(input_variables=['completion', 'prompt'], template='Prompt:\\n{prompt}\\nCompletion:\\n{completion}\\n\\nAbove, the Completion did not satisfy the constraints given in the Prompt.\\nPlease try again:'), max_retries: int = 1)  RetryOutputParser[T][source] Create an OutputFixingParser from a language model and a parser. Parameters llm  llm to use for fixing parser  parser to use for parsing prompt  prompt to use for fixing max_retries  Maximum number of retries to parse. Returns RetryOutputParser classmethod from_orm(obj: Any)  Model get_format_instructions()  str[source] Instructions on how the LLM output should be formatted. get_input_schema(config: Optional[RunnableConfig] = None)  Type[BaseModel] Get a pydantic model that can be used to validate input to the runnable. Runnables that leverage the configurable_fields and configurable_alternatives methods will have a dynamic input schema that depends on which configuration the runnable is invoked with. This method allows to get an input schema for a specific configuration. Parameters config  A config to use when generating the schema. Returns A pydantic model that can be used to validate input. classmethod get_lc_namespace()  List[str] Get the namespace of the langchain object. For example, if the class is langchain.llms.openai.OpenAI, then the namespace is [langchain, llms, openai] get_output_schema(config: Optional[RunnableConfig] = None)  Type[BaseModel] Get a pydantic model that can be used to validate output to the runnable. Runnables that leverage the configurable_fields and configurable_alternatives methods will have a dynamic output schema that depends on which configuration the runnable is invoked with. This method allows to get an output schema for a specific configuration. Parameters config  A config to use when generating the schema. Returns A pydantic model that can be used to validate output. invoke(input: Union[str, BaseMessage], config: Optional[RunnableConfig] = None)  T Transform a single input into an output. Override to implement. Parameters input  The input to the runnable. config  A config to use when invoking the runnable. The config supports standard keys like 'tags', 'metadata' for tracing purposes, 'max_concurrency' for controlling how much work to do in parallel, and other keys. Please refer to the RunnableConfig for more details. Returns The output of the runnable. classmethod is_lc_serializable()  bool Is this class serializable? json(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any)  unicode Generate a JSON representation of the model, include and exclude arguments as per dict(). encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps(). classmethod lc_id()  List[str] A unique identifier for this class for serialization purposes. The unique identifier is a list of strings that describes the path to the object. map()  Runnable[List[Input], List[Output]] Return a new Runnable that maps a list of inputs to a list of outputs, by calling invoke() with each input. parse(completion: str)  T[source] Parse a single string model output into some structure. Parameters text  String output of a language model. Returns Structured output. classmethod parse_file(path: Union[str, Path], *, content_type: unicode = None, encoding: unicode = 'utf8', proto: Protocol = None, allow_pickle: bool = False)  Model classmethod parse_obj(obj: Any)  Model classmethod parse_raw(b: Union[str, bytes], *, content_type: unicode = None, encoding: unicode = 'utf8', proto: Protocol = None, allow_pickle: bool = False)  Model parse_result(result: List[Generation], *, partial: bool = False)  T Parse a list of candidate model Generations into a specific format. The return value is parsed from only the first Generation in the result, whichis assumed to be the highest-likelihood Generation. Parameters result  A list of Generations to be parsed. The Generations are assumed to be different candidate outputs for a single model input. Returns Structured output. parse_with_prompt(completion: str, prompt_value: PromptValue)  T[source] Parse the output of an LLM call using a wrapped parser. Parameters completion  The chain completion to parse. prompt_value  The prompt to use to parse the completion. Returns The parsed completion. classmethod schema(by_alias: bool = True, ref_template: unicode = '#/definitions/{model}')  DictStrAny classmethod schema_json(*, by_alias: bool = True, ref_template: unicode = '#/definitions/{model}', **dumps_kwargs: Any)  unicode stream(input: Input, config: Optional[RunnableConfig] = None, **kwargs: Optional[Any])  Iterator[Output] Default implementation of stream, which calls invoke. Subclasses should override this method if they support streaming output. to_json()  Union[SerializedConstructor, SerializedNotImplemented] to_json_not_implemented()  SerializedNotImplemented transform(input: Iterator[Input], config: Optional[RunnableConfig] = None, **kwargs: Optional[Any])  Iterator[Output] Default implementation of transform, which buffers input and then calls stream. Subclasses should override this method if they can start producing output while input is still being generated. classmethod update_forward_refs(**localns: Any)  None Try to update ForwardRefs on fields based on this Model, globalns and localns. classmethod validate(value: Any)  Model with_config(config: Optional[RunnableConfig] = None, **kwargs: Any)  Runnable[Input, Output] Bind config to a Runnable, returning a new Runnable. with_fallbacks(fallbacks: Sequence[Runnable[Input, Output]], *, exceptions_to_handle: Tuple[Type[BaseException], ...] = (,))  RunnableWithFallbacksT[Input, Output] Add fallbacks to a runnable, returning a new Runnable. Parameters fallbacks  A sequence of runnables to try if the original runnable fails. exceptions_to_handle  A tuple of exception types to handle. Returns A new Runnable that will try the original runnable, and then each fallback in order, upon failures. with_listeners(*, on_start: Optional[Listener] = None, on_end: Optional[Listener] = None, on_error: Optional[Listener] = None)  Runnable[Input, Output] Bind lifecycle listeners to a Runnable, returning a new Runnable. on_start: Called before the runnable starts running, with the Run object. on_end: Called after the runnable finishes running, with the Run object. on_error: Called if the runnable throws an error, with the Run object. The Run object contains information about the run, including its id, type, input, output, error, start_time, end_time, and any tags or metadata added to the run. with_retry(*, retry_if_exception_type: ~typing.Tuple[~typing.Type[BaseException], ...] = (,), wait_exponential_jitter: bool = True, stop_after_attempt: int = 3)  Runnable[Input, Output] Create a new Runnable that retries the original runnable on exceptions. Parameters retry_if_exception_type  A tuple of exception types to retry on wait_exponential_jitter  Whether to add jitter to the wait time between retries stop_after_attempt  The maximum number of attempts to make before giving up Returns A new Runnable that retries the original runnable on exceptions. with_types(*, input_type: Optional[Type[Input]] = None, output_type: Optional[Type[Output]] = None)  Runnable[Input, Output] Bind input and output types to a Runnable, returning a new Runnable. property InputType: Any The type of input this runnable accepts specified as a type annotation. property OutputType: Type[langchain.schema.output_parser.T] The type of output this runnable produces specified as a type annotation. property config_specs: List[langchain.schema.runnable.utils.ConfigurableFieldSpec] List configurable fields for this runnable. property input_schema: Type[pydantic.main.BaseModel] The type of input this runnable accepts specified as a pydantic model. property lc_attributes: Dict List of attribute names that should be included in the serialized kwargs. These attributes must be accepted by the constructor. property lc_secrets: Dict[str, str] A map of constructor argument names to secret ids. For example,{openai_api_key: OPENAI_API_KEY} property output_schema: Type[pydantic.main.BaseModel] The type of output this runnable produces specified as a pydantic model. Examples using RetryOutputParser Retry parser  2023, Harrison Chase. Last updated on Nov 17, 2023. Show this page source"", 'Router | Router Routing allows you to create non-deterministic chains where the output of a previous step defines the next step. Routing helps provide structure and consistency around interactions with LLMs. As a very simple example, let\'s suppose we have two templates optimized for different types of questions, and we want to choose the template based on the user input. ```python from langchain.prompts import PromptTemplate physics_template = """"""You are a very smart physics professor. \\ You are great at answering questions about physics in a concise and easy to understand manner. \\ When you don\'t know the answer to a question you admit that you don\'t know. Here is a question: {input}"""""" physics_prompt = PromptTemplate.from_template(physics_template) math_template = """"""You are a very good mathematician. You are great at answering math questions. \\ You are so good because you are able to break down hard problems into their component parts, \\ answer the component parts, and then put them together to answer the broader question. Here is a question: {input}"""""" math_prompt = PromptTemplate.from_template(math_template) ``` ## Using LCEL We can easily do this using a `RunnableBranch`. A `RunnableBranch` is initialized with a list of (condition, runnable) pairs and a default runnable. It selects which branch by passing each condition the input it\'s invoked with. It selects the first condition to evaluate to True, and runs the corresponding runnable to that condition with the input. If no provided conditions match, it runs the default runnable. ```python from langchain.chat_models import ChatOpenAI from langchain.schema.output_parser import StrOutputParser from langchain.schema.runnable import RunnableBranch ``` ```python general_prompt = PromptTemplate.from_template( ""You are a helpful assistant. Answer the question as accurately as you can.\\n\\n{input}"" ) prompt_branch = RunnableBranch( (lambda x: x[""topic""] == ""math"", math_prompt), (lambda x: x[""topic""] == ""physics"", physics_prompt), general_prompt, ) ``` ```python from typing import Literal from langchain.output_parsers.openai_functions import PydanticAttrOutputFunctionsParser from langchain.pydantic_v1 import BaseModel from langchain.utils.openai_functions import convert_pydantic_to_openai_function class TopicClassifier(BaseModel): ""Classify the topic of the user question"" topic: Literal[""math"", ""physics"", ""general""] ""The topic of the user question. One of \'math\', \'physics\' or \'general\'."" classifier_function = convert_pydantic_to_openai_function(TopicClassifier) llm = ChatOpenAI().bind( functions=[classifier_function], function_call={""name"": ""TopicClassifier""} ) parser = PydanticAttrOutputFunctionsParser( pydantic_schema=TopicClassifier, attr_name=""topic"" ) classifier_chain = llm | parser ``` ```python from operator import itemgetter from langchain.schema.output_parser import StrOutputParser from langchain.schema.runnable import RunnablePassthrough final_chain = ( RunnablePassthrough.assign(topic=itemgetter(""input"") | classifier_chain) | prompt_branch | ChatOpenAI() | StrOutputParser() ) ``` ```python final_chain.invoke( { ""input"": ""What is the first prime number greater than 40 such that one plus the prime number is divisible by 3?"" } ) ``` ```text ""Thank you for your kind words! I\'ll be happy to help you with this math question.\\n\\nTo find the first prime number greater than 40 that satisfies the given condition, we need to follow a step-by-step approach. \\n\\nFirstly, let\'s list the prime numbers greater than 40:\\n41, 43, 47, 53, 59, 61, 67, 71, ...\\n\\nNow, we need to check if one plus each of these prime numbers is divisible by 3. We can do this by calculating the remainder when dividing each number by 3.\\n\\nFor 41, (41 + 1) % 3 = 42 % 3 = 0. It is divisible by 3.\\n\\nFor 43, (43 + 1) % 3 = 44 % 3 = 2. It is not divisible by 3.\\n\\nFor 47, (47 + 1) % 3 = 48 % 3 = 0. It is divisible by 3.\\n\\nSince 41 and 47 are both greater than 40 and satisfy the condition, the first prime number greater than 40 such that one plus the prime number is divisible by 3 is 41.\\n\\nTherefore, the answer to the question is 41."" ``` For more on routing with LCEL [head here](/docs/expression_language/how_to/routing). ## [Legacy] RouterChain The preferred approach as of version `0.0.293` is to use LCEL as above.Here we show how to use the `RouterChain` paradigm to create a chain that dynamically selects the next chain to use for a given input. Router chains are made up of two components: - The `RouterChain` itself (responsible for selecting the next chain to call) - `destination_chains`: chains that the router chain can route to In this example, we will focus on the different types of routing chains. We will show these routing chains used in a `MultiPromptChain` to create a question-answering chain that selects the prompt which is most relevant for a given question, and then answers the question using that prompt. ```python from langchain.chains import ConversationChain from langchain.chains.llm import LLMChain from langchain.chains.router import MultiPromptChain from langchain.llms import OpenAI ``` ### [Legacy] LLMRouterChain This chain uses an LLM to determine how to route things. ```python prompt_infos = [ { ""name"": ""physics"", ""description"": ""Good for answering questions about physics"", ""prompt_template"": physics_template, }, { ""name"": ""math"", ""description"": ""Good for answering math questions"", ""prompt_template"": math_template, }, ] ``` ```python llm = OpenAI() ``` ```python destination_chains = {} for p_info in prompt_infos: name = p_info[""name""] prompt_template = p_info[""prompt_template""] prompt = PromptTemplate(template=prompt_template, input_variables=[""input""]) chain = LLMChain(llm=llm, prompt=prompt) destination_chains[name] = chain default_chain = ConversationChain(llm=llm, output_key=""text"") ``` ```python from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE ``` ```python destinations = [f""{p[\'name\']}: {p[\'description\']}"" for p in prompt_infos] destinations_str = ""\\n"".join(destinations) router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations_str) router_prompt = PromptTemplate( template=router_template, input_variables=[""input""], output_parser=RouterOutputParser(), ) router_chain = LLMRouterChain.from_llm(llm, router_prompt) ``` ```python chain = MultiPromptChain( router_chain=router_chain, destination_chains=destination_chains, default_chain=default_chain, verbose=True, ) ``` ```python print(chain.run(""What is black body radiation?"")) ``` ```text > Entering new MultiPromptChain chain... /Users/bagatur/langchain/libs/langchain/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain. warnings.warn( physics: {\'input\': \'What is black body radiation?\'} > Finished chain. Black body radiation is the thermal electromagnetic radiation within or surrounding a body in thermodynamic equilibrium with its environment, or emitted by a black body (an idealized physical body which absorbs all incident electromagnetic radiation). It is a characteristic of the temperature of the body; if the body has a uniform temperature, the radiation is also uniform across the spectrum of frequencies. The spectral characteristics of the radiation are determined by the temperature of the body, which implies that a black body at a given temperature will emit the same amount of radiation at every frequency. ``` ```python print( chain.run( ""What is the first prime number greater than 40 such that one plus the prime number is divisible by 3?"" ) ) ``` ```text > Entering new MultiPromptChain chain... /Users/bagatur/langchain/libs/langchain/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain. warnings.warn( math: {\'input\': \'What is the first prime number greater than 40 such that one plus the prime number is divisible by 3?\'} > Finished chain. The first prime number greater than 40 such that one plus the prime number is divisible by 3 is 43. This can be seen by breaking down the problem: 1) We know that a prime number is a number that is only divisible by itself and one. 2) We also know that if a number is divisible by 3, the sum of its digits must be divisible by 3. So, if we want to find the first prime number greater than 40 such that one plus the prime number is divisible by 3, we can start counting up from 40, testing each number to see if it is prime and if the sum of the number and one is divisible by three. The first number we come to that satisfies these conditions is 43. ``` ```python print(chain.run(""What is the name of the type of cloud that rains?"")) ``` ```text > Entering new MultiPromptChain chain... /Users/bagatur/langchain/libs/langchain/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain. warnings.warn( physics: {\'input\': \'What is the name of the type of cloud that rains?\'} > Finished chain. The type of cloud that rains is called a cumulonimbus cloud. ``` ## [Legacy] EmbeddingRouterChain The `EmbeddingRouterChain` uses embeddings and similarity to route between destination chains. ```python from langchain.chains.router.embedding_router import EmbeddingRouterChain from langchain.embeddings import CohereEmbeddings from langchain.vectorstores import Chroma ``` ```python names_and_descriptions = [ (""physics"", [""for questions about physics""]), (""math"", [""for questions about math""]), ] ``` ```python router_chain = EmbeddingRouterChain.from_names_and_descriptions( names_and_descriptions, Chroma, CohereEmbeddings(), routing_keys=[""input""] ) ``` ```python chain = MultiPromptChain( router_chain=router_chain, destination_chains=destination_chains, default_chain=default_chain, verbose=True, ) ``` ```python print(chain.run(""What is black body radiation?"")) ``` ```text > Entering new MultiPromptChain chain... physics: {\'input\': \'What is black body radiation?\'} > Finished chain. Black body radiation is the electromagnetic radiation emitted by a black body, which is an idealized physical body that absorbs all incident electromagnetic radiation. This radiation is related to the temperature of the body, with higher temperatures leading to higher radiation levels. The spectrum of the radiation is continuous, and is described by the Planck\'s law of black body radiation. ``` ```python print( chain.run( ""What is the first prime number greater than 40 such that one plus the prime number is divisible by 3?"" ) ) ``` ```text > Entering new MultiPromptChain chain... math: {\'input\': \'What is the first prime number greater than 40 such that one plus the prime number is divisible by 3?\'} > Finished chain. The first prime number greater than 40 such that one plus the prime number is divisible by 3 is 43. This is because 43 is a prime number, and 1 + 43 = 44, which is divisible by 3. ``` - [Using LCEL](#using-lcel) - [Legacy RouterChain](#legacy-routerchain)']","The Retry parser is used to handle parsing errors by retrying the parsing process when the initial attempt fails. It does this by passing the original prompt and the incomplete or incorrect output to another language model (LLM), which attempts to correct the output based on the context provided by the prompt. This helps in cases where the output is not just incorrectly formatted but is also partially complete, which might not be fixable by the parser alone.",The Retry parser is used when the LLM output is not in the expected format. It passes the input and initial output back to the LLM so it can try again with the correct format.,0.99999999995,1.0,1.0,0.21449459478473418,0.2831858407079646
30,What does ConversationBufferWindowMemory do?,"['Conversation Buffer Window | Conversation Buffer Window `ConversationBufferWindowMemory` keeps a list of the interactions of the conversation over time. It only uses the last K interactions. This can be useful for keeping a sliding window of the most recent interactions, so the buffer does not get too large. Let\'s first explore the basic functionality of this type of memory. ```python from langchain.memory import ConversationBufferWindowMemory ``` ```python memory = ConversationBufferWindowMemory( k=1) memory.save_context({""input"": ""hi""}, {""output"": ""whats up""}) memory.save_context({""input"": ""not much you""}, {""output"": ""not much""}) ``` ```python memory.load_memory_variables({}) ``` ```text {\'history\': \'Human: not much you\\nAI: not much\'} ``` We can also get the history as a list of messages (this is useful if you are using this with a chat model). ```python memory = ConversationBufferWindowMemory( k=1, return_messages=True) memory.save_context({""input"": ""hi""}, {""output"": ""whats up""}) memory.save_context({""input"": ""not much you""}, {""output"": ""not much""}) ``` ```python memory.load_memory_variables({}) ``` ```text {\'history\': [HumanMessage(content=\'not much you\', additional_kwargs={}), AIMessage(content=\'not much\', additional_kwargs={})]} ``` ## Using in a chain Let\'s walk through an example, again setting `verbose=True` so we can see the prompt. ```python from langchain.llms import OpenAI from langchain.chains import ConversationChain conversation_with_summary = ConversationChain( llm=OpenAI(temperature=0), # We set a low k=2, to only keep the last 2 interactions in memory memory=ConversationBufferWindowMemory(k=2), verbose=True ) conversation_with_summary.predict(input=""Hi, what\'s up?"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: Human: Hi, what\'s up? AI: > Finished chain. "" Hi there! I\'m doing great. I\'m currently helping a customer with a technical issue. How about you?"" ``` ```python conversation_with_summary.predict(input=""What\'s their issues?"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: Human: Hi, what\'s up? AI: Hi there! I\'m doing great. I\'m currently helping a customer with a technical issue. How about you? Human: What\'s their issues? AI: > Finished chain. "" The customer is having trouble connecting to their Wi-Fi network. I\'m helping them troubleshoot the issue and get them connected."" ``` ```python conversation_with_summary.predict(input=""Is it going well?"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: Human: Hi, what\'s up? AI: Hi there! I\'m doing great. I\'m currently helping a customer with a technical issue. How about you? Human: What\'s their issues? AI: The customer is having trouble connecting to their Wi-Fi network. I\'m helping them troubleshoot the issue and get them connected. Human: Is it going well? AI: > Finished chain. "" Yes, it\'s going well so far. We\'ve already identified the problem and are now working on a solution."" ``` ```python # Notice here that the first interaction does not appear. conversation_with_summary.predict(input=""What\'s the solution?"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: Human: What\'s their issues? AI: The customer is having trouble connecting to their Wi-Fi network. I\'m helping them troubleshoot the issue and get them connected. Human: Is it going well? AI: Yes, it\'s going well so far. We\'ve already identified the problem and are now working on a solution. Human: What\'s the solution? AI: > Finished chain. "" The solution is to reset the router and reconfigure the settings. We\'re currently in the process of doing that."" ``` - [Using in a chain](#using-in-a-chain)', 'Chatbots | Chatbots []( ## Use case Chatbots are one of the central LLM use-cases. The core features of chatbots are that they can have long-running conversations and have access to information that users want to know about. Aside from basic prompting and LLMs, memory and retrieval are the core components of a chatbot. Memory allows a chatbot to remember past interactions, and retrieval provides a chatbot with up-to-date, domain-specific information. ![Image description](/assets/images/chat_use_case-eb8a4883931d726e9f23628a0d22e315.png) ## Overview The chat model interface is based around messages rather than raw text. Several components are important to consider for chat: - `chat model`: See [here](/docs/integrations/chat) for a list of chat model integrations and [here](/docs/modules/model_io/chat) for documentation on the chat model interface in LangChain. You can use `LLMs` (see [here](/docs/modules/model_io/llms)) for chatbots as well, but chat models have a more conversational tone and natively support a message interface. - `prompt template`: Prompt templates make it easy to assemble prompts that combine default messages, user input, chat history, and (optionally) additional retrieved context. - `memory`: [See here](/docs/modules/memory/) for in-depth documentation on memory types - `retriever` (optional): [See here](/docs/modules/data_connection/retrievers) for in-depth documentation on retrieval systems. These are useful if you want to build a chatbot with domain-specific knowledge. ## Quickstart Here\'s a quick preview of how we can create chatbot interfaces. First let\'s install some dependencies and set the required credentials: ```bash pip install langchain openai # Set env var OPENAI_API_KEY or load from a .env file: # import dotenv # dotenv.load_dotenv() ``` With a plain chat model, we can get chat completions by [passing one or more messages](/docs/modules/model_io/chat) to the model. The chat model will respond with a message. ```python from langchain.chat_models import ChatOpenAI from langchain.schema import HumanMessage, SystemMessage chat = ChatOpenAI() chat( [ HumanMessage( content=""Translate this sentence from English to French: I love programming."" ) ] ) ``` ```text AIMessage(content=""J\'adore la programmation."", additional_kwargs={}, example=False) ``` And if we pass in a list of messages: ```python messages = [ SystemMessage( content=""You are a helpful assistant that translates English to French."" ), HumanMessage(content=""I love programming.""), ] chat(messages) ``` ```text AIMessage(content=""J\'adore la programmation."", additional_kwargs={}, example=False) ``` We can then wrap our chat model in a `ConversationChain`, which has built-in memory for remembering past user inputs and model outputs. ```python from langchain.chains import ConversationChain conversation = ConversationChain(llm=chat) conversation.run(""Translate this sentence from English to French: I love programming."") ``` ```text \'Je adore la programmation.\' ``` ```python conversation.run(""Translate it to German."") ``` ```text \'Ich liebe Programmieren.\' ``` ## Memory As we mentioned above, the core component of chatbots is the memory system. One of the simplest and most commonly used forms of memory is `ConversationBufferMemory`: - This memory allows for storing of messages in a `buffer` - When called in a chain, it returns all of the messages it has stored LangChain comes with many other types of memory, too. [See here](/docs/modules/memory/) for in-depth documentation on memory types. For now let\'s take a quick look at ConversationBufferMemory. We can manually add a few chat messages to the memory like so: ```python from langchain.memory import ConversationBufferMemory memory = ConversationBufferMemory() memory.chat_memory.add_user_message(""hi!"") memory.chat_memory.add_ai_message(""whats up?"") ``` And now we can load from our memory. The key method exposed by all `Memory` classes is `load_memory_variables`. This takes in any initial chain input and returns a list of memory variables which are added to the chain input. Since this simple memory type doesn\'t actually take into account the chain input when loading memory, we can pass in an empty input for now: ```python memory.load_memory_variables({}) ``` ```text {\'history\': \'Human: hi!\\nAI: whats up?\'} ``` We can also keep a sliding window of the most recent `k` interactions using `ConversationBufferWindowMemory`. ```python from langchain.memory import ConversationBufferWindowMemory memory = ConversationBufferWindowMemory(k=1) memory.save_context({""input"": ""hi""}, {""output"": ""whats up""}) memory.save_context({""input"": ""not much you""}, {""output"": ""not much""}) memory.load_memory_variables({}) ``` ```text {\'history\': \'Human: not much you\\nAI: not much\'} ``` `ConversationSummaryMemory` is an extension of this theme. It creates a summary of the conversation over time. This memory is most useful for longer conversations where the full message history would consume many tokens. ```python from langchain.llms import OpenAI from langchain.memory import ConversationSummaryMemory llm = OpenAI(temperature=0) memory = ConversationSummaryMemory(llm=llm) memory.save_context({""input"": ""hi""}, {""output"": ""whats up""}) memory.save_context( {""input"": ""im working on better docs for chatbots""}, {""output"": ""oh, that sounds like a lot of work""}, ) memory.save_context( {""input"": ""yes, but it\'s worth the effort""}, {""output"": ""agreed, good docs are important!""}, ) ``` ```python memory.load_memory_variables({}) ``` ```text {\'history\': \'\\nThe human greets the AI, to which the AI responds. The human then mentions they are working on better docs for chatbots, to which the AI responds that it sounds like a lot of work. The human agrees that it is worth the effort, and the AI agrees that good docs are important.\'} ``` `ConversationSummaryBufferMemory` extends this a bit further: It uses token length rather than number of interactions to determine when to flush interactions. ```python from langchain.memory import ConversationSummaryBufferMemory memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=10) memory.save_context({""input"": ""hi""}, {""output"": ""whats up""}) memory.save_context({""input"": ""not much you""}, {""output"": ""not much""}) ``` ## Conversation We can unpack what goes under the hood with `ConversationChain`. We can specify our memory, `ConversationSummaryMemory` and we can specify the prompt. ```python from langchain.chains import LLMChain from langchain.prompts import ( ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate, ) # LLM llm = ChatOpenAI() # Prompt prompt = ChatPromptTemplate( messages=[ SystemMessagePromptTemplate.from_template( ""You are a nice chatbot having a conversation with a human."" ), # The `variable_name` here is what must align with memory MessagesPlaceholder(variable_name=""chat_history""), HumanMessagePromptTemplate.from_template(""{question}""), ] ) # Notice that we `return_messages=True` to fit into the MessagesPlaceholder # Notice that `""chat_history""` aligns with the MessagesPlaceholder name memory = ConversationBufferMemory(memory_key=""chat_history"", return_messages=True) conversation = LLMChain(llm=llm, prompt=prompt, verbose=True, memory=memory) # Notice that we just pass in the `question` variables - `chat_history` gets populated by memory conversation({""question"": ""hi""}) ``` ```text > Entering new LLMChain chain... Prompt after formatting: System: You are a nice chatbot having a conversation with a human. Human: hi > Finished chain. {\'question\': \'hi\', \'chat_history\': [HumanMessage(content=\'hi\', additional_kwargs={}, example=False), AIMessage(content=\'Hello! How can I assist you today?\', additional_kwargs={}, example=False)], \'text\': \'Hello! How can I assist you today?\'} ``` ```python conversation( {""question"": ""Translate this sentence from English to French: I love programming.""} ) ``` ```text > Entering new LLMChain chain... Prompt after formatting: System: You are a nice chatbot having a conversation with a human. Human: hi AI: Hello! How can I assist you today? Human: Translate this sentence from English to French: I love programming. > Finished chain. {\'question\': \'Translate this sentence from English to French: I love programming.\', \'chat_history\': [HumanMessage(content=\'hi\', additional_kwargs={}, example=False), AIMessage(content=\'Hello! How can I assist you today?\', additional_kwargs={}, example=False), HumanMessage(content=\'Translate this sentence from English to French: I love programming.\', additional_kwargs={}, example=False), AIMessage(content=\'Sure! The translation of ""I love programming"" from English to French is ""J\\\'adore programmer.""\', additional_kwargs={}, example=False)], \'text\': \'Sure! The translation of ""I love programming"" from English to French is ""J\\\'adore programmer.""\'} ``` ```python conversation({""question"": ""Now translate the sentence to German.""}) ``` ```text > Entering new LLMChain chain... Prompt after formatting: System: You are a nice chatbot having a conversation with a human. Human: hi AI: Hello! How can I assist you today? Human: Translate this sentence from English to French: I love programming. AI: Sure! The translation of ""I love programming"" from English to French is ""J\'adore programmer."" Human: Now translate the sentence to German. > Finished chain. {\'question\': \'Now translate the sentence to German.\', \'chat_history\': [HumanMessage(content=\'hi\', additional_kwargs={}, example=False), AIMessage(content=\'Hello! How can I assist you today?\', additional_kwargs={}, example=False), HumanMessage(content=\'Translate this sentence from English to French: I love programming.\', additional_kwargs={}, example=False), AIMessage(content=\'Sure! The translation of ""I love programming"" from English to French is ""J\\\'adore programmer.""\', additional_kwargs={}, example=False), HumanMessage(content=\'Now translate the sentence to German.\', additional_kwargs={}, example=False), AIMessage(content=\'Certainly! The translation of ""I love programming"" from English to German is ""Ich liebe das Programmieren.""\', additional_kwargs={}, example=False)], \'text\': \'Certainly! The translation of ""I love programming"" from English to German is ""Ich liebe das Programmieren.""\'} ``` We can see the chat history preserved in the prompt using the [LangSmith trace]( ![Image description](/assets/images/chat_use_case_2-a76871806149f125d08ff149363e0c2c.png) ## Chat Retrieval Now, suppose we want to [chat with documents]( or some other source of knowledge. This is popular use case, combining chat with [document retrieval](/docs/use_cases/question_answering). It allows us to chat with specific information that the model was not trained on. ```bash pip install tiktoken chromadb ``` Load a blog post. ```python from langchain.document_loaders import WebBaseLoader loader = WebBaseLoader("" data = loader.load() ``` Split and store this in a vector. ```python from langchain.text_splitter import RecursiveCharacterTextSplitter text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0) all_splits = text_splitter.split_documents(data) from langchain.embeddings import OpenAIEmbeddings from langchain.vectorstores import Chroma vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings()) ``` Create our memory, as before, but\'s let\'s use `ConversationSummaryMemory`. ```python memory = ConversationSummaryMemory( llm=llm, memory_key=""chat_history"", return_messages=True ) ``` ```python from langchain.chains import ConversationalRetrievalChain from langchain.chat_models import ChatOpenAI llm = ChatOpenAI() retriever = vectorstore.as_retriever() qa = ConversationalRetrievalChain.from_llm(llm, retriever=retriever, memory=memory) ``` ```python qa(""How do agents use Task decomposition?"") ``` ```text {\'question\': \'How do agents use Task decomposition?\', \'chat_history\': [SystemMessage(content=\'\', additional_kwargs={})], \'answer\': \'Agents can use task decomposition in several ways:\\n\\n1. Simple prompting: Agents can use Language Model based prompting to break down tasks into subgoals. For example, by providing prompts like ""Steps for XYZ"" or ""What are the subgoals for achieving XYZ?"", the agent can generate a sequence of smaller steps that lead to the completion of the overall task.\\n\\n2. Task-specific instructions: Agents can be given task-specific instructions to guide their planning process. For example, if the task is to write a novel, the agent can be instructed to ""Write a story outline."" This provides a high-level structure for the task and helps in breaking it down into smaller components.\\n\\n3. Human inputs: Agents can also take inputs from humans to decompose tasks. This can be done through direct communication or by leveraging human expertise. Humans can provide guidance and insights to help the agent break down complex tasks into manageable subgoals.\\n\\nOverall, task decomposition allows agents to break down large tasks into smaller, more manageable subgoals, enabling them to plan and execute complex tasks efficiently.\'} ``` ```python qa(""What are the various ways to implement memory to support it?"") ``` ```text {\'question\': \'What are the various ways to implement memory to support it?\', \'chat_history\': [SystemMessage(content=\'The human asks how agents use task decomposition. The AI explains that agents can use task decomposition in several ways, including simple prompting, task-specific instructions, and human inputs. Task decomposition allows agents to break down large tasks into smaller, more manageable subgoals, enabling them to plan and execute complex tasks efficiently.\', additional_kwargs={})], \'answer\': \'There are several ways to implement memory to support task decomposition:\\n\\n1. Long-Term Memory Management: This involves storing and organizing information in a long-term memory system. The agent can retrieve past experiences, knowledge, and learned strategies to guide the task decomposition process.\\n\\n2. Internet Access: The agent can use internet access to search for relevant information and gather resources to aid in task decomposition. This allows the agent to access a vast amount of information and utilize it in the decomposition process.\\n\\n3. GPT-3.5 Powered Agents: The agent can delegate simple tasks to GPT-3.5 powered agents. These agents can perform specific tasks or provide assistance in task decomposition, allowing the main agent to focus on higher-level planning and decision-making.\\n\\n4. File Output: The agent can store the results of task decomposition in files or documents. This allows for easy retrieval and reference during the execution of the task.\\n\\nThese memory resources help the agent in organizing and managing information, making informed decisions, and effectively decomposing complex tasks into smaller, manageable subgoals.\'} ``` Again, we can use the [LangSmith trace]( to explore the prompt structure. ### Going deeper - Agents, such as the [conversational retrieval agent](/docs/use_cases/question_answering/conversational_retrieval_agents), can be used for retrieval when necessary while also holding a conversation. - [Use case](#use-case) - [Overview](#overview) - [Quickstart](#quickstart) - [Memory](#memory) - [Conversation](#conversation) - [Chat Retrieval](#chat-retrieval)- [Going deeper](#going-deeper)', 'Customizing Conversational Memory | Customizing Conversational Memory This notebook walks through a few ways to customize conversational memory. ```python from langchain.chains import ConversationChain from langchain.llms import OpenAI from langchain.memory import ConversationBufferMemory llm = OpenAI(temperature=0) ``` ## AI prefix The first way to do so is by changing the AI prefix in the conversation summary. By default, this is set to ""AI"", but you can set this to be anything you want. Note that if you change this, you should also change the prompt used in the chain to reflect this naming change. Let\'s walk through an example of that in the example below. ```python # Here it is by default set to ""AI"" conversation = ConversationChain( llm=llm, verbose=True, memory=ConversationBufferMemory() ) ``` ```python conversation.predict(input=""Hi there!"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: Human: Hi there! AI: > Finished ConversationChain chain. "" Hi there! It\'s nice to meet you. How can I help you today?"" ``` ```python conversation.predict(input=""What\'s the weather?"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: Human: Hi there! AI: Hi there! It\'s nice to meet you. How can I help you today? Human: What\'s the weather? AI: > Finished ConversationChain chain. \' The current weather is sunny and warm with a temperature of 75 degrees Fahrenheit. The forecast for the next few days is sunny with temperatures in the mid-70s.\' ``` ```python # Now we can override it and set it to ""AI Assistant"" from langchain.prompts.prompt import PromptTemplate template = """"""The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: {history} Human: {input} AI Assistant:"""""" PROMPT = PromptTemplate(input_variables=[""history"", ""input""], template=template) conversation = ConversationChain( prompt=PROMPT, llm=llm, verbose=True, memory=ConversationBufferMemory(ai_prefix=""AI Assistant""), ) ``` ```python conversation.predict(input=""Hi there!"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: Human: Hi there! AI Assistant: > Finished ConversationChain chain. "" Hi there! It\'s nice to meet you. How can I help you today?"" ``` ```python conversation.predict(input=""What\'s the weather?"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: Human: Hi there! AI Assistant: Hi there! It\'s nice to meet you. How can I help you today? Human: What\'s the weather? AI Assistant: > Finished ConversationChain chain. \' The current weather is sunny and warm with a temperature of 75 degrees Fahrenheit. The forecast for the rest of the day is sunny with a high of 78 degrees and a low of 65 degrees.\' ``` ## Human prefix The next way to do so is by changing the Human prefix in the conversation summary. By default, this is set to ""Human"", but you can set this to be anything you want. Note that if you change this, you should also change the prompt used in the chain to reflect this naming change. Let\'s walk through an example of that in the example below. ```python # Now we can override it and set it to ""Friend"" from langchain.prompts.prompt import PromptTemplate template = """"""The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: {history} Friend: {input} AI:"""""" PROMPT = PromptTemplate(input_variables=[""history"", ""input""], template=template) conversation = ConversationChain( prompt=PROMPT, llm=llm, verbose=True, memory=ConversationBufferMemory(human_prefix=""Friend""), ) ``` ```python conversation.predict(input=""Hi there!"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: Friend: Hi there! AI: > Finished ConversationChain chain. "" Hi there! It\'s nice to meet you. How can I help you today?"" ``` ```python conversation.predict(input=""What\'s the weather?"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: Friend: Hi there! AI: Hi there! It\'s nice to meet you. How can I help you today? Friend: What\'s the weather? AI: > Finished ConversationChain chain. \' The weather right now is sunny and warm with a temperature of 75 degrees Fahrenheit. The forecast for the rest of the day is mostly sunny with a high of 82 degrees.\' ``` - [AI prefix](#ai-prefix) - [Human prefix](#human-prefix)']",ConversationBufferWindowMemory keeps a sliding window of the last K interactions in a conversation. It's useful for maintaining a manageable memory size while retaining recent context.,"ConversationBufferWindowMemory keeps a list of the interactions of the conversation over time. It only uses the last K interactions. This can be useful for keeping a sliding window of the most recent interactions, so the buffer does not get too large.",0.9999999999666667,1.0,1.0,0.5676721706387804,0.35820895522388063
31,What is the main purpose of the time-weighted vector store retriever?,"['Time-weighted vector store retriever | Time-weighted vector store retriever This retriever uses a combination of semantic similarity and a time decay. The algorithm for scoring them is: ```text semantic_similarity + (1.0 - decay_rate) ^ hours_passed ``` Notably, `hours_passed` refers to the hours passed since the object in the retriever **was last accessed**, not since it was created. This means that frequently accessed objects remain ""fresh"". ```python import faiss from datetime import datetime, timedelta from langchain.docstore import InMemoryDocstore from langchain.embeddings import OpenAIEmbeddings from langchain.retrievers import TimeWeightedVectorStoreRetriever from langchain.schema import Document from langchain.vectorstores import FAISS ``` ## Low decay rate A low `decay rate` (in this, to be extreme, we will set it close to 0) means memories will be ""remembered"" for longer. A `decay rate` of 0 means memories never be forgotten, making this retriever equivalent to the vector lookup. ```python # Define your embedding model embeddings_model = OpenAIEmbeddings() # Initialize the vectorstore as empty embedding_size = 1536 index = faiss.IndexFlatL2(embedding_size) vectorstore = FAISS(embeddings_model.embed_query, index, InMemoryDocstore({}), {}) retriever = TimeWeightedVectorStoreRetriever(vectorstore=vectorstore, decay_rate=.0000000000000000000000001, k=1) ``` ```python yesterday = datetime.now() - timedelta(days=1) retriever.add_documents([Document(page_content=""hello world"", metadata={""last_accessed_at"": yesterday})]) retriever.add_documents([Document(page_content=""hello foo"")]) ``` ```text [\'d7f85756-2371-4bdf-9140-052780a0f9b3\'] ``` ```python # ""Hello World"" is returned first because it is most salient, and the decay rate is close to 0., meaning it\'s still recent enough retriever.get_relevant_documents(""hello world"") ``` ```text [Document(page_content=\'hello world\', metadata={\'last_accessed_at\': datetime.datetime(2023, 5, 13, 21, 0, 27, 678341), \'created_at\': datetime.datetime(2023, 5, 13, 21, 0, 27, 279596), \'buffer_idx\': 0})] ``` ## High decay rate With a high `decay rate` (e.g., several 9\'s), the `recency score` quickly goes to 0! If you set this all the way to 1, `recency` is 0 for all objects, once again making this equivalent to a vector lookup. ```python # Define your embedding model embeddings_model = OpenAIEmbeddings() # Initialize the vectorstore as empty embedding_size = 1536 index = faiss.IndexFlatL2(embedding_size) vectorstore = FAISS(embeddings_model.embed_query, index, InMemoryDocstore({}), {}) retriever = TimeWeightedVectorStoreRetriever(vectorstore=vectorstore, decay_rate=.999, k=1) ``` ```python yesterday = datetime.now() - timedelta(days=1) retriever.add_documents([Document(page_content=""hello world"", metadata={""last_accessed_at"": yesterday})]) retriever.add_documents([Document(page_content=""hello foo"")]) ``` ```text [\'40011466-5bbe-4101-bfd1-e22e7f505de2\'] ``` ```python # ""Hello Foo"" is returned first because ""hello world"" is mostly forgotten retriever.get_relevant_documents(""hello world"") ``` ```text [Document(page_content=\'hello foo\', metadata={\'last_accessed_at\': datetime.datetime(2023, 4, 16, 22, 9, 2, 494798), \'created_at\': datetime.datetime(2023, 4, 16, 22, 9, 2, 178722), \'buffer_idx\': 1})] ``` ## Virtual time Using some utils in LangChain, you can mock out the time component. ```python from langchain.utils import mock_now import datetime ``` ```python # Notice the last access time is that date time with mock_now(datetime.datetime(2011, 2, 3, 10, 11)): print(retriever.get_relevant_documents(""hello world"")) ``` ```text [Document(page_content=\'hello world\', metadata={\'last_accessed_at\': MockDateTime(2011, 2, 3, 10, 11), \'created_at\': datetime.datetime(2023, 5, 13, 21, 0, 27, 279596), \'buffer_idx\': 0})] ``` - [Low decay rate](#low-decay-rate) - [High decay rate](#high-decay-rate) - [Virtual time](#virtual-time)', 'Fleet AI Libraries Context | Fleet AI Libraries Context The Fleet AI team is on a mission to embed the world\'s most important data. They\'ve started by embedding the top 1200 Python libraries to enable code generation with up-to-date knowledge. They\'ve been kind enough to share their embeddings of the [LangChain docs]( and [API reference]( Let\'s take a look at how we can use these embeddings to power a docs retrieval system and ultimately a simple code generating chain! ```bash pip install langchain fleet-context openai pandas faiss-cpu # faiss-gpu for CUDA supported GPU ``` ```python from operator import itemgetter from typing import Any, Optional, Type import pandas as pd from langchain.embeddings import OpenAIEmbeddings from langchain.retrievers import MultiVectorRetriever from langchain.schema import Document from langchain.schema.storage import BaseStore from langchain.schema.vectorstore import VectorStore from langchain.vectorstores import FAISS def load_fleet_retriever( df: pd.DataFrame, *, vectorstore_cls: Type[VectorStore] = FAISS, docstore: Optional[BaseStore] = None, **kwargs: Any, ): vectorstore = _populate_vectorstore(df, vectorstore_cls) if docstore is None: return vectorstore.as_retriever(**kwargs) else: _populate_docstore(df, docstore) return MultiVectorRetriever( vectorstore=vectorstore, docstore=docstore, id_key=""parent"", **kwargs ) def _populate_vectorstore( df: pd.DataFrame, vectorstore_cls: Type[VectorStore], ) -> VectorStore: if not hasattr(vectorstore_cls, ""from_embeddings""): raise ValueError( f""Incompatible vector store class {vectorstore_cls}."" ""Must implement `from_embeddings` class method."" ) texts_embeddings = [] metadatas = [] for _, row in df.iterrows(): texts_embeddings.append((row.metadata[""text""], row[""dense_embeddings""])) metadatas.append(row.metadata) return vectorstore_cls.from_embeddings( texts_embeddings, OpenAIEmbeddings(model=""text-embedding-ada-002""), metadatas=metadatas, ) def _populate_docstore(df: pd.DataFrame, docstore: BaseStore) -> None: parent_docs = [] df = df.copy() df[""parent""] = df.metadata.apply(itemgetter(""parent"")) for parent_id, group in df.groupby(""parent""): sorted_group = group.iloc[ group.metadata.apply(itemgetter(""section_index"")).argsort() ] text = """".join(sorted_group.metadata.apply(itemgetter(""text""))) metadata = { k: sorted_group.iloc[0].metadata[k] for k in (""title"", ""type"", ""url"") } text = metadata[""title""] + ""\\n"" + text metadata[""id""] = parent_id parent_docs.append(Document(page_content=text, metadata=metadata)) docstore.mset(((d.metadata[""id""], d) for d in parent_docs)) ``` ## Retriever chunks As part of their embedding process, the Fleet AI team first chunked long documents before embedding them. This means the vectors correspond to sections of pages in the LangChain docs, not entire pages. By default, when we spin up a retriever from these embeddings, we\'ll be retrieving these embedded chunks. We will be using Fleet Context\'s `download_embeddings()` to grab Langchain\'s documentation embeddings. You can view all supported libraries\' documentation at [ ```python from context import download_embeddings df = download_embeddings(""langchain"") vecstore_retriever = load_fleet_retriever(df) ``` ```python vecstore_retriever.get_relevant_documents(""How does the multi vector retriever work"") ``` ```text [Document(page_content=""# Vector store-backed retriever A vector store retriever is a retriever that uses a vector store to retrieve documents. It is a lightweight wrapper around the vector store class to make it conform to the retriever interface. It uses the search methods implemented by a vector store, like similarity search and MMR, to query the texts in the vector store. Once you construct a vector store, it\'s very easy to construct a retriever. Let\'s walk through an example."", metadata={\'id\': \'f509f20d-4c63-4a5a-a40a-5c4c0f099839\', \'library_id\': \'4506492b-70de-49f1-ba2e-d65bd7048a28\', \'page_id\': \'d78cf422-2dab-4860-80fe-d71a3619b02f\', \'parent\': \'c153ebd9-2611-4a43-9db6-daa1f5f214f6\', \'section_id\': \'\', \'section_index\': 0, \'text\': ""# Vector store-backed retriever A vector store retriever is a retriever that uses a vector store to retrieve documents. It is a lightweight wrapper around the vector store class to make it conform to the retriever interface. It uses the search methods implemented by a vector store, like similarity search and MMR, to query the texts in the vector store. Once you construct a vector store, it\'s very easy to construct a retriever. Let\'s walk through an example."", \'title\': \'Vector store-backed retriever | Langchain\', \'type\': None, \'url\': \' Document(page_content=\'# MultiVector Retriever It can often be beneficial to store multiple vectors per document. There are multiple use cases where this is beneficial. LangChain has a base `MultiVectorRetriever` which makes querying this type of setup easy. A lot of the complexity lies in how to create the multiple vectors per document. This notebook covers some of the common ways to create those vectors and use the `MultiVectorRetriever`. The methods to create multiple vectors per document include: - Smaller chunks: split a document into smaller chunks, and embed those (this is ParentDocumentRetriever). - Summary: create a summary for each document, embed that along with (or instead of) the document. - Hypothetical questions: create hypothetical questions that each document would be appropriate to answer, embed those along with (or instead of) the document. Note that this also enables another method of adding embeddings - manually.\', metadata={\'id\': \'e06e6bb5-127a-49f7-9511-247d279d0d83\', \'library_id\': \'4506492b-70de-49f1-ba2e-d65bd7048a28\', \'page_id\': \'7c7dd0de-25e0-4e1d-9237-cfd2674c29d4\', \'parent\': \'beec5531-16a7-453c-80ab-c5628e0236ce\', \'section_id\': \'\', \'section_index\': 0, \'text\': \'# MultiVector Retriever It can often be beneficial to store multiple vectors per document. There are multiple use cases where this is beneficial. LangChain has a base `MultiVectorRetriever` which makes querying this type of setup easy. A lot of the complexity lies in how to create the multiple vectors per document. This notebook covers some of the common ways to create those vectors and use the `MultiVectorRetriever`. The methods to create multiple vectors per document include: - Smaller chunks: split a document into smaller chunks, and embed those (this is ParentDocumentRetriever). - Summary: create a summary for each document, embed that along with (or instead of) the document. - Hypothetical questions: create hypothetical questions that each document would be appropriate to answer, embed those along with (or instead of) the document. Note that this also enables another method of adding embeddings - manually.\', \'title\': \'MultiVector Retriever | Langchain\', \'type\': None, \'url\': \' Document(page_content=\'# MultiQueryRetriever Distance-based vector database retrieval embeds (represents) queries in high-dimensional space and finds similar embedded documents based on ""distance"". But, retrieval may produce different results with subtle changes in query wording or if the embeddings do not capture the semantics of the data well. Prompt engineering / tuning is sometimes done to manually address these problems, but can be tedious. The `MultiQueryRetriever` automates the process of prompt tuning by using an LLM to generate multiple queries from different perspectives for a given user input query. For each query, it retrieves a set of relevant documents and takes the unique union across all queries to get a larger set of potentially relevant documents. By generating multiple perspectives on the same question, the `MultiQueryRetriever` might be able to overcome some of the limitations of the distance-based retrieval and get a richer set of results.\', metadata={\'id\': \'dc3b8e5b-a591-4a46-bfeb-a91b36affae1\', \'library_id\': \'4506492b-70de-49f1-ba2e-d65bd7048a28\', \'page_id\': \'31f80e84-c5db-4da2-939c-bccf519864a3\', \'parent\': \'f7c20633-6a60-4ca3-96b1-13fee66e321d\', \'section_id\': \'\', \'section_index\': 0, \'text\': \'# MultiQueryRetriever Distance-based vector database retrieval embeds (represents) queries in high-dimensional space and finds similar embedded documents based on ""distance"". But, retrieval may produce different results with subtle changes in query wording or if the embeddings do not capture the semantics of the data well. Prompt engineering / tuning is sometimes done to manually address these problems, but can be tedious. The `MultiQueryRetriever` automates the process of prompt tuning by using an LLM to generate multiple queries from different perspectives for a given user input query. For each query, it retrieves a set of relevant documents and takes the unique union across all queries to get a larger set of potentially relevant documents. By generating multiple perspectives on the same question, the `MultiQueryRetriever` might be able to overcome some of the limitations of the distance-based retrieval and get a richer set of results.\', \'title\': \'MultiQueryRetriever | Langchain\', \'type\': None, \'url\': \' Document(page_content=\'# `langchain.retrievers.multi_vector`.MultiVectorRetriever[](#langchain-retrievers-multi-vector-multivectorretriever) *class *langchain.retrievers.multi_vector.MultiVectorRetriever[[source]](../_modules/langchain/retrievers/multi_vector.html#MultiVectorRetriever)[](#langchain.retrievers.multi_vector.MultiVectorRetriever) # Examples using MultiVectorRetriever[](#langchain-retrievers-multi-vector-multivectorretriever) - [MultiVector Retriever]( metadata={\'id\': \'1f4ca702-35b8-44c0-b33b-18c09a1f787d\', \'library_id\': \'6254c672-7aa0-4233-b0a6-804bd273752b\', \'page_id\': \'87f5d1fb-a1c6-4080-9d2b-7b88794eb6bf\', \'parent\': \'1820c44d-7783-4846-a11c-106b18da015d\', \'section_id\': \'langchain-retrievers-multi-vector-multivectorretriever\', \'section_index\': 0, \'text\': \'# `langchain.retrievers.multi_vector`.MultiVectorRetriever[](#langchain-retrievers-multi-vector-multivectorretriever) *class *langchain.retrievers.multi_vector.MultiVectorRetriever[[source]](../_modules/langchain/retrievers/multi_vector.html#MultiVectorRetriever)[](#langchain.retrievers.multi_vector.MultiVectorRetriever) # Examples using MultiVectorRetriever[](#langchain-retrievers-multi-vector-multivectorretriever) - [MultiVector Retriever]( \'title\': \'langchain.retrievers.multi_vector.MultiVectorRetriever LangChain 0.0.322\', \'type\': None, \'url\': \' ``` ## Other packages You can download and use other embeddings from [this Dropbox link]( ## Retrieve parent docs The embeddings provided by Fleet AI contain metadata that indicates which embedding chunks correspond to the same original document page. If we\'d like we can use this information to retrieve whole parent documents, and not just embedded chunks. Under the hood, we\'ll use a MultiVectorRetriever and a BaseStore object to search for relevant chunks and then map them to their parent document. ```python from langchain.storage import InMemoryStore parent_retriever = load_fleet_retriever( "" docstore=InMemoryStore(), ) ``` ```python parent_retriever.get_relevant_documents(""How does the multi vector retriever work"") ``` ```text [Document(page_content=\'Vector store-backed retriever | Langchain\\n# Vector store-backed retriever A vector store retriever is a retriever that uses a vector store to retrieve documents. It is a lightweight wrapper around the vector store class to make it conform to the retriever interface. It uses the search methods implemented by a vector store, like similarity search and MMR, to query the texts in the vector store. Once you construct a vector store, it\\\'s very easy to construct a retriever. Let\\\'s walk through an example.Once you construct a vector store, it\\\'s very easy to construct a retriever. Let\\\'s walk through an example. ``` from langchain.document_loaders import TextLoaderloader = TextLoader(\\\'../../../state_of_the_union.txt\\\') ``` ``` from langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import FAISSfrom langchain.embeddings import OpenAIEmbeddingsdocuments = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)texts = text_splitter.split_documents(documents)embeddings = OpenAIEmbeddings()db = FAISS.from_documents(texts, embeddings) ``` ``` Exiting: Cleaning up .chroma directory ``` ``` retriever = db.as_retriever() ``` ``` docs = retriever.get_relevant_documents(""what did he say about ketanji brown jackson"") ``` ## Maximum marginal relevance retrieval[\\u200b](#maximum-marginal-relevance-retrieval) By default, the vector store retriever uses similarity search.If the underlying vector store supports maximum marginal relevance search, you can specify that as the search type. ``` retriever = db.as_retriever(search_type=""mmr"") ``` ``` docs = retriever.get_relevant_documents(""what did he say about ketanji brown jackson"") ``` ## Similarity score threshold retrieval[\\u200b](#similarity-score-threshold-retrieval) You can also a retrieval method that sets a similarity score threshold and only returns documents with a score above that threshold. ``` retriever = db.as_retriever(search_type=""similarity_score_threshold"", search_kwargs={""score_threshold"": .5}) ``` ``` docs = retriever.get_relevant_documents(""what did he say about ketanji brown jackson"") ``` ## Specifying top k[\\u200b](#specifying-top-k) You can also specify search kwargs like `k` to use when doing retrieval.``` retriever = db.as_retriever(search_kwargs={""k"": 1}) ``` ``` docs = retriever.get_relevant_documents(""what did he say about ketanji brown jackson"") ``` ``` len(docs) ``` ``` 1 ```\', metadata={\'title\': \'Vector store-backed retriever | Langchain\', \'type\': None, \'url\': \' \'id\': \'c153ebd9-2611-4a43-9db6-daa1f5f214f6\'}), Document(page_content=\'MultiVector Retriever | Langchain\\n# MultiVector Retriever It can often be beneficial to store multiple vectors per document. There are multiple use cases where this is beneficial. LangChain has a base `MultiVectorRetriever` which makes querying this type of setup easy. A lot of the complexity lies in how to create the multiple vectors per document. This notebook covers some of the common ways to create those vectors and use the `MultiVectorRetriever`. The methods to create multiple vectors per document include: - Smaller chunks: split a document into smaller chunks, and embed those (this is ParentDocumentRetriever). - Summary: create a summary for each document, embed that along with (or instead of) the document. - Hypothetical questions: create hypothetical questions that each document would be appropriate to answer, embed those along with (or instead of) the document. Note that this also enables another method of adding embeddings - manually.Note that this also enables another method of adding embeddings - manually. This is great because you can explicitly add questions or queries that should lead to a document being recovered, giving you more control. ``` from langchain.retrievers.multi_vector import MultiVectorRetriever ``` ``` from langchain.vectorstores import Chromafrom langchain.embeddings import OpenAIEmbeddingsfrom langchain.text_splitter import RecursiveCharacterTextSplitterfrom langchain.storage import InMemoryStorefrom langchain.document_loaders import TextLoader ``` ``` loaders = [ TextLoader(\\\'../../paul_graham_essay.txt\\\'), TextLoader(\\\'../../state_of_the_union.txt\\\'),]docs = []for l in loaders: docs.extend(l.load())text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000)docs = text_splitter.split_documents(docs) ``` ## Smaller chunks[\\u200b](#smaller-chunks) Often times it can be useful to retrieve larger chunks of information, but embed smaller chunks.This allows for embeddings to capture the semantic meaning as closely as possible, but for as much context as possible to be passed downstream. Note that this is what the `ParentDocumentRetriever` does. Here we show what is going on under the hood.``` # The vectorstore to use to index the child chunksvectorstore = Chroma( collection_name=""full_documents"", embedding_function=OpenAIEmbeddings())# The storage layer for the parent documentsstore = InMemoryStore()id_key = ""doc_id""# The retriever (empty to start)retriever = MultiVectorRetriever( vectorstore=vectorstore, docstore=store, id_key=id_key,)import uuiddoc_ids = [str(uuid.uuid4()) for _ in docs] ``` ``` # The splitter to use to create smaller chunkschild_text_splitter = RecursiveCharacterTextSplitter(chunk_size=400) ``` ``` sub_docs = []for i, doc in enumerate(docs): _id = doc_ids[i] _sub_docs = child_text_splitter.split_documents([doc]) for _doc in _sub_docs: _doc.metadata[id_key] = _id sub_docs.extend(_sub_docs) ``` ``` retriever.vectorstore.add_documents(sub_docs)retriever.docstore.mset(list(zip(doc_ids, docs))) ``` ``` # Vectorstore alone retrieves the small chunksretriever.vectorstore.similarity_search(""justice breyer"")[0] ``` ``` Document(page_content=\\\'Tonight, I\'d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyeran Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court.Justice Breyer, thank you for your service. \\\\n\\\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\\', metadata={\\\'doc_id\\\': \\\'10e9cbc0-4ba5-4d79-a09b-c033d1ba7b01\\\', \\\'source\\\': \\\'../../state_of_the_union.txt\\\'}) ``` ``` # Retriever returns larger chunkslen(retriever.get_relevant_documents(""justice breyer"")[0].page_content) ``` ``` 9874 ``` ## Summary[\\u200b](#summary) Oftentimes a summary may be able to distill more accurately what a chunk is about, leading to better retrieval. Here we show how to create summaries, and then embed those.``` from langchain.chat_models import ChatOpenAIfrom langchain.prompts import ChatPromptTemplatefrom langchain.schema.output_parser import StrOutputParserimport uuidfrom langchain.schema.document import Document ``` ``` chain = ( {""doc"": lambda x: x.page_content} | ChatPromptTemplate.from_template(""Summarize the following document:\\\\n\\\\n{doc}"") | ChatOpenAI(max_retries=0) | StrOutputParser()) ``` ``` summaries = chain.batch(docs, {""max_concurrency"": 5}) ``` ``` # The vectorstore to use to index the child chunksvectorstore = Chroma( collection_name=""summaries"", embedding_function=OpenAIEmbeddings())# The storage layer for the parent documentsstore = InMemoryStore()id_key = ""doc_id""# The retriever (empty to start)retriever = MultiVectorRetriever( vectorstore=vectorstore, docstore=store, id_key=id_key,)doc_ids = [str(uuid.uuid4()) for _ in docs] ``` ``` summary_docs = [Document(page_content=s,metadata={id_key: doc_ids[i]}) for i, s in enumerate(summaries)] ``` ``` retriever.vectorstore.add_documents(summary_docs)retriever.docstore.mset(list(zip(doc_ids, docs))) ``` ``` # # We can also add the original chunks to the vectorstore if we so want# for i, doc in enumerate(docs):# doc.metadata[id_key] = doc_ids[i]# retriever.vectorstore.add_documents(docs) ``` ``` sub_docs = vectorstore.similarity_search(""justice breyer"") ``` ``` sub_docs[0] ``` ``` Document(page_content=""The document is a transcript of a speech given by the President of the United States.The President discusses several important issues and initiatives, including the nomination of a Supreme Court Justice, border security and immigration reform, protecting women\\\'s rights, advancing LGBTQ+ equality, bipartisan legislation, addressing the opioid epidemic and mental health, supporting veterans, investigating the health effects of burn pits on military personnel, ending cancer, and the strength and resilience of the American people. "", metadata={\\\'doc_id\\\': \\\'79fa2e9f-28d9-4372-8af3-2caf4f1de312\\\'}) ``` ``` retrieved_docs = retriever.get_relevant_documents(""justice breyer"") ``` ``` len(retrieved_docs[0].page_content) ``` ``` 9194 ``` ## Hypothetical Queries[\\u200b](#hypothetical-queries) An LLM can also be used to generate a list of hypothetical questions that could be asked of a particular document.These questions can then be embedded ``` functions = [ { ""name"": ""hypothetical_questions"", ""description"": ""Generate hypothetical questions"", ""parameters"": { ""type"": ""object"", ""properties"": { ""questions"": { ""type"": ""array"", ""items"": { ""type"": ""string"" }, }, }, ""required"": [""questions""] } } ] ``` ``` from langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParserchain = ( {""doc"": lambda x: x.page_content} # Only asking for 3 hypothetical questions, but this could be adjusted | ChatPromptTemplate.from_template(""Generate a list of 3 hypothetical questions that the below document could be used to answer:\\\\n\\\\n{doc}"") | ChatOpenAI(max_retries=0, model=""gpt-4"").bind(functions=functions, function_call={""name"": ""hypothetical_questions""}) | JsonKeyOutputFunctionsParser(key_name=""questions"")) ``` ``` chain.invoke(docs[0]) ``` ``` [""What was the author\\\'s initial impression of philosophy as a field of study, and how did it change when they got to college?"", \\\'Why did the author decide to switch their focus to Artificial Intelligence (AI)? \\\', ""What led to the author\\\'s disillusionment with the field of AI as it was practiced at the time?""]``` ``` hypothetical_questions = chain.batch(docs, {""max_concurrency"": 5}) ``` ``` # The vectorstore to use to index the child chunksvectorstore = Chroma( collection_name=""hypo-questions"", embedding_function=OpenAIEmbeddings())# The storage layer for the parent documentsstore = InMemoryStore()id_key = ""doc_id""# The retriever (empty to start)retriever = MultiVectorRetriever( vectorstore=vectorstore, docstore=store, id_key=id_key,)doc_ids = [str(uuid.uuid4()) for _ in docs] ``` ``` question_docs = []for i, question_list in enumerate(hypothetical_questions): question_docs.extend([Document(page_content=s,metadata={id_key: doc_ids[i]}) for s in question_list]) ``` ``` retriever.vectorstore.add_documents(question_docs)retriever.docstore.mset(list(zip(doc_ids, docs))) ``` ``` sub_docs = vectorstore.similarity_search(""justice breyer"") ``` ``` sub_docs ``` ``` [Document(page_content=""What is the President\\\'s stance on immigration reform?"", metadata={\\\'doc_id\\\': \\\'505d73e3-8350-46ec-a58e-3af032f04ab3\\\'}), Document(page_content=""What is the President\\\'s stance on immigration reform? "", metadata={\\\'doc_id\\\': \\\'1c9618f0-7660-4b4f-a37c-509cbbbf6dba\\\'}), Document(page_content=""What is the President\\\'s stance on immigration reform? "", metadata={\\\'doc_id\\\': \\\'82c08209-b904-46a8-9532-edd2380950b7\\\'}), Document(page_content=\\\'What measures is the President proposing to protect the rights of LGBTQ+ Americans? \\\', metadata={\\\'doc_id\\\': \\\'82c08209-b904-46a8-9532-edd2380950b7\\\'})] ``` ``` retrieved_docs = retriever.get_relevant_documents(""justice breyer"") ``` ``` len(retrieved_docs[0].page_content) ``` ``` 9194 ```\', metadata={\'title\': \'MultiVector Retriever | Langchain\', \'type\': None, \'url\': \' \'id\': \'beec5531-16a7-453c-80ab-c5628e0236ce\'}), Document(page_content=\'MultiQueryRetriever | Langchain\\n# MultiQueryRetriever Distance-based vector database retrieval embeds (represents) queries in high-dimensional space and finds similar embedded documents based on ""distance"". But, retrieval may produce different results with subtle changes in query wording or if the embeddings do not capture the semantics of the data well. Prompt engineering / tuning is sometimes done to manually address these problems, but can be tedious. The `MultiQueryRetriever` automates the process of prompt tuning by using an LLM to generate multiple queries from different perspectives for a given user input query. For each query, it retrieves a set of relevant documents and takes the unique union across all queries to get a larger set of potentially relevant documents. By generating multiple perspectives on the same question, the `MultiQueryRetriever` might be able to overcome some of the limitations of the distance-based retrieval and get a richer set of results.By generating multiple perspectives on the same question, the `MultiQueryRetriever` might be able to overcome some of the limitations of the distance-based retrieval and get a richer set of results. ``` # Build a sample vectorDBfrom langchain.vectorstores import Chromafrom langchain.document_loaders import WebBaseLoaderfrom langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.text_splitter import RecursiveCharacterTextSplitter# Load blog postloader = WebBaseLoader("" = loader.load()# Splittext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)splits = text_splitter.split_documents(data)# VectorDBembedding = OpenAIEmbeddings()vectordb = Chroma.from_documents(documents=splits, embedding=embedding) ``` #### Simple usage[\\u200b](#simple-usage) Specify the LLM to use for query generation, and the retriever will do the rest.``` from langchain.chat_models import ChatOpenAIfrom langchain.retrievers.multi_query import MultiQueryRetrieverquestion = ""What are the approaches to Task Decomposition? ""llm = ChatOpenAI(temperature=0)retriever_from_llm = MultiQueryRetriever.from_llm( retriever=vectordb.as_retriever(), llm=llm) ``` ``` # Set logging for the queriesimport logginglogging.basicConfig()logging.getLogger(""langchain.retrievers.multi_query"").setLevel(logging.INFO) ``` ``` unique_docs = retriever_from_llm.get_relevant_documents(query=question)len(unique_docs) ``` ``` INFO:langchain.retrievers.multi_query:Generated queries: [\\\'1. How can Task Decomposition be approached? \\\', \\\'2. What are the different methods for Task Decomposition? \\\', \\\'3. What are the various approaches to decomposing tasks?\\\'] 5 ``` #### Supplying your own prompt[\\u200b](#supplying-your-own-prompt) You can also supply a prompt along with an output parser to split the results into a list of queries.5 ``` #### Supplying your own prompt[\\u200b](#supplying-your-own-prompt) You can also supply a prompt along with an output parser to split the results into a list of queries. ``` from typing import Listfrom langchain.chains import LLMChainfrom pydantic import BaseModel, Fieldfrom langchain.prompts import PromptTemplatefrom langchain.output_parsers import PydanticOutputParser# Output parser will split the LLM result into a list of queriesclass LineList(BaseModel): # ""lines"" is the key (attribute name) of the parsed output lines: List[str] = Field(description=""Lines of text"")class LineListOutputParser(PydanticOutputParser): def __init__(self) -> None: super().__init__(pydantic_object=LineList) def parse(self, text: str) -> LineList: lines = text.strip().split(""\\\\n"") return LineList(lines=lines)output_parser = LineListOutputParser()QUERY_PROMPT = PromptTemplate( input_variables=[""question""], template=""""""You are an AI language model assistant.Your task is to generate five different versions of the given user question to retrieve relevant documents from a vector database. By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based similarity search. Provide these alternative questions separated by newlines. Original question: {question}"""""",)llm = ChatOpenAI(temperature=0)# Chainllm_chain = LLMChain(llm=llm, prompt=QUERY_PROMPT, output_parser=output_parser)# Other inputsquestion = ""What are the approaches to Task Decomposition?"" ``` ``` # Runretriever = MultiQueryRetriever( retriever=vectordb.as_retriever(), llm_chain=llm_chain, parser_key=""lines"") # ""lines"" is the key (attribute name) of the parsed output# Resultsunique_docs = retriever.get_relevant_documents( query=""What does the course say about regression? "")len(unique_docs) ``` ``` INFO:langchain.retrievers.multi_query:Generated queries: [""1."")len(unique_docs) ``` ``` INFO:langchain.retrievers.multi_query:Generated queries: [""1. What is the course\\\'s perspective on regression? "", \\\'2. Can you provide information on regression as discussed in the course? \\\', \\\'3. How does the course cover the topic of regression? \\\', ""4. What are the course\\\'s teachings on regression? "", \\\'5. In relation to the course, what is mentioned about regression?\\\'] 11 ```\', metadata={\'title\': \'MultiQueryRetriever | Langchain\', \'type\': None, \'url\': \' \'id\': \'f7c20633-6a60-4ca3-96b1-13fee66e321d\'}), Document(page_content=\'langchain.retrievers.multi_vector.MultiVectorRetriever LangChain 0.0.322\\n# `langchain.retrievers.multi_vector`.MultiVectorRetriever[](#langchain-retrievers-multi-vector-multivectorretriever) *class *langchain.retrievers.multi_vector.MultiVectorRetriever[[source]](../_modules/langchain/retrievers/multi_vector.html#MultiVectorRetriever)[](#langchain.retrievers.multi_vector.MultiVectorRetriever) # Examples using MultiVectorRetriever[](#langchain-retrievers-multi-vector-multivectorretriever) - [MultiVector Retriever]( metadata={\'title\': \'langchain.retrievers.multi_vector.MultiVectorRetriever LangChain 0.0.322\', \'type\': None, \'url\': \' \'id\': \'1820c44d-7783-4846-a11c-106b18da015d\'})] ``` ## Putting it in a chain Let\'s try using our retrieval systems in a simple chain! ```python from langchain.chat_models import ChatOpenAI from langchain.prompts import ChatPromptTemplate from langchain.schema import StrOutputParser from langchain.schema.runnable import RunnablePassthrough prompt = ChatPromptTemplate.from_messages( [ ( ""system"", """"""You are a great software engineer who is very familiar \\ with Python. Given a user question or request about a new Python library called LangChain and \\ parts of the LangChain documentation, answer the question or generate the requested code. \\ Your answers must be accurate, should include code whenever possible, and should assume anything \\ about LangChain which is note explicitly stated in the LangChain documentation. If the required \\ information is not available, just say so. LangChain Documentation ------------------ {context}"""""", ), (""human"", ""{question}""), ] ) model = ChatOpenAI(model=""gpt-3.5-turbo-16k"") chain = ( { ""question"": RunnablePassthrough(), ""context"": parent_retriever | (lambda docs: ""\\n\\n"".join(d.page_content for d in docs)), } | prompt | model | StrOutputParser() ) ``` ```python for chunk in chain.invoke( ""How do I create a FAISS vector store retriever that returns 10 documents per search query"" ): print(chunk, end="""", flush=True) ``` ```text To create a FAISS vector store retriever that returns 10 documents per search query, you can use the following code: ```python from langchain.embeddings import OpenAIEmbeddings from langchain.vectorstores import FAISS # Assuming you have already loaded and split your documents # into `texts` and initialized your `embeddings` object # Create the FAISS vector store db = FAISS.from_documents(texts, embeddings) # Create the retriever with the desired search kwargs retriever = db.as_retriever(search_kwargs={""k"": 10}) ``` Now, you can use the `retriever` object to get relevant documents using the `get_relevant_documents` method. For example: ```python docs = retriever.get_relevant_documents(""your search query"") ``` This will return a list of 10 documents that are most relevant to the given search query. ``` - [Retriever chunks](#retriever-chunks) - [Other packages](#other-packages) - [Retrieve parent docs](#retrieve-parent-docs) - [Putting it in a chain](#putting-it-in-a-chain)', 'Retrieval-augmented generation (RAG) | Retrieval-augmented generation (RAG) []( ## Overview ### What is RAG? RAG is a technique for augmenting LLM knowledge with additional, often private or real-time, data. LLMs can reason about wide-ranging topics, but their knowledge is limited to the public data up to a specific point in time that they were trained on. If you want to build AI applications that can reason about private data or data introduced after a model\'s cutoff date, you need to augment the knowledge of the model with the specific information it needs. The process of bringing the appropriate information and inserting it into the model prompt is known as Retrieval Augmented Generation (RAG). ### What\'s in this guide? LangChain has a number of components specifically designed to help build RAG applications. To familiarize ourselves with these, we\'ll build a simple question-answering application over a text data source. Specifically, we\'ll build a QA bot over the [LLM Powered Autonomous Agents]( blog post by Lilian Weng. Along the way we\'ll go over a typical QA architecture, discuss the relevant LangChain components, and highlight additional resources for more advanced QA techniques. We\'ll also see how LangSmith can help us trace and understand our application. LangSmith will become increasingly helpful as our application grows in complexity. **Note** Here we focus on RAG for unstructured data. Two RAG use cases which we cover elsewhere are: - [QA over structured data](/docs/use_cases/qa_structured/sql) (e.g., SQL) - [QA over code](/docs/use_cases/question_answering/code_understanding) (e.g., Python) ## Architecture A typical RAG application has two main components: **Indexing**: a pipeline for ingesting data from a source and indexing it. _This usually happen offline._ **Retrieval and generation**: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model. The most common full sequence from raw data to answer looks like: #### Indexing 1. **Load**: First we need to load our data. We\'ll use [DocumentLoaders](/docs/modules/data_connection/document_loaders/) for this. 2. **Split**: [Text splitters](/docs/modules/data_connection/document_transformers/) break large `Documents` into smaller chunks. This is useful both for indexing data and for passing it in to a model, since large chunks are harder to search over and won\'t in a model\'s finite context window. 3. **Store**: We need somewhere to store and index our splits, so that they can later be searched over. This is often done using a [VectorStore](/docs/modules/data_connection/vectorstores/) and [Embeddings](/docs/modules/data_connection/text_embedding/) model. #### Retrieval and generation 1. **Retrieve**: Given a user input, relevant splits are retrieved from storage using a [Retriever](/docs/modules/data_connection/retrievers/). 2. **Generate**: A [ChatModel](/docs/modules/model_io/chat_models) / [LLM](/docs/modules/model_io/llms/) produces an answer using a prompt that includes the question and the retrieved data ![flow.jpeg](/assets/images/qa_flow-9fbd91de9282eb806bda1c6db501ecec.jpeg) ## Setup ### Dependencies We\'ll use an OpenAI chat model and embeddings and a Chroma vector store in this walkthrough, but everything shown here works with any [ChatModel](/docs/integrations/chat/) or [LLM](/docs/integrations/llms/), [Embeddings](/docs/integrations/text_embedding/), and [VectorStore](/docs/integrations/vectorstores/) or [Retriever](/docs/integrations/retrievers). We\'ll use the following packages: ```bash pip install -U langchain openai chromadb langchainhub bs4 ``` We need to set environment variable `OPENAI_API_KEY`, which can be done directly or loaded from a `.env` file like so: ```python import getpass import os os.environ[""OPENAI_API_KEY""] = getpass.getpass() # import dotenv # dotenv.load_dotenv() ``` ### LangSmith Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with [LangSmith]( Note that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces: ```python os.environ[""LANGCHAIN_TRACING_V2""] = ""true"" os.environ[""LANGCHAIN_API_KEY""] = getpass.getpass() ``` ## Quickstart Suppose we want to build a QA app over the [LLM Powered Autonomous Agents]( blog post by Lilian Weng. We can create a simple pipeline for this in ~20 lines of code: ```python import bs4 from langchain import hub from langchain.chat_models import ChatOpenAI from langchain.document_loaders import WebBaseLoader from langchain.embeddings import OpenAIEmbeddings from langchain.schema import StrOutputParser from langchain.schema.runnable import RunnablePassthrough from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain.vectorstores import Chroma ``` ```python loader = WebBaseLoader( web_paths=("" bs_kwargs=dict( parse_only=bs4.SoupStrainer( class_=(""post-content"", ""post-title"", ""post-header"") ) ), ) docs = loader.load() text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200) splits = text_splitter.split_documents(docs) vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings()) retriever = vectorstore.as_retriever() prompt = hub.pull(""rlm/rag-prompt"") llm = ChatOpenAI(model_name=""gpt-3.5-turbo"", temperature=0) def format_docs(docs): return ""\\n\\n"".join(doc.page_content for doc in docs) rag_chain = ( {""context"": retriever | format_docs, ""question"": RunnablePassthrough()} | prompt | llm | StrOutputParser() ) ``` ```python rag_chain.invoke(""What is Task Decomposition?"") ``` ```text \'Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It can be done through prompting techniques like Chain of Thought or Tree of Thoughts, or by using task-specific instructions or human inputs. Task decomposition helps agents plan ahead and manage complicated tasks more effectively.\' ``` ```python # cleanup vectorstore.delete_collection() ``` Check out the [LangSmith trace]( ## Detailed walkthrough Let\'s go through the above code step-by-step to really understand what\'s going on. ## Step 1. Load We need to first load the blog post contents. We can use `DocumentLoader`s for this, which are objects that load in data from a source as `Documents`. A `Document` is an object with `page_content` (str) and `metadata` (dict) attributes. In this case we\'ll use the `WebBaseLoader`, which uses `urllib` and `BeautifulSoup` to load and parse the passed in web urls, returning one `Document` per url. We can customize the html -> text parsing by passing in parameters to the `BeautifulSoup` parser via `bs_kwargs` (see [BeautifulSoup docs]( In this case only HTML tags with class ""post-content"", ""post-title"", or ""post-header"" are relevant, so we\'ll remove all others. ```python from langchain.document_loaders import WebBaseLoader loader = WebBaseLoader( web_paths=("" bs_kwargs={ ""parse_only"": bs4.SoupStrainer( class_=(""post-content"", ""post-title"", ""post-header"") ) }, ) docs = loader.load() ``` ```python len(docs[0].page_content) ``` ```text 42824 ``` ```python print(docs[0].page_content[:500]) ``` ```text LLM Powered Autonomous Agents Date: June 23, 2023 | Estimated Reading Time: 31 min | Author: Lilian Weng Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver. Agent System Overview# In ``` ### Go deeper `DocumentLoader`: Object that load data from a source as `Documents`. - [Docs](/docs/modules/data_connection/document_loaders/): Further documentation on how to use `DocumentLoader`s. - [Integrations](/docs/integrations/document_loaders/): Find the relevant `DocumentLoader` integration (of the > 160 of them) for your use case. ## Step 2. Split Our loaded document is over 42k characters long. This is too long to fit in the context window of many models. And even for those models that could fit the full post in their context window, empirically models struggle to find the relevant context in very long prompts. So we\'ll split the `Document` into chunks for embedding and vector storage. This should help us retrieve only the most relevant bits of the blog post at run time. In this case we\'ll split our documents into chunks of 1000 characters with 200 characters of overlap between chunks. The overlap helps mitigate the possibility of separating a statement from important context related to it. We use the `RecursiveCharacterTextSplitter`, which will (recursively) split the document using common separators (like new lines) until each chunk is the appropriate size. ```python from langchain.text_splitter import RecursiveCharacterTextSplitter text_splitter = RecursiveCharacterTextSplitter( chunk_size=1000, chunk_overlap=200, add_start_index=True ) all_splits = text_splitter.split_documents(docs) ``` ```python len(all_splits) ``` ```text 66 ``` ```python len(all_splits[0].page_content) ``` ```text 969 ``` ```python all_splits[10].metadata ``` ```text {\'source\': \' \'start_index\': 7056} ``` ### Go deeper `DocumentSplitter`: Object that splits a list of `Document`s into smaller chunks. Subclass of `DocumentTransformer`s. - Explore `Context-aware splitters`, which keep the location (""context"") of each split in the original `Document`:- [Markdown files](/docs/use_cases/question_answering/document-context-aware-QA) - [Code (py or js)](/docs/use_cases/question_answering/docs/integrations/document_loaders/source_code) - [Scientific papers](/docs/integrations/document_loaders/grobid) `DocumentTransformer`: Object that performs a transformation on a list of `Document`s. - [Docs](/docs/modules/data_connection/document_transformers/): Further documentation on how to use `DocumentTransformer`s - [Integrations](/docs/integrations/document_transformers/) ## Step 3. Store Now that we\'ve got 66 text chunks in memory, we need to store and index them so that we can search them later in our RAG app. The most common way to do this is to embed the contents of each document split and upload those embeddings to a vector store. Then, when we want to search over our splits, we take the search query, embed it as well, and perform some sort of ""similarity"" search to identify the stored splits with the most similar embeddings to our query embedding. The simplest similarity measure is cosine similarity we measure the cosine of the angle between each pair of embeddings (which are just very high dimensional vectors). We can embed and store all of our document splits in a single command using the `Chroma` vector store and `OpenAIEmbeddings` model. ```python from langchain.embeddings import OpenAIEmbeddings from langchain.vectorstores import Chroma vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings()) ``` ### Go deeper `Embeddings`: Wrapper around a text embedding model, used for converting text to embeddings. - [Docs](/docs/modules/data_connection/text_embedding): Further documentation on the interface. - [Integrations](/docs/integrations/text_embedding/): Browse the > 30 text embedding integrations `VectorStore`: Wrapper around a vector database, used for storing and querying embeddings. - [Docs](/docs/modules/data_connection/vectorstores/): Further documentation on the interface. - [Integrations](/docs/integrations/vectorstores/): Browse the > 40 `VectorStore` integrations. This completes the **Indexing** portion of the pipeline. At this point we have an query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question: ## Step 4. Retrieve Now let\'s write the actual application logic. We want to create a simple application that let\'s the user ask a question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and finally returns an answer. LangChain defines a `Retriever` interface which wraps an index that can return relevant documents given a string query. All retrievers implement a common method `get_relevant_documents()` (and its asynchronous variant `aget_relevant_documents()`). The most common type of `Retriever` is the `VectorStoreRetriever`, which uses the similarity search capabilities of a vector store to facillitate retrieval. Any `VectorStore` can easily be turned into a `Retriever`: ```python retriever = vectorstore.as_retriever(search_type=""similarity"", search_kwargs={""k"": 6}) ``` ```python retrieved_docs = retriever.get_relevant_documents( ""What are the approaches to Task Decomposition?"" ) ``` ```python len(retrieved_docs) ``` ```text 6 ``` ```python print(retrieved_docs[0].page_content) ``` ```text Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote. Task decomposition can be done (1) by LLM with simple prompting like ""Steps for XYZ.\\n1."", ""What are the subgoals for achieving XYZ?"", (2) by using task-specific instructions; e.g. ""Write a story outline."" for writing a novel, or (3) with human inputs. ``` ### Go deeper Vector stores are commonly used for retrieval, but there are plenty of other ways to do retrieval. `Retriever`: An object that returns `Document`s given a text query - [Docs](/docs/modules/data_connection/retrievers/): Further documentation on the interface and built-in retrieval techniques. Some of which include:- `MultiQueryRetriever` [generates variants of the input question](/docs/modules/data_connection/retrievers/MultiQueryRetriever) to improve retrieval hit rate. - `MultiVectorRetriever` (diagram below) instead generates [variants of the embeddings](/docs/modules/data_connection/retrievers/multi_vector), also in order to improve retrieval hit rate. - `Max marginal relevance` selects for [relevance and diversity]( among the retrieved documents to avoid passing in duplicate context. - Documents can be filtered during vector store retrieval using [metadata filters](/docs/use_cases/question_answering/document-context-aware-QA). - [Integrations](/docs/integrations/retrievers/): Integrations with retrieval services. ## Step 5. Generate Let\'s put it all together into a chain that takes a question, retrieves relevant documents, constructs a prompt, passes that to a model, and parses the output. We\'ll use the gpt-3.5-turbo OpenAI chat model, but any LangChain `LLM` or `ChatModel` could be substituted in. ```python from langchain.chat_models import ChatOpenAI llm = ChatOpenAI(model_name=""gpt-3.5-turbo"", temperature=0) ``` We\'ll use a prompt for RAG that is checked into the LangChain prompt hub ([here]( ```python from langchain import hub prompt = hub.pull(""rlm/rag-prompt"") ``` ```python print( prompt.invoke( {""context"": ""filler context"", ""question"": ""filler question""} ).to_string() ) ``` ```text Human: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise. Question: filler question Context: filler context Answer: ``` We\'ll use the [LCEL Runnable]( protocol to define the chain, allowing us to - pipe together components and functions in a transparent way - automatically trace our chain in LangSmith - get streaming, async, and batched calling out of the box ```python from langchain.schema import StrOutputParser from langchain.schema.runnable import RunnablePassthrough def format_docs(docs): return ""\\n\\n"".join(doc.page_content for doc in docs) rag_chain = ( {""context"": retriever | format_docs, ""question"": RunnablePassthrough()} | prompt | llm | StrOutputParser() ) ``` ```python for chunk in rag_chain.stream(""What is Task Decomposition?""): print(chunk, end="""", flush=True) ``` ```text Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It can be done through methods like Chain of Thought (CoT) or Tree of Thoughts, which involve dividing the task into manageable subtasks and exploring multiple reasoning possibilities at each step. Task decomposition can be performed by AI models with prompting, task-specific instructions, or human inputs. ``` Check out the [LangSmith trace]( ### Go deeper #### Choosing LLMs `ChatModel`: An LLM-backed chat model wrapper. Takes in a sequence of messages and returns a message. - [Docs](/docs/modules/model_io/chat/) - [Integrations](/docs/integrations/chat/): Explore over 25 `ChatModel` integrations. `LLM`: A text-in-text-out LLM. Takes in a string and returns a string. - [Docs](/docs/modules/model_io/llms) - [Integrations](/docs/integrations/llms): Explore over 75 `LLM` integrations. See a guide on RAG with locally-running models [here](/docs/modules/use_cases/question_answering/local_retrieval_qa). #### Customizing the prompt As shown above, we can load prompts (e.g., [this RAG prompt]( from the prompt hub. The prompt can also be easily customized: ```python from langchain.prompts import PromptTemplate template = """"""Use the following pieces of context to answer the question at the end. If you don\'t know the answer, just say that you don\'t know, don\'t try to make up an answer. Use three sentences maximum and keep the answer as concise as possible. Always say ""thanks for asking!"" at the end of the answer. {context} Question: {question} Helpful Answer:"""""" rag_prompt_custom = PromptTemplate.from_template(template) rag_chain = ( {""context"": retriever | format_docs, ""question"": RunnablePassthrough()} | rag_prompt_custom | llm | StrOutputParser() ) rag_chain.invoke(""What is Task Decomposition?"") ``` ```text \'Task decomposition is the process of breaking down a complex task into smaller and simpler steps. It can be done through techniques like Chain of Thought (CoT) or Tree of Thoughts, which involve dividing the problem into multiple thought steps and generating multiple thoughts per step. Task decomposition helps in enhancing model performance and understanding the thinking process of the model. Thanks for asking!\' ``` Check out the [LangSmith trace]( ### Adding sources With LCEL it\'s easy to return the retrieved documents or certain source metadata from the documents: ```python from operator import itemgetter from langchain.schema.runnable import RunnableMap rag_chain_from_docs = ( { ""context"": lambda input: format_docs(input[""documents""]), ""question"": itemgetter(""question""), } | rag_prompt_custom | llm | StrOutputParser() ) rag_chain_with_source = RunnableMap( {""documents"": retriever, ""question"": RunnablePassthrough()} ) | { ""documents"": lambda input: [doc.metadata for doc in input[""documents""]], ""answer"": rag_chain_from_docs, } rag_chain_with_source.invoke(""What is Task Decomposition"") ``` ```text {\'documents\': [{\'source\': \' \'start_index\': 1585}, {\'source\': \' \'start_index\': 2192}, {\'source\': \' \'start_index\': 17804}, {\'source\': \' \'start_index\': 17414}, {\'source\': \' \'start_index\': 29630}, {\'source\': \' \'start_index\': 19373}], \'answer\': \'Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It involves transforming big tasks into multiple manageable tasks, allowing for a more systematic and organized approach to problem-solving. Thanks for asking!\'} ``` Check out the [LangSmith trace]( Adding memory Suppose we want to create a stateful application that remembers past user inputs. There are two main things we need to do to support this. 1. Add a messages placeholder to our chain which allows us to pass in historical messages 2. Add a chain that takes the latest user query and reformulates it in the context of the chat history into a standalone question that can be passed to our retriever. Let\'s start with 2. We can build a ""condense question"" chain that looks something like this: ```python from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder condense_q_system_prompt = """"""Given a chat history and the latest user question \\ which might reference the chat history, formulate a standalone question \\ which can be understood without the chat history. Do NOT answer the question, \\ just reformulate it if needed and otherwise return it as is."""""" condense_q_prompt = ChatPromptTemplate.from_messages( [ (""system"", condense_q_system_prompt), MessagesPlaceholder(variable_name=""chat_history""), (""human"", ""{question}""), ] ) condense_q_chain = condense_q_prompt | llm | StrOutputParser() ``` ```python from langchain.schema.messages import AIMessage, HumanMessage condense_q_chain.invoke( { ""chat_history"": [ HumanMessage(content=""What does LLM stand for?""), AIMessage(content=""Large language model""), ], ""question"": ""What is meant by large"", } ) ``` ```text \'What is the definition of ""large"" in the context of a language model?\' ``` ```python condense_q_chain.invoke( { ""chat_history"": [ HumanMessage(content=""What does LLM stand for?""), AIMessage(content=""Large language model""), ], ""question"": ""How do transformers work"", } ) ``` ```text \'How do transformer models function?\' ``` And now we can build our full QA chain. Notice we add some routing functionality to only run the ""condense question chain"" when our chat history isn\'t empty. ```python qa_system_prompt = """"""You are an assistant for question-answering tasks. \\ Use the following pieces of retrieved context to answer the question. \\ If you don\'t know the answer, just say that you don\'t know. \\ Use three sentences maximum and keep the answer concise.\\ {context}"""""" qa_prompt = ChatPromptTemplate.from_messages( [ (""system"", qa_system_prompt), MessagesPlaceholder(variable_name=""chat_history""), (""human"", ""{question}""), ] ) def condense_question(input: dict): if input.get(""chat_history""): return condense_q_chain else: return input[""question""] rag_chain = ( RunnablePassthrough.assign(context=condense_question | retriever | format_docs) | qa_prompt | llm ) ``` ```python chat_history = [] question = ""What is Task Decomposition?"" ai_msg = rag_chain.invoke({""question"": question, ""chat_history"": chat_history}) chat_history.extend([HumanMessage(content=question), ai_msg]) second_question = ""What are common ways of doing it?"" rag_chain.invoke({""question"": second_question, ""chat_history"": chat_history}) ``` ```text AIMessage(content=\'Common ways of task decomposition include:\\n\\n1. Using Chain of Thought (CoT): CoT is a prompting technique that instructs the model to ""think step by step"" and decompose complex tasks into smaller and simpler steps. It utilizes more test-time computation and sheds light on the model\\\'s thinking process.\\n\\n2. Prompting with LLM: Language Model (LLM) can be used to prompt the model with simple instructions like ""Steps for XYZ"" or ""What are the subgoals for achieving XYZ?"" This allows the model to generate a sequence of subtasks or thought steps.\\n\\n3. Task-specific instructions: For certain tasks, task-specific instructions can be provided to guide the model in decomposing the task. For example, for writing a novel, the instruction ""Write a story outline"" can be given to break down the task into manageable steps.\\n\\n4. Human inputs: In some cases, human inputs can be used to assist in task decomposition. Humans can provide their expertise and knowledge to identify and break down complex tasks into smaller subtasks.\') ``` Check out the [LangSmith trace]( Here we\'ve gone over how to add chain logic for incorporating historical outputs. But how do we actually store and retrieve historical outputs for different sessions? For that check out the LCEL [How to add message history (memory)](/docs/expression_language/how_to/message_history) page. ## Next steps That\'s a lot of content we\'ve covered in a short amount of time. There\'s plenty of nuances, features, integrations, etc to explore in each of the above sections. Aside from the sources mentioned above, good next steps include: - Reading up on more advanced retrieval techniques in the [Retrievers](/docs/modules/data_connection/retrievers/) section. - Learning about the LangChain [Indexing API](/docs/modules/data_connection/indexing), which helps repeatedly sync data sources and vector stores without redundant computation or storage. - Exploring RAG [LangChain Templates](/docs/templates/#-advanced-retrieval), which are reference applications that can easily be deployed with [LangServe](/docs/langserve). - Learning about [evaluating RAG applications with LangSmith]( - [Overview](#overview)- [What is RAG?](#what-is-rag) - [What\'s in this guide?](#whats-in-this-guide) - [Architecture](#architecture) - [Setup](#setup)- [Dependencies](#dependencies) - [LangSmith](#langsmith) - [Quickstart](#quickstart) - [Detailed walkthrough](#detailed-walkthrough) - [Step 1. Load](#step-1-load)- [Go deeper](#go-deeper) - [Step 2. Split](#step-2-split)- [Go deeper](#go-deeper-1) - [Step 3. Store](#step-3-store)- [Go deeper](#go-deeper-2) - [Step 4. Retrieve](#step-4-retrieve)- [Go deeper](#go-deeper-3) - [Step 5. Generate](#step-5-generate)- [Go deeper](#go-deeper-4) - [Adding sources](#adding-sources) - [Adding memory](#adding-memory) - [Next steps](#next-steps)']","The main purpose of the time-weighted vector store retriever is to combine semantic similarity with a time decay factor, ensuring that frequently accessed objects remain ""fresh"" while less frequently accessed objects gradually lose relevance over time.",The main purpose of the time-weighted vector store retriever is to use a combination of semantic similarity and a time decay to retrieve relevant vectors.,0.9999999999666667,1.0,1.0,0.2996353753310469,0.6031746031746031
32,What serialization format is used to serialize chains to and from disk?,"[""langchain.smith.evaluation.string_run_evaluator.LLMStringRunMapper LangChain 0.0.337 API Experimental Python Docs Toggle Menu PrevUp Next LangChain 0.0.337 langchain.smith.evaluation.string_run_evaluator.LLMStringRunMapper langchain.smith.evaluation.string_run_evaluator.LLMStringRunMapper class langchain.smith.evaluation.string_run_evaluator.LLMStringRunMapper[source] Bases: StringRunMapper Extract items to evaluate from the run object. Create a new model by parsing and validating input data from keyword arguments. Raises ValidationError if the input data cannot be parsed to form a valid model. __call__(run: Run) Dict[str, str] Maps the Run to a dictionary. classmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) Model Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values copy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters include fields to include in new model exclude fields to exclude from new model, as with values this takes precedence over include update values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data deep set to True to make a deep copy of the model Returns new model instance dict(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False) DictStrAny Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. classmethod from_orm(obj: Any) Model classmethod get_lc_namespace() List[str] Get the namespace of the langchain object. For example, if the class is langchain.llms.openai.OpenAI, then the namespace is [langchain, llms, openai] classmethod is_lc_serializable() bool Is this class serializable? json(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) unicode Generate a JSON representation of the model, include and exclude arguments as per dict(). encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps(). classmethod lc_id() List[str] A unique identifier for this class for serialization purposes. The unique identifier is a list of strings that describes the path to the object. map(run: Run) Dict[str, str][source] Maps the Run to a dictionary. classmethod parse_file(path: Union[str, Path], *, content_type: unicode = None, encoding: unicode = 'utf8', proto: Protocol = None, allow_pickle: bool = False)  Model classmethod parse_obj(obj: Any)  Model classmethod parse_raw(b: Union[str, bytes], *, content_type: unicode = None, encoding: unicode = 'utf8', proto: Protocol = None, allow_pickle: bool = False)  Model classmethod schema(by_alias: bool = True, ref_template: unicode = '#/definitions/{model}')  DictStrAny classmethod schema_json(*, by_alias: bool = True, ref_template: unicode = '#/definitions/{model}', **dumps_kwargs: Any)  unicode serialize_chat_messages(messages: List[Dict])  str[source] Extract the input messages from the run. serialize_inputs(inputs: Dict)  str[source] serialize_outputs(outputs: Dict)  str[source] to_json()  Union[SerializedConstructor, SerializedNotImplemented] to_json_not_implemented()  SerializedNotImplemented classmethod update_forward_refs(**localns: Any)  None Try to update ForwardRefs on fields based on this Model, globalns and localns. classmethod validate(value: Any)  Model property lc_attributes: Dict List of attribute names that should be included in the serialized kwargs. These attributes must be accepted by the constructor. property lc_secrets: Dict[str, str] A map of constructor argument names to secret ids. For example,{openai_api_key: OPENAI_API_KEY} property output_keys: List[str] The keys to extract from the run.  2023, Harrison Chase. Last updated on Nov 17, 2023. Show this page source"", 'Router | Router Routing allows you to create non-deterministic chains where the output of a previous step defines the next step. Routing helps provide structure and consistency around interactions with LLMs. As a very simple example, let\'s suppose we have two templates optimized for different types of questions, and we want to choose the template based on the user input. ```python from langchain.prompts import PromptTemplate physics_template = """"""You are a very smart physics professor. \\ You are great at answering questions about physics in a concise and easy to understand manner. \\ When you don\'t know the answer to a question you admit that you don\'t know. Here is a question: {input}"""""" physics_prompt = PromptTemplate.from_template(physics_template) math_template = """"""You are a very good mathematician. You are great at answering math questions. \\ You are so good because you are able to break down hard problems into their component parts, \\ answer the component parts, and then put them together to answer the broader question. Here is a question: {input}"""""" math_prompt = PromptTemplate.from_template(math_template) ``` ## Using LCEL We can easily do this using a `RunnableBranch`. A `RunnableBranch` is initialized with a list of (condition, runnable) pairs and a default runnable. It selects which branch by passing each condition the input it\'s invoked with. It selects the first condition to evaluate to True, and runs the corresponding runnable to that condition with the input. If no provided conditions match, it runs the default runnable. ```python from langchain.chat_models import ChatOpenAI from langchain.schema.output_parser import StrOutputParser from langchain.schema.runnable import RunnableBranch ``` ```python general_prompt = PromptTemplate.from_template( ""You are a helpful assistant. Answer the question as accurately as you can.\\n\\n{input}"" ) prompt_branch = RunnableBranch( (lambda x: x[""topic""] == ""math"", math_prompt), (lambda x: x[""topic""] == ""physics"", physics_prompt), general_prompt, ) ``` ```python from typing import Literal from langchain.output_parsers.openai_functions import PydanticAttrOutputFunctionsParser from langchain.pydantic_v1 import BaseModel from langchain.utils.openai_functions import convert_pydantic_to_openai_function class TopicClassifier(BaseModel): ""Classify the topic of the user question"" topic: Literal[""math"", ""physics"", ""general""] ""The topic of the user question. One of \'math\', \'physics\' or \'general\'."" classifier_function = convert_pydantic_to_openai_function(TopicClassifier) llm = ChatOpenAI().bind( functions=[classifier_function], function_call={""name"": ""TopicClassifier""} ) parser = PydanticAttrOutputFunctionsParser( pydantic_schema=TopicClassifier, attr_name=""topic"" ) classifier_chain = llm | parser ``` ```python from operator import itemgetter from langchain.schema.output_parser import StrOutputParser from langchain.schema.runnable import RunnablePassthrough final_chain = ( RunnablePassthrough.assign(topic=itemgetter(""input"") | classifier_chain) | prompt_branch | ChatOpenAI() | StrOutputParser() ) ``` ```python final_chain.invoke( { ""input"": ""What is the first prime number greater than 40 such that one plus the prime number is divisible by 3?"" } ) ``` ```text ""Thank you for your kind words! I\'ll be happy to help you with this math question.\\n\\nTo find the first prime number greater than 40 that satisfies the given condition, we need to follow a step-by-step approach. \\n\\nFirstly, let\'s list the prime numbers greater than 40:\\n41, 43, 47, 53, 59, 61, 67, 71, ...\\n\\nNow, we need to check if one plus each of these prime numbers is divisible by 3. We can do this by calculating the remainder when dividing each number by 3.\\n\\nFor 41, (41 + 1) % 3 = 42 % 3 = 0. It is divisible by 3.\\n\\nFor 43, (43 + 1) % 3 = 44 % 3 = 2. It is not divisible by 3.\\n\\nFor 47, (47 + 1) % 3 = 48 % 3 = 0. It is divisible by 3.\\n\\nSince 41 and 47 are both greater than 40 and satisfy the condition, the first prime number greater than 40 such that one plus the prime number is divisible by 3 is 41.\\n\\nTherefore, the answer to the question is 41."" ``` For more on routing with LCEL [head here](/docs/expression_language/how_to/routing). ## [Legacy] RouterChain The preferred approach as of version `0.0.293` is to use LCEL as above.Here we show how to use the `RouterChain` paradigm to create a chain that dynamically selects the next chain to use for a given input. Router chains are made up of two components: - The `RouterChain` itself (responsible for selecting the next chain to call) - `destination_chains`: chains that the router chain can route to In this example, we will focus on the different types of routing chains. We will show these routing chains used in a `MultiPromptChain` to create a question-answering chain that selects the prompt which is most relevant for a given question, and then answers the question using that prompt. ```python from langchain.chains import ConversationChain from langchain.chains.llm import LLMChain from langchain.chains.router import MultiPromptChain from langchain.llms import OpenAI ``` ### [Legacy] LLMRouterChain This chain uses an LLM to determine how to route things. ```python prompt_infos = [ { ""name"": ""physics"", ""description"": ""Good for answering questions about physics"", ""prompt_template"": physics_template, }, { ""name"": ""math"", ""description"": ""Good for answering math questions"", ""prompt_template"": math_template, }, ] ``` ```python llm = OpenAI() ``` ```python destination_chains = {} for p_info in prompt_infos: name = p_info[""name""] prompt_template = p_info[""prompt_template""] prompt = PromptTemplate(template=prompt_template, input_variables=[""input""]) chain = LLMChain(llm=llm, prompt=prompt) destination_chains[name] = chain default_chain = ConversationChain(llm=llm, output_key=""text"") ``` ```python from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE ``` ```python destinations = [f""{p[\'name\']}: {p[\'description\']}"" for p in prompt_infos] destinations_str = ""\\n"".join(destinations) router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations_str) router_prompt = PromptTemplate( template=router_template, input_variables=[""input""], output_parser=RouterOutputParser(), ) router_chain = LLMRouterChain.from_llm(llm, router_prompt) ``` ```python chain = MultiPromptChain( router_chain=router_chain, destination_chains=destination_chains, default_chain=default_chain, verbose=True, ) ``` ```python print(chain.run(""What is black body radiation?"")) ``` ```text > Entering new MultiPromptChain chain... /Users/bagatur/langchain/libs/langchain/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain. warnings.warn( physics: {\'input\': \'What is black body radiation?\'} > Finished chain. Black body radiation is the thermal electromagnetic radiation within or surrounding a body in thermodynamic equilibrium with its environment, or emitted by a black body (an idealized physical body which absorbs all incident electromagnetic radiation). It is a characteristic of the temperature of the body; if the body has a uniform temperature, the radiation is also uniform across the spectrum of frequencies. The spectral characteristics of the radiation are determined by the temperature of the body, which implies that a black body at a given temperature will emit the same amount of radiation at every frequency. ``` ```python print( chain.run( ""What is the first prime number greater than 40 such that one plus the prime number is divisible by 3?"" ) ) ``` ```text > Entering new MultiPromptChain chain... /Users/bagatur/langchain/libs/langchain/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain. warnings.warn( math: {\'input\': \'What is the first prime number greater than 40 such that one plus the prime number is divisible by 3?\'} > Finished chain. The first prime number greater than 40 such that one plus the prime number is divisible by 3 is 43. This can be seen by breaking down the problem: 1) We know that a prime number is a number that is only divisible by itself and one. 2) We also know that if a number is divisible by 3, the sum of its digits must be divisible by 3. So, if we want to find the first prime number greater than 40 such that one plus the prime number is divisible by 3, we can start counting up from 40, testing each number to see if it is prime and if the sum of the number and one is divisible by three. The first number we come to that satisfies these conditions is 43. ``` ```python print(chain.run(""What is the name of the type of cloud that rains?"")) ``` ```text > Entering new MultiPromptChain chain... /Users/bagatur/langchain/libs/langchain/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain. warnings.warn( physics: {\'input\': \'What is the name of the type of cloud that rains?\'} > Finished chain. The type of cloud that rains is called a cumulonimbus cloud. ``` ## [Legacy] EmbeddingRouterChain The `EmbeddingRouterChain` uses embeddings and similarity to route between destination chains. ```python from langchain.chains.router.embedding_router import EmbeddingRouterChain from langchain.embeddings import CohereEmbeddings from langchain.vectorstores import Chroma ``` ```python names_and_descriptions = [ (""physics"", [""for questions about physics""]), (""math"", [""for questions about math""]), ] ``` ```python router_chain = EmbeddingRouterChain.from_names_and_descriptions( names_and_descriptions, Chroma, CohereEmbeddings(), routing_keys=[""input""] ) ``` ```python chain = MultiPromptChain( router_chain=router_chain, destination_chains=destination_chains, default_chain=default_chain, verbose=True, ) ``` ```python print(chain.run(""What is black body radiation?"")) ``` ```text > Entering new MultiPromptChain chain... physics: {\'input\': \'What is black body radiation?\'} > Finished chain. Black body radiation is the electromagnetic radiation emitted by a black body, which is an idealized physical body that absorbs all incident electromagnetic radiation. This radiation is related to the temperature of the body, with higher temperatures leading to higher radiation levels. The spectrum of the radiation is continuous, and is described by the Planck\'s law of black body radiation. ``` ```python print( chain.run( ""What is the first prime number greater than 40 such that one plus the prime number is divisible by 3?"" ) ) ``` ```text > Entering new MultiPromptChain chain... math: {\'input\': \'What is the first prime number greater than 40 such that one plus the prime number is divisible by 3?\'} > Finished chain. The first prime number greater than 40 such that one plus the prime number is divisible by 3 is 43. This is because 43 is a prime number, and 1 + 43 = 44, which is divisible by 3. ``` - [Using LCEL](#using-lcel) - [Legacy RouterChain](#legacy-routerchain)', 'LangSmith Walkthrough | LangSmith Walkthrough []( LangChain makes it easy to prototype LLM applications and Agents. However, delivering LLM applications to production can be deceptively difficult. You will likely have to heavily customize and iterate on your prompts, chains, and other components to create a high-quality product. To aid in this process, we\'ve launched LangSmith, a unified platform for debugging, testing, and monitoring your LLM applications. When might this come in handy? You may find it useful when you want to: - Quickly debug a new chain, agent, or set of tools - Visualize how components (chains, llms, retrievers, etc.) relate and are used - Evaluate different prompts and LLMs for a single component - Run a given chain several times over a dataset to ensure it consistently meets a quality bar - Capture usage traces and using LLMs or analytics pipelines to generate insights ## Prerequisites **Create a LangSmith account and create an API key (see bottom left corner). Familiarize yourself with the platform by looking through the docs** Note LangSmith is in closed beta; we\'re in the process of rolling it out to more users. However, you can fill out the form on the website for expedited access. Now, let\'s get started! ## Log runs to LangSmith First, configure your environment variables to tell LangChain to log traces. This is done by setting the `LANGCHAIN_TRACING_V2` environment variable to true. You can tell LangChain which project to log to by setting the `LANGCHAIN_PROJECT` environment variable (if this isn\'t set, runs will be logged to the `default` project). This will automatically create the project for you if it doesn\'t exist. You must also set the `LANGCHAIN_ENDPOINT` and `LANGCHAIN_API_KEY` environment variables. For more information on other ways to set up tracing, please reference the [LangSmith documentation]( **NOTE:** You must also set your `OPENAI_API_KEY` environment variables in order to run the following tutorial. **NOTE:** You can only access an API key when you first create it. Keep it somewhere safe. **NOTE:** You can also use a context manager in python to log traces using ```python from langchain.callbacks.manager import tracing_v2_enabled with tracing_v2_enabled(project_name=""My Project""): agent.run(""How many people live in canada as of 2023?"") ``` However, in this example, we will use environment variables. ```python %pip install openai tiktoken pandas duckduckgo-search --quiet ``` ```python import os from uuid import uuid4 unique_id = uuid4().hex[0:8] os.environ[""LANGCHAIN_TRACING_V2""] = ""true"" os.environ[""LANGCHAIN_PROJECT""] = f""Tracing Walkthrough - {unique_id}"" os.environ[""LANGCHAIN_ENDPOINT""] = "" os.environ[""LANGCHAIN_API_KEY""] = """" # Update to your API key # Used by the agent in this tutorial os.environ[""OPENAI_API_KEY""] = """" ``` Create the langsmith client to interact with the API ```python from langsmith import Client client = Client() ``` Create a LangChain component and log runs to the platform. In this example, we will create a ReAct-style agent with access to a general search tool (DuckDuckGo). The agent\'s prompt can be viewed in the [Hub here]( ```python from langchain import hub from langchain.agents import AgentExecutor from langchain.agents.format_scratchpad import format_to_openai_function_messages from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser from langchain.chat_models import ChatOpenAI from langchain.tools import DuckDuckGoSearchResults from langchain.tools.render import format_tool_to_openai_function # Fetches the latest version of this prompt prompt = hub.pull(""wfh/langsmith-agent-prompt:latest"") llm = ChatOpenAI( model=""gpt-3.5-turbo-16k"", temperature=0, ) tools = [ DuckDuckGoSearchResults( name=""duck_duck_go"" ), # General internet search using DuckDuckGo ] llm_with_tools = llm.bind(functions=[format_tool_to_openai_function(t) for t in tools]) runnable_agent = ( { ""input"": lambda x: x[""input""], ""agent_scratchpad"": lambda x: format_to_openai_function_messages( x[""intermediate_steps""] ), } | prompt | llm_with_tools | OpenAIFunctionsAgentOutputParser() ) agent_executor = AgentExecutor( agent=runnable_agent, tools=tools, handle_parsing_errors=True ) ``` We are running the agent concurrently on multiple inputs to reduce latency. Runs get logged to LangSmith in the background so execution latency is unaffected. ```python inputs = [ ""What is LangChain?"", ""What\'s LangSmith?"", ""When was Llama-v2 released?"", ""What is the langsmith cookbook?"", ""When did langchain first announce the hub?"", ] results = agent_executor.batch([{""input"": x} for x in inputs], return_exceptions=True) ``` ```python results[:2] ``` ```text [{\'input\': \'What is LangChain?\', \'output\': \'I\\\'m sorry, but I couldn\\\'t find any information about ""LangChain"". Could you please provide more context or clarify your question?\'}, {\'input\': ""What\'s LangSmith?"", \'output\': \'I\\\'m sorry, but I couldn\\\'t find any information about ""LangSmith"". It could be a specific term or a company that is not widely known. Can you provide more context or clarify what you are referring to?\'}] ``` Assuming you\'ve successfully set up your environment, your agent traces should show up in the `Projects` section in the [app]( Congrats! ![Initial Runs](/assets/images/log_traces-edd14f0c4d5c320263362395793babdc.png) It looks like the agent isn\'t effectively using the tools though. Let\'s evaluate this so we have a baseline. ## Evaluate Agent In addition to logging runs, LangSmith also allows you to test and evaluate your LLM applications. In this section, you will leverage LangSmith to create a benchmark dataset and run AI-assisted evaluators on an agent. You will do so in a few steps: 1. Create a dataset 2. Initialize a new agent to benchmark 3. Configure evaluators to grade an agent\'s output 4. Run the agent over the dataset and evaluate the results ### 1. Create a LangSmith dataset Below, we use the LangSmith client to create a dataset from the input questions from above and a list labels. You will use these later to measure performance for a new agent. A dataset is a collection of examples, which are nothing more than input-output pairs you can use as test cases to your application. For more information on datasets, including how to create them from CSVs or other files or how to create them in the platform, please refer to the [LangSmith documentation]( ```python outputs = [ ""LangChain is an open-source framework for building applications using large language models. It is also the name of the company building LangSmith."", ""LangSmith is a unified platform for debugging, testing, and monitoring language model applications and agents powered by LangChain"", ""July 18, 2023"", ""The langsmith cookbook is a github repository containing detailed examples of how to use LangSmith to debug, evaluate, and monitor large language model-powered applications."", ""September 5, 2023"", ] ``` ```python dataset_name = f""agent-qa-{unique_id}"" dataset = client.create_dataset( dataset_name, description=""An example dataset of questions over the LangSmith documentation."", ) for query, answer in zip(inputs, outputs): client.create_example( inputs={""input"": query}, outputs={""output"": answer}, dataset_id=dataset.id ) ``` ### 2. Initialize a new agent to benchmark LangSmith lets you evaluate any LLM, chain, agent, or even a custom function. Conversational agents are stateful (they have memory); to ensure that this state isn\'t shared between dataset runs, we will pass in a `chain_factory` (aka a `constructor`) function to initialize for each call. In this case, we will test an agent that uses OpenAI\'s function calling endpoints. ```python from langchain import hub from langchain.agents import AgentExecutor, AgentType, initialize_agent, load_tools from langchain.agents.format_scratchpad import format_to_openai_function_messages from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser from langchain.chat_models import ChatOpenAI from langchain.tools.render import format_tool_to_openai_function # Since chains can be stateful (e.g. they can have memory), we provide # a way to initialize a new chain for each row in the dataset. This is done # by passing in a factory function that returns a new chain for each row. def agent_factory(prompt): llm_with_tools = llm.bind( functions=[format_tool_to_openai_function(t) for t in tools] ) runnable_agent = ( { ""input"": lambda x: x[""input""], ""agent_scratchpad"": lambda x: format_to_openai_function_messages( x[""intermediate_steps""] ), } | prompt | llm_with_tools | OpenAIFunctionsAgentOutputParser() ) return AgentExecutor(agent=runnable_agent, tools=tools, handle_parsing_errors=True) ``` ### 3. Configure evaluation Manually comparing the results of chains in the UI is effective, but it can be time consuming. It can be helpful to use automated metrics and AI-assisted feedback to evaluate your component\'s performance. Below, we will create some pre-implemented run evaluators that do the following: - Compare results against ground truth labels. - Measure semantic (dis)similarity using embedding distance - Evaluate \'aspects\' of the agent\'s response in a reference-free manner using custom criteria For a longer discussion of how to select an appropriate evaluator for your use case and how to create your own custom evaluators, please refer to the [LangSmith documentation]( ```python from langchain.evaluation import EvaluatorType from langchain.smith import RunEvalConfig evaluation_config = RunEvalConfig( # Evaluators can either be an evaluator type (e.g., ""qa"", ""criteria"", ""embedding_distance"", etc.) or a configuration for that evaluator evaluators=[ # Measures whether a QA response is ""Correct"", based on a reference answer # You can also select via the raw string ""qa"" EvaluatorType.QA, # Measure the embedding distance between the output and the reference answer # Equivalent to: EvalConfig.EmbeddingDistance(embeddings=OpenAIEmbeddings()) EvaluatorType.EMBEDDING_DISTANCE, # Grade whether the output satisfies the stated criteria. # You can select a default one such as ""helpfulness"" or provide your own. RunEvalConfig.LabeledCriteria(""helpfulness""), # The LabeledScoreString evaluator outputs a score on a scale from 1-10. # You can use default criteria or write our own rubric RunEvalConfig.LabeledScoreString( { ""accuracy"": """""" Score 1: The answer is completely unrelated to the reference. Score 3: The answer has minor relevance but does not align with the reference. Score 5: The answer has moderate relevance but contains inaccuracies. Score 7: The answer aligns with the reference but has minor errors or omissions. Score 10: The answer is completely accurate and aligns perfectly with the reference."""""" }, normalize_by=10, ), ], # You can add custom StringEvaluator or RunEvaluator objects here as well, which will automatically be # applied to each prediction. Check out the docs for examples. custom_evaluators=[], ) ``` ### 4. Run the agent and evaluators Use the [run_on_dataset]( (or asynchronous [arun_on_dataset]( function to evaluate your model. This will: 1. Fetch example rows from the specified dataset. 2. Run your agent (or any custom function) on each example. 3. Apply evaluators to the resulting run traces and corresponding reference examples to generate automated feedback. The results will be visible in the LangSmith app. ```python from langchain import hub # We will test this version of the prompt prompt = hub.pull(""wfh/langsmith-agent-prompt:798e7324"") ``` ```python import functools from langchain.smith import ( arun_on_dataset, run_on_dataset, ) chain_results = run_on_dataset( dataset_name=dataset_name, llm_or_chain_factory=functools.partial(agent_factory, prompt=prompt), evaluation=evaluation_config, verbose=True, client=client, project_name=f""runnable-agent-test-5d466cbc-{unique_id}"", tags=[ ""testing-notebook"", ""prompt:5d466cbc"", ], # Optional, adds a tag to the resulting chain runs ) # Sometimes, the agent will error due to parsing issues, incompatible tool inputs, etc. # These are logged as warnings here and captured as errors in the tracing UI. ``` ```text View the evaluation results for project \'runnable-agent-test-5d466cbc-bf2162aa\' at: [> ] 0/5 Chain failed for example 54b4fce8-4492-409d-94af-708f51698b39 with inputs {\'input\': \'Who trained Llama-v2?\'} Error Type: TypeError, Message: DuckDuckGoSearchResults._run() got an unexpected keyword argument \'arg1\' [------------------------------------------------->] 5/5 Eval quantiles: 0.25 0.5 0.75 mean mode embedding_cosine_distance 0.086614 0.118841 0.183672 0.151444 0.050158 correctness 0.000000 0.500000 1.000000 0.500000 0.000000 score_string:accuracy 0.775000 1.000000 1.000000 0.775000 1.000000 helpfulness 0.750000 1.000000 1.000000 0.750000 1.000000 ``` ### Review the test results You can review the test results tracing UI below by clicking the URL in the output above or navigating to the ""Testing & Datasets"" page in LangSmith **""agent-qa-{unique_id}""** dataset. ![test results](/assets/images/test_results-15649f0f4500fd64ef2209229951a6c1.png) This will show the new runs and the feedback logged from the selected evaluators. You can also explore a summary of the results in tabular format below. ```python chain_results.to_dataframe() ``` ```html .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } embedding_cosine_distance correctness score_string:accuracy helpfulness input output reference 42b639a2-17c4-4031-88a9-0ce2c45781ce 0.317938 0.0 1.0 1.0 {\'input\': \'What is the langsmith cookbook?\'} {\'input\': \'What is the langsmith cookbook?\', \'... {\'output\': \'September 5, 2023\'} 54b4fce8-4492-409d-94af-708f51698b39 NaN NaN NaN NaN {\'input\': \'Who trained Llama-v2?\'} {\'Error\': \'TypeError(""DuckDuckGoSearchResults.... {\'output\': \'The langsmith cookbook is a github... 8ae5104e-bbb4-42cc-a84e-f9b8cfc92b8e 0.138916 1.0 1.0 1.0 {\'input\': \'When was Llama-v2 released?\'} {\'input\': \'When was Llama-v2 released?\', \'outp... {\'output\': \'July 18, 2023\'} 678c0363-3ed1-410a-811f-ebadef2e783a 0.050158 1.0 1.0 1.0 {\'input\': \'What\'s LangSmith?\'} {\'input\': \'What\'s LangSmith?\', \'output\': \'Lang... {\'output\': \'LangSmith is a unified platform fo... 762a616c-7aab-419c-9001-b43ab6200d26 0.098766 0.0 0.1 0.0 {\'input\': \'What is LangChain?\'} {\'input\': \'What is LangChain?\', \'output\': \'Lan... {\'output\': \'LangChain is an open-source framew... ``` ### (Optional) Compare to another prompt Now that we have our test run results, we can make changes to our agent and benchmark them. Let\'s try this again with a different prompt and see the results. ```python candidate_prompt = hub.pull(""wfh/langsmith-agent-prompt:39f3bbd0"") chain_results = run_on_dataset( dataset_name=dataset_name, llm_or_chain_factory=functools.partial(agent_factory, prompt=candidate_prompt), evaluation=evaluation_config, verbose=True, client=client, project_name=f""runnable-agent-test-39f3bbd0-{unique_id}"", tags=[ ""testing-notebook"", ""prompt:39f3bbd0"", ], # Optional, adds a tag to the resulting chain runs ) ``` ```text View the evaluation results for project \'runnable-agent-test-39f3bbd0-bf2162aa\' at: [------------------------------------------------->] 5/5 Eval quantiles: 0.25 0.5 0.75 mean mode embedding_cosine_distance 0.059506 0.155538 0.212864 0.157915 0.043119 correctness 0.000000 0.000000 1.000000 0.400000 0.000000 score_string:accuracy 0.700000 1.000000 1.000000 0.880000 1.000000 helpfulness 1.000000 1.000000 1.000000 0.800000 1.000000 ``` ## Exporting datasets and runs LangSmith lets you export data to common formats such as CSV or JSONL directly in the web app. You can also use the client to fetch runs for further analysis, to store in your own database, or to share with others. Let\'s fetch the run traces from the evaluation run. **Note: It may be a few moments before all the runs are accessible.** ```python runs = client.list_runs(project_name=chain_results[""project_name""], execution_order=1) ``` ```python # After some time, these will be populated. client.read_project(project_name=chain_results[""project_name""]).feedback_stats ``` ## Conclusion Congratulations! You have successfully traced and evaluated an agent using LangSmith! This was a quick guide to get started, but there are many more ways to use LangSmith to speed up your developer flow and produce better results. For more information on how you can get the most out of LangSmith, check out [LangSmith documentation]( and please reach out with questions, feature requests, or feedback at [support@langchain.dev](mailto:support@langchain.dev). - [Prerequisites](#prerequisites) - [Log runs to LangSmith](#log-runs-to-langsmith) - [Evaluate Agent](#evaluate-agent)- [1. Create a LangSmith dataset](#1-create-a-langsmith-dataset) - [2. Initialize a new agent to benchmark](#2-initialize-a-new-agent-to-benchmark) - [3. Configure evaluation](#3-configure-evaluation) - [4. Run the agent and evaluators](#4-run-the-agent-and-evaluators) - [Review the test results](#review-the-test-results) - [(Optional) Compare to another prompt](#optional-compare-to-another-prompt) - [Exporting datasets and runs](#exporting-datasets-and-runs) - [Conclusion](#conclusion)']",LangChain uses JSON as the serialization format to save and load chains from disk.,The serialization format used is JSON or YAML.,0.9999999999,0.5,0.0,0.06250381527944883,0.2727272727272727
33,whats an agent type that works well with Anthropic's models and supports structured output?,"['Dynamically route logic based on input | Dynamically route logic based on input This notebook covers how to do routing in the LangChain Expression Language. Routing allows you to create non-deterministic chains where the output of a previous step defines the next step. Routing helps provide structure and consistency around interactions with LLMs. There are two ways to perform routing: 1. Using a `RunnableBranch`. 2. Writing custom factory function that takes the input of a previous step and returns a **runnable**. Importantly, this should return a **runnable** and NOT actually execute. We\'ll illustrate both methods using a two step sequence where the first step classifies an input question as being about `LangChain`, `Anthropic`, or `Other`, then routes to a corresponding prompt chain. ## Using a RunnableBranch A `RunnableBranch` is initialized with a list of (condition, runnable) pairs and a default runnable. It selects which branch by passing each condition the input it\'s invoked with. It selects the first condition to evaluate to True, and runs the corresponding runnable to that condition with the input. If no provided conditions match, it runs the default runnable. Here\'s an example of what it looks like in action: ```python from langchain.chat_models import ChatAnthropic from langchain.prompts import PromptTemplate from langchain.schema.output_parser import StrOutputParser ``` First, let\'s create a chain that will identify incoming questions as being about `LangChain`, `Anthropic`, or `Other`: ```python chain = ( PromptTemplate.from_template( """"""Given the user question below, classify it as either being about `LangChain`, `Anthropic`, or `Other`. Do not respond with more than one word. {question} Classification:"""""" ) | ChatAnthropic() | StrOutputParser() ) ``` ```python chain.invoke({""question"": ""how do I call Anthropic?""}) ``` ```text \' Anthropic\' ``` Now, let\'s create three sub chains: ```python langchain_chain = ( PromptTemplate.from_template( """"""You are an expert in langchain. \\ Always answer questions starting with ""As Harrison Chase told me"". \\ Respond to the following question: Question: {question} Answer:"""""" ) | ChatAnthropic() ) anthropic_chain = ( PromptTemplate.from_template( """"""You are an expert in anthropic. \\ Always answer questions starting with ""As Dario Amodei told me"". \\ Respond to the following question: Question: {question} Answer:"""""" ) | ChatAnthropic() ) general_chain = ( PromptTemplate.from_template( """"""Respond to the following question: Question: {question} Answer:"""""" ) | ChatAnthropic() ) ``` ```python from langchain.schema.runnable import RunnableBranch branch = RunnableBranch( (lambda x: ""anthropic"" in x[""topic""].lower(), anthropic_chain), (lambda x: ""langchain"" in x[""topic""].lower(), langchain_chain), general_chain, ) ``` ```python full_chain = {""topic"": chain, ""question"": lambda x: x[""question""]} | branch ``` ```python full_chain.invoke({""question"": ""how do I use Anthropic?""}) ``` ```text AIMessage(content="" As Dario Amodei told me, here are some ways to use Anthropic:\\n\\n- Sign up for an account on Anthropic\'s website to access tools like Claude, Constitutional AI, and Writer. \\n\\n- Use Claude for tasks like email generation, customer service chat, and QA. Claude can understand natural language prompts and provide helpful responses.\\n\\n- Use Constitutional AI if you need an AI assistant that is harmless, honest, and helpful. It is designed to be safe and aligned with human values.\\n\\n- Use Writer to generate natural language content for things like marketing copy, stories, reports, and more. Give it a topic and prompt and it will create high-quality written content.\\n\\n- Check out Anthropic\'s documentation and blog for tips, tutorials, examples, and announcements about new capabilities as they continue to develop their AI technology.\\n\\n- Follow Anthropic on social media or subscribe to their newsletter to stay up to date on new features and releases.\\n\\n- For most people, the easiest way to leverage Anthropic\'s technology is through their website - just create an account to get started!"", additional_kwargs={}, example=False) ``` ```python full_chain.invoke({""question"": ""how do I use LangChain?""}) ``` ```text AIMessage(content=\' As Harrison Chase told me, here is how you use LangChain:\\n\\nLangChain is an AI assistant that can have conversations, answer questions, and generate text. To use LangChain, you simply type or speak your input and LangChain will respond. \\n\\nYou can ask LangChain questions, have discussions, get summaries or explanations about topics, and request it to generate text on a subject. Some examples of interactions:\\n\\n- Ask general knowledge questions and LangChain will try to answer factually. For example ""What is the capital of France?""\\n\\n- Have conversations on topics by taking turns speaking. You can prompt the start of a conversation by saying something like ""Let\\\'s discuss machine learning""\\n\\n- Ask for summaries or high-level explanations on subjects. For example ""Can you summarize the main themes in Shakespeare\\\'s Hamlet?"" \\n\\n- Give creative writing prompts or requests to have LangChain generate text in different styles. For example ""Write a short children\\\'s story about a mouse"" or ""Generate a poem in the style of Robert Frost about nature""\\n\\n- Correct LangChain if it makes an inaccurate statement and provide the right information. This helps train it.\\n\\nThe key is interacting naturally and giving it clear prompts and requests\', additional_kwargs={}, example=False) ``` ```python full_chain.invoke({""question"": ""whats 2 + 2""}) ``` ```text AIMessage(content=\' 2 + 2 = 4\', additional_kwargs={}, example=False) ``` ## Using a custom function You can also use a custom function to route between different outputs. Here\'s an example: ```python def route(info): if ""anthropic"" in info[""topic""].lower(): return anthropic_chain elif ""langchain"" in info[""topic""].lower(): return langchain_chain else: return general_chain ``` ```python from langchain.schema.runnable import RunnableLambda full_chain = {""topic"": chain, ""question"": lambda x: x[""question""]} | RunnableLambda( route ) ``` ```python full_chain.invoke({""question"": ""how do I use Anthroipc?""}) ``` ```text AIMessage(content=\' As Dario Amodei told me, to use Anthropic IPC you first need to import it:\\n\\n```python\\nfrom anthroipc import ic\\n```\\n\\nThen you can create a client and connect to the server:\\n\\n```python \\nclient = ic.connect()\\n```\\n\\nAfter that, you can call methods on the client and get responses:\\n\\n```python\\nresponse = client.ask(""What is the meaning of life?"")\\nprint(response)\\n```\\n\\nYou can also register callbacks to handle events: \\n\\n```python\\ndef on_poke(event):\\n print(""Got poked!"")\\n\\nclient.on(\\\'poke\\\', on_poke)\\n```\\n\\nAnd that\\\'s the basics of using the Anthropic IPC client library for Python! Let me know if you have any other questions!\', additional_kwargs={}, example=False) ``` ```python full_chain.invoke({""question"": ""how do I use LangChain?""}) ``` ```text AIMessage(content=\' As Harrison Chase told me, to use LangChain you first need to sign up for an API key at platform.langchain.com. Once you have your API key, you can install the Python library and write a simple Python script to call the LangChain API. Here is some sample code to get started:\\n\\n```python\\nimport langchain\\n\\napi_key = ""YOUR_API_KEY""\\n\\nlangchain.set_key(api_key)\\n\\nresponse = langchain.ask(""What is the capital of France?"")\\n\\nprint(response.response)\\n```\\n\\nThis will send the question ""What is the capital of France?"" to the LangChain API and print the response. You can customize the request by providing parameters like max_tokens, temperature, etc. The LangChain Python library documentation has more details on the available options. The key things are getting an API key and calling langchain.ask() with your question text. Let me know if you have any other questions!\', additional_kwargs={}, example=False) ``` ```python full_chain.invoke({""question"": ""whats 2 + 2""}) ``` ```text AIMessage(content=\' 4\', additional_kwargs={}, example=False) ``` - [Using a RunnableBranch](#using-a-runnablebranch) - [Using a custom function](#using-a-custom-function)', 'XML Agent | XML Agent Some language models (like Anthropic\'s Claude) are particularly good at reasoning/writing XML. This goes over how to use an agent that uses XML when prompting. ## Initialize the tools We will initialize some fake tools for demo purposes ```python from langchain.agents import tool @tool def search(query: str) -> str: """"""Search things about current events."""""" return ""32 degrees"" ``` ```python tools = [search] ``` ```python from langchain.chat_models import ChatAnthropic model = ChatAnthropic(model=""claude-2"") ``` ## Use LangChain Expression Language We will first show how to create this agent using LangChain Expression Language ```python from langchain import hub from langchain.agents.format_scratchpad import format_xml from langchain.agents.output_parsers import XMLAgentOutputParser from langchain.tools.render import render_text_description ``` ```python prompt = hub.pull(""hwchase17/xml-agent"") ``` ```python prompt = prompt.partial( tools=render_text_description(tools), tool_names="", "".join([t.name for t in tools]), ) ``` ```python llm_with_stop = model.bind(stop=[""""]) ``` ```python agent = ( { ""question"": lambda x: x[""question""], ""agent_scratchpad"": lambda x: format_xml(x[""intermediate_steps""]), } | prompt | llm_with_stop | XMLAgentOutputParser() ) ``` ```python from langchain.agents import AgentExecutor ``` ```python agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True) ``` ```python agent_executor.invoke({""question"": ""whats the weather in New york?""}) ``` ```text > Entering new AgentExecutor chain... search weather in new york32 degrees search weather in new york32 degrees The weather in New York is 32 degrees. > Finished chain. {\'question\': \'whats the weather in New york?\', \'output\': \'\\nThe weather in New York is 32 degrees.\\n\'} ``` ## Use off-the-shelf agent ```python from langchain.agents import XMLAgent from langchain.chains import LLMChain ``` ```python chain = LLMChain( llm=model, prompt=XMLAgent.get_default_prompt(), output_parser=XMLAgent.get_default_output_parser(), ) agent = XMLAgent(tools=tools, llm_chain=chain) ``` ```python agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True) ``` ```python agent_executor.invoke({""input"": ""whats the weather in New york?""}) ``` ```text > Entering new AgentExecutor chain... search weather in new york32 degrees The weather in New York is 32 degrees > Finished chain. {\'input\': \'whats the weather in New york?\', \'output\': \'The weather in New York is 32 degrees\'} ``` - [Initialize the tools](#initialize-the-tools) - [Use LangChain Expression Language](#use-langchain-expression-language) - [Use off-the-shelf agent](#use-off-the-shelf-agent)', 'Anthropic | Anthropic All functionality related to Anthropic models. [Anthropic]( is an AI safety and research company, and is the creator of Claude. This page covers all integrations between Anthropic models and LangChain. ## Prompting Overview Claude is chat-based model, meaning it is trained on conversation data. However, it is a text based API, meaning it takes in single string. It expects this string to be in a particular format. This means that it is up the user to ensure that is the case. LangChain provides several utilities and helper functions to make sure prompts that you write - whether formatted as a string or as a list of messages - end up formatted correctly. Specifically, Claude is trained to fill in text for the Assistant role as part of an ongoing dialogue between a human user (`Human:`) and an AI assistant (`Assistant:`). Prompts sent via the API must contain `\\n\\nHuman:` and `\\n\\nAssistant:` as the signals of who\'s speaking. The final turn must always be `\\n\\nAssistant:` - the input string cannot have `\\n\\nHuman:` as the final role. Because Claude is chat-based but accepts a string as input, it can be treated as either a LangChain `ChatModel` or `LLM`. This means there are two wrappers in LangChain - `ChatAnthropic` and `Anthropic`. It is generally recommended to use the `ChatAnthropic` wrapper, and format your prompts as `ChatMessage`s (we will show examples of this below). This is because it keeps your prompt in a general format that you can easily then also use with other models (should you want to). However, if you want more fine-grained control over the prompt, you can use the `Anthropic` wrapper - we will show and example of this as well. The `Anthropic` wrapper however is deprecated, as all functionality can be achieved in a more generic way using `ChatAnthropic`. ## Prompting Best Practices Anthropic models have several prompting best practices compared to OpenAI models. **No System Messages** Anthropic models are not trained on the concept of a ""system message"". We have worked with the Anthropic team to handle them somewhat appropriately (a Human message with an `admin` tag) but this is largely a hack and it is recommended that you do not use system messages. **AI Messages Can Continue** A completion from Claude is a continuation of the last text in the string which allows you further control over Claude\'s output. For example, putting words in Claude\'s mouth in a prompt like this: `\\n\\nHuman: Tell me a joke about bears\\n\\nAssistant: What do you call a bear with no teeth?` This will return a completion like this `A gummy bear!` instead of a whole new assistant message with a different random bear joke. ## ChatAnthropic `ChatAnthropic` is a subclass of LangChain\'s `ChatModel`, meaning it works best with `ChatPromptTemplate`. You can import this wrapper with the following code: ```text from langchain.chat_models import ChatAnthropic model = ChatAnthropic() ``` When working with ChatModels, it is preferred that you design your prompts as `ChatPromptTemplate`s. Here is an example below of doing that: ```text from langchain.prompts import ChatPromptTemplate prompt = ChatPromptTemplate.from_messages([ (""system"", ""You are a helpful chatbot""), (""human"", ""Tell me a joke about {topic}""), ]) ``` You can then use this in a chain as follows: ```text chain = prompt | model chain.invoke({""topic"": ""bears""}) ``` How is the prompt actually being formatted under the hood? We can see that by running the following code ```text prompt_value = prompt.format_prompt(topic=""bears"") model.convert_prompt(prompt_value) ``` This produces the following formatted string: ```text \'\\n\\nYou are a helpful chatbot\\n\\nHuman: Tell me a joke about bears\\n\\nAssistant:\' ``` We can see that under the hood LangChain is not appending any prefix/suffix to `SystemMessage`\'s. This is because Anthropic has no concept of `SystemMessage`. Anthropic requires all prompts to end with assistant messages. This means if the last message is not an assistant message, the suffix `Assistant:` will automatically be inserted. If you decide instead to use a normal PromptTemplate (one that just works on a single string) let\'s take a look at what happens: ```text from langchain.prompts import PromptTemplate prompt = PromptTemplate.from_template(""Tell me a joke about {topic}"") prompt_value = prompt.format_prompt(topic=""bears"") model.convert_prompt(prompt_value) ``` This produces the following formatted string: ```text \'\\n\\nHuman: Tell me a joke about bears\\n\\nAssistant:\' ``` We can see that it automatically adds the Human and Assistant tags. What is happening under the hood? First: the string gets converted to a single human message. This happens generically (because we are using a subclass of `ChatModel`). Then, similarly to the above example, an empty Assistant message is getting appended. This is Anthropic specific. ## [Deprecated] Anthropic This `Anthropic` wrapper is subclassed from `LLM`. We can import it with: ```text from langchain.llms import Anthropic model = Anthropic() ``` This model class is designed to work with normal PromptTemplates. An example of that is below: ```text prompt = PromptTemplate.from_template(""Tell me a joke about {topic}"") chain = prompt | model chain.invoke({""topic"": ""bears""}) ``` Let\'s see what is going on with the prompt templating under the hood! ```text prompt_value = prompt.format_prompt(topic=""bears"") model.convert_prompt(prompt_value) ``` This outputs the following ```text \'\\n\\nHuman: Tell me a joke about bears\\n\\nAssistant: Sure, here you go:\\n\' ``` Notice that it adds the Human tag at the start of the string, and then finishes it with `\\n\\nAssistant: Sure, here you go:`. The extra `Sure, here you go` was added on purpose by the Anthropic team. What happens if we have those symbols in the prompt directly? ```text prompt = PromptTemplate.from_template(""Human: Tell me a joke about {topic}"") prompt_value = prompt.format_prompt(topic=""bears"") model.convert_prompt(prompt_value) ``` This outputs: ```text \'\\n\\nHuman: Tell me a joke about bears\' ``` We can see that we detect that the user is trying to use the special tokens, and so we don\'t do any formatting. - [Prompting Overview](#prompting-overview) - [Prompting Best Practices](#prompting-best-practices) - [ChatAnthropic](#chatanthropic) - [Deprecated Anthropic](#deprecated-anthropic)']","An agent type that works well with Anthropic's models and supports structured output is the **XML Agent**. This type of agent can utilize XML to structure its output, which is particularly useful for parsing and processing the data in a well-defined format. 

Here's a brief example of how to set it up:

```python
from langchain.agents import tool
from langchain.chat_models import ChatAnthropic
from langchain import hub
from langchain.agents.format_scratchpad import format_xml
from langchain.agents.output_parsers import XMLAgentOutputParser
from langchain.tools.render import render_text_description

@tool
def search(query: str) -> str:
    """"""Search things about current events.""""""
    return ""32 degrees""

tools = [search]

model = ChatAnthropic(model=""claude-2"")

prompt = hub.pull(""hwchase17/xml-agent"")
prompt = prompt.partial(tools=render_text_description(tools), tool_names="", "".join([t.name for t in tools]), )

llm_with_stop = model.bind(stop=[""""])

agent = (
    {""question"": lambda x: x[""question""], ""agent_scratchpad"": lambda x: format_xml(x[""intermediate_steps""])}
    | prompt
    | llm_with_stop
    | XMLAgentOutputParser()
)

agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)
```

This setup allows the agent to return structured XML output, which can be easily parsed and processed.","The XML agent works well with Anthropic's Claude 2 model. Here's some code for doing so:

```python
from langchain.agents import XMLAgent, tool, AgentExecutor
from langchain.chat_models import ChatAnthropic
from langchain.chains import LLMChain
model = ChatAnthropic(model='claude-2')
chain = LLMChain(
llm=model,
prompt=XMLAgent.get_default_prompt(),
output_parser=XMLAgent.get_default_output_parser()
)
agent = XMLAgent(tools=tool_list, llm_chain=chain)
agent_executor = AgentExecutor(agent=agent, tools=tool_list, verbose=True)
```",0.9999999999666667,1.0,,0.13520459769143478,0.32330827067669177
34,How do i run gpt-4 locally?,"['GPT4All | GPT4All [GPT4All]( is a free-to-use, locally running, privacy-aware chatbot. There is no GPU or internet required. It features popular models and its own models such as GPT4All Falcon, Wizard, etc. This notebook explains how to use [GPT4All embeddings]( with LangChain. ## Install GPT4All\'s Python Bindings ```python %pip install gpt4all > /dev/null ``` Note: you may need to restart the kernel to use updated packages. ```python from langchain.embeddings import GPT4AllEmbeddings ``` ```python gpt4all_embd = GPT4AllEmbeddings() ``` ```text 100%|| 45.5M/45.5M [00:02<00:00, 18.5MiB/s] Model downloaded at: /Users/rlm/.cache/gpt4all/ggml-all-MiniLM-L6-v2-f16.bin objc[45711]: Class GGMLMetalClass is implemented in both /Users/rlm/anaconda3/envs/lcn2/lib/python3.9/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libreplit-mainline-metal.dylib (0x29fe18208) and /Users/rlm/anaconda3/envs/lcn2/lib/python3.9/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libllamamodel-mainline-metal.dylib (0x2a0244208). One of the two will be used. Which one is undefined. ``` ```python text = ""This is a test document."" ``` ## Embed the Textual Data ```python query_result = gpt4all_embd.embed_query(text) ``` With embed_documents you can embed multiple pieces of text. You can also map these embeddings with [Nomic\'s Atlas]( to see a visual representation of your data. ```python doc_result = gpt4all_embd.embed_documents([text]) ``` - [Install GPT4All\'s Python Bindings](#install-gpt4alls-python-bindings) - [Embed the Textual Data](#embed-the-textual-data)', 'GPT4All | GPT4All [GitHub:nomic-ai/gpt4all]( an ecosystem of open-source chatbots trained on a massive collections of clean assistant data including code, stories and dialogue. This example goes over how to use LangChain to interact with `GPT4All` models. ```python %pip install gpt4all > /dev/null ``` ```text Note: you may need to restart the kernel to use updated packages. ``` ### Import GPT4All ```python from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler from langchain.chains import LLMChain from langchain.llms import GPT4All from langchain.prompts import PromptTemplate ``` ### Set Up Question to pass to LLM ```python template = """"""Question: {question} Answer: Let\'s think step by step."""""" prompt = PromptTemplate(template=template, input_variables=[""question""]) ``` ### Specify Model To run locally, download a compatible ggml-formatted model. The [gpt4all page]( has a useful `Model Explorer` section: - Select a model of interest - Download using the UI and move the `.bin` to the `local_path` (noted below) For more info, visit [ ```python local_path = ( ""./models/ggml-gpt4all-l13b-snoozy.bin"" # replace with your desired local file path ) ``` ```python # Callbacks support token-wise streaming callbacks = [StreamingStdOutCallbackHandler()] # Verbose is required to pass to the callback manager llm = GPT4All(model=local_path, callbacks=callbacks, verbose=True) # If you want to use a custom model add the backend parameter # Check for supported backends llm = GPT4All(model=local_path, backend=""gptj"", callbacks=callbacks, verbose=True) ``` ```python llm_chain = LLMChain(prompt=prompt, llm=llm) ``` ```python question = ""What NFL team won the Super Bowl in the year Justin Bieber was born?"" llm_chain.run(question) ``` Justin Bieber was born on March 1, 1994. In 1994, The Cowboys won Super Bowl XXVIII. - [Import GPT4All](#import-gpt4all) - [Set Up Question to pass to LLM](#set-up-question-to-pass-to-llm) - [Specify Model](#specify-model)', 'LLM Caching integrations | LLM Caching integrations This notebook covers how to cache results of individual LLM calls using different caches. ```python from langchain.globals import set_llm_cache from langchain.llms import OpenAI # To make the caching really obvious, lets use a slower model. llm = OpenAI(model_name=""text-davinci-002"", n=2, best_of=2) ``` ## In Memory Cache ```python from langchain.cache import InMemoryCache set_llm_cache(InMemoryCache()) ``` ```python # The first time, it is not yet in cache, so it should take longer llm(""Tell me a joke"") ``` ```text CPU times: user 52.2 ms, sys: 15.2 ms, total: 67.4 ms Wall time: 1.19 s ""\\n\\nWhy couldn\'t the bicycle stand up by itself? Because it was...two tired!"" ``` ```python # The second time it is, so it goes faster llm(""Tell me a joke"") ``` ```text CPU times: user 191 s, sys: 11 s, total: 202 s Wall time: 205 s ""\\n\\nWhy couldn\'t the bicycle stand up by itself? Because it was...two tired!"" ``` ## SQLite Cache ```bash rm .langchain.db ``` ```python # We can do the same thing with a SQLite cache from langchain.cache import SQLiteCache set_llm_cache(SQLiteCache(database_path="".langchain.db"")) ``` ```python # The first time, it is not yet in cache, so it should take longer llm(""Tell me a joke"") ``` ```text CPU times: user 33.2 ms, sys: 18.1 ms, total: 51.2 ms Wall time: 667 ms \'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.\' ``` ```python # The second time it is, so it goes faster llm(""Tell me a joke"") ``` ```text CPU times: user 4.86 ms, sys: 1.97 ms, total: 6.83 ms Wall time: 5.79 ms \'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.\' ``` ## Upstash Redis Cache ### Standard Cache Use [Upstash Redis]( to cache prompts and responses with a serverless HTTP API. ```python from langchain.cache import UpstashRedisCache from upstash_redis import Redis URL = """" TOKEN = """" langchain.llm_cache = UpstashRedisCache(redis_=Redis(url=URL, token=TOKEN)) ``` ```python # The first time, it is not yet in cache, so it should take longer llm(""Tell me a joke"") ``` ```text CPU times: user 7.56 ms, sys: 2.98 ms, total: 10.5 ms Wall time: 1.14 s \'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!\' ``` ```python # The first time, it is not yet in cache, so it should take longer llm(""Tell me a joke"") ``` ```text CPU times: user 2.78 ms, sys: 1.95 ms, total: 4.73 ms Wall time: 82.9 ms \'\\n\\nTwo guys stole a calendar. They got six months each.\' ``` ## Redis Cache ### Standard Cache Use [Redis](/docs/ecosystem/integrations/redis) to cache prompts and responses. ```python # We can do the same thing with a Redis cache # (make sure your local Redis instance is running first before running this example) from langchain.cache import RedisCache from redis import Redis set_llm_cache(RedisCache(redis_=Redis())) ``` ```python # The first time, it is not yet in cache, so it should take longer llm(""Tell me a joke"") ``` ```text CPU times: user 6.88 ms, sys: 8.75 ms, total: 15.6 ms Wall time: 1.04 s \'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!\' ``` ```python # The second time it is, so it goes faster llm(""Tell me a joke"") ``` ```text CPU times: user 1.59 ms, sys: 610 s, total: 2.2 ms Wall time: 5.58 ms \'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!\' ``` ### Semantic Cache Use [Redis](/docs/ecosystem/integrations/redis) to cache prompts and responses and evaluate hits based on semantic similarity. ```python from langchain.cache import RedisSemanticCache from langchain.embeddings import OpenAIEmbeddings set_llm_cache( RedisSemanticCache(redis_url=""redis://localhost:6379"", embedding=OpenAIEmbeddings()) ) ``` ```python # The first time, it is not yet in cache, so it should take longer llm(""Tell me a joke"") ``` ```text CPU times: user 351 ms, sys: 156 ms, total: 507 ms Wall time: 3.37 s ""\\n\\nWhy don\'t scientists trust atoms?\\nBecause they make up everything."" ``` ```python # The second time, while not a direct hit, the question is semantically similar to the original question, # so it uses the cached result! llm(""Tell me one joke"") ``` ```text CPU times: user 6.25 ms, sys: 2.72 ms, total: 8.97 ms Wall time: 262 ms ""\\n\\nWhy don\'t scientists trust atoms?\\nBecause they make up everything."" ``` ## GPTCache We can use [GPTCache]( for exact match caching OR to cache results based on semantic similarity Let\'s first start with an example of exact match ```python import hashlib from gptcache import Cache from gptcache.manager.factory import manager_factory from gptcache.processor.pre import get_prompt from langchain.cache import GPTCache def get_hashed_name(name): return hashlib.sha256(name.encode()).hexdigest() def init_gptcache(cache_obj: Cache, llm: str): hashed_llm = get_hashed_name(llm) cache_obj.init( pre_embedding_func=get_prompt, data_manager=manager_factory(manager=""map"", data_dir=f""map_cache_{hashed_llm}""), ) set_llm_cache(GPTCache(init_gptcache)) ``` ```python # The first time, it is not yet in cache, so it should take longer llm(""Tell me a joke"") ``` ```text CPU times: user 21.5 ms, sys: 21.3 ms, total: 42.8 ms Wall time: 6.2 s \'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!\' ``` ```python # The second time it is, so it goes faster llm(""Tell me a joke"") ``` ```text CPU times: user 571 s, sys: 43 s, total: 614 s Wall time: 635 s \'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!\' ``` Let\'s now show an example of similarity caching ```python import hashlib from gptcache import Cache from gptcache.adapter.api import init_similar_cache from langchain.cache import GPTCache def get_hashed_name(name): return hashlib.sha256(name.encode()).hexdigest() def init_gptcache(cache_obj: Cache, llm: str): hashed_llm = get_hashed_name(llm) init_similar_cache(cache_obj=cache_obj, data_dir=f""similar_cache_{hashed_llm}"") set_llm_cache(GPTCache(init_gptcache)) ``` ```python # The first time, it is not yet in cache, so it should take longer llm(""Tell me a joke"") ``` ```text CPU times: user 1.42 s, sys: 279 ms, total: 1.7 s Wall time: 8.44 s \'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.\' ``` ```python # This is an exact match, so it finds it in the cache llm(""Tell me a joke"") ``` ```text CPU times: user 866 ms, sys: 20 ms, total: 886 ms Wall time: 226 ms \'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.\' ``` ```python # This is not an exact match, but semantically within distance so it hits! llm(""Tell me joke"") ``` ```text CPU times: user 853 ms, sys: 14.8 ms, total: 868 ms Wall time: 224 ms \'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.\' ``` ## Momento Cache Use [Momento](/docs/ecosystem/integrations/momento) to cache prompts and responses. Requires momento to use, uncomment below to install: ```python # !pip install momento ``` You\'ll need to get a Momento auth token to use this class. This can either be passed in to a momento.CacheClient if you\'d like to instantiate that directly, as a named parameter `auth_token` to `MomentoChatMessageHistory.from_client_params`, or can just be set as an environment variable `MOMENTO_AUTH_TOKEN`. ```python from datetime import timedelta from langchain.cache import MomentoCache cache_name = ""langchain"" ttl = timedelta(days=1) set_llm_cache(MomentoCache.from_client_params(cache_name, ttl)) ``` ```python # The first time, it is not yet in cache, so it should take longer llm(""Tell me a joke"") ``` ```text CPU times: user 40.7 ms, sys: 16.5 ms, total: 57.2 ms Wall time: 1.73 s \'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!\' ``` ```python # The second time it is, so it goes faster # When run in the same region as the cache, latencies are single digit ms llm(""Tell me a joke"") ``` ```text CPU times: user 3.16 ms, sys: 2.98 ms, total: 6.14 ms Wall time: 57.9 ms \'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!\' ``` ## SQLAlchemy Cache You can use `SQLAlchemyCache` to cache with any SQL database supported by `SQLAlchemy`. ```python # from langchain.cache import SQLAlchemyCache # from sqlalchemy import create_engine # engine = create_engine(""postgresql://postgres:postgres@localhost:5432/postgres"") # set_llm_cache(SQLAlchemyCache(engine)) ``` ### Custom SQLAlchemy Schemas ```python # You can define your own declarative SQLAlchemyCache child class to customize the schema used for caching. For example, to support high-speed fulltext prompt indexing with Postgres, use: from langchain.cache import SQLAlchemyCache from sqlalchemy import Column, Computed, Index, Integer, Sequence, String, create_engine from sqlalchemy.ext.declarative import declarative_base from sqlalchemy_utils import TSVectorType Base = declarative_base() class FulltextLLMCache(Base): # type: ignore """"""Postgres table for fulltext-indexed LLM Cache"""""" __tablename__ = ""llm_cache_fulltext"" id = Column(Integer, Sequence(""cache_id""), primary_key=True) prompt = Column(String, nullable=False) llm = Column(String, nullable=False) idx = Column(Integer) response = Column(String) prompt_tsv = Column( TSVectorType(), Computed(""to_tsvector(\'english\', llm || \' \' || prompt)"", persisted=True), ) __table_args__ = ( Index(""idx_fulltext_prompt_tsv"", prompt_tsv, postgresql_using=""gin""), ) engine = create_engine(""postgresql://postgres:postgres@localhost:5432/postgres"") set_llm_cache(SQLAlchemyCache(engine, FulltextLLMCache)) ``` ## Cassandra caches You can use Cassandra / Astra DB for caching LLM responses, choosing from the exact-match `CassandraCache` or the (vector-similarity-based) `CassandraSemanticCache`. Let\'s see both in action in the following cells. #### Connect to the DB First you need to establish a `Session` to the DB and to specify a _keyspace_ for the cache table(s). The following gets you started with an Astra DB instance (see e.g. [here]( for more backends and connection options). ```python import getpass keyspace = input(""\\nKeyspace name? "") ASTRA_DB_APPLICATION_TOKEN = getpass.getpass(\'\\nAstra DB Token (""AstraCS:..."") \') ASTRA_DB_SECURE_BUNDLE_PATH = input(""Full path to your Secure Connect Bundle? "") ``` ```text Keyspace name? my_keyspace Astra DB Token (""AstraCS:..."") Full path to your Secure Connect Bundle? /path/to/secure-connect-databasename.zip ``` ```python from cassandra.auth import PlainTextAuthProvider from cassandra.cluster import Cluster cluster = Cluster( cloud={ ""secure_connect_bundle"": ASTRA_DB_SECURE_BUNDLE_PATH, }, auth_provider=PlainTextAuthProvider(""token"", ASTRA_DB_APPLICATION_TOKEN), ) session = cluster.connect() ``` ### Exact cache This will avoid invoking the LLM when the supplied prompt is _exactly_ the same as one encountered already: ```python from langchain.cache import CassandraCache from langchain.globals import set_llm_cache set_llm_cache(CassandraCache(session=session, keyspace=keyspace)) ``` ```python print(llm(""Why is the Moon always showing the same side?"")) ``` ```text The Moon always shows the same side because it is tidally locked to Earth. CPU times: user 41.7 ms, sys: 153 s, total: 41.8 ms Wall time: 1.96 s ``` ```python print(llm(""Why is the Moon always showing the same side?"")) ``` ```text The Moon always shows the same side because it is tidally locked to Earth. CPU times: user 4.09 ms, sys: 0 ns, total: 4.09 ms Wall time: 119 ms ``` ### Semantic cache\u200b This cache will do a semantic similarity search and return a hit if it finds a cached entry that is similar enough, For this, you need to provide an `Embeddings` instance of your choice. ```python from langchain.embeddings import OpenAIEmbeddings embedding = OpenAIEmbeddings() ``` ```python from langchain.cache import CassandraSemanticCache set_llm_cache( CassandraSemanticCache( session=session, keyspace=keyspace, embedding=embedding, table_name=""cass_sem_cache"", ) ) ``` ```python print(llm(""Why is the Moon always showing the same side?"")) ``` ```text The Moon always shows the same side because it is tidally locked with Earth. This means that the same side of the Moon always faces Earth. CPU times: user 21.3 ms, sys: 177 s, total: 21.4 ms Wall time: 3.09 s ``` ```python print(llm(""How come we always see one face of the moon?"")) ``` ```text The Moon always shows the same side because it is tidally locked with Earth. This means that the same side of the Moon always faces Earth. CPU times: user 10.9 ms, sys: 17 s, total: 10.9 ms Wall time: 461 ms ``` ## Optional Caching\u200b You can also turn off caching for specific LLMs should you choose. In the example below, even though global caching is enabled, we turn it off for a specific LLM ```python llm = OpenAI(model_name=""text-davinci-002"", n=2, best_of=2, cache=False) ``` ```python llm(""Tell me a joke"") ``` ```text CPU times: user 5.8 ms, sys: 2.71 ms, total: 8.51 ms Wall time: 745 ms \'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!\' ``` ```python llm(""Tell me a joke"") ``` ```text CPU times: user 4.91 ms, sys: 2.64 ms, total: 7.55 ms Wall time: 623 ms \'\\n\\nTwo guys stole a calendar. They got six months each.\' ``` ## Optional Caching in Chains\u200b You can also turn off caching for particular nodes in chains. Note that because of certain interfaces, its often easier to construct the chain first, and then edit the LLM afterwards. As an example, we will load a summarizer map-reduce chain. We will cache results for the map-step, but then not freeze it for the combine step. ```python llm = OpenAI(model_name=""text-davinci-002"") no_cache_llm = OpenAI(model_name=""text-davinci-002"", cache=False) ``` ```python from langchain.text_splitter import CharacterTextSplitter text_splitter = CharacterTextSplitter() ``` ```python with open(""../../modules/state_of_the_union.txt"") as f: state_of_the_union = f.read() texts = text_splitter.split_text(state_of_the_union) ``` ```python from langchain.docstore.document import Document docs = [Document(page_content=t) for t in texts[:3]] from langchain.chains.summarize import load_summarize_chain ``` ```python chain = load_summarize_chain(llm, chain_type=""map_reduce"", reduce_llm=no_cache_llm) ``` ```python chain.run(docs) ``` ```text CPU times: user 452 ms, sys: 60.3 ms, total: 512 ms Wall time: 5.09 s \'\\n\\nPresident Biden is discussing the American Rescue Plan and the Bipartisan Infrastructure Law, which will create jobs and help Americans. He also talks about his vision for America, which includes investing in education and infrastructure. In response to Russian aggression in Ukraine, the United States is joining with European allies to impose sanctions and isolate Russia. American forces are being mobilized to protect NATO countries in the event that Putin decides to keep moving west. The Ukrainians are bravely fighting back, but the next few weeks will be hard for them. Putin will pay a high price for his actions in the long run. Americans should not be alarmed, as the United States is taking action to protect its interests and allies.\' ``` When we run it again, we see that it runs substantially faster but the final answer is different. This is due to caching at the map steps, but not at the reduce step. ```python chain.run(docs) ``` ```text CPU times: user 11.5 ms, sys: 4.33 ms, total: 15.8 ms Wall time: 1.04 s \'\\n\\nPresident Biden is discussing the American Rescue Plan and the Bipartisan Infrastructure Law, which will create jobs and help Americans. He also talks about his vision for America, which includes investing in education and infrastructure.\' ``` ```bash rm .langchain.db sqlite.db ``` - [In Memory Cache](#in-memory-cache) - [SQLite Cache](#sqlite-cache) - [Upstash Redis Cache](#upstash-redis-cache)- [Standard Cache](#standard-cache) - [Redis Cache](#redis-cache)- [Standard Cache](#standard-cache-1) - [Semantic Cache](#semantic-cache) - [GPTCache](#gptcache) - [Momento Cache](#momento-cache) - [SQLAlchemy Cache](#sqlalchemy-cache)- [Custom SQLAlchemy Schemas](#custom-sqlalchemy-schemas) - [Cassandra caches](#cassandra-caches)- [Exact cache](#exact-cache) - [Semantic cache](#semantic-cache-1) - [Optional Caching](#optional-caching) - [Optional Caching in Chains](#optional-caching-in-chains)']","To run [gpt4all](https://gpt4all.io/) locally, follow these steps:

1. **Install Dependencies**:
   ```bash
   pip install gpt4all
   ```

2. **Download a Model**:
   Visit the [gpt4all model page](https://gpt4all.io/models/) and download a model of your choice. Place the downloaded `.bin` file in a directory you'll reference later.

3. **Set Up Python Script**:
   Create a Python script to load the model and run it locally.

   ```python
   from langchain.llms import GPT4All
   from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

   local_path = ""./path/to/your/model.bin""  # Update this path
   llm = GPT4All(model=local_path, callbacks=[StreamingStdOutCallbackHandler()])

   # Example query
   print(llm(""What is the capital of France?""))
   ```

4. **Run the Script**:
   Execute the script in your terminal:
   ```bash
   python your_script_name.py
   ```

This will run the gpt4all model locally and output responses to your queries.","GPT-4 is a closed-source model provided through an API by OpenAI. While you can connect to GPT-4 via the API, you cannot run the model locally.",0.0,1.0,,0.024426633221059636,0.12195121951219512
35,I want to save the configuration for a given LLM. Show me how to do that.,"['HuggingFace dataset | HuggingFace dataset The [Hugging Face Hub]( is home to over 5,000 [datasets]( in more than 100 languages that can be used for a broad range of tasks across NLP, Computer Vision, and Audio. They used for a diverse range of tasks such as translation, automatic speech recognition, and image classification. This notebook shows how to load `Hugging Face Hub` datasets to LangChain. ```python from langchain.document_loaders import HuggingFaceDatasetLoader ``` ```python dataset_name = ""imdb"" page_content_column = ""text"" loader = HuggingFaceDatasetLoader(dataset_name, page_content_column) ``` ```python data = loader.load() ``` ```python data[:15] ``` ```text [Document(page_content=\'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered ""controversial"" I really had to see this for myself.The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\\'t have much of a plot.\', metadata={\'label\': 0}), Document(page_content=\'""I Am Curious: Yellow"" is a risible and pretentious steaming pile. It doesn\\\'t matter what one\\\'s political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn\\\'t true. I\\\'ve seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don\\\'t exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we\\\'re treated to the site of Vincent Gallo\\\'s throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) ""double-standard"" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. In fact, you generally won\\\'t see female genitals in an American film in anything short of porn or explicit erotica. This alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women\\\'s bodies.\', metadata={\'label\': 0}), Document(page_content=""If only to avoid making this type of film in the future. This film is interesting as an experiment but tells no cogent story.One might feel virtuous for sitting thru it because it touches on so many IMPORTANT issues but it does so without any discernable motive. The viewer comes away with no new perspectives (unless one comes up with one while one\'s mind wanders, as it will invariably do during this pointless film).One might better spend one\'s time staring out a window at a tree growing."", metadata={\'label\': 0}), Document(page_content=""This film was probably inspired by Godard\'s Masculin, fminin and I urge you to see that film instead.The film has two strong elements and those are, (1) the realistic acting (2) the impressive, undeservedly good, photo. Apart from that, what strikes me most is the endless stream of silliness. Lena Nyman has to be most annoying actress in the world. She acts so stupid and with all the nudity in this film,...it\'s unattractive. Comparing to Godard\'s film, intellectuality has been replaced with stupidity. Without going too far on this subject, I would say that follows from the difference in ideals between the French and the Swedish society.A movie of its time, and place. 2/10."", metadata={\'label\': 0}), Document(page_content=\'Oh, brother...after hearing about this ridiculous film for umpteen years all I can think of is that old Peggy Lee song..""Is that all there is??"" ...I was just an early teen when this smoked fish hit the U.S. I was too young to get in the theater (although I did manage to sneak into ""Goodbye Columbus""). Then a screening at a local film museum beckoned - Finally I could see this film, except now I was as old as my parents were when they schlepped to see it!!The ONLY reason this film was not condemned to the anonymous sands of time was because of the obscenity case sparked by its U.S. release. MILLIONS of people flocked to this stinker, thinking they were going to see a sex film...Instead, they got lots of closeups of gnarly, repulsive Swedes, on-street interviews in bland shopping malls, asinie political pretension...and feeble who-cares simulated sex scenes with saggy, pale actors.Cultural icon, holy grail, historic artifact..whatever this thing was, shred it, burn it, then stuff the ashes in a lead box!Elite esthetes still scrape to find value in its boring pseudo revolutionary political spewings..But if it weren\\\'t for the censorship scandal, it would have been ignored, then forgotten.Instead, the ""I Am Blank, Blank"" rhythymed title was repeated endlessly for years as a titilation for porno films (I am Curious, Lavender - for gay films, I Am Curious, Black - for blaxploitation films, etc..) and every ten years or so the thing rises from the dead, to be viewed by a new generation of suckers who want to see that ""naughty sex film"" that ""revolutionized the film industry""...Yeesh, avoid like the plague..Or if you MUST see it - rent the video and fast forward to the ""dirty"" parts, just to get it over with.\', metadata={\'label\': 0}), Document(page_content=""I would put this at the top of my list of films in the category of unwatchable trash! There are films that are bad, but the worst kind are the ones that are unwatchable but you are suppose to like them because they are supposed to be good for you! The sex sequences, so shocking in its day, couldn\'t even arouse a rabbit. The so called controversial politics is strictly high school sophomore amateur night Marxism. The film is self-consciously arty in the worst sense of the term. The photography is in a harsh grainy black and white. Some scenes are out of focus or taken from the wrong angle. Even the sound is bad! And some people call this art?"", metadata={\'label\': 0}), Document(page_content=""Whoever wrote the screenplay for this movie obviously never consulted any books about Lucille Ball, especially her autobiography. I\'ve never seen so many mistakes in a biopic, ranging from her early years in Celoron and Jamestown to her later years with Desi. I could write a whole list of factual errors, but it would go on for pages. In all, I believe that Lucille Ball is one of those inimitable people who simply cannot be portrayed by anyone other than themselves. If I were Lucie Arnaz and Desi, Jr., I would be irate at how many mistakes were made in this film. The filmmakers tried hard, but the movie seems awfully sloppy to me."", metadata={\'label\': 0}), Document(page_content=\'When I first saw a glimpse of this movie, I quickly noticed the actress who was playing the role of Lucille Ball. Rachel York\\\'s portrayal of Lucy is absolutely awful. Lucille Ball was an astounding comedian with incredible talent. To think about a legend like Lucille Ball being portrayed the way she was in the movie is horrendous. I cannot believe out of all the actresses in the world who could play a much better Lucy, the producers decided to get Rachel York. She might be a good actress in other roles but to play the role of Lucille Ball is tough. It is pretty hard to find someone who could resemble Lucille Ball, but they could at least find someone a bit similar in looks and talent. If you noticed York\\\'s portrayal of Lucy in episodes of I Love Lucy like the chocolate factory or vitavetavegamin, nothing is similar in any way-her expression, voice, or movement.To top it all off, Danny Pino playing Desi Arnaz is horrible. Pino does not qualify to play as Ricky. He\\\'s small and skinny, his accent is unreal, and once again, his acting is unbelievable. Although Fred and Ethel were not similar either, they were not as bad as the characters of Lucy and Ricky.Overall, extremely horrible casting and the story is badly told. If people want to understand the real life situation of Lucille Ball, I suggest watching A&E Biography of Lucy and Desi, read the book from Lucille Ball herself, or PBS\\\' American Masters: Finding Lucy. If you want to see a docudrama, ""Before the Laughter"" would be a better choice. The casting of Lucille Ball and Desi Arnaz in ""Before the Laughter"" is much better compared to this. At least, a similar aspect is shown rather than nothing.\', metadata={\'label\': 0}), Document(page_content=\'Who are these ""They""- the actors? the filmmakers? Certainly couldn\\\'t be the audience- this is among the most air-puffed productions in existence. It\\\'s the kind of movie that looks like it was a lot of fun to shoot\\x97 TOO much fun, nobody is getting any actual work done, and that almost always makes for a movie that\\\'s no fun to watch.Ritter dons glasses so as to hammer home his character\\\'s status as a sort of doppleganger of the bespectacled Bogdanovich; the scenes with the breezy Ms. Stratten are sweet, but have an embarrassing, look-guys-I\\\'m-dating-the-prom-queen feel to them. Ben Gazzara sports his usual cat\\\'s-got-canary grin in a futile attempt to elevate the meager plot, which requires him to pursue Audrey Hepburn with all the interest of a narcoleptic at an insomnia clinic. In the meantime, the budding couple\\\'s respective children (nepotism alert: Bogdanovich\\\'s daughters) spew cute and pick up some fairly disturbing pointers on \\\'love\\\' while observing their parents. (Ms. Hepburn, drawing on her dignity, manages to rise above the proceedings- but she has the monumental challenge of playing herself, ostensibly.) Everybody looks great, but so what? It\\\'s a movie and we can expect that much, if that\\\'s what you\\\'re looking for you\\\'d be better off picking up a copy of Vogue.Oh- and it has to be mentioned that Colleen Camp thoroughly annoys, even apart from her singing, which, while competent, is wholly unconvincing... the country and western numbers are woefully mismatched with the standards on the soundtrack. Surely this is NOT what Gershwin (who wrote the song from which the movie\\\'s title is derived) had in mind; his stage musicals of the 20\\\'s may have been slight, but at least they were long on charm. ""They All Laughed"" tries to coast on its good intentions, but nobody- least of all Peter Bogdanovich - has the good sense to put on the brakes.Due in no small part to the tragic death of Dorothy Stratten, this movie has a special place in the heart of Mr. Bogdanovich- he even bought it back from its producers, then distributed it on his own and went bankrupt when it didn\\\'t prove popular. His rise and fall is among the more sympathetic and tragic of Hollywood stories, so there\\\'s no joy in criticizing the film... there _is_ real emotional investment in Ms. Stratten\\\'s scenes. But ""Laughed"" is a faint echo of ""The Last Picture Show"", ""Paper Moon"" or ""What\\\'s Up, Doc""- following ""Daisy Miller"" and ""At Long Last Love"", it was a thundering confirmation of the phase from which P.B. has never emerged.All in all, though, the movie is harmless, only a waste of rental. I want to watch people having a good time, I\\\'ll go to the park on a sunny day. For filmic expressions of joy and love, I\\\'ll stick to Ernest Lubitsch and Jaques Demy...\', metadata={\'label\': 0}), Document(page_content=""This is said to be a personal film for Peter Bogdonavitch. He based it on his life but changed things around to fit the characters, who are detectives. These detectives date beautiful models and have no problem getting them. Sounds more like a millionaire playboy filmmaker than a detective, doesn\'t it? This entire movie was written by Peter, and it shows how out of touch with real people he was. You\'re supposed to write what you know, and he did that, indeed. And leaves the audience bored and confused, and jealous, for that matter. This is a curio for people who want to see Dorothy Stratten, who was murdered right after filming. But Patti Hanson, who would, in real life, marry Keith Richards, was also a model, like Stratten, but is a lot better and has a more ample part. In fact, Stratten\'s part seemed forced; added. She doesn\'t have a lot to do with the story, which is pretty convoluted to begin with. All in all, every character in this film is somebody that very few people can relate with, unless you\'re millionaire from Manhattan with beautiful supermodels at your beckon call. For the rest of us, it\'s an irritating snore fest. That\'s what happens when you\'re out of touch. You entertain your few friends with inside jokes, and bore all the rest."", metadata={\'label\': 0}), Document(page_content=\'It was great to see some of my favorite stars of 30 years ago including John Ritter, Ben Gazarra and Audrey Hepburn. They looked quite wonderful. But that was it. They were not given any characters or good lines to work with. I neither understood or cared what the characters were doing.Some of the smaller female roles were fine, Patty Henson and Colleen Camp were quite competent and confident in their small sidekick parts. They showed some talent and it is sad they didn\\\'t go on to star in more and better films. Sadly, I didn\\\'t think Dorothy Stratten got a chance to act in this her only important film role.The film appears to have some fans, and I was very open-minded when I started watching it. I am a big Peter Bogdanovich fan and I enjoyed his last movie, ""Cat\\\'s Meow"" and all his early ones from ""Targets"" to ""Nickleodeon"". So, it really surprised me that I was barely able to keep awake watching this one.It is ironic that this movie is about a detective agency where the detectives and clients get romantically involved with each other. Five years later, Bogdanovich\\\'s ex-girlfriend, Cybil Shepherd had a hit television series called ""Moonlighting"" stealing the story idea from Bogdanovich. Of course, there was a great difference in that the series relied on tons of witty dialogue, while this tries to make do with slapstick and a few screwball lines.Bottom line: It ain\\\'t no ""Paper Moon"" and only a very pale version of ""What\\\'s Up, Doc"".\', metadata={\'label\': 0}), Document(page_content=""I can\'t believe that those praising this movie herein aren\'t thinking of some other film. I was prepared for the possibility that this would be awful, but the script (or lack thereof) makes for a film that\'s also pointless. On the plus side, the general level of craft on the part of the actors and technical crew is quite competent, but when you\'ve got a sow\'s ear to work with you can\'t make a silk purse. Ben G fans should stick with just about any other movie he\'s been in. Dorothy S fans should stick to Galaxina. Peter B fans should stick to Last Picture Show and Target. Fans of cheap laughs at the expense of those who seem to be asking for it should stick to Peter B\'s amazingly awful book, Killing of the Unicorn."", metadata={\'label\': 0}), Document(page_content=\'Never cast models and Playboy bunnies in your films! Bob Fosse\\\'s ""Star 80"" about Dorothy Stratten, of whom Bogdanovich was obsessed enough to have married her SISTER after her murder at the hands of her low-life husband, is a zillion times more interesting than Dorothy herself on the silver screen. Patty Hansen is no actress either..I expected to see some sort of lost masterpiece a la Orson Welles but instead got Audrey Hepburn cavorting in jeans and a god-awful ""poodlesque"" hair-do....Very disappointing....""Paper Moon"" and ""The Last Picture Show"" I could watch again and again. This clunker I could barely sit through once. This movie was reputedly not released because of the brouhaha surrounding Ms. Stratten\\\'s tawdry death; I think the real reason was because it was so bad!\', metadata={\'label\': 0}), Document(page_content=""Its not the cast. A finer group of actors, you could not find. Its not the setting. The director is in love with New York City, and by the end of the film, so are we all! Woody Allen could not improve upon what Bogdonovich has done here. If you are going to fall in love, or find love, Manhattan is the place to go. No, the problem with the movie is the script. There is none. The actors fall in love at first sight, words are unnecessary. In the director\'s own experience in Hollywood that is what happens when they go to work on the set. It is reality to him, and his peers, but it is a fantasy to most of us in the real world. So, in the end, the movie is hollow, and shallow, and message-less."", metadata={\'label\': 0}), Document(page_content=\'Today I found ""They All Laughed"" on VHS on sale in a rental. It was a really old and very used VHS, I had no information about this movie, but I liked the references listed on its cover: the names of Peter Bogdanovich, Audrey Hepburn, John Ritter and specially Dorothy Stratten attracted me, the price was very low and I decided to risk and buy it. I searched IMDb, and the User Rating of 6.0 was an excellent reference. I looked in ""Mick Martin & Marsha Porter Video & DVD Guide 2003"" and \\x96 wow \\x96 four stars! So, I decided that I could not waste more time and immediately see it. Indeed, I have just finished watching ""They All Laughed"" and I found it a very boring overrated movie. The characters are badly developed, and I spent lots of minutes to understand their roles in the story. The plot is supposed to be funny (private eyes who fall in love for the women they are chasing), but I have not laughed along the whole story. The coincidences, in a huge city like New York, are ridiculous. Ben Gazarra as an attractive and very seductive man, with the women falling for him as if her were a Brad Pitt, Antonio Banderas or George Clooney, is quite ridiculous. In the end, the greater attractions certainly are the presence of the Playboy centerfold and playmate of the year Dorothy Stratten, murdered by her husband pretty after the release of this movie, and whose life was showed in ""Star 80"" and ""Death of a Centerfold: The Dorothy Stratten Story""; the amazing beauty of the sexy Patti Hansen, the future Mrs. Keith Richards; the always wonderful, even being fifty-two years old, Audrey Hepburn; and the song ""Amigo"", from Roberto Carlos. Although I do not like him, Roberto Carlos has been the most popular Brazilian singer since the end of the 60\\\'s and is called by his fans as ""The King"". I will keep this movie in my collection only because of these attractions (manly Dorothy Stratten). My vote is four.Title (Brazil): ""Muito Riso e Muita Alegria"" (""Many Laughs and Lots of Happiness"")\', metadata={\'label\': 0})] ``` ### Example In this example, we use data from a dataset to answer a question ```python from langchain.document_loaders.hugging_face_dataset import HuggingFaceDatasetLoader from langchain.indexes import VectorstoreIndexCreator ``` ```python dataset_name = ""tweet_eval"" page_content_column = ""text"" name = ""stance_climate"" loader = HuggingFaceDatasetLoader(dataset_name, page_content_column, name) ``` ```python index = VectorstoreIndexCreator().from_loaders([loader]) ``` ```text Found cached dataset tweet_eval 0%| | 0/3 [00:00<?, ?it/s] Using embedded DuckDB without persistence: data will be transient ``` ```python query = ""What are the most used hashtag?"" result = index.query(query) ``` ```python result ``` ```text \' The most used hashtags in this context are #UKClimate2015, #Sustainability, #TakeDownTheFlag, #LoveWins, #CSOTA, #ClimateSummitoftheAmericas, #SM, and #SocialMedia.\' ``` - [Example](#example)', 'Configure chain internals at runtime | Configure chain internals at runtime Oftentimes you may want to experiment with, or even expose to the end user, multiple different ways of doing things. In order to make this experience as easy as possible, we have defined two methods. First, a `configurable_fields` method. This lets you configure particular fields of a runnable. Second, a `configurable_alternatives` method. With this method, you can list out alternatives for any particular runnable that can be set during runtime. ## Configuration Fields ### With LLMs With LLMs we can configure things like temperature ```python from langchain.chat_models import ChatOpenAI from langchain.prompts import PromptTemplate model = ChatOpenAI(temperature=0).configurable_fields( temperature=ConfigurableField( id=""llm_temperature"", name=""LLM Temperature"", description=""The temperature of the LLM"", ) ) ``` ```python model.invoke(""pick a random number"") ``` ```text AIMessage(content=\'7\') ``` ```python model.with_config(configurable={""llm_temperature"": 0.9}).invoke(""pick a random number"") ``` ```text AIMessage(content=\'34\') ``` We can also do this when its used as part of a chain ```python prompt = PromptTemplate.from_template(""Pick a random number above {x}"") chain = prompt | model ``` ```python chain.invoke({""x"": 0}) ``` ```text AIMessage(content=\'57\') ``` ```python chain.with_config(configurable={""llm_temperature"": 0.9}).invoke({""x"": 0}) ``` ```text AIMessage(content=\'6\') ``` ### With HubRunnables This is useful to allow for switching of prompts ```python from langchain.runnables.hub import HubRunnable ``` ```python prompt = HubRunnable(""rlm/rag-prompt"").configurable_fields( owner_repo_commit=ConfigurableField( id=""hub_commit"", name=""Hub Commit"", description=""The Hub commit to pull from"", ) ) ``` ```python prompt.invoke({""question"": ""foo"", ""context"": ""bar""}) ``` ```text ChatPromptValue(messages=[HumanMessage(content=""You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise.\\nQuestion: foo \\nContext: bar \\nAnswer:"")]) ``` ```python prompt.with_config(configurable={""hub_commit"": ""rlm/rag-prompt-llama""}).invoke( {""question"": ""foo"", ""context"": ""bar""} ) ``` ```text ChatPromptValue(messages=[HumanMessage(content=""[INST]> You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise.> \\nQuestion: foo \\nContext: bar \\nAnswer: [/INST]"")]) ``` ## Configurable Alternatives ### With LLMs Let\'s take a look at doing this with LLMs ```python from langchain.chat_models import ChatAnthropic, ChatOpenAI from langchain.prompts import PromptTemplate from langchain.schema.runnable import ConfigurableField ``` ```python llm = ChatAnthropic(temperature=0).configurable_alternatives( # This gives this field an id # When configuring the end runnable, we can then use this id to configure this field ConfigurableField(id=""llm""), # This sets a default_key. # If we specify this key, the default LLM (ChatAnthropic initialized above) will be used default_key=""anthropic"", # This adds a new option, with name `openai` that is equal to `ChatOpenAI()` openai=ChatOpenAI(), # This adds a new option, with name `gpt4` that is equal to `ChatOpenAI(model=""gpt-4"")` gpt4=ChatOpenAI(model=""gpt-4""), # You can add more configuration options here ) prompt = PromptTemplate.from_template(""Tell me a joke about {topic}"") chain = prompt | llm ``` ```python # By default it will call Anthropic chain.invoke({""topic"": ""bears""}) ``` ```text AIMessage(content="" Here\'s a silly joke about bears:\\n\\nWhat do you call a bear with no teeth?\\nA gummy bear!"") ``` ```python # We can use `.with_config(configurable={""llm"": ""openai""})` to specify an llm to use chain.with_config(configurable={""llm"": ""openai""}).invoke({""topic"": ""bears""}) ``` ```text AIMessage(content=""Sure, here\'s a bear joke for you:\\n\\nWhy don\'t bears wear shoes?\\n\\nBecause they already have bear feet!"") ``` ```python # If we use the `default_key` then it uses the default chain.with_config(configurable={""llm"": ""anthropic""}).invoke({""topic"": ""bears""}) ``` ```text AIMessage(content="" Here\'s a silly joke about bears:\\n\\nWhat do you call a bear with no teeth?\\nA gummy bear!"") ``` ### With Prompts We can do a similar thing, but alternate between prompts ```python llm = ChatAnthropic(temperature=0) prompt = PromptTemplate.from_template( ""Tell me a joke about {topic}"" ).configurable_alternatives( # This gives this field an id # When configuring the end runnable, we can then use this id to configure this field ConfigurableField(id=""prompt""), # This sets a default_key. # If we specify this key, the default LLM (ChatAnthropic initialized above) will be used default_key=""joke"", # This adds a new option, with name `poem` poem=PromptTemplate.from_template(""Write a short poem about {topic}""), # You can add more configuration options here ) chain = prompt | llm ``` ```python # By default it will write a joke chain.invoke({""topic"": ""bears""}) ``` ```text AIMessage(content="" Here\'s a silly joke about bears:\\n\\nWhat do you call a bear with no teeth?\\nA gummy bear!"") ``` ```python # We can configure it write a poem chain.with_config(configurable={""prompt"": ""poem""}).invoke({""topic"": ""bears""}) ``` ```text AIMessage(content=\' Here is a short poem about bears:\\n\\nThe bears awaken from their sleep\\nAnd lumber out into the deep\\nForests filled with trees so tall\\nForaging for food before nightfall \\nTheir furry coats and claws so sharp\\nSniffing for berries and fish to nab\\nLumbering about without a care\\nThe mighty grizzly and black bear\\nProud creatures, wild and free\\nRuling their domain majestically\\nWandering the woods they call their own\\nBefore returning to their dens alone\') ``` ### With Prompts and LLMs We can also have multiple things configurable! Here\'s an example doing that with both prompts and LLMs. ```python llm = ChatAnthropic(temperature=0).configurable_alternatives( # This gives this field an id # When configuring the end runnable, we can then use this id to configure this field ConfigurableField(id=""llm""), # This sets a default_key. # If we specify this key, the default LLM (ChatAnthropic initialized above) will be used default_key=""anthropic"", # This adds a new option, with name `openai` that is equal to `ChatOpenAI()` openai=ChatOpenAI(), # This adds a new option, with name `gpt4` that is equal to `ChatOpenAI(model=""gpt-4"")` gpt4=ChatOpenAI(model=""gpt-4""), # You can add more configuration options here ) prompt = PromptTemplate.from_template( ""Tell me a joke about {topic}"" ).configurable_alternatives( # This gives this field an id # When configuring the end runnable, we can then use this id to configure this field ConfigurableField(id=""prompt""), # This sets a default_key. # If we specify this key, the default LLM (ChatAnthropic initialized above) will be used default_key=""joke"", # This adds a new option, with name `poem` poem=PromptTemplate.from_template(""Write a short poem about {topic}""), # You can add more configuration options here ) chain = prompt | llm ``` ```python # We can configure it write a poem with OpenAI chain.with_config(configurable={""prompt"": ""poem"", ""llm"": ""openai""}).invoke( {""topic"": ""bears""} ) ``` ```text AIMessage(content=""In the forest, where tall trees sway,\\nA creature roams, both fierce and gray.\\nWith mighty paws and piercing eyes,\\nThe bear, a symbol of strength, defies.\\n\\nThrough snow-kissed mountains, it does roam,\\nA guardian of its woodland home.\\nWith fur so thick, a shield of might,\\nIt braves the coldest winter night.\\n\\nA gentle giant, yet wild and free,\\nThe bear commands respect, you see.\\nWith every step, it leaves a trace,\\nOf untamed power and ancient grace.\\n\\nFrom honeyed feast to salmon\'s leap,\\nIt takes its place, in nature\'s keep.\\nA symbol of untamed delight,\\nThe bear, a wonder, day and night.\\n\\nSo let us honor this noble beast,\\nIn forests where its soul finds peace.\\nFor in its presence, we come to know,\\nThe untamed spirit that in us also flows."") ``` ```python # We can always just configure only one if we want chain.with_config(configurable={""llm"": ""openai""}).invoke({""topic"": ""bears""}) ``` ```text AIMessage(content=""Sure, here\'s a bear joke for you:\\n\\nWhy don\'t bears wear shoes?\\n\\nBecause they have bear feet!"") ``` ### Saving configurations We can also easily save configured chains as their own objects ```python openai_poem = chain.with_config(configurable={""llm"": ""openai""}) ``` ```python openai_poem.invoke({""topic"": ""bears""}) ``` ```text AIMessage(content=""Why don\'t bears wear shoes?\\n\\nBecause they have bear feet!"") ``` - [Configuration Fields](#configuration-fields)- [With LLMs](#with-llms) - [With HubRunnables](#with-hubrunnables) - [Configurable Alternatives](#configurable-alternatives)- [With LLMs](#with-llms-1) - [With Prompts](#with-prompts) - [With Prompts and LLMs](#with-prompts-and-llms) - [Saving configurations](#saving-configurations)', 'iFixit | iFixit [iFixit]( is the largest, open repair community on the web. The site contains nearly 100k repair manuals, 200k Questions & Answers on 42k devices, and all the data is licensed under CC-BY-NC-SA 3.0. This loader will allow you to download the text of a repair guide, text of Q&A\'s and wikis from devices on `iFixit` using their open APIs. It\'s incredibly useful for context related to technical documents and answers to questions about devices in the corpus of data on `iFixit`. ```python from langchain.document_loaders import IFixitLoader ``` ```python loader = IFixitLoader("" data = loader.load() ``` ```python data ``` ```text [Document(page_content=""# Banana Teardown\\nIn this teardown, we open a banana to see what\'s inside. Yellow and delicious, but most importantly, yellow.\\n\\n\\n###Tools Required:\\n\\n - Fingers\\n\\n - Teeth\\n\\n - Thumbs\\n\\n\\n###Parts Required:\\n\\n - None\\n\\n\\n## Step 1\\nTake one banana from the bunch.\\nDon\'t squeeze too hard!\\n\\n\\n## Step 2\\nHold the banana in your left hand and grip the stem between your right thumb and forefinger.\\n\\n\\n## Step 3\\nPull the stem downward until the peel splits.\\n\\n\\n## Step 4\\nInsert your thumbs into the split of the peel and pull the two sides apart.\\nExpose the top of the banana. It may be slightly squished from pulling on the stem, but this will not affect the flavor.\\n\\n\\n## Step 5\\nPull open the peel, starting from your original split, and opening it along the length of the banana.\\n\\n\\n## Step 6\\nRemove fruit from peel.\\n\\n\\n## Step 7\\nEat and enjoy!\\nThis is where you\'ll need your teeth.\\nDo not choke on banana!\\n"", lookup_str=\'\', metadata={\'source\': \' \'title\': \'Banana Teardown\'}, lookup_index=0)] ``` ```python loader = IFixitLoader( "" ) data = loader.load() ``` ```python data ``` ```text [Document(page_content=\'# My iPhone 6 is typing and opening apps by itself\\nmy iphone 6 is typing and opening apps by itself. How do i fix this. I just bought it last week.\\nI restored as manufactures cleaned up the screen\\nthe problem continues\\n\\n## 27 Answers\\n\\nFilter by: \\n\\nMost Helpful\\nNewest\\nOldest\\n\\n### Accepted Answer\\nHi,\\nWhere did you buy it? If you bought it from Apple or from an official retailer like Carphone warehouse etc. Then you\\\'ll have a year warranty and can get it replaced free.\\nIf you bought it second hand, from a third part repair shop or online, then it may still have warranty, unless it is refurbished and has been repaired elsewhere.\\nIf this is the case, it may be the screen that needs replacing to solve your issue.\\nEither way, wherever you got it, it\\\'s best to return it and get a refund or a replacement device. :-)\\n\\n\\n\\n### Most Helpful Answer\\nI had the same issues, screen freezing, opening apps by itself, selecting the screens and typing on it\\\'s own. I first suspected aliens and then ghosts and then hackers.\\niPhone 6 is weak physically and tend to bend on pressure. And my phone had no case or cover.\\nI took the phone to apple stores and they said sensors need to be replaced and possibly screen replacement as well. My phone is just 17 months old.\\nHere is what I did two days ago and since then it is working like a charm..\\nHold the phone in portrait (as if watching a movie). Twist it very very gently. do it few times.Rest the phone for 10 mins (put it on a flat surface). You can now notice those self typing things gone and screen getting stabilized.\\nThen, reset the hardware (hold the power and home button till the screen goes off and comes back with apple logo). release the buttons when you see this.\\nThen, connect to your laptop and log in to iTunes and reset your phone completely. (please take a back-up first).\\nAnd your phone should be good to use again.\\nWhat really happened here for me is that the sensors might have stuck to the screen and with mild twisting, they got disengaged/released.\\nI posted this in Apple Community and the moderators deleted it, for the best reasons known to them.\\nInstead of throwing away your phone (or selling cheaply), try this and you could be saving your phone.\\nLet me know how it goes.\\n\\n\\n\\n### Other Answer\\nIt was the charging cord! I bought a gas station braided cord and it was the culprit. Once I plugged my OEM cord into the phone the GHOSTS went away.\\n\\n\\n\\n### Other Answer\\nI\\\'ve same issue that I just get resolved. I first tried to restore it from iCloud back, however it was not a software issue or any virus issue, so after restore same problem continues. Then I get my phone to local area iphone repairing lab, and they detected that it is an LCD issue. LCD get out of order without any reason (It was neither hit or nor slipped, but LCD get out of order all and sudden, while using it) it started opening things at random. I get LCD replaced with new one, that cost me $80.00 in total ($70.00 LCD charges + $10.00 as labor charges to fix it). iPhone is back to perfect mode now. It was iphone 6s. Thanks.\\n\\n\\n\\n### Other Answer\\nI was having the same issue with my 6 plus, I took it to a repair shop, they opened the phone, disconnected the three ribbons the screen has, blew up and cleaned the connectors and connected the screen again and it solved the issue it\'s hardware, not software.\\n\\n\\n\\n### Other Answer\\nHey.\\nJust had this problem now. As it turns out, you just need to plug in your phone. I use a case and when I took it off I noticed that there was a lot of dust and dirt around the areas that the case didn\\\'t cover. I shined a light in my ports and noticed they were filled with dust. Tomorrow I plan on using pressurized air to clean it out and the problem should be solved. If you plug in your phone and unplug it and it stops the issue, I recommend cleaning your phone thoroughly.\\n\\n\\n\\n### Other Answer\\nI simply changed the power supply and problem was gone. The block that plugs in the wall not the sub cord. The cord was fine but not the block.\\n\\n\\n\\n### Other Answer\\nSomeone ask! I purchased my iPhone 6s Plus for 1000 from at&t. Before I touched it, I purchased a otter defender case. I read where at&t said touch desease was due to dropping! Bullshit!! I am 56 I have never dropped it!! Looks brand new! Never dropped or abused any way! I have my original charger. I am going to clean it and try everyone\'s advice. It really sucks! I had 40,000,000 on my heart of Vegas slots! I play every day. I would be spinning and my fingers were no where max buttons and it would light up and switch to max. It did it 3 times before I caught it light up by its self. It sucks. Hope I can fix it!!!!\\n\\n\\n\\n### Other Answer\\nNo answer, but same problem with iPhone 6 plus--random, self-generated jumping amongst apps and typing on its own--plus freezing regularly (aha--maybe that\\\'s what the ""plus"" in ""6 plus"" refers to?). An Apple Genius recommended upgrading to iOS 11.3.1 from 11.2.2, to see if that fixed the trouble. If it didn\\\'t, Apple will sell me a new phone for $168! Of couese the OS upgrade didn\\\'t fix the problem. Thanks for helping me figure out that it\\\'s most likely a hardware problem--which the ""genius"" probably knows too.\\nI\\\'m getting ready to go Android.\\n\\n\\n\\n### Other Answer\\nI experienced similar ghost touches. Two weeks ago, I changed my iPhone 6 Plus shell (I had forced the phone into it because it\'s pretty tight), and also put a new glass screen protector (the edges of the protector don\'t stick to the screen, weird, so I brushed pressure on the edges at times to see if they may smooth out one day miraculously). I\'m not sure if I accidentally bend the phone when I installed the shell, or, if I got a defective glass protector that messes up the touch sensor. Well, yesterday was the worse day, keeps dropping calls and ghost pressing keys for me when I was on a call. I got fed up, so I removed the screen protector, and so far problems have not reoccurred yet. I\'m crossing my fingers that problems indeed solved.\\n\\n\\n\\n### Other Answer\\nthank you so much for this post! i was struggling doing the reset because i cannot type userids and passwords correctly because the iphone 6 plus i have kept on typing letters incorrectly. I have been doing it for a day until i come across this article. Very helpful! God bless you!!\\n\\n\\n\\n### Other Answer\\nI just turned it off, and turned it back on.\\n\\n\\n\\n### Other Answer\\nMy problem has not gone away completely but its better now i changed my charger and turned off prediction ....,,,now it rarely happens\\n\\n\\n\\n### Other Answer\\nI tried all of the above. I then turned off my home cleaned it with isopropyl alcohol 90%. Then I baked it in my oven on warm for an hour and a half over foil. Took it out and set it cool completely on the glass top stove. Then I turned on and it worked.\\n\\n\\n\\n### Other Answer\\nI think at& t should man up and fix your phone for free! You pay a lot for a Apple they should back it. I did the next 30 month payments and finally have it paid off in June. My iPad sept. Looking forward to a almost 100 drop in my phone bill! Now this crap!!! Really\\n\\n\\n\\n### Other Answer\\nIf your phone is JailBroken, suggest downloading a virus. While all my symptoms were similar, there was indeed a virus/malware on the phone which allowed for remote control of my iphone (even while in lock mode). My mistake for buying a third party iphone i suppose. Anyway i have since had the phone restored to factory and everything is working as expected for now. I will of course keep you posted if this changes. Thanks to all for the helpful posts, really helped me narrow a few things down.\\n\\n\\n\\n### Other Answer\\nWhen my phone was doing this, it ended up being the screen protector that i got from 5 below. I took it off and it stopped. I ordered more protectors from amazon and replaced it\\n\\n\\n\\n### Other Answer\\niPhone 6 Plus first generation.I had the same issues as all above, apps opening by themselves, self typing, ultra sensitive screen, items jumping around all over.it even called someone on FaceTime twice by itself when I was not in the room..I thought the phone was toast and i\'d have to buy a new one took me a while to figure out but it was the extra cheap block plug I bought at a dollar store for convenience of an extra charging station when I move around the house from den to living room..cord was fine but bought a new Apple brand block plugno more problems works just fine now. This issue was a recent event so had to narrow things down to what had changed recently to my phone so I could figure it out.\\nI even had the same problem on a laptop with documents opening up by themselves..a laptop that was plugged in to the same wall plug as my phone charger with the dollar store block plug.until I changed the block plug.\\n\\n\\n\\n### Other Answer\\nHad the problem: Inherited a 6s Plus from my wife. She had no problem with it.\\nLooks like it was merely the cheap phone case I purchased on Amazon. It was either pinching the edges or torquing the screen/body of the phone. Problem solved.\\n\\n\\n\\n### Other Answer\\nI bought my phone on march 6 and it was a brand new, but It sucks me uo because it freezing, shaking and control by itself. I went to the store where I bought this and I told them to replacr it, but they told me I have to pay it because Its about lcd issue. Please help me what other ways to fix it. Or should I try to remove the screen or should I follow your step above.\\n\\n\\n\\n### Other Answer\\nI tried everything and it seems to come back to needing the original iPhone cableor at least another 1 that would have come with another iPhonenot the $5 Store fast charging cables. My original cable is pretty beat up - like most that I see - but I\'ve been beaten up much MUCH less by sticking with its use! I didn\'t find that the casing/shell around it or not made any diff.\\n\\n\\n\\n### Other Answer\\ngreat now I have to wait one more hour to reset my phone and while I was tryin to connect my phone to my computer the computer also restarted smh does anyone else knows how I can get my phone to work my problem is I have a black dot on the bottom left of my screen an it wont allow me to touch a certain part of my screen unless I rotate my phone and I know the password but the first number is a 2 and it won\\\'t let me touch 1,2, or 3 so now I have to find a way to get rid of my password and all of a sudden my phone wants to touch stuff on its own which got my phone disabled many times to the point where I have to wait a whole hour and I really need to finish something on my phone today PLEASE HELPPPP\\n\\n\\n\\n### Other Answer\\nIn my case , iphone 6 screen was faulty. I got it replaced at local repair shop, so far phone is working fine.\\n\\n\\n\\n### Other Answer\\nthis problem in iphone 6 has many different scenarios and solutions, first try to reconnect the lcd screen to the motherboard again, if didnt solve, try to replace the lcd connector on the motherboard, if not solved, then remains two issues, lcd screen it self or touch IC. in my country some repair shops just change them all for almost 40$ since they dont want to troubleshoot one by one. readers of this comment also should know that partial screen not responding in other iphone models might also have an issue in LCD connector on the motherboard, specially if you lock/unlock screen and screen works again for sometime. lcd connectors gets disconnected lightly from the motherboard due to multiple falls and hits after sometime. best of luck for all\\n\\n\\n\\n### Other Answer\\nI am facing the same issue whereby these ghost touches type and open apps , I am using an original Iphone cable , how to I fix this issue.\\n\\n\\n\\n### Other Answer\\nThere were two issues with the phone I had troubles with. It was my dads and turns out he carried it in his pocket. The phone itself had a little bend in it as a result. A little pressure in the opposite direction helped the issue. But it also had a tiny crack in the screen which wasnt obvious, once we added a screen protector this fixed the issues entirely.\\n\\n\\n\\n### Other Answer\\nI had the same problem with my 64Gb iPhone 6+. Tried a lot of things and eventually downloaded all my images and videos to my PC and restarted the phone - problem solved. Been working now for two days.\', lookup_str=\'\', metadata={\'source\': \' \'title\': \'My iPhone 6 is typing and opening apps by itself\'}, lookup_index=0)] ``` ```python loader = IFixitLoader("" data = loader.load() ``` ```python data ``` ```text [Document(page_content=""Standard iPad\\nThe standard edition of the tablet computer made by Apple.\\n== Background Information ==\\n\\nOriginally introduced in January 2010, the iPad is Apple\'s standard edition of their tablet computer. In total, there have been ten generations of the standard edition of the iPad.\\n\\n== Additional Information ==\\n\\n* [link| Apple Product Page]\\n* [link| iPad Wikipedia]"", lookup_str=\'\', metadata={\'source\': \' \'title\': \'Standard iPad\'}, lookup_index=0)] ``` ## Searching iFixit using /suggest If you\'re looking for a more general way to search iFixit based on a keyword or phrase, the /suggest endpoint will return content related to the search term, then the loader will load the content from each of the suggested items and prep and return the documents. ```python data = IFixitLoader.load_suggestions(""Banana"") ``` ```python data ``` ```text [Document(page_content=\'Banana\\nTasty fruit. Good source of potassium. Yellow.\\n== Background Information ==\\n\\nCommonly misspelled, this wildly popular, phone shaped fruit serves as nutrition and an obstacle to slow down vehicles racing close behind you. Also used commonly as a synonym for crazy or insane.\\n\\nBotanically, the banana is considered a berry, although it isn\'t included in the culinary berry category containing strawberries and raspberries. Belonging to the genus Musa, the banana originated in Southeast Asia and Australia. Now largely cultivated throughout South and Central America, bananas are largely available throughout the world. They are especially valued as a staple food group in developing countries due to the banana tree\'s ability to produce fruit year round.\\n\\nThe banana can be easily opened. Simply remove the outer yellow shell by cracking the top of the stem. Then, with the broken piece, peel downward on each side until the fruity components on the inside are exposed. Once the shell has been removed it cannot be put back together.\\n\\n== Technical Specifications ==\\n\\n* Dimensions: Variable depending on genetics of the parent tree\\n* Color: Variable depending on ripeness, region, and season\\n\\n== Additional Information ==\\n\\n[link| Banana]\', lookup_str=\'\', metadata={\'source\': \' \'title\': \'Banana\'}, lookup_index=0), Document(page_content=""# Banana Teardown\\nIn this teardown, we open a banana to see what\'s inside. Yellow and delicious, but most importantly, yellow.\\n\\n\\n###Tools Required:\\n\\n - Fingers\\n\\n - Teeth\\n\\n - Thumbs\\n\\n\\n###Parts Required:\\n\\n - None\\n\\n\\n## Step 1\\nTake one banana from the bunch.\\nDon\'t squeeze too hard!\\n\\n\\n## Step 2\\nHold the banana in your left hand and grip the stem between your right thumb and forefinger.\\n\\n\\n## Step 3\\nPull the stem downward until the peel splits.\\n\\n\\n## Step 4\\nInsert your thumbs into the split of the peel and pull the two sides apart.\\nExpose the top of the banana. It may be slightly squished from pulling on the stem, but this will not affect the flavor.\\n\\n\\n## Step 5\\nPull open the peel, starting from your original split, and opening it along the length of the banana.\\n\\n\\n## Step 6\\nRemove fruit from peel.\\n\\n\\n## Step 7\\nEat and enjoy!\\nThis is where you\'ll need your teeth.\\nDo not choke on banana!\\n"", lookup_str=\'\', metadata={\'source\': \' \'title\': \'Banana Teardown\'}, lookup_index=0)] ``` - [Searching iFixit using /suggest](#searching-ifixit-using-suggest)']","To save the configuration for a given LLM (Large Language Model), you can create a new object with the desired configuration and then use that object as needed. Here's a simple example using the `ConfigurableField` and `configurable_alternatives` methods:

```python
from langchain.chat_models import ChatAnthropic, ChatOpenAI
from langchain.schema.runnable import ConfigurableField

# Define the LLM with alternatives
llm = ChatAnthropic(temperature=0).configurable_alternatives(
    ConfigurableField(id=""llm"", 
                      default_key=""anthropic"", 
                      openai=ChatOpenAI(), 
                      gpt4=ChatOpenAI(model=""gpt-4"")
                     )
)

# Create a configured LLM object
configured_llm = llm.with_config(configurable={""llm"": ""openai""})

# Now you can use `configured_llm` as needed
```

In this example, `configured_llm` is an object that has been configured to use `ChatOpenAI` instead of the default `ChatAnthropic`. You can now use this configured LLM in your applications.

Remember to refer to the official documentation for the most up-to-date and detailed information on configuring LLMs.","To save the configuration for a given LLM, you can follow these steps:

First, let's go over loading an LLM configuration from disk. LLM configurations can be saved in two formats: JSON or YAML, but they are loaded in the same way.

For a JSON-formatted LLM configuration (llm.json):

```json
{
    ""model_name"": ""text-davinci-003"",
    ""temperature"": 0.7,
    ""max_tokens"": 256,
    ""top_p"": 1.0,
    ""frequency_penalty"": 0.0,
    ""presence_penalty"": 0.0,
    ""n"": 1,
    ""best_of"": 1,
    ""request_timeout"": null,
    ""_type"": ""openai""
}
```

Load the configuration like this:

```python
from langchain.llms.loading import load_llm

llm = load_llm(""llm.json"")
```

For a YAML-formatted LLM configuration (llm.yaml):

```yaml
_type: openai
best_of: 1
frequency_penalty: 0.0
max_tokens: 256
model_name: text-davinci-003
n: 1
presence_penalty: 0.0
request_timeout: null
temperature: 0.7
top_p: 1.0
```

Load the configuration like this:

```python
from langchain.llms.loading import load_llm

llm = load_llm(""llm.yaml"")
```

To save an LLM configuration from memory to a serialized version, you can use the `.save` method. This method supports both JSON and YAML formats:

```python
llm.save(""llm.json"")
llm.save(""llm.yaml"")
```

This way, you can easily save and load LLM configurations to and from disk.",0.99999999995,,,0.29436489775753666,0.22284122562674097
36,"Show me an example using Weaviate, but customizing the VectorStoreRetriever to return the top 10 k nearest neighbors. ","['langchain.vectorstores.weaviate.Weaviate LangChain 0.0.337 API Experimental Python Docs Toggle Menu PrevUp Next LangChain 0.0.337 langchain.vectorstores.weaviate.Weaviate langchain.vectorstores.weaviate.Weaviate class langchain.vectorstores.weaviate.Weaviate(client: ~typing.Any, index_name: str, text_key: str, embedding: ~typing.Optional[~langchain.schema.embeddings.Embeddings] = None, attributes: ~typing.Optional[~typing.List[str]] = None, relevance_score_fn: ~typing.Optional[~typing.Callable[[float], float]] = , by_text: bool = True)[source] Weaviate vector store. To use, you should have the weaviate-client python package installed. Example import weaviate from langchain.vectorstores import Weaviate client = weaviate.Client(url=os.environ[""WEAVIATE_URL""], ...) weaviate = Weaviate(client, index_name, text_key) Initialize with Weaviate client. Attributes embeddings Access the query embedding object if available. Methods __init__(client,index_name,text_key[,...]) Initialize with Weaviate client. aadd_documents(documents,**kwargs) Run more documents through the embeddings and add to the vectorstore. aadd_texts(texts[,metadatas]) Run more texts through the embeddings and add to the vectorstore. add_documents(documents,**kwargs) Run more documents through the embeddings and add to the vectorstore. add_texts(texts[,metadatas]) Upload texts with metadata (properties) to Weaviate. adelete([ids]) Delete by vector ID or other criteria. afrom_documents(documents,embedding,**kwargs) Return VectorStore initialized from documents and embeddings. afrom_texts(texts,embedding[,metadatas]) Return VectorStore initialized from texts and embeddings. amax_marginal_relevance_search(query[,k,...]) Return docs selected using the maximal marginal relevance. amax_marginal_relevance_search_by_vector(...) Return docs selected using the maximal marginal relevance. as_retriever(**kwargs) Return VectorStoreRetriever initialized from this VectorStore. asearch(query,search_type,**kwargs) Return docs most similar to query using specified search type. asimilarity_search(query[,k]) Return docs most similar to query. asimilarity_search_by_vector(embedding[,k]) Return docs most similar to embedding vector. asimilarity_search_with_relevance_scores(query) Return docs and relevance scores in the range [0, 1], asynchronously. asimilarity_search_with_score(*args,**kwargs) Run similarity search with distance asynchronously. delete([ids]) Delete by vector IDs. from_documents(documents,embedding,**kwargs) Return VectorStore initialized from documents and embeddings. from_texts(texts,embedding[,metadatas,...]) Construct Weaviate wrapper from raw documents. max_marginal_relevance_search(query[,k,...]) Return docs selected using the maximal marginal relevance. max_marginal_relevance_search_by_vector(...) Return docs selected using the maximal marginal relevance. search(query,search_type,**kwargs) Return docs most similar to query using specified search type. similarity_search(query[,k]) Return docs most similar to query. similarity_search_by_text(query[,k]) Return docs most similar to query. similarity_search_by_vector(embedding[, k]) Look up similar documents by embedding vector in Weaviate. similarity_search_with_relevance_scores(query) Return docs and relevance scores in the range [0, 1]. similarity_search_with_score(query[, k]) Return list of documents most similar to the query text and cosine distance in float for each. __init__(client: ~typing.Any, index_name: str, text_key: str, embedding: ~typing.Optional[~langchain.schema.embeddings.Embeddings] = None, attributes: ~typing.Optional[~typing.List[str]] = None, relevance_score_fn: ~typing.Optional[~typing.Callable[[float], float]] = , by_text: bool = True)[source] Initialize with Weaviate client. async aadd_documents(documents: List[Document], **kwargs: Any)  List[str] Run more documents through the embeddings and add to the vectorstore. Parameters (List[Document] (documents)  Documents to add to the vectorstore. Returns List of IDs of the added texts. Return type List[str] async aadd_texts(texts: Iterable[str], metadatas: Optional[List[dict]] = None, **kwargs: Any)  List[str] Run more texts through the embeddings and add to the vectorstore. add_documents(documents: List[Document], **kwargs: Any)  List[str] Run more documents through the embeddings and add to the vectorstore. Parameters (List[Document] (documents)  Documents to add to the vectorstore. Returns List of IDs of the added texts. Return type List[str] add_texts(texts: Iterable[str], metadatas: Optional[List[dict]] = None, **kwargs: Any)  List[str][source] Upload texts with metadata (properties) to Weaviate. async adelete(ids: Optional[List[str]] = None, **kwargs: Any)  Optional[bool] Delete by vector ID or other criteria. Parameters ids  List of ids to delete. **kwargs  Other keyword arguments that subclasses might use. Returns True if deletion is successful, False otherwise, None if not implemented. Return type Optional[bool] async classmethod afrom_documents(documents: List[Document], embedding: Embeddings, **kwargs: Any)  VST Return VectorStore initialized from documents and embeddings. async classmethod afrom_texts(texts: List[str], embedding: Embeddings, metadatas: Optional[List[dict]] = None, **kwargs: Any)  VST Return VectorStore initialized from texts and embeddings. async amax_marginal_relevance_search(query: str, k: int = 4, fetch_k: int = 20, lambda_mult: float = 0.5, **kwargs: Any)  List[Document] Return docs selected using the maximal marginal relevance. async amax_marginal_relevance_search_by_vector(embedding: List[float], k: int = 4, fetch_k: int = 20, lambda_mult: float = 0.5, **kwargs: Any)  List[Document] Return docs selected using the maximal marginal relevance. as_retriever(**kwargs: Any)  VectorStoreRetriever Return VectorStoreRetriever initialized from this VectorStore. Parameters search_type (Optional[str])  Defines the type of search that the Retriever should perform. Can be similarity (default), mmr, or similarity_score_threshold. search_kwargs (Optional[Dict])  Keyword arguments to pass to the search function. Can include things like: k: Amount of documents to return (Default: 4) score_threshold: Minimum relevance threshold for similarity_score_threshold fetch_k: Amount of documents to pass to MMR algorithm (Default: 20) lambda_mult: Diversity of results returned by MMR; 1 for minimum diversity and 0 for maximum. (Default: 0.5) filter: Filter by document metadata Returns Retriever class for VectorStore. Return type VectorStoreRetriever Examples: # Retrieve more documents with higher diversity # Useful if your dataset has many similar documents docsearch.as_retriever( search_type=""mmr"", search_kwargs={\'k\': 6, \'lambda_mult\': 0.25} ) # Fetch more documents for the MMR algorithm to consider # But only return the top 5 docsearch.as_retriever( search_type=""mmr"", search_kwargs={\'k\': 5, \'fetch_k\': 50} ) # Only retrieve documents that have a relevance score # Above a certain threshold docsearch.as_retriever( search_type=""similarity_score_threshold"", search_kwargs={\'score_threshold\': 0.8} ) # Only get the single most similar document from the dataset docsearch.as_retriever(search_kwargs={\'k\': 1}) # Use a filter to only retrieve documents from a specific paper docsearch.as_retriever( search_kwargs={\'filter\': {\'paper_title\':\'GPT-4 Technical Report\'}} ) async asearch(query: str, search_type: str, **kwargs: Any)  List[Document] Return docs most similar to query using specified search type. async asimilarity_search(query: str, k: int = 4, **kwargs: Any)  List[Document] Return docs most similar to query. async asimilarity_search_by_vector(embedding: List[float], k: int = 4, **kwargs: Any)  List[Document] Return docs most similar to embedding vector. async asimilarity_search_with_relevance_scores(query: str, k: int = 4, **kwargs: Any)  List[Tuple[Document, float]] Return docs and relevance scores in the range [0, 1], asynchronously. 0 is dissimilar, 1 is most similar. Parameters query  input text k  Number of Documents to return. Defaults to 4. **kwargs  kwargs to be passed to similarity search. Should include: score_threshold: Optional, a floating point value between 0 to 1 to filter the resulting set of retrieved docs Returns List of Tuples of (doc, similarity_score) async asimilarity_search_with_score(*args: Any, **kwargs: Any)  List[Tuple[Document, float]] Run similarity search with distance asynchronously. delete(ids: Optional[List[str]] = None, **kwargs: Any)  None[source] Delete by vector IDs. Parameters ids  List of ids to delete. classmethod from_documents(documents: List[Document], embedding: Embeddings, **kwargs: Any)  VST Return VectorStore initialized from documents and embeddings. classmethod from_texts(texts: List[str], embedding: Embeddings, metadatas: Optional[List[dict]] = None, *, client: Optional[weaviate.Client] = None, weaviate_url: Optional[str] = None, weaviate_api_key: Optional[str] = None, batch_size: Optional[int] = None, index_name: Optional[str] = None, text_key: str = \'text\', by_text: bool = False, relevance_score_fn: Optional[Callable[[float], float]] = , **kwargs: Any)  Weaviate[source] Construct Weaviate wrapper from raw documents. This is a user-friendly interface that: Embeds documents. Creates a new index for the embeddings in the Weaviate instance. Adds the documents to the newly created Weaviate index. This is intended to be a quick way to get started. Parameters texts  Texts to add to vector store. embedding  Text embedding model to use. metadatas  Metadata associated with each text. client  weaviate.Client to use. weaviate_url  The Weaviate URL. If using Weaviate Cloud Services get it from the Details tab. Can be passed in as a named param or by setting the environment variable WEAVIATE_URL. Should not be specified if client is provided. weaviate_api_key  The Weaviate API key. If enabled and using Weaviate Cloud Services, get it from Details tab. Can be passed in as a named param or by setting the environment variable WEAVIATE_API_KEY. Should not be specified if client is provided. batch_size  Size of batch operations. index_name  Index name. text_key  Key to use for uploading/retrieving text to/from vectorstore. by_text  Whether to search by text or by embedding. relevance_score_fn  Function for converting whatever distance function the vector store uses to a relevance score, which is a normalized similarity score (0 means dissimilar, 1 means similar). **kwargs  Additional named parameters to pass to Weaviate.__init__(). Example from langchain.embeddings import OpenAIEmbeddings from langchain.vectorstores import Weaviate embeddings = OpenAIEmbeddings() weaviate = Weaviate.from_texts( texts, embeddings, weaviate_url="" ) max_marginal_relevance_search(query: str, k: int = 4, fetch_k: int = 20, lambda_mult: float = 0.5, **kwargs: Any)  List[Document][source] Return docs selected using the maximal marginal relevance. Maximal marginal relevance optimizes for similarity to query AND diversity among selected documents. Parameters query  Text to look up documents similar to. k  Number of Documents to return. Defaults to 4. fetch_k  Number of Documents to fetch to pass to MMR algorithm. lambda_mult  Number between 0 and 1 that determines the degree of diversity among the results with 0 corresponding to maximum diversity and 1 to minimum diversity. Defaults to 0.5. Returns List of Documents selected by maximal marginal relevance. max_marginal_relevance_search_by_vector(embedding: List[float], k: int = 4, fetch_k: int = 20, lambda_mult: float = 0.5, **kwargs: Any)  List[Document][source] Return docs selected using the maximal marginal relevance. Maximal marginal relevance optimizes for similarity to query AND diversity among selected documents. Parameters embedding  Embedding to look up documents similar to. k  Number of Documents to return. Defaults to 4. fetch_k  Number of Documents to fetch to pass to MMR algorithm. lambda_mult  Number between 0 and 1 that determines the degree of diversity among the results with 0 corresponding to maximum diversity and 1 to minimum diversity. Defaults to 0.5. Returns List of Documents selected by maximal marginal relevance. search(query: str, search_type: str, **kwargs: Any)  List[Document] Return docs most similar to query using specified search type. similarity_search(query: str, k: int = 4, **kwargs: Any)  List[Document][source] Return docs most similar to query. Parameters query  Text to look up documents similar to. k  Number of Documents to return. Defaults to 4. Returns List of Documents most similar to the query. similarity_search_by_text(query: str, k: int = 4, **kwargs: Any)  List[Document][source] Return docs most similar to query. Parameters query  Text to look up documents similar to. k  Number of Documents to return. Defaults to 4. Returns List of Documents most similar to the query. similarity_search_by_vector(embedding: List[float], k: int = 4, **kwargs: Any)  List[Document][source] Look up similar documents by embedding vector in Weaviate. similarity_search_with_relevance_scores(query: str, k: int = 4, **kwargs: Any)  List[Tuple[Document, float]] Return docs and relevance scores in the range [0, 1]. 0 is dissimilar, 1 is most similar. Parameters query  input text k  Number of Documents to return. Defaults to 4. **kwargs  kwargs to be passed to similarity search. Should include: score_threshold: Optional, a floating point value between 0 to 1 to filter the resulting set of retrieved docs Returns List of Tuples of (doc, similarity_score) similarity_search_with_score(query: str, k: int = 4, **kwargs: Any)  List[Tuple[Document, float]][source] Return list of documents most similar to the query text and cosine distance in float for each. Lower score represents more similarity. Examples using Weaviate Weaviate  2023, Harrison Chase. Last updated on Nov 17, 2023. Show this page source', ""Retrievers | Retrievers [ Activeloop Deep MemoryActiveloop Deep Memory is a suite of tools that enables you to optimize your Vector Store for your use-case and achieve higher accuracy in your LLM apps.](/docs/integrations/retrievers/Activeloop DeepMemory+LangChain)[ Amazon KendraAmazon Kendra is an intelligent search service provided by Amazon Web Services (AWS). It utilizes advanced natural language processing (NLP) and machine learning algorithms to enable powerful search capabilities across various data sources within an organization. Kendra is designed to help users find the information they need quickly and accurately, improving productivity and decision-making.](/docs/integrations/retrievers/amazon_kendra_retriever)[ Arcee RetrieverThis notebook demonstrates how to use the ArceeRetriever class to retrieve relevant document(s) for Arcee's Domain Adapted Language Models (DALMs).](/docs/integrations/retrievers/arcee)[ ArxivarXiv is an open-access archive for 2 million scholarly articles in the fields of physics, mathematics, computer science, quantitative biology, quantitative finance, statistics, electrical engineering and systems science, and economics.](/docs/integrations/retrievers/arxiv)[ Azure Cognitive SearchAzure Cognitive Search (formerly known as Azure Search) is a cloud search service that gives developers infrastructure, APIs, and tools for building a rich search experience over private, heterogeneous content in web, mobile, and enterprise applications.](/docs/integrations/retrievers/azure_cognitive_search)[ BM25BM25 also known as the Okapi BM25, is a ranking function used in information retrieval systems to estimate the relevance of documents to a given search query.](/docs/integrations/retrievers/bm25)[ ChaindeskChaindesk platform brings data from anywhere (Datsources: Text, PDF, Word, PowerPpoint, Excel, Notion, Airtable, Google Sheets, etc..) into Datastores (container of multiple Datasources).](/docs/integrations/retrievers/chaindesk)[ ChatGPT PluginOpenAI plugins connect ChatGPT to third-party applications. These plugins enable ChatGPT to interact with APIs defined by developers, enhancing ChatGPT's capabilities and allowing it to perform a wide range of actions.](/docs/integrations/retrievers/chatgpt-plugin)[ Cohere RerankerCohere is a Canadian startup that provides natural language processing models that help companies improve human-machine interactions.](/docs/integrations/retrievers/cohere-reranker)[ Cohere RAG retrieverThis notebook covers how to get started with Cohere RAG retriever. This allows you to leverage the ability to search documents over various connectors or by supplying your own.](/docs/integrations/retrievers/cohere)[ DocArrayDocArray is a versatile, open-source tool for managing your multi-modal data. It lets you shape your data however you want, and offers the flexibility to store and search it using various document index backends. Plus, it gets even better - you can utilize your DocArray document index to create a DocArrayRetriever, and build awesome Langchain apps!](/docs/integrations/retrievers/docarray_retriever)[ ElasticSearch BM25Elasticsearch is a distributed, RESTful search and analytics engine. It provides a distributed, multitenant-capable full-text search engine with an HTTP web interface and schema-free JSON documents.](/docs/integrations/retrievers/elastic_search_bm25)[ Fleet AI Libraries ContextThe Fleet AI team is on a mission to embed the world's most important data. They've started by embedding the top 1200 Python libraries to enable code generation with up-to-date knowledge. They've been kind enough to share their embeddings of the LangChain docs and API reference.](/docs/integrations/retrievers/fleet_context)[ Google DriveThis notebook covers how to retrieve documents from Google Drive.](/docs/integrations/retrievers/google_drive)[ Google Vertex AI SearchVertex AI Search (formerly known as Enterprise Search on Generative AI App Builder) is a part of the Vertex AI machine learning platform offered by Google Cloud.](/docs/integrations/retrievers/google_vertex_ai_search)[ Kay.aiData API built for RAG We are curating the world's largest datasets as high-quality embeddings so your AI agents can retrieve context on the fly. Latest models, fast retrieval, and zero infra.](/docs/integrations/retrievers/kay)[ kNNIn statistics, the k-nearest neighbors algorithm (k-NN) is a non-parametric supervised learning method first developed by Evelyn Fix and Joseph Hodges in 1951, and later expanded by Thomas Cover. It is used for classification and regression.](/docs/integrations/retrievers/knn)[ LOTR (Merger Retriever)Lord of the Retrievers, also known as MergerRetriever, takes a list of retrievers as input and merges the results of their getrelevantdocuments() methods into a single list. The merged results will be a list of documents that are relevant to the query and that have been ranked by the different retrievers.](/docs/integrations/retrievers/merger_retriever)[ MetalMetal is a managed service for ML Embeddings.](/docs/integrations/retrievers/metal)[ Pinecone Hybrid SearchPinecone is a vector database with broad functionality.](/docs/integrations/retrievers/pinecone_hybrid_search)[ PubMedPubMed by The National Center for Biotechnology Information, National Library of Medicine comprises more than 35 million citations for biomedical literature from MEDLINE, life science journals, and online books. Citations may include links to full text content from PubMed Central and publisher web sites.](/docs/integrations/retrievers/pubmed)[ RePhraseQueryRePhraseQuery is a simple retriever that applies an LLM between the user input and the query passed by the retriever.](/docs/integrations/retrievers/re_phrase)[ SEC filingThe SEC filing is a financial statement or other formal document submitted to the U.S. Securities and Exchange Commission (SEC). Public companies, certain insiders, and broker-dealers are required to make regular SEC filings. Investors and financial professionals rely on these filings for information about companies they are evaluating for investment purposes.](/docs/integrations/retrievers/sec_filings)[ Self-querying retriever14 items](/docs/integrations/retrievers/self_query/)[ SingleStoreDBSingleStoreDB is a high-performance distributed SQL database that supports deployment both in the cloud and on-premises. It provides vector storage, and vector functions including dotproduct and euclideandistance, thereby supporting AI applications that require text similarity matching.](/docs/integrations/retrievers/singlestoredb)[ SVMSupport vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.](/docs/integrations/retrievers/svm)[ Tavily Search APITavily's Search API is a search engine built specifically for AI agents (LLMs), delivering real-time, accurate, and factual results at speed.](/docs/integrations/retrievers/tavily)[ TF-IDFTF-IDF means term-frequency times inverse document-frequency.](/docs/integrations/retrievers/tf_idf)[ VespaVespa is a fully featured search engine and vector database. It supports vector search (ANN), lexical search, and search in structured data, all in the same query.](/docs/integrations/retrievers/vespa)[ Weaviate Hybrid SearchWeaviate is an open-source vector database.](/docs/integrations/retrievers/weaviate-hybrid)[ WikipediaWikipedia is a multilingual free online encyclopedia written and maintained by a community of volunteers, known as Wikipedians, through open collaboration and using a wiki-based editing system called MediaWiki. Wikipedia is the largest and most-read reference work in history.](/docs/integrations/retrievers/wikipedia)[ you-retrieverUsing the You.com Retriever](/docs/integrations/retrievers/you-retriever)[ ZepRetriever Example for Zep](/docs/integrations/retrievers/zep_memorystore)"", 'Weaviate | Weaviate [Weaviate]( is an open-source vector database. It allows you to store data objects and vector embeddings from your favorite ML models, and scale seamlessly into billions of data objects. In the notebook, we\'ll demo the `SelfQueryRetriever` wrapped around a `Weaviate` vector store. ## Creating a Weaviate vector store First we\'ll want to create a Weaviate vector store and seed it with some data. We\'ve created a small demo set of documents that contain summaries of movies. **Note:** The self-query retriever requires you to have `lark` installed (`pip install lark`). We also need the `weaviate-client` package. ```python #!pip install lark weaviate-client ``` ```python from langchain.embeddings.openai import OpenAIEmbeddings from langchain.schema import Document from langchain.vectorstores import Weaviate embeddings = OpenAIEmbeddings() ``` ```python docs = [ Document( page_content=""A bunch of scientists bring back dinosaurs and mayhem breaks loose"", metadata={""year"": 1993, ""rating"": 7.7, ""genre"": ""science fiction""}, ), Document( page_content=""Leo DiCaprio gets lost in a dream within a dream within a dream within a ..."", metadata={""year"": 2010, ""director"": ""Christopher Nolan"", ""rating"": 8.2}, ), Document( page_content=""A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea"", metadata={""year"": 2006, ""director"": ""Satoshi Kon"", ""rating"": 8.6}, ), Document( page_content=""A bunch of normal-sized women are supremely wholesome and some men pine after them"", metadata={""year"": 2019, ""director"": ""Greta Gerwig"", ""rating"": 8.3}, ), Document( page_content=""Toys come alive and have a blast doing so"", metadata={""year"": 1995, ""genre"": ""animated""}, ), Document( page_content=""Three men walk into the Zone, three men walk out of the Zone"", metadata={ ""year"": 1979, ""director"": ""Andrei Tarkovsky"", ""genre"": ""science fiction"", ""rating"": 9.9, }, ), ] vectorstore = Weaviate.from_documents( docs, embeddings, weaviate_url="" ) ``` ## Creating our self-querying retriever Now we can instantiate our retriever. To do this we\'ll need to provide some information upfront about the metadata fields that our documents support and a short description of the document contents. ```python from langchain.chains.query_constructor.base import AttributeInfo from langchain.llms import OpenAI from langchain.retrievers.self_query.base import SelfQueryRetriever metadata_field_info = [ AttributeInfo( name=""genre"", description=""The genre of the movie"", type=""string or list[string]"", ), AttributeInfo( name=""year"", description=""The year the movie was released"", type=""integer"", ), AttributeInfo( name=""director"", description=""The name of the movie director"", type=""string"", ), AttributeInfo( name=""rating"", description=""A 1-10 rating for the movie"", type=""float"" ), ] document_content_description = ""Brief summary of a movie"" llm = OpenAI(temperature=0) retriever = SelfQueryRetriever.from_llm( llm, vectorstore, document_content_description, metadata_field_info, verbose=True ) ``` ## Testing it out And now we can try actually using our retriever! ```python # This example only specifies a relevant query retriever.get_relevant_documents(""What are some movies about dinosaurs"") ``` ```text query=\'dinosaur\' filter=None limit=None [Document(page_content=\'A bunch of scientists bring back dinosaurs and mayhem breaks loose\', metadata={\'genre\': \'science fiction\', \'rating\': 7.7, \'year\': 1993}), Document(page_content=\'Toys come alive and have a blast doing so\', metadata={\'genre\': \'animated\', \'rating\': None, \'year\': 1995}), Document(page_content=\'Three men walk into the Zone, three men walk out of the Zone\', metadata={\'genre\': \'science fiction\', \'rating\': 9.9, \'year\': 1979}), Document(page_content=\'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\', metadata={\'genre\': None, \'rating\': 8.6, \'year\': 2006})] ``` ```python # This example specifies a query and a filter retriever.get_relevant_documents(""Has Greta Gerwig directed any movies about women"") ``` ```text query=\'women\' filter=Comparison(comparator=, attribute=\'director\', value=\'Greta Gerwig\') limit=None [Document(page_content=\'A bunch of normal-sized women are supremely wholesome and some men pine after them\', metadata={\'genre\': None, \'rating\': 8.3, \'year\': 2019})] ``` ## Filter k We can also use the self query retriever to specify `k`: the number of documents to fetch. We can do this by passing `enable_limit=True` to the constructor. ```python retriever = SelfQueryRetriever.from_llm( llm, vectorstore, document_content_description, metadata_field_info, enable_limit=True, verbose=True, ) ``` ```python # This example only specifies a relevant query retriever.get_relevant_documents(""what are two movies about dinosaurs"") ``` ```text query=\'dinosaur\' filter=None limit=2 [Document(page_content=\'A bunch of scientists bring back dinosaurs and mayhem breaks loose\', metadata={\'genre\': \'science fiction\', \'rating\': 7.7, \'year\': 1993}), Document(page_content=\'Toys come alive and have a blast doing so\', metadata={\'genre\': \'animated\', \'rating\': None, \'year\': 1995})] ``` - [Creating a Weaviate vector store](#creating-a-weaviate-vector-store) - [Creating our self-querying retriever](#creating-our-self-querying-retriever) - [Testing it out](#testing-it-out) - [Filter k](#filter-k)']","Certainly! Here's an example of how to customize the `VectorStoreRetriever` to return the top 10 nearest neighbors using Weaviate:

```python
from langchain.embeddings import OpenAIEmbeddings
from langchain.schema import Document
from langchain.vectorstores import Weaviate
from langchain.retrievers.vector_store import VectorStoreRetriever

# Initialize embeddings
embeddings = OpenAIEmbeddings()

# Sample documents
docs = [
    Document(page_content=""A bunch of scientists bring back dinosaurs and mayhem breaks loose"", metadata={""year"": 1993, ""rating"": 7.7, ""genre"": ""science fiction""}),
    Document(page_content=""Leo DiCaprio gets lost in a dream within a dream within a ..."", metadata={""year"": 2010, ""director"": ""Christopher Nolan"", ""rating"": 8.2}),
    Document(page_content=""A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea"", metadata={""year"": 2006, ""director"": ""Satoshi Kon"", ""rating"": 8.6}),
    Document(page_content=""A bunch of normal-sized women are supremely wholesome and some men pine after them"", metadata={""year"": 2019, ""director"": ""Greta Gerwig"", ""rating"": 8.3}),
    Document(page_content=""Toys come alive and have a blast doing so"", metadata={""year"": 1995, ""genre"": ""animated""}),
    Document(page_content=""Three men walk into the Zone, three men walk out of the Zone"", metadata={""year"": 1979, ""director"": ""Andrei Tarkovsky"", ""genre"": ""science fiction"", ""rating"": 9.9}),
]

# Create Weaviate vector store
vectorstore = Weaviate.from_documents(docs, embeddings, weaviate_url=""your_weaviate_url_here"")

# Initialize VectorStoreRetriever with k=10
retriever = VectorStoreRetriever(vectorstore=vectorstore, k=10)

# Example query
query = ""What are some movies about dinosaurs""

# Retrieve top 10 nearest neighbors
results = retriever.get_relevant_documents(query)

# Print results
for doc in results","To customize the Weaviate client and return the top 10 k nearest neighbors, you can utilize the `as_retriever` method with the appropriate parameters. Here's how you can achieve this:

```python
# Assuming you have imported the necessary modules and classes

# Create the Weaviate client
client = weaviate.Client(url=os.environ[""WEAVIATE_URL""], ...)

# Initialize the Weaviate wrapper
weaviate = Weaviate(client, index_name, text_key)

# Customize the client to return top 10 k nearest neighbors using as_retriever
custom_retriever = weaviate.as_retriever(
    search_type=""similarity"",
    search_kwargs={
        'k': 10  # Customize the value of k as needed
    }
)

# Now you can use the custom_retriever to perform searches
results = custom_retriever.search(query, ...)
```",0.8333333332916666,,0.0,0.03570169375565,0.15384615384615383
37,What is BabyAGI,"[""YouTube videos | YouTube videos icon marks a new addition [last update 2023-09-21] ### Official LangChain YouTube channel ### Introduction to LangChain with Harrison Chase, creator of LangChain - [Building the Future with LLMs, LangChain, & Pinecone]( by [Pinecone]( - [LangChain and Weaviate with Harrison Chase and Bob van Luijt - Weaviate Podcast #36]( by [Weaviate Vector Database]( - [LangChain Demo + Q&A with Harrison Chase]( by [Full Stack Deep Learning]( - [LangChain Agents: Build Personal Assistants For Your Data (Q&A with Harrison Chase and Mayo Oshin)]( by [Chat with data]( ## Videos (sorted by views) - [Using ChatGPT with YOUR OWN Data. This is magical. (LangChain OpenAI API)]( by [TechLead]( - [First look - ChatGPT + WolframAlpha (GPT-3.5 and Wolfram|Alpha via LangChain by James Weaver)]( by [Dr Alan D. Thompson]( - [LangChain explained - The hottest new Python framework]( by [AssemblyAI]( - [Chatbot with INFINITE MEMORY using OpenAI & Pinecone - GPT-3, Embeddings, ADA, Vector DB, Semantic]( by [David Shapiro ~ AI]( - [LangChain for LLMs is... basically just an Ansible playbook]( by [David Shapiro ~ AI]( - [Build your own LLM Apps with LangChain & GPT-Index]( by [1littlecoder]( - [BabyAGI - New System of Autonomous AI Agents with LangChain]( by [1littlecoder]( - [Run BabyAGI with Langchain Agents (with Python Code)]( by [1littlecoder]( - [How to Use Langchain With Zapier | Write and Send Email with GPT-3 | OpenAI API Tutorial]( by [StarMorph AI]( - [Use Your Locally Stored Files To Get Response From GPT - OpenAI | Langchain | Python]( by [Shweta Lodha]( - [Langchain JS | How to Use GPT-3, GPT-4 to Reference your own Data | OpenAI Embeddings Intro]( by [StarMorph AI]( - [The easiest way to work with large language models | Learn LangChain in 10min]( by [Sophia Yang]( - [4 Autonomous AI Agents: Westworld simulation BabyAGI, AutoGPT, Camel, LangChain]( by [Sophia Yang]( - [AI CAN SEARCH THE INTERNET? Langchain Agents + OpenAI ChatGPT]( by [tylerwhatsgood]( - [Query Your Data with GPT-4 | Embeddings, Vector Databases | Langchain JS Knowledgebase]( by [StarMorph AI]( - [Weaviate + LangChain for LLM apps presented by Erika Cardenas]( by [Weaviate Vector Database]( - [Langchain Overview How to Use Langchain & ChatGPT]( by [Python In Office]( - [Langchain Overview - How to Use Langchain & ChatGPT]( by [Python In Office]( - [LangChain Tutorials]( by [Edrick]( [LangChain, Chroma DB, OpenAI Beginner Guide | ChatGPT with your PDF]( - [LangChain 101: The Complete Beginner's Guide]( - [Custom langchain Agent & Tools with memory. Turn any Python function into langchain tool with Gpt 3]( by [echohive]( - [Building AI LLM Apps with LangChain (and more?) - LIVE STREAM]( by [Nicholas Renotte]( - [ChatGPT with any YouTube video using langchain and chromadb]( by [echohive]( - [How to Talk to a PDF using LangChain and ChatGPT]( by [Automata Learning Lab]( - [Langchain Document Loaders Part 1: Unstructured Files]( by [Merk]( - [LangChain - Prompt Templates (what all the best prompt engineers use)]( by [Nick Daigler]( - [LangChain. Crear aplicaciones Python impulsadas por GPT]( by [Jess Conde]( - [Easiest Way to Use GPT In Your Products | LangChain Basics Tutorial]( by [Rachel Woods]( - [BabyAGI + GPT-4 Langchain Agent with Internet Access]( by [tylerwhatsgood]( - [Learning LLM Agents. How does it actually work? LangChain, AutoGPT & OpenAI]( by [Arnoldas Kemeklis]( - [Get Started with LangChain in Node.js]( by [Developers Digest]( - [LangChain + OpenAI tutorial: Building a Q&A system w/ own text data]( by [Samuel Chan]( - [Langchain + Zapier Agent]( by [Merk]( - [Connecting the Internet with ChatGPT (LLMs) using Langchain And Answers Your Questions]( by [Kamalraj M M]( - [Build More Powerful LLM Applications for Business's with LangChain (Beginners Guide)]( by[ No Code Blackbox]( - [LangFlow LLM Agent Demo for LangChain]( by [Cobus Greyling]( - [Chatbot Factory: Streamline Python Chatbot Creation with LLMs and Langchain]( by [Finxter]( - [LangChain Tutorial - ChatGPT mit eigenen Daten]( by [Coding Crashkurse]( - [Chat with a CSV | LangChain Agents Tutorial (Beginners)]( by [GoDataProf]( - [Introduo ao Langchain - #Cortes - Live DataHackers]( by [Prof. Joo Gabriel Lima]( - [LangChain: Level up ChatGPT !? | LangChain Tutorial Part 1]( by [Code Affinity]( - [KI schreibt krasses Youtube Skript | LangChain Tutorial Deutsch]( by [SimpleKI]( - [Chat with Audio: Langchain, Chroma DB, OpenAI, and Assembly AI]( by [AI Anytime]( - [QA over documents with Auto vector index selection with Langchain router chains]( by [echohive]( - [Build your own custom LLM application with Bubble.io & Langchain (No Code & Beginner friendly)]( by [No Code Blackbox]( - [Simple App to Question Your Docs: Leveraging Streamlit, Hugging Face Spaces, LangChain, and Claude!]( by [Chris Alexiuk]( - [LANGCHAIN AI- ConstitutionalChainAI + Databutton AI ASSISTANT Web App]( by [Avra]( - [LANGCHAIN AI AUTONOMOUS AGENT WEB APP - BABY AGI with EMAIL AUTOMATION using DATABUTTON]( by [Avra]( - [The Future of Data Analysis: Using A.I. Models in Data Analysis (LangChain)]( by [Absent Data]( - [Memory in LangChain | Deep dive (python)]( by [Eden Marco]( - [9 LangChain UseCases | Beginner's Guide | 2023]( by [Data Science Basics]( - [Use Large Language Models in Jupyter Notebook | LangChain | Agents & Indexes]( by [Abhinaw Tiwari]( - [How to Talk to Your Langchain Agent | 11 Labs + Whisper]( by [VRSEN]( - [LangChain Deep Dive: 5 FUN AI App Ideas To Build Quickly and Easily]( by [James NoCode]( - [LangChain 101: Models]( by [Mckay Wrigley]( - [LangChain with JavaScript Tutorial #1 | Setup & Using LLMs]( by [Leon van Zyl]( - [LangChain Overview & Tutorial for Beginners: Build Powerful AI Apps Quickly & Easily (ZERO CODE)]( by [James NoCode]( - [LangChain In Action: Real-World Use Case With Step-by-Step Tutorial]( by [Rabbitmetrics]( - [Summarizing and Querying Multiple Papers with LangChain]( by [Automata Learning Lab]( - [Using Langchain (and Replit) through Tana, ask Google/Wikipedia/Wolfram Alpha to fill out a table]( by [Stian Hklev]( - [Langchain PDF App (GUI) | Create a ChatGPT For Your PDF in Python]( by [Alejandro AO - Software & Ai]( - [Auto-GPT with LangChain | Create Your Own Personal AI Assistant]( by [Data Science Basics]( - [Create Your OWN Slack AI Assistant with Python & LangChain]( by [Dave Ebbelaar]( - [How to Create LOCAL Chatbots with GPT4All and LangChain [Full Guide]]( by [Liam Ottley]( - [Build a Multilingual PDF Search App with LangChain, Cohere and Bubble]( by [Menlo Park Lab]( - [Building a LangChain Agent (code-free!) Using Bubble and Flowise]( by [Menlo Park Lab]( - [Build a LangChain-based Semantic PDF Search App with No-Code Tools Bubble and Flowise]( by [Menlo Park Lab]( - [LangChain Memory Tutorial | Building a ChatGPT Clone in Python]( by [Alejandro AO - Software & Ai]( - [ChatGPT For Your DATA | Chat with Multiple Documents Using LangChain]( by [Data Science Basics]( - [Llama Index: Chat with Documentation using URL Loader]( by [Merk]( - [Using OpenAI, LangChain, and Gradio to Build Custom GenAI Applications]( by [David Hundley]( - [LangChain, Chroma DB, OpenAI Beginner Guide | ChatGPT with your PDF]( - [Build AI chatbot with custom knowledge base using OpenAI API and GPT Index]( by [Irina Nik]( - [Build Your Own Auto-GPT Apps with LangChain (Python Tutorial)]( by [Dave Ebbelaar]( - [Chat with Multiple PDFs | LangChain App Tutorial in Python (Free LLMs and Embeddings)]( by [Alejandro AO - Software & Ai]( - [Chat with a CSV | LangChain Agents Tutorial (Beginners)]( by [Alejandro AO - Software & Ai]( - [Create Your Own ChatGPT with PDF Data in 5 Minutes (LangChain Tutorial)]( by [Liam Ottley]( - [Build a Custom Chatbot with OpenAI: GPT-Index & LangChain | Step-by-Step Tutorial]( by [Fabrikod]( - [Flowise is an open-source no-code UI visual tool to build LangChain applications]( by [Cobus Greyling]( - [LangChain & GPT 4 For Data Analysis: The Pandas Dataframe Agent]( by [Rabbitmetrics]( - [GirlfriendGPT - AI girlfriend with LangChain]( by [Toolfinder AI]( - [How to build with Langchain 10x easier | LangFlow & Flowise]( by [AI Jason]( - [Getting Started With LangChain In 20 Minutes- Build Celebrity Search Application]( by [Krish Naik]( - [Vector Embeddings Tutorial Code Your Own AI Assistant with GPT-4 API + LangChain + NLP]( by [FreeCodeCamp.org]( - [Fully LOCAL Llama 2 Q&A with LangChain]( by [1littlecoder]( - [Fully LOCAL Llama 2 Langchain on CPU]( by [1littlecoder]( - [Build LangChain Audio Apps with Python in 5 Minutes]( by [AssemblyAI]( - [Voiceflow & Flowise: Want to Beat Competition? New Tutorial with Real AI Chatbot]( by [AI SIMP]( - [THIS Is How You Build Production-Ready AI Apps (LangSmith Tutorial)]( by [Dave Ebbelaar]( - [Build POWERFUL LLM Bots EASILY with Your Own Data - Embedchain - Langchain 2.0? (Tutorial)]( by [WorldofAI]( - [Code Llama powered Gradio App for Coding: Runs on CPU]( by [AI Anytime]( - [LangChain Complete Course in One Video | Develop LangChain (AI) Based Solutions for Your Business]( by [UBprogrammer]( - [How to Run LLaMA Locally on CPU or GPU | Python & Langchain & CTransformers Guide]( by [Code With Prince]( - [PyData Heidelberg #11 - TimeSeries Forecasting & LLM Langchain]( by [PyData]( - [Prompt Engineering in Web Development | Using LangChain and Templates with OpenAI]( by [Akamai Developer ]( - [Retrieval-Augmented Generation (RAG) using LangChain and Pinecone - The RAG Special Episode]( by [Generative AI and Data Science On AWS]( - [LLAMA2 70b-chat Multiple Documents Chatbot with Langchain & Streamlit |All OPEN SOURCE|Replicate API]( by [DataInsightEdge]( - [Chatting with 44K Fashion Products: LangChain Opportunities and Pitfalls]( by [Rabbitmetrics]( - [Structured Data Extraction from ChatGPT with LangChain]( by [MG]( - [Chat with Multiple PDFs using Llama 2, Pinecone and LangChain (Free LLMs and Embeddings)]( by [Muhammad Moin]( - [Integrate Audio into LangChain.js apps in 5 Minutes]( by [AssemblyAI]( - [ChatGPT for your data with Local LLM]( by [Jacob Jedryszek]( - [Training Chatgpt with your personal data using langchain step by step in detail]( by [NextGen Machines]( - [Use ANY language in LangSmith with REST]( by [Nerding I/O]( - [How to Leverage the Full Potential of LLMs for Your Business with Langchain - Leon Ruddat]( by [PyData]( - [ChatCSV App: Chat with CSV files using LangChain and Llama 2]( by [Muhammad Moin]( ### Prompt Engineering and LangChain by Venelin Valkov\u200b - [Getting Started with LangChain: Load Custom Data, Run OpenAI Models, Embeddings and ChatGPT]( - [Loaders, Indexes & Vectorstores in LangChain: Question Answering on PDF files with ChatGPT]( - [LangChain Models: ChatGPT, Flan Alpaca, OpenAI Embeddings, Prompt Templates & Streaming]( - [LangChain Chains: Use ChatGPT to Build Conversational Agents, Summaries and Q&A on Text With LLMs]( - [Analyze Custom CSV Data with GPT-4 using Langchain]( - [Build ChatGPT Chatbots with LangChain Memory: Understanding and Implementing Memory in Conversations]( icon marks a new addition [last update 2023-09-21] - [Official LangChain YouTube channel](#official-langchain-youtube-channel) - [Introduction to LangChain with Harrison Chase, creator of LangChain](#introduction-to-langchain-with-harrison-chase-creator-of-langchain) - [Videos (sorted by views)](#videos-sorted-by-views)- [Prompt Engineering and LangChain by Venelin Valkov](#prompt-engineering-and-langchain-by-venelin-valkov)"", ""Tutorials | Tutorials Below are links to tutorials and courses on LangChain. For written guides on common use cases for LangChain, check out the [use cases guides](/docs/use_cases). icon marks a new addition [last update 2023-09-21] ### LangChain on Wikipedia ### DeepLearning.AI courses by [Harrison Chase]( and [Andrew Ng]( - [LangChain for LLM Application Development]( - [LangChain Chat with Your Data]( - [Functions, Tools and Agents with LangChain]( ### Handbook [LangChain AI Handbook]( By **James Briggs** and **Francisco Ingham** ### Short Tutorials [LangChain Explained in 13 Minutes | QuickStart Tutorial for Beginners]( by [Rabbitmetrics]( [LangChain Crash Course: Build an AutoGPT app in 25 minutes]( by [Nicholas Renotte]( [LangChain Crash Course - Build apps with language models]( by [Patrick Loeber]( ## Tutorials ### LangChain for Gen AI and LLMs by James Briggs - #1 [Getting Started with GPT-3 vs. Open Source LLMs]( - #2 [Prompt Templates for GPT 3.5 and other LLMs]( - #3 [LLM Chains using GPT 3.5 and other LLMs]( - [LangChain Data Loaders, Tokenizers, Chunking, and Datasets - Data Prep 101]( - #4 [Chatbot Memory for Chat-GPT, Davinci + other LLMs]( - #5 [Chat with OpenAI in LangChain]( - #6 [Fixing LLM Hallucinations with Retrieval Augmentation in LangChain]( - #7 [LangChain Agents Deep Dive with GPT 3.5]( - #8 [Create Custom Tools for Chatbots in LangChain]( - #9 [Build Conversational Agents with Vector DBs]( - [Using NEW MPT-7B in Hugging Face and LangChain]( - [MPT-30B Chatbot with LangChain]( - [Fine-tuning OpenAI's GPT 3.5 for LangChain Agents]( - [Chatbots with RAG: LangChain Full Walkthrough]( ### LangChain 101 by Greg Kamradt (Data Indy) - [What Is LangChain? - LangChain + ChatGPT Overview]( - [Quickstart Guide]( - [Beginner's Guide To 7 Essential Concepts]( - [Beginner's Guide To 9 Use Cases]( - [Agents Overview + Google Searches]( - [OpenAI + Wolfram Alpha]( - [Ask Questions On Your Custom (or Private) Files]( - [Connect Google Drive Files To OpenAI]( - [YouTube Transcripts + OpenAI]( - [Question A 300 Page Book (w/ OpenAI + Pinecone)]( - [Workaround OpenAI's Token Limit With Chain Types]( - [Build Your Own OpenAI + LangChain Web App in 23 Minutes]( - [Working With The New ChatGPT API]( - [OpenAI + LangChain Wrote Me 100 Custom Sales Emails]( - [Structured Output From OpenAI (Clean Dirty Data)]( - [Connect OpenAI To +5,000 Tools (LangChain + Zapier)]( - [Use LLMs To Extract Data From Text (Expert Mode)]( - [Extract Insights From Interview Transcripts Using LLMs]( - [5 Levels Of LLM Summarizing: Novice to Expert]( - [Control Tone & Writing Style Of Your LLM Output]( - [Build Your Own AI Twitter Bot Using LLMs]( - [ChatGPT made my interview questions for me (Streamlit + LangChain)]( - [Function Calling via ChatGPT API - First Look With LangChain]( - [Extract Topics From Video/Audio With LLMs (Topic Modeling w/ LangChain)]( ### LangChain How to and guides by Sam Witteveen - [LangChain Basics - LLMs & PromptTemplates with Colab]( - [LangChain Basics - Tools and Chains]( - [ChatGPT API Announcement & Code Walkthrough with LangChain]( - [Conversations with Memory (explanation & code walkthrough)]( - [Chat with Flan20B]( - [Using Hugging Face Models locally (code walkthrough)]( - [PAL: Program-aided Language Models with LangChain code]( - [Building a Summarization System with LangChain and GPT-3 - Part 1]( - [Building a Summarization System with LangChain and GPT-3 - Part 2]( - [Microsoft's Visual ChatGPT using LangChain]( - [LangChain Agents - Joining Tools and Chains with Decisions]( - [Comparing LLMs with LangChain]( - [Using Constitutional AI in LangChain]( - [Talking to Alpaca with LangChain - Creating an Alpaca Chatbot]( - [Talk to your CSV & Excel with LangChain]( - [BabyAGI: Discover the Power of Task-Driven Autonomous Agents!]( - [Improve your BabyAGI with LangChain]( - [Master PDF Chat with LangChain - Your essential guide to queries on documents]( - [Using LangChain with DuckDuckGO, Wikipedia & PythonREPL Tools]( - [Building Custom Tools and Agents with LangChain (gpt-3.5-turbo)]( - [LangChain Retrieval QA Over Multiple Files with ChromaDB]( - [LangChain Retrieval QA with Instructor Embeddings & ChromaDB for PDFs]( - [LangChain + Retrieval Local LLMs for Retrieval QA - No OpenAI!!!]( - [Camel + LangChain for Synthetic Data & Market Research]( - [Information Extraction with LangChain & Kor]( - [Converting a LangChain App from OpenAI to OpenSource]( - [Using LangChain Output Parsers to get what you want out of LLMs]( - [Building a LangChain Custom Medical Agent with Memory]( - [Understanding ReACT with LangChain]( - [OpenAI Functions + LangChain : Building a Multi Tool Agent]( - [What can you do with 16K tokens in LangChain?]( - [Tagging and Extraction - Classification using OpenAI Functions]( - [HOW to Make Conversational Form with LangChain]( - [Claude-2 meets LangChain!]( - [PaLM 2 Meets LangChain]( - [LLaMA2 with LangChain - Basics | LangChain TUTORIAL]( - [Serving LLaMA2 with Replicate]( - [NEW LangChain Expression Language]( - [Building a RCI Chain for Agents with LangChain Expression Language]( - [How to Run LLaMA-2-70B on the Together AI]( - [RetrievalQA with LLaMA 2 70b & Chroma DB]( - [How to use BGE Embeddings for LangChain]( - [How to use Custom Prompts for RetrievalQA on LLaMA-2 7B]( ### LangChain by Prompt Engineering - [LangChain Crash Course All You Need to Know to Build Powerful Apps with LLMs]( - [Working with MULTIPLE PDF Files in LangChain: ChatGPT for your Data]( - [ChatGPT for YOUR OWN PDF files with LangChain]( - [Talk to YOUR DATA without OpenAI APIs: LangChain]( - [LangChain: PDF Chat App (GUI) | ChatGPT for Your PDF FILES]( - [LangFlow: Build Chatbots without Writing Code]( - [LangChain: Giving Memory to LLMs]( - [BEST OPEN Alternative to OPENAI's EMBEDDINGs for Retrieval QA: LangChain]( - [LangChain: Run Language Models Locally - Hugging Face Models]( - [Slash API Costs: Mastering Caching for LLM Applications]( - [Avoid PROMPT INJECTION with Constitutional AI - LangChain]( ### LangChain by Chat with data - [LangChain Beginner's Tutorial for Typescript/Javascript]( - [GPT-4 Tutorial: How to Chat With Multiple PDF Files (~1000 pages of Tesla's 10-K Annual Reports)]( - [GPT-4 & LangChain Tutorial: How to Chat With A 56-Page PDF Document (w/Pinecone)]( - [LangChain & Supabase Tutorial: How to Build a ChatGPT Chatbot For Your Website]( - [LangChain Agents: Build Personal Assistants For Your Data (Q&A with Harrison Chase and Mayo Oshin)]( ### Codebase Analysis - [Codebase Analysis: Langchain Agents]( icon marks a new addition [last update 2023-09-21] - [LangChain on Wikipedia](#langchain-on-wikipedia) - [DeepLearning.AI courses](#deeplearningai-courses) - [Handbook](#handbook) - [Short Tutorials](#short-tutorials) - [Tutorials](#tutorials-1)- [LangChain for Gen AI and LLMs by James Briggs](#langchain-for-gen-ai-and-llms-by-james-briggs) - [LangChain 101 by Greg Kamradt (Data Indy)](#langchain-101-by-greg-kamradt-data-indy) - [LangChain How to and guides by Sam Witteveen](#langchain-how-to-and-guides-by-sam-witteveen) - [LangChain by Prompt Engineering](#langchain-by-prompt-engineering) - [LangChain by Chat with data](#langchain-by-chat-with-data) - [Codebase Analysis](#codebase-analysis)"", 'Retrieval-augmented generation (RAG) | Retrieval-augmented generation (RAG) []( ## Overview ### What is RAG? RAG is a technique for augmenting LLM knowledge with additional, often private or real-time, data. LLMs can reason about wide-ranging topics, but their knowledge is limited to the public data up to a specific point in time that they were trained on. If you want to build AI applications that can reason about private data or data introduced after a model\'s cutoff date, you need to augment the knowledge of the model with the specific information it needs. The process of bringing the appropriate information and inserting it into the model prompt is known as Retrieval Augmented Generation (RAG). ### What\'s in this guide? LangChain has a number of components specifically designed to help build RAG applications. To familiarize ourselves with these, we\'ll build a simple question-answering application over a text data source. Specifically, we\'ll build a QA bot over the [LLM Powered Autonomous Agents]( blog post by Lilian Weng. Along the way we\'ll go over a typical QA architecture, discuss the relevant LangChain components, and highlight additional resources for more advanced QA techniques. We\'ll also see how LangSmith can help us trace and understand our application. LangSmith will become increasingly helpful as our application grows in complexity. **Note** Here we focus on RAG for unstructured data. Two RAG use cases which we cover elsewhere are: - [QA over structured data](/docs/use_cases/qa_structured/sql) (e.g., SQL) - [QA over code](/docs/use_cases/question_answering/code_understanding) (e.g., Python) ## Architecture A typical RAG application has two main components: **Indexing**: a pipeline for ingesting data from a source and indexing it. _This usually happen offline._ **Retrieval and generation**: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model. The most common full sequence from raw data to answer looks like: #### Indexing 1. **Load**: First we need to load our data. We\'ll use [DocumentLoaders](/docs/modules/data_connection/document_loaders/) for this. 2. **Split**: [Text splitters](/docs/modules/data_connection/document_transformers/) break large `Documents` into smaller chunks. This is useful both for indexing data and for passing it in to a model, since large chunks are harder to search over and won\'t in a model\'s finite context window. 3. **Store**: We need somewhere to store and index our splits, so that they can later be searched over. This is often done using a [VectorStore](/docs/modules/data_connection/vectorstores/) and [Embeddings](/docs/modules/data_connection/text_embedding/) model. #### Retrieval and generation 1. **Retrieve**: Given a user input, relevant splits are retrieved from storage using a [Retriever](/docs/modules/data_connection/retrievers/). 2. **Generate**: A [ChatModel](/docs/modules/model_io/chat_models) / [LLM](/docs/modules/model_io/llms/) produces an answer using a prompt that includes the question and the retrieved data ![flow.jpeg](/assets/images/qa_flow-9fbd91de9282eb806bda1c6db501ecec.jpeg) ## Setup ### Dependencies We\'ll use an OpenAI chat model and embeddings and a Chroma vector store in this walkthrough, but everything shown here works with any [ChatModel](/docs/integrations/chat/) or [LLM](/docs/integrations/llms/), [Embeddings](/docs/integrations/text_embedding/), and [VectorStore](/docs/integrations/vectorstores/) or [Retriever](/docs/integrations/retrievers). We\'ll use the following packages: ```bash pip install -U langchain openai chromadb langchainhub bs4 ``` We need to set environment variable `OPENAI_API_KEY`, which can be done directly or loaded from a `.env` file like so: ```python import getpass import os os.environ[""OPENAI_API_KEY""] = getpass.getpass() # import dotenv # dotenv.load_dotenv() ``` ### LangSmith Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with [LangSmith]( Note that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces: ```python os.environ[""LANGCHAIN_TRACING_V2""] = ""true"" os.environ[""LANGCHAIN_API_KEY""] = getpass.getpass() ``` ## Quickstart Suppose we want to build a QA app over the [LLM Powered Autonomous Agents]( blog post by Lilian Weng. We can create a simple pipeline for this in ~20 lines of code: ```python import bs4 from langchain import hub from langchain.chat_models import ChatOpenAI from langchain.document_loaders import WebBaseLoader from langchain.embeddings import OpenAIEmbeddings from langchain.schema import StrOutputParser from langchain.schema.runnable import RunnablePassthrough from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain.vectorstores import Chroma ``` ```python loader = WebBaseLoader( web_paths=("" bs_kwargs=dict( parse_only=bs4.SoupStrainer( class_=(""post-content"", ""post-title"", ""post-header"") ) ), ) docs = loader.load() text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200) splits = text_splitter.split_documents(docs) vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings()) retriever = vectorstore.as_retriever() prompt = hub.pull(""rlm/rag-prompt"") llm = ChatOpenAI(model_name=""gpt-3.5-turbo"", temperature=0) def format_docs(docs): return ""\\n\\n"".join(doc.page_content for doc in docs) rag_chain = ( {""context"": retriever | format_docs, ""question"": RunnablePassthrough()} | prompt | llm | StrOutputParser() ) ``` ```python rag_chain.invoke(""What is Task Decomposition?"") ``` ```text \'Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It can be done through prompting techniques like Chain of Thought or Tree of Thoughts, or by using task-specific instructions or human inputs. Task decomposition helps agents plan ahead and manage complicated tasks more effectively.\' ``` ```python # cleanup vectorstore.delete_collection() ``` Check out the [LangSmith trace]( ## Detailed walkthrough Let\'s go through the above code step-by-step to really understand what\'s going on. ## Step 1. Load We need to first load the blog post contents. We can use `DocumentLoader`s for this, which are objects that load in data from a source as `Documents`. A `Document` is an object with `page_content` (str) and `metadata` (dict) attributes. In this case we\'ll use the `WebBaseLoader`, which uses `urllib` and `BeautifulSoup` to load and parse the passed in web urls, returning one `Document` per url. We can customize the html -> text parsing by passing in parameters to the `BeautifulSoup` parser via `bs_kwargs` (see [BeautifulSoup docs]( In this case only HTML tags with class ""post-content"", ""post-title"", or ""post-header"" are relevant, so we\'ll remove all others. ```python from langchain.document_loaders import WebBaseLoader loader = WebBaseLoader( web_paths=("" bs_kwargs={ ""parse_only"": bs4.SoupStrainer( class_=(""post-content"", ""post-title"", ""post-header"") ) }, ) docs = loader.load() ``` ```python len(docs[0].page_content) ``` ```text 42824 ``` ```python print(docs[0].page_content[:500]) ``` ```text LLM Powered Autonomous Agents Date: June 23, 2023 | Estimated Reading Time: 31 min | Author: Lilian Weng Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver. Agent System Overview# In ``` ### Go deeper `DocumentLoader`: Object that load data from a source as `Documents`. - [Docs](/docs/modules/data_connection/document_loaders/): Further documentation on how to use `DocumentLoader`s. - [Integrations](/docs/integrations/document_loaders/): Find the relevant `DocumentLoader` integration (of the > 160 of them) for your use case. ## Step 2. Split Our loaded document is over 42k characters long. This is too long to fit in the context window of many models. And even for those models that could fit the full post in their context window, empirically models struggle to find the relevant context in very long prompts. So we\'ll split the `Document` into chunks for embedding and vector storage. This should help us retrieve only the most relevant bits of the blog post at run time. In this case we\'ll split our documents into chunks of 1000 characters with 200 characters of overlap between chunks. The overlap helps mitigate the possibility of separating a statement from important context related to it. We use the `RecursiveCharacterTextSplitter`, which will (recursively) split the document using common separators (like new lines) until each chunk is the appropriate size. ```python from langchain.text_splitter import RecursiveCharacterTextSplitter text_splitter = RecursiveCharacterTextSplitter( chunk_size=1000, chunk_overlap=200, add_start_index=True ) all_splits = text_splitter.split_documents(docs) ``` ```python len(all_splits) ``` ```text 66 ``` ```python len(all_splits[0].page_content) ``` ```text 969 ``` ```python all_splits[10].metadata ``` ```text {\'source\': \' \'start_index\': 7056} ``` ### Go deeper `DocumentSplitter`: Object that splits a list of `Document`s into smaller chunks. Subclass of `DocumentTransformer`s. - Explore `Context-aware splitters`, which keep the location (""context"") of each split in the original `Document`:- [Markdown files](/docs/use_cases/question_answering/document-context-aware-QA) - [Code (py or js)](/docs/use_cases/question_answering/docs/integrations/document_loaders/source_code) - [Scientific papers](/docs/integrations/document_loaders/grobid) `DocumentTransformer`: Object that performs a transformation on a list of `Document`s. - [Docs](/docs/modules/data_connection/document_transformers/): Further documentation on how to use `DocumentTransformer`s - [Integrations](/docs/integrations/document_transformers/) ## Step 3. Store Now that we\'ve got 66 text chunks in memory, we need to store and index them so that we can search them later in our RAG app. The most common way to do this is to embed the contents of each document split and upload those embeddings to a vector store. Then, when we want to search over our splits, we take the search query, embed it as well, and perform some sort of ""similarity"" search to identify the stored splits with the most similar embeddings to our query embedding. The simplest similarity measure is cosine similarity we measure the cosine of the angle between each pair of embeddings (which are just very high dimensional vectors). We can embed and store all of our document splits in a single command using the `Chroma` vector store and `OpenAIEmbeddings` model. ```python from langchain.embeddings import OpenAIEmbeddings from langchain.vectorstores import Chroma vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings()) ``` ### Go deeper `Embeddings`: Wrapper around a text embedding model, used for converting text to embeddings. - [Docs](/docs/modules/data_connection/text_embedding): Further documentation on the interface. - [Integrations](/docs/integrations/text_embedding/): Browse the > 30 text embedding integrations `VectorStore`: Wrapper around a vector database, used for storing and querying embeddings. - [Docs](/docs/modules/data_connection/vectorstores/): Further documentation on the interface. - [Integrations](/docs/integrations/vectorstores/): Browse the > 40 `VectorStore` integrations. This completes the **Indexing** portion of the pipeline. At this point we have an query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question: ## Step 4. Retrieve Now let\'s write the actual application logic. We want to create a simple application that let\'s the user ask a question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and finally returns an answer. LangChain defines a `Retriever` interface which wraps an index that can return relevant documents given a string query. All retrievers implement a common method `get_relevant_documents()` (and its asynchronous variant `aget_relevant_documents()`). The most common type of `Retriever` is the `VectorStoreRetriever`, which uses the similarity search capabilities of a vector store to facillitate retrieval. Any `VectorStore` can easily be turned into a `Retriever`: ```python retriever = vectorstore.as_retriever(search_type=""similarity"", search_kwargs={""k"": 6}) ``` ```python retrieved_docs = retriever.get_relevant_documents( ""What are the approaches to Task Decomposition?"" ) ``` ```python len(retrieved_docs) ``` ```text 6 ``` ```python print(retrieved_docs[0].page_content) ``` ```text Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote. Task decomposition can be done (1) by LLM with simple prompting like ""Steps for XYZ.\\n1."", ""What are the subgoals for achieving XYZ?"", (2) by using task-specific instructions; e.g. ""Write a story outline."" for writing a novel, or (3) with human inputs. ``` ### Go deeper Vector stores are commonly used for retrieval, but there are plenty of other ways to do retrieval. `Retriever`: An object that returns `Document`s given a text query - [Docs](/docs/modules/data_connection/retrievers/): Further documentation on the interface and built-in retrieval techniques. Some of which include:- `MultiQueryRetriever` [generates variants of the input question](/docs/modules/data_connection/retrievers/MultiQueryRetriever) to improve retrieval hit rate. - `MultiVectorRetriever` (diagram below) instead generates [variants of the embeddings](/docs/modules/data_connection/retrievers/multi_vector), also in order to improve retrieval hit rate. - `Max marginal relevance` selects for [relevance and diversity]( among the retrieved documents to avoid passing in duplicate context. - Documents can be filtered during vector store retrieval using [metadata filters](/docs/use_cases/question_answering/document-context-aware-QA). - [Integrations](/docs/integrations/retrievers/): Integrations with retrieval services. ## Step 5. Generate Let\'s put it all together into a chain that takes a question, retrieves relevant documents, constructs a prompt, passes that to a model, and parses the output. We\'ll use the gpt-3.5-turbo OpenAI chat model, but any LangChain `LLM` or `ChatModel` could be substituted in. ```python from langchain.chat_models import ChatOpenAI llm = ChatOpenAI(model_name=""gpt-3.5-turbo"", temperature=0) ``` We\'ll use a prompt for RAG that is checked into the LangChain prompt hub ([here]( ```python from langchain import hub prompt = hub.pull(""rlm/rag-prompt"") ``` ```python print( prompt.invoke( {""context"": ""filler context"", ""question"": ""filler question""} ).to_string() ) ``` ```text Human: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise. Question: filler question Context: filler context Answer: ``` We\'ll use the [LCEL Runnable]( protocol to define the chain, allowing us to - pipe together components and functions in a transparent way - automatically trace our chain in LangSmith - get streaming, async, and batched calling out of the box ```python from langchain.schema import StrOutputParser from langchain.schema.runnable import RunnablePassthrough def format_docs(docs): return ""\\n\\n"".join(doc.page_content for doc in docs) rag_chain = ( {""context"": retriever | format_docs, ""question"": RunnablePassthrough()} | prompt | llm | StrOutputParser() ) ``` ```python for chunk in rag_chain.stream(""What is Task Decomposition?""): print(chunk, end="""", flush=True) ``` ```text Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It can be done through methods like Chain of Thought (CoT) or Tree of Thoughts, which involve dividing the task into manageable subtasks and exploring multiple reasoning possibilities at each step. Task decomposition can be performed by AI models with prompting, task-specific instructions, or human inputs. ``` Check out the [LangSmith trace]( ### Go deeper #### Choosing LLMs `ChatModel`: An LLM-backed chat model wrapper. Takes in a sequence of messages and returns a message. - [Docs](/docs/modules/model_io/chat/) - [Integrations](/docs/integrations/chat/): Explore over 25 `ChatModel` integrations. `LLM`: A text-in-text-out LLM. Takes in a string and returns a string. - [Docs](/docs/modules/model_io/llms) - [Integrations](/docs/integrations/llms): Explore over 75 `LLM` integrations. See a guide on RAG with locally-running models [here](/docs/modules/use_cases/question_answering/local_retrieval_qa). #### Customizing the prompt As shown above, we can load prompts (e.g., [this RAG prompt]( from the prompt hub. The prompt can also be easily customized: ```python from langchain.prompts import PromptTemplate template = """"""Use the following pieces of context to answer the question at the end. If you don\'t know the answer, just say that you don\'t know, don\'t try to make up an answer. Use three sentences maximum and keep the answer as concise as possible. Always say ""thanks for asking!"" at the end of the answer. {context} Question: {question} Helpful Answer:"""""" rag_prompt_custom = PromptTemplate.from_template(template) rag_chain = ( {""context"": retriever | format_docs, ""question"": RunnablePassthrough()} | rag_prompt_custom | llm | StrOutputParser() ) rag_chain.invoke(""What is Task Decomposition?"") ``` ```text \'Task decomposition is the process of breaking down a complex task into smaller and simpler steps. It can be done through techniques like Chain of Thought (CoT) or Tree of Thoughts, which involve dividing the problem into multiple thought steps and generating multiple thoughts per step. Task decomposition helps in enhancing model performance and understanding the thinking process of the model. Thanks for asking!\' ``` Check out the [LangSmith trace]( ### Adding sources With LCEL it\'s easy to return the retrieved documents or certain source metadata from the documents: ```python from operator import itemgetter from langchain.schema.runnable import RunnableMap rag_chain_from_docs = ( { ""context"": lambda input: format_docs(input[""documents""]), ""question"": itemgetter(""question""), } | rag_prompt_custom | llm | StrOutputParser() ) rag_chain_with_source = RunnableMap( {""documents"": retriever, ""question"": RunnablePassthrough()} ) | { ""documents"": lambda input: [doc.metadata for doc in input[""documents""]], ""answer"": rag_chain_from_docs, } rag_chain_with_source.invoke(""What is Task Decomposition"") ``` ```text {\'documents\': [{\'source\': \' \'start_index\': 1585}, {\'source\': \' \'start_index\': 2192}, {\'source\': \' \'start_index\': 17804}, {\'source\': \' \'start_index\': 17414}, {\'source\': \' \'start_index\': 29630}, {\'source\': \' \'start_index\': 19373}], \'answer\': \'Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It involves transforming big tasks into multiple manageable tasks, allowing for a more systematic and organized approach to problem-solving. Thanks for asking!\'} ``` Check out the [LangSmith trace]( Adding memory Suppose we want to create a stateful application that remembers past user inputs. There are two main things we need to do to support this. 1. Add a messages placeholder to our chain which allows us to pass in historical messages 2. Add a chain that takes the latest user query and reformulates it in the context of the chat history into a standalone question that can be passed to our retriever. Let\'s start with 2. We can build a ""condense question"" chain that looks something like this: ```python from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder condense_q_system_prompt = """"""Given a chat history and the latest user question \\ which might reference the chat history, formulate a standalone question \\ which can be understood without the chat history. Do NOT answer the question, \\ just reformulate it if needed and otherwise return it as is."""""" condense_q_prompt = ChatPromptTemplate.from_messages( [ (""system"", condense_q_system_prompt), MessagesPlaceholder(variable_name=""chat_history""), (""human"", ""{question}""), ] ) condense_q_chain = condense_q_prompt | llm | StrOutputParser() ``` ```python from langchain.schema.messages import AIMessage, HumanMessage condense_q_chain.invoke( { ""chat_history"": [ HumanMessage(content=""What does LLM stand for?""), AIMessage(content=""Large language model""), ], ""question"": ""What is meant by large"", } ) ``` ```text \'What is the definition of ""large"" in the context of a language model?\' ``` ```python condense_q_chain.invoke( { ""chat_history"": [ HumanMessage(content=""What does LLM stand for?""), AIMessage(content=""Large language model""), ], ""question"": ""How do transformers work"", } ) ``` ```text \'How do transformer models function?\' ``` And now we can build our full QA chain. Notice we add some routing functionality to only run the ""condense question chain"" when our chat history isn\'t empty. ```python qa_system_prompt = """"""You are an assistant for question-answering tasks. \\ Use the following pieces of retrieved context to answer the question. \\ If you don\'t know the answer, just say that you don\'t know. \\ Use three sentences maximum and keep the answer concise.\\ {context}"""""" qa_prompt = ChatPromptTemplate.from_messages( [ (""system"", qa_system_prompt), MessagesPlaceholder(variable_name=""chat_history""), (""human"", ""{question}""), ] ) def condense_question(input: dict): if input.get(""chat_history""): return condense_q_chain else: return input[""question""] rag_chain = ( RunnablePassthrough.assign(context=condense_question | retriever | format_docs) | qa_prompt | llm ) ``` ```python chat_history = [] question = ""What is Task Decomposition?"" ai_msg = rag_chain.invoke({""question"": question, ""chat_history"": chat_history}) chat_history.extend([HumanMessage(content=question), ai_msg]) second_question = ""What are common ways of doing it?"" rag_chain.invoke({""question"": second_question, ""chat_history"": chat_history}) ``` ```text AIMessage(content=\'Common ways of task decomposition include:\\n\\n1. Using Chain of Thought (CoT): CoT is a prompting technique that instructs the model to ""think step by step"" and decompose complex tasks into smaller and simpler steps. It utilizes more test-time computation and sheds light on the model\\\'s thinking process.\\n\\n2. Prompting with LLM: Language Model (LLM) can be used to prompt the model with simple instructions like ""Steps for XYZ"" or ""What are the subgoals for achieving XYZ?"" This allows the model to generate a sequence of subtasks or thought steps.\\n\\n3. Task-specific instructions: For certain tasks, task-specific instructions can be provided to guide the model in decomposing the task. For example, for writing a novel, the instruction ""Write a story outline"" can be given to break down the task into manageable steps.\\n\\n4. Human inputs: In some cases, human inputs can be used to assist in task decomposition. Humans can provide their expertise and knowledge to identify and break down complex tasks into smaller subtasks.\') ``` Check out the [LangSmith trace]( Here we\'ve gone over how to add chain logic for incorporating historical outputs. But how do we actually store and retrieve historical outputs for different sessions? For that check out the LCEL [How to add message history (memory)](/docs/expression_language/how_to/message_history) page. ## Next steps That\'s a lot of content we\'ve covered in a short amount of time. There\'s plenty of nuances, features, integrations, etc to explore in each of the above sections. Aside from the sources mentioned above, good next steps include: - Reading up on more advanced retrieval techniques in the [Retrievers](/docs/modules/data_connection/retrievers/) section. - Learning about the LangChain [Indexing API](/docs/modules/data_connection/indexing), which helps repeatedly sync data sources and vector stores without redundant computation or storage. - Exploring RAG [LangChain Templates](/docs/templates/#-advanced-retrieval), which are reference applications that can easily be deployed with [LangServe](/docs/langserve). - Learning about [evaluating RAG applications with LangSmith]( - [Overview](#overview)- [What is RAG?](#what-is-rag) - [What\'s in this guide?](#whats-in-this-guide) - [Architecture](#architecture) - [Setup](#setup)- [Dependencies](#dependencies) - [LangSmith](#langsmith) - [Quickstart](#quickstart) - [Detailed walkthrough](#detailed-walkthrough) - [Step 1. Load](#step-1-load)- [Go deeper](#go-deeper) - [Step 2. Split](#step-2-split)- [Go deeper](#go-deeper-1) - [Step 3. Store](#step-3-store)- [Go deeper](#go-deeper-2) - [Step 4. Retrieve](#step-4-retrieve)- [Go deeper](#go-deeper-3) - [Step 5. Generate](#step-5-generate)- [Go deeper](#go-deeper-4) - [Adding sources](#adding-sources) - [Adding memory](#adding-memory) - [Next steps](#next-steps)']","BabyAGI is a concept related to autonomous AI agents that can perform task-driven operations without human intervention. It's an extension of the AGI (Artificial General Intelligence) idea, where the goal is to create systems that can understand, learn, and apply intelligence across a wide range of tasks. BabyAGI aims to build upon existing LLM (Large Language Model) technologies to create agents that can manage and execute a set of tasks in an autonomous manner.

For more detailed information, you might want to explore resources and tutorials specifically focused on BabyAGI and its implementation using frameworks like LangChain. These resources often provide step-by-step guides and code examples to help you understand and build such systems. Always ensure to follow best practices and guidelines when working with AI technologies.","BabyAGI is an example of an ""autonomous AI agent"" that can generate and simulate the execution of tasks based on a given objective.",0.99999999995,1.0,,0.048491394006509564,0.16883116883116883
38,What is the difference between ChatPromptTemplate and PromptTemplate?,"['Prompt templates | Prompt templates Prompt templates are pre-defined recipes for generating prompts for language models. A template may include instructions, few-shot examples, and specific context and questions appropriate for a given task. LangChain provides tooling to create and work with prompt templates. LangChain strives to create model agnostic templates to make it easy to reuse existing templates across different language models. Typically, language models expect the prompt to either be a string or else a list of chat messages. ## PromptTemplate Use `PromptTemplate` to create a template for a string prompt. By default, `PromptTemplate` uses [Python\'s str.format]( syntax for templating. ```python from langchain.prompts import PromptTemplate prompt_template = PromptTemplate.from_template( ""Tell me a {adjective} joke about {content}."" ) prompt_template.format(adjective=""funny"", content=""chickens"") ``` ```text \'Tell me a funny joke about chickens.\' ``` The template supports any number of variables, including no variables: ```python from langchain.prompts import PromptTemplate prompt_template = PromptTemplate.from_template(""Tell me a joke"") prompt_template.format() ``` ```text \'Tell me a joke\' ``` For additional validation, specify `input_variables` explicitly. These variables will be compared against the variables present in the template string during instantiation, **raising an exception if there is a mismatch**. For example: ```python from langchain.prompts import PromptTemplate invalid_prompt = PromptTemplate( input_variables=[""adjective""], template=""Tell me a {adjective} joke about {content}."", ) ``` ```text --------------------------------------------------------------------------- ValidationError Traceback (most recent call last) Cell In[19], line 3 1 from langchain.prompts import PromptTemplate ----> 3 invalid_prompt = PromptTemplate( 4 input_variables=[""adjective""], 5 template=""Tell me a {adjective} joke about {content}."" 6 ) File ~/langchain/libs/langchain/langchain/load/serializable.py:97, in Serializable.__init__(self, **kwargs) 96 def __init__(self, **kwargs: Any) -> None: ---> 97 super().__init__(**kwargs) 98 self._lc_kwargs = kwargs File ~/langchain/.venv/lib/python3.9/site-packages/pydantic/main.py:341, in pydantic.main.BaseModel.__init__() ValidationError: 1 validation error for PromptTemplate __root__ Invalid prompt schema; check for mismatched or missing input parameters. \'content\' (type=value_error) ``` You can create custom prompt templates that format the prompt in any way you want. For more information, see [Custom Prompt Templates](/docs/modules/model_io/prompts/prompt_templates/custom_prompt_template.html). ## ChatPromptTemplate The prompt to [chat models](/docs/modules/model_io/prompts/models/chat) is a list of chat messages. Each chat message is associated with content, and an additional parameter called `role`. For example, in the OpenAI [Chat Completions API]( a chat message can be associated with an AI assistant, a human or a system role. Create a chat prompt template like this: ```python from langchain.prompts import ChatPromptTemplate chat_template = ChatPromptTemplate.from_messages( [ (""system"", ""You are a helpful AI bot. Your name is {name}.""), (""human"", ""Hello, how are you doing?""), (""ai"", ""I\'m doing well, thanks!""), (""human"", ""{user_input}""), ] ) messages = chat_template.format_messages(name=""Bob"", user_input=""What is your name?"") ``` `ChatPromptTemplate.from_messages` accepts a variety of message representations. For example, in addition to using the 2-tuple representation of (type, content) used above, you could pass in an instance of `MessagePromptTemplate` or `BaseMessage`. ```python from langchain.chat_models import ChatOpenAI from langchain.prompts import HumanMessagePromptTemplate from langchain.schema.messages import SystemMessage chat_template = ChatPromptTemplate.from_messages( [ SystemMessage( content=( ""You are a helpful assistant that re-writes the user\'s text to "" ""sound more upbeat."" ) ), HumanMessagePromptTemplate.from_template(""{text}""), ] ) llm = ChatOpenAI() llm(chat_template.format_messages(text=""i dont like eating tasty things."")) ``` ```text AIMessage(content=\'I absolutely love indulging in delicious treats!\') ``` This provides you with a lot of flexibility in how you construct your chat prompts. ## LCEL `PromptTemplate` and `ChatPromptTemplate` implement the [Runnable interface](/docs/expression_language/interface), the basic building block of the [LangChain Expression Language (LCEL)](/docs/expression_language/). This means they support `invoke`, `ainvoke`, `stream`, `astream`, `batch`, `abatch`, `astream_log` calls. `PromptTemplate` accepts a dictionary (of the prompt variables) and returns a `StringPromptValue`. A `ChatPromptTemplate` accepts a dictionary and returns a `ChatPromptValue`. ```python prompt_val = prompt_template.invoke({""adjective"": ""funny"", ""content"": ""chickens""}) prompt_val ``` ```text StringPromptValue(text=\'Tell me a joke\') ``` ```python prompt_val.to_string() ``` ```text \'Tell me a joke\' ``` ```python prompt_val.to_messages() ``` ```text [HumanMessage(content=\'Tell me a joke\')] ``` ```python chat_val = chat_template.invoke({""text"": ""i dont like eating tasty things.""}) ``` ```python chat_val.to_messages() ``` ```text [SystemMessage(content=""You are a helpful assistant that re-writes the user\'s text to sound more upbeat.""), HumanMessage(content=\'i dont like eating tasty things.\')] ``` ```python chat_val.to_string() ``` ```text ""System: You are a helpful assistant that re-writes the user\'s text to sound more upbeat.\\nHuman: i dont like eating tasty things."" ``` - [PromptTemplate](#prompttemplate) - [ChatPromptTemplate](#chatprompttemplate) - [LCEL](#lcel)', 'Baseten | Baseten [Baseten]( provides all the infrastructure you need to deploy and serve ML models performantly, scalably, and cost-efficiently. This example demonstrates using Langchain with models deployed on Baseten. # Setup To run this notebook, you\'ll need a [Baseten account]( and an [API key]( You\'ll also need to install the Baseten Python package: ```bash pip install baseten ``` ```python import baseten baseten.login(""YOUR_API_KEY"") ``` # Single model call First, you\'ll need to deploy a model to Baseten. You can deploy foundation models like WizardLM and Alpaca with one click from the [Baseten model library]( or if you have your own model, [deploy it with this tutorial]( In this example, we\'ll work with WizardLM. [Deploy WizardLM here]( and follow along with the deployed [model\'s version ID]( ```python from langchain.llms import Baseten ``` ```python # Load the model wizardlm = Baseten(model=""MODEL_VERSION_ID"", verbose=True) ``` ```python # Prompt the model wizardlm(""What is the difference between a Wizard and a Sorcerer?"") ``` # Chained model calls We can chain together multiple calls to one or multiple models, which is the whole point of Langchain! This example uses WizardLM to plan a meal with an entree, three sides, and an alcoholic and non-alcoholic beverage pairing. ```python from langchain.chains import LLMChain, SimpleSequentialChain from langchain.prompts import PromptTemplate ``` ```python # Build the first link in the chain prompt = PromptTemplate( input_variables=[""cuisine""], template=""Name a complex entree for a {cuisine} dinner. Respond with just the name of a single dish."", ) link_one = LLMChain(llm=wizardlm, prompt=prompt) ``` ```python # Build the second link in the chain prompt = PromptTemplate( input_variables=[""entree""], template=""What are three sides that would go with {entree}. Respond with only a list of the sides."", ) link_two = LLMChain(llm=wizardlm, prompt=prompt) ``` ```python # Build the third link in the chain prompt = PromptTemplate( input_variables=[""sides""], template=""What is one alcoholic and one non-alcoholic beverage that would go well with this list of sides: {sides}. Respond with only the names of the beverages."", ) link_three = LLMChain(llm=wizardlm, prompt=prompt) ``` ```python # Run the full chain! menu_maker = SimpleSequentialChain( chains=[link_one, link_two, link_three], verbose=True ) menu_maker.run(""South Indian"") ```', 'Quickstart | Quickstart In this quickstart we\'ll show you how to: - Get setup with LangChain, LangSmith and LangServe - Use the most basic and common components of LangChain: prompt templates, models, and output parsers - Use LangChain Expression Language, the protocol that LangChain is built on and which facilitates component chaining - Build a simple application with LangChain - Trace your application with LangSmith - Serve your application with LangServe That\'s a fair amount to cover! Let\'s dive in. ## Setup ### Installation To install LangChain run: Pip ```bash pip install langchain ``` Conda ```bash conda install langchain -c conda-forge ``` For more details, see our [Installation guide](/docs/get_started/installation). ### Environment Using LangChain will usually require integrations with one or more model providers, data stores, APIs, etc. For this example, we\'ll use OpenAI\'s model APIs. First we\'ll need to install their Python package: ```bash pip install openai ``` Accessing the API requires an API key, which you can get by creating an account and heading [here]( Once we have a key we\'ll want to set it as an environment variable by running: ```bash export OPENAI_API_KEY=""..."" ``` If you\'d prefer not to set an environment variable you can pass the key in directly via the `openai_api_key` named parameter when initiating the OpenAI LLM class: ```python from langchain.chat_models import ChatOpenAI llm = ChatOpenAI(openai_api_key=""..."") ``` ### LangSmith Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with [LangSmith]( Note that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces: ```shell export LANGCHAIN_TRACING_V2=""true"" export LANGCHAIN_API_KEY=... ``` ### LangServe LangServe helps developers deploy LangChain chains as a REST API. You do not need to use LangServe to use LangChain, but in this guide we\'ll show how you can deploy your app with LangServe. Install with: ```bash pip install ""langserve[all]"" ``` ## Building with LangChain LangChain provides many modules that can be used to build language model applications. Modules can be used as standalones in simple applications and they can be composed for more complex use cases. Composition is powered by **LangChain Expression Language** (LCEL), which defines a unified `Runnable` interface that many modules implement, making it possible to seamlessly chain components. The simplest and most common chain contains three things: - LLM/Chat Model: The language model is the core reasoning engine here. In order to work with LangChain, you need to understand the different types of language models and how to work with them. - Prompt Template: This provides instructions to the language model. This controls what the language model outputs, so understanding how to construct prompts and different prompting strategies is crucial. - Output Parser: These translate the raw response from the language model to a more workable format, making it easy to use the output downstream. In this guide we\'ll cover those three components individually, and then go over how to combine them. Understanding these concepts will set you up well for being able to use and customize LangChain applications. Most LangChain applications allow you to configure the model and/or the prompt, so knowing how to take advantage of this will be a big enabler. ### LLM / Chat Model There are two types of language models: - `LLM`: underlying model takes a string as input and returns a string - `ChatModel`: underlying model takes a list of messages as input and returns a message Strings are simple, but what exactly are messages? The base message interface is defined by `BaseMessage`, which has two required attributes: - `content`: The content of the message. Usually a string. - `role`: The entity from which the `BaseMessage` is coming. LangChain provides several objects to easily distinguish between different roles: - `HumanMessage`: A `BaseMessage` coming from a human/user. - `AIMessage`: A `BaseMessage` coming from an AI/assistant. - `SystemMessage`: A `BaseMessage` coming from the system. - `FunctionMessage` / `ToolMessage`: A `BaseMessage` containing the output of a function or tool call. If none of those roles sound right, there is also a `ChatMessage` class where you can specify the role manually. LangChain provides a common interface that\'s shared by both `LLM`s and `ChatModel`s. However it\'s useful to understand the difference in order to most effectively construct prompts for a given language model. The simplest way to call an `LLM` or `ChatModel` is using `.invoke()`, the universal synchronous call method for all LangChain Expression Language (LCEL) objects: - `LLM.invoke`: Takes in a string, returns a string. - `ChatModel.invoke`: Takes in a list of `BaseMessage`, returns a `BaseMessage`. The input types for these methods are actually more general than this, but for simplicity here we can assume LLMs only take strings and Chat models only takes lists of messages. Check out the ""Go deeper"" section below to learn more about model invocation. Let\'s see how to work with these different types of models and these different types of inputs. First, let\'s import an LLM and a ChatModel. ```python from langchain.llms import OpenAI from langchain.chat_models import ChatOpenAI llm = OpenAI() chat_model = ChatOpenAI() ``` `LLM` and `ChatModel` objects are effectively configuration objects. You can initialize them with parameters like `temperature` and others, and pass them around. ```python from langchain.schema import HumanMessage text = ""What would be a good company name for a company that makes colorful socks?"" messages = [HumanMessage(content=text)] llm.invoke(text) # >> Feetful of Fun chat_model.invoke(messages) # >> AIMessage(content=""Socks O\'Color"") ``` Go deeper `LLM.invoke` and `ChatModel.invoke` actually both support as input any of `Union[str, List[BaseMessage], PromptValue]`. `PromptValue` is an object that defines it\'s own custom logic for returning it\'s inputs either as a string or as messages. `LLM`s have logic for coercing any of these into a string, and `ChatModel`s have logic for coercing any of these to messages. The fact that `LLM` and `ChatModel` accept the same inputs means that you can directly swap them for one another in most chains without breaking anything, though it\'s of course important to think about how inputs are being coerced and how that may affect model performance. To dive deeper on models head to the [Language models](/docs/modules/model_io/models) section. ### Prompt templates Most LLM applications do not pass user input directly into an LLM. Usually they will add the user input to a larger piece of text, called a prompt template, that provides additional context on the specific task at hand. In the previous example, the text we passed to the model contained instructions to generate a company name. For our application, it would be great if the user only had to provide the description of a company/product, without having to worry about giving the model instructions. PromptTemplates help with exactly this! They bundle up all the logic for going from user input into a fully formatted prompt. This can start off very simple - for example, a prompt to produce the above string would just be: ```python from langchain.prompts import PromptTemplate prompt = PromptTemplate.from_template(""What is a good name for a company that makes {product}?"") prompt.format(product=""colorful socks"") ``` ```python What is a good name for a company that makes colorful socks? ``` However, the advantages of using these over raw string formatting are several. You can ""partial"" out variables - e.g. you can format only some of the variables at a time. You can compose them together, easily combining different templates into a single prompt. For explanations of these functionalities, see the [section on prompts](/docs/modules/model_io/prompts) for more detail. `PromptTemplate`s can also be used to produce a list of messages. In this case, the prompt not only contains information about the content, but also each message (its role, its position in the list, etc.). Here, what happens most often is a `ChatPromptTemplate` is a list of `ChatMessageTemplates`. Each `ChatMessageTemplate` contains instructions for how to format that `ChatMessage` - its role, and then also its content. Let\'s take a look at this below: ```python from langchain.prompts.chat import ChatPromptTemplate template = ""You are a helpful assistant that translates {input_language} to {output_language}."" human_template = ""{text}"" chat_prompt = ChatPromptTemplate.from_messages([ (""system"", template), (""human"", human_template), ]) chat_prompt.format_messages(input_language=""English"", output_language=""French"", text=""I love programming."") ``` ```pycon [ SystemMessage(content=""You are a helpful assistant that translates English to French."", additional_kwargs={}), HumanMessage(content=""I love programming."") ] ``` ChatPromptTemplates can also be constructed in other ways - see the [section on prompts](/docs/modules/model_io/prompts) for more detail. ### Output parsers `OutputParsers` convert the raw output of a language model into a format that can be used downstream. There are few main types of `OutputParser`s, including: - Convert text from `LLM` into structured information (e.g. JSON) - Convert a `ChatMessage` into just a string - Convert the extra information returned from a call besides the message (like OpenAI function invocation) into a string. For full information on this, see the [section on output parsers](/docs/modules/model_io/output_parsers). In this getting started guide, we will write our own output parser - one that converts a comma separated list into a list. ```python from langchain.schema import BaseOutputParser class CommaSeparatedListOutputParser(BaseOutputParser): """"""Parse the output of an LLM call to a comma-separated list."""""" def parse(self, text: str): """"""Parse the output of an LLM call."""""" return text.strip().split("", "") CommaSeparatedListOutputParser().parse(""hi, bye"") # >> [\'hi\', \'bye\'] ``` ### Composing with LCEL We can now combine all these into one chain. This chain will take input variables, pass those to a prompt template to create a prompt, pass the prompt to a language model, and then pass the output through an (optional) output parser. This is a convenient way to bundle up a modular piece of logic. Let\'s see it in action! ```python from typing import List from langchain.chat_models import ChatOpenAI from langchain.prompts import ChatPromptTemplate from langchain.schema import BaseOutputParser class CommaSeparatedListOutputParser(BaseOutputParser[List[str]]): """"""Parse the output of an LLM call to a comma-separated list."""""" def parse(self, text: str) -> List[str]: """"""Parse the output of an LLM call."""""" return text.strip().split("", "") template = """"""You are a helpful assistant who generates comma separated lists. A user will pass in a category, and you should generate 5 objects in that category in a comma separated list. ONLY return a comma separated list, and nothing more."""""" human_template = ""{text}"" chat_prompt = ChatPromptTemplate.from_messages([ (""system"", template), (""human"", human_template), ]) chain = chat_prompt | ChatOpenAI() | CommaSeparatedListOutputParser() chain.invoke({""text"": ""colors""}) # >> [\'red\', \'blue\', \'green\', \'yellow\', \'orange\'] ``` Note that we are using the `|` syntax to join these components together. This `|` syntax is powered by the LangChain Expression Language (LCEL) and relies on the universal `Runnable` interface that all of these objects implement. To learn more about LCEL, read the documentation [here](/docs/expression_language). ## Tracing with LangSmith Assuming we\'ve set our environment variables as shown in the beginning, all of the model and chain calls we\'ve been making will have been automatically logged to LangSmith. Once there, we can use LangSmith to debug and annotate our application traces, then turn them into datasets for evaluating future iterations of the application. Check out what the trace for the above chain would look like: [ For more on LangSmith [head here](/docs/langsmith/). ## Serving with LangServe Now that we\'ve built an application, we need to serve it. That\'s where LangServe comes in. LangServe helps developers deploy LCEL chains as a REST API. The library is integrated with FastAPI and uses pydantic for data validation. ### Server To create a server for our application we\'ll make a `serve.py` file with three things: 1. The definition of our chain (same as above) 2. Our FastAPI app 3. A definition of a route from which to serve the chain, which is done with `langserve.add_routes` ```python #!/usr/bin/env python from typing import List from fastapi import FastAPI from langchain.prompts import ChatPromptTemplate from langchain.chat_models import ChatOpenAI from langchain.schema import BaseOutputParser from langserve import add_routes # 1. Chain definition class CommaSeparatedListOutputParser(BaseOutputParser[List[str]]): """"""Parse the output of an LLM call to a comma-separated list."""""" def parse(self, text: str) -> List[str]: """"""Parse the output of an LLM call."""""" return text.strip().split("", "") template = """"""You are a helpful assistant who generates comma separated lists. A user will pass in a category, and you should generate 5 objects in that category in a comma separated list. ONLY return a comma separated list, and nothing more."""""" human_template = ""{text}"" chat_prompt = ChatPromptTemplate.from_messages([ (""system"", template), (""human"", human_template), ]) category_chain = chat_prompt | ChatOpenAI() | CommaSeparatedListOutputParser() # 2. App definition app = FastAPI( title=""LangChain Server"", version=""1.0"", description=""A simple api server using Langchain\'s Runnable interfaces"", ) # 3. Adding chain route add_routes( app, category_chain, path=""/category_chain"", ) if __name__ == ""__main__"": import uvicorn uvicorn.run(app, host=""localhost"", port=8000) ``` And that\'s it! If we execute this file: ```bash python serve.py ``` we should see our chain being served at localhost:8000. ### Playground Every LangServe service comes with a simple built-in UI for configuring and invoking the application with streaming output and visibility into intermediate steps. Head to to try it out! ### Client Now let\'s set up a client for programmatically interacting with our service. We can easily do this with the `langserve.RemoteRunnable`. Using this, we can interact with the served chain as if it were running client-side. ```python from langserve import RemoteRunnable remote_chain = RemoteRunnable("" remote_chain.invoke({""text"": ""colors""}) # >> [\'red\', \'blue\', \'green\', \'yellow\', \'orange\'] ``` To learn more about the many other features of LangServe [head here](/docs/langserve). ## Next steps We\'ve touched on how to build an application with LangChain, how to trace it with LangSmith, and how to serve it with LangServe. There are a lot more features in all three of these than we can cover here. To continue on your journey: - Read up on [LangChain Expression Language (LCEL)](/docs/expression_language) to learn how to chain these components together - [Dive deeper](/docs/modules/model_io) into LLMs, prompts, and output parsers and learn the other [key components](/docs/modules) - Explore common [end-to-end use cases](/docs/use_cases/qa_structured/sql) and [template applications](/docs/templates) - [Read up on LangSmith](/docs/langsmith/), the platform for debugging, testing, monitoring and more - Learn more about serving your applications with [LangServe](/docs/langserve) - [Setup](#setup)- [Installation](#installation) - [Environment](#environment) - [LangSmith](#langsmith) - [LangServe](#langserve) - [Building with LangChain](#building-with-langchain)- [LLM / Chat Model](#llm--chat-model) - [Prompt templates](#prompt-templates) - [Output parsers](#output-parsers) - [Composing with LCEL](#composing-with-lcel) - [Tracing with LangSmith](#tracing-with-langsmith) - [Serving with LangServe](#serving-with-langserve)- [Server](#server) - [Playground](#playground) - [Client](#client) - [Next steps](#next-steps)']","`ChatPromptTemplate` is used for chat models, which take a list of messages as input, while `PromptTemplate` is for language models that take a string as input. `ChatPromptTemplate` includes system, human, and AI messages, whereas `PromptTemplate` is a simple string template.","ChatPromptTemplate and PromptTemplate are similar in their functions. PromptTemplate takes a prompt as a string that can contain variables. ChatPromptTemplate separates the prompt into structured messages of different kinds. Each chat message is associated with content, and an additional parameter called role. Some example roles include system, human (or user), and AI (or assistant).",0.8333333332916666,1.0,0.3333333333333333,0.03704295381390839,0.25531914893617025
39,Show me how to use RecursiveURLLoader,"[""langchain.document_loaders.url.UnstructuredURLLoader LangChain 0.0.337 API Experimental Python Docs Toggle Menu PrevUp Next LangChain 0.0.337 langchain.document_loaders.url.UnstructuredURLLoader langchain.document_loaders.url.UnstructuredURLLoader class langchain.document_loaders.url.UnstructuredURLLoader(urls: List[str], continue_on_failure: bool = True, mode: str = 'single', show_progress_bar: bool = False, **unstructured_kwargs: Any)[source] Load files from remote URLs using Unstructured. Use the unstructured partition function to detect the MIME type and route the file to the appropriate partitioner. You can run the loader in one of two modes: single and elements. If you use single mode, the document will be returned as a single langchain Document object. If you use elements mode, the unstructured library will split the document into elements such as Title and NarrativeText. You can pass in additional unstructured kwargs after mode to apply different unstructured settings. Examples from langchain.document_loaders import UnstructuredURLLoader loader = UnstructuredURLLoader(urls=[, ], mode=elements, strategy=fast, ) docs = loader.load() References Initialize with file path. Methods __init__(urls[,continue_on_failure,mode,...]) Initialize with file path. lazy_load() A lazy loader for Documents. load() Load file. load_and_split([text_splitter]) Load Documents and split into chunks. __init__(urls: List[str], continue_on_failure: bool = True, mode: str = 'single', show_progress_bar: bool = False, **unstructured_kwargs: Any)[source] Initialize with file path. lazy_load() Iterator[Document] A lazy loader for Documents. load() List[Document][source] Load file. load_and_split(text_splitter: Optional[TextSplitter] = None) List[Document] Load Documents and split into chunks. Chunks are returned as Documents. Parameters text_splitter TextSplitter instance to use for splitting documents. Defaults to RecursiveCharacterTextSplitter. Returns List of Documents. Examples using UnstructuredURLLoader URL 2023, Harrison Chase. Last updated on Nov 17, 2023. Show this page source"", 'langchain.document_loaders.news.NewsURLLoader LangChain 0.0.337 API Experimental Python Docs Toggle Menu PrevUp Next LangChain 0.0.337 langchain.document_loaders.news.NewsURLLoader langchain.document_loaders.news.NewsURLLoader class langchain.document_loaders.news.NewsURLLoader(urls: List[str], text_mode: bool = True, nlp: bool = False, continue_on_failure: bool = True, show_progress_bar: bool = False, **newspaper_kwargs: Any)[source] Load news articles from URLs using Unstructured. Parameters urls URLs to load. Each is loaded into its own document. text_mode If True, extract text from URL and use that for page content. Otherwise, extract raw HTML. nlp If True, perform NLP on the extracted contents, like providing a summary and extracting keywords. continue_on_failure If True, continue loading documents even if loading fails for a particular URL. show_progress_bar If True, use tqdm to show a loading progress bar. Requires tqdm to be installed, pip install tqdm. **newspaper_kwargs Any additional named arguments to pass to newspaper.Article(). Example from langchain.document_loaders import NewsURLLoader loader = NewsURLLoader( urls=["""", """"], ) docs = loader.load() Newspaper reference: Initialize with file path. Methods __init__(urls[,text_mode,nlp,...]) Initialize with file path. lazy_load() A lazy loader for Documents. load() Load data into Document objects. load_and_split([text_splitter]) Load Documents and split into chunks. __init__(urls: List[str], text_mode: bool = True, nlp: bool = False, continue_on_failure: bool = True, show_progress_bar: bool = False, **newspaper_kwargs: Any) None[source] Initialize with file path. lazy_load() Iterator[Document][source] A lazy loader for Documents. load() List[Document][source] Load data into Document objects. load_and_split(text_splitter: Optional[TextSplitter] = None) List[Document] Load Documents and split into chunks. Chunks are returned as Documents. Parameters text_splitter TextSplitter instance to use for splitting documents. Defaults to RecursiveCharacterTextSplitter. Returns List of Documents. Examples using NewsURLLoader News URL 2023, Harrison Chase. Last updated on Nov 17, 2023. Show this page source', 'URL | URL This covers how to load HTML documents from a list of URLs into a document format that we can use downstream. ```python from langchain.document_loaders import UnstructuredURLLoader ``` ```python urls = [ "" "" ] ``` Pass in ssl_verify=False with headers=headers to get past ssl_verification error. ```python loader = UnstructuredURLLoader(urls=urls) ``` ```python data = loader.load() ``` # Selenium URL Loader This covers how to load HTML documents from a list of URLs using the `SeleniumURLLoader`. Using selenium allows us to load pages that require JavaScript to render. ## Setup To use the `SeleniumURLLoader`, you will need to install `selenium` and `unstructured`. ```python from langchain.document_loaders import SeleniumURLLoader ``` ```python urls = [ "" "" ] ``` ```python loader = SeleniumURLLoader(urls=urls) ``` ```python data = loader.load() ``` # Playwright URL Loader This covers how to load HTML documents from a list of URLs using the `PlaywrightURLLoader`. As in the Selenium case, Playwright allows us to load pages that need JavaScript to render. ## Setup To use the `PlaywrightURLLoader`, you will need to install `playwright` and `unstructured`. Additionally, you will need to install the Playwright Chromium browser: ```bash # Install playwright pip install ""playwright"" pip install ""unstructured"" playwright install ``` ```python from langchain.document_loaders import PlaywrightURLLoader ``` ```python urls = [ "" "" ] ``` ```python loader = PlaywrightURLLoader(urls=urls, remove_selectors=[""header"", ""footer""]) ``` ```python data = loader.load() ``` - [Setup](#setup) - [Setup](#setup-1)']","To use `RecursiveURLLoader`, you need to import it and pass a list of URLs to load. Here's a brief example:

```python
from langchain.document_loaders import RecursiveURLLoader

urls = [""http://example.com""]
loader = RecursiveURLLoader(urls=urls)
data = loader.load()
```

This will load the initial URLs and recursively follow links to load more documents.","The RecursiveURLLoader comes from the langchain.document_loaders.recursive_url_loader module. Here's an example of how to use it:

```python
from langchain.document_loaders.recursive_url_loader import RecursiveUrlLoader

# Create an instance of RecursiveUrlLoader with the URL you want to load
loader = RecursiveUrlLoader(url=""https://example.com"")

# Load all child links from the URL page
documents = loader.load()

# Fetch the documents
for doc in documents:
    print(doc)
```

Make sure to replace ""https://example.com"" with the actual URL you want to load.",0.0,0.0,,0.062005030008404925,0.33576642335766427
40,what are the main methods supported by Runnables,"['Multiple chains | Multiple chains Runnables can easily be used to string together multiple Chains ```python from operator import itemgetter from langchain.chat_models import ChatOpenAI from langchain.prompts import ChatPromptTemplate from langchain.schema import StrOutputParser prompt1 = ChatPromptTemplate.from_template(""what is the city {person} is from?"") prompt2 = ChatPromptTemplate.from_template( ""what country is the city {city} in? respond in {language}"" ) model = ChatOpenAI() chain1 = prompt1 | model | StrOutputParser() chain2 = ( {""city"": chain1, ""language"": itemgetter(""language"")} | prompt2 | model | StrOutputParser() ) chain2.invoke({""person"": ""obama"", ""language"": ""spanish""}) ``` ```text \'El pas donde se encuentra la ciudad de Honolulu, donde naci Barack Obama, el 44 Presidente de los Estados Unidos, es Estados Unidos. Honolulu se encuentra en la isla de Oahu, en el estado de Hawi.\' ``` ```python from langchain.schema.runnable import RunnablePassthrough prompt1 = ChatPromptTemplate.from_template( ""generate a {attribute} color. Return the name of the color and nothing else:"" ) prompt2 = ChatPromptTemplate.from_template( ""what is a fruit of color: {color}. Return the name of the fruit and nothing else:"" ) prompt3 = ChatPromptTemplate.from_template( ""what is a country with a flag that has the color: {color}. Return the name of the country and nothing else:"" ) prompt4 = ChatPromptTemplate.from_template( ""What is the color of {fruit} and the flag of {country}?"" ) model_parser = model | StrOutputParser() color_generator = ( {""attribute"": RunnablePassthrough()} | prompt1 | {""color"": model_parser} ) color_to_fruit = prompt2 | model_parser color_to_country = prompt3 | model_parser question_generator = ( color_generator | {""fruit"": color_to_fruit, ""country"": color_to_country} | prompt4 ) ``` ```python question_generator.invoke(""warm"") ``` ```text ChatPromptValue(messages=[HumanMessage(content=\'What is the color of strawberry and the flag of China?\', additional_kwargs={}, example=False)]) ``` ```python prompt = question_generator.invoke(""warm"") model.invoke(prompt) ``` ```text AIMessage(content=\'The color of an apple is typically red or green. The flag of China is predominantly red with a large yellow star in the upper left corner and four smaller yellow stars surrounding it.\', additional_kwargs={}, example=False) ``` ### Branching and Merging You may want the output of one component to be processed by 2 or more other components. [RunnableMaps]( let you split or fork the chain so multiple components can process the input in parallel. Later, other components can join or merge the results to synthesize a final response. This type of chain creates a computation graph that looks like the following: ```text Input / \\ / \\ Branch1 Branch2 \\ / \\ / Combine ``` ```python planner = ( ChatPromptTemplate.from_template(""Generate an argument about: {input}"") | ChatOpenAI() | StrOutputParser() | {""base_response"": RunnablePassthrough()} ) arguments_for = ( ChatPromptTemplate.from_template( ""List the pros or positive aspects of {base_response}"" ) | ChatOpenAI() | StrOutputParser() ) arguments_against = ( ChatPromptTemplate.from_template( ""List the cons or negative aspects of {base_response}"" ) | ChatOpenAI() | StrOutputParser() ) final_responder = ( ChatPromptTemplate.from_messages( [ (""ai"", ""{original_response}""), (""human"", ""Pros:\\n{results_1}\\n\\nCons:\\n{results_2}""), (""system"", ""Generate a final response given the critique""), ] ) | ChatOpenAI() | StrOutputParser() ) chain = ( planner | { ""results_1"": arguments_for, ""results_2"": arguments_against, ""original_response"": itemgetter(""base_response""), } | final_responder ) ``` ```python chain.invoke({""input"": ""scrum""}) ``` ```text \'While Scrum has its potential cons and challenges, many organizations have successfully embraced and implemented this project management framework to great effect. The cons mentioned above can be mitigated or overcome with proper training, support, and a commitment to continuous improvement. It is also important to note that not all cons may be applicable to every organization or project.\\n\\nFor example, while Scrum may be complex initially, with proper training and guidance, teams can quickly grasp the concepts and practices. The lack of predictability can be mitigated by implementing techniques such as velocity tracking and release planning. The limited documentation can be addressed by maintaining a balance between lightweight documentation and clear communication among team members. The dependency on team collaboration can be improved through effective communication channels and regular team-building activities.\\n\\nScrum can be scaled and adapted to larger projects by using frameworks like Scrum of Scrums or LeSS (Large Scale Scrum). Concerns about speed versus quality can be addressed by incorporating quality assurance practices, such as continuous integration and automated testing, into the Scrum process. Scope creep can be managed by having a well-defined and prioritized product backlog, and a strong product owner can be developed through training and mentorship.\\n\\nResistance to change can be overcome by providing proper education and communication to stakeholders and involving them in the decision-making process. Ultimately, the cons of Scrum can be seen as opportunities for growth and improvement, and with the right mindset and support, they can be effectively managed.\\n\\nIn conclusion, while Scrum may have its challenges and potential cons, the benefits and advantages it offers in terms of collaboration, flexibility, adaptability, transparency, and customer satisfaction make it a widely adopted and successful project management framework. With proper implementation and continuous improvement, organizations can leverage Scrum to drive innovation, efficiency, and project success.\' ``` - [Branching and Merging](#branching-and-merging)', 'langchain.schema.runnable.retry.RunnableRetry LangChain 0.0.337 API Experimental Python Docs Toggle Menu PrevUp Next LangChain 0.0.337 langchain.schema.runnable.retry.RunnableRetry langchain.schema.runnable.retry.RunnableRetry class langchain.schema.runnable.retry.RunnableRetry[source] Bases: RunnableBindingBase[Input, Output] Retry a Runnable if it fails. A RunnableRetry helps can be used to add retry logic to any object that subclasses the base Runnable. Such retries are especially useful for network calls that may fail due to transient errors. The RunnableRetry is implemented as a RunnableBinding. The easiest way to use it is through the .with_retry() method on all Runnables. Example: Here\'s an example that uses a RunnableLambda to raise an exception import time def foo(input) -> None: \'\'\'Fake function that raises an exception.\'\'\' raise ValueError(""Invoking foo failed. At time {time.time()}"") runnable = RunnableLambda(foo) runnable_with_retries = runnable.with_retry( retry_exception_types=(ValueError,), # Retry only on ValueError wait_exponential_jitter=True, # Add jitter to the exponential backoff max_attempt_number=2, # Try twice ) # The method invocation above is equivalent to the longer form below: runnable_with_retries = RunnableRetry( bound=runnable, retry_exception_types=(ValueError,), max_attempt_number=2, wait_exponential_jitter=True ) This logic can be used to retry any Runnable, including a chain of Runnables, but in general it\'s best practice to keep the scope of the retry as small as possible. For example, if you have a chain of Runnables, you should only retry the Runnable that is likely to fail, not the entire chain. Example from langchain.chat_models import ChatOpenAI from langchain.prompts import PromptTemplate template = PromptTemplate.from_template(""tell me a joke about {topic}."") model = ChatOpenAI(temperature=0.5) # Good chain = template | model.with_retry() # Bad chain = template | model retryable_chain = chain.with_retry() Create a new model by parsing and validating input data from keyword arguments. Raises ValidationError if the input data cannot be parsed to form a valid model. param bound: Runnable[Input, Output] [Required] param config: RunnableConfig [Optional] param config_factories: List[Callable[[RunnableConfig], RunnableConfig]] [Optional] param custom_input_type: Optional[Any] = None param custom_output_type: Optional[Any] = None param kwargs: Mapping[str, Any] [Optional] param max_attempt_number: int = 3 The maximum number of attempts to retry the runnable. param retry_exception_types: Tuple[Type[BaseException], ...] = (,) The exception types to retry on. By default all exceptions are retried. In general you should only retry on exceptions that are likely to be transient, such as network errors. Good exceptions to retry are all server errors (5xx) and selected client errors (4xx) such as 429 Too Many Requests. param wait_exponential_jitter: bool = True Whether to add jitter to the exponential backoff. async abatch(inputs: List[Input], config: Optional[Union[RunnableConfig, List[RunnableConfig]]] = None, *, return_exceptions: bool = False, **kwargs: Any) List[Output][source] Default implementation runs ainvoke in parallel using asyncio.gather. The default implementation of batch works well for IO bound runnables. Subclasses should override this method if they can batch more efficiently; e.g., if the underlying runnable uses an API which supports a batch mode. async ainvoke(input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any) Output[source] Default implementation of ainvoke, calls invoke from a thread. The default implementation allows usage of async code even if the runnable did not implement a native async version of invoke. Subclasses should override this method if they can run asynchronously. async astream(input: Input, config: Optional[RunnableConfig] = None, **kwargs: Optional[Any]) AsyncIterator[Output] Default implementation of astream, which calls ainvoke. Subclasses should override this method if they support streaming output. async astream_log(input: Any, config: Optional[RunnableConfig] = None, *, diff: bool = True, include_names: Optional[Sequence[str]] = None, include_types: Optional[Sequence[str]] = None, include_tags: Optional[Sequence[str]] = None, exclude_names: Optional[Sequence[str]] = None, exclude_types: Optional[Sequence[str]] = None, exclude_tags: Optional[Sequence[str]] = None, **kwargs: Optional[Any]) Union[AsyncIterator[RunLogPatch], AsyncIterator[RunLog]] Stream all output from a runnable, as reported to the callback system. This includes all inner runs of LLMs, Retrievers, Tools, etc. Output is streamed as Log objects, which include a list of jsonpatch ops that describe how the state of the run has changed in each step, and the final state of the run. The jsonpatch ops can be applied in order to construct state. async atransform(input: AsyncIterator[Input], config: Optional[RunnableConfig] = None, **kwargs: Any) AsyncIterator[Output] Default implementation of atransform, which buffers input and calls astream. Subclasses should override this method if they can start producing output while input is still being generated. batch(inputs: List[Input], config: Optional[Union[RunnableConfig, List[RunnableConfig]]] = None, *, return_exceptions: bool = False, **kwargs: Any) List[Output][source] Default implementation runs invoke in parallel using a thread pool executor. The default implementation of batch works well for IO bound runnables. Subclasses should override this method if they can batch more efficiently; e.g., if the underlying runnable uses an API which supports a batch mode. bind(**kwargs: Any) Runnable[Input, Output] Bind arguments to a Runnable, returning a new Runnable. config_schema(*, include: Optional[Sequence[str]] = None) Type[BaseModel] The type of config this runnable accepts specified as a pydantic model. To mark a field as configurable, see the configurable_fields and configurable_alternatives methods. Parameters include A list of fields to include in the config schema. Returns A pydantic model that can be used to validate config. configurable_alternatives(which: ConfigurableField, default_key: str = \'default\', **kwargs: Union[Runnable[Input, Output], Callable[[], Runnable[Input, Output]]]) RunnableSerializable[Input, Output] configurable_fields(**kwargs: Union[ConfigurableField, ConfigurableFieldSingleOption, ConfigurableFieldMultiOption]) RunnableSerializable[Input, Output] classmethod construct(_fields_set: Optional[SetStr] = None, **values: Any)  Model Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = \'allow\' was set since it adds all passed values copy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False)  Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters include  fields to include in new model exclude  fields to exclude from new model, as with values this takes precedence over include update  values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data deep  set to True to make a deep copy of the model Returns new model instance dict(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False)  DictStrAny Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. classmethod from_orm(obj: Any)  Model get_input_schema(config: Optional[RunnableConfig] = None)  Type[BaseModel] Get a pydantic model that can be used to validate input to the runnable. Runnables that leverage the configurable_fields and configurable_alternatives methods will have a dynamic input schema that depends on which configuration the runnable is invoked with. This method allows to get an input schema for a specific configuration. Parameters config  A config to use when generating the schema. Returns A pydantic model that can be used to validate input. classmethod get_lc_namespace()  List[str] Get the namespace of the langchain object. For example, if the class is langchain.llms.openai.OpenAI, then the namespace is [langchain, llms, openai] get_output_schema(config: Optional[RunnableConfig] = None)  Type[BaseModel] Get a pydantic model that can be used to validate output to the runnable. Runnables that leverage the configurable_fields and configurable_alternatives methods will have a dynamic output schema that depends on which configuration the runnable is invoked with. This method allows to get an output schema for a specific configuration. Parameters config  A config to use when generating the schema. Returns A pydantic model that can be used to validate output. invoke(input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any)  Output[source] Transform a single input into an output. Override to implement. Parameters input  The input to the runnable. config  A config to use when invoking the runnable. The config supports standard keys like \'tags\', \'metadata\' for tracing purposes, \'max_concurrency\' for controlling how much work to do in parallel, and other keys. Please refer to the RunnableConfig for more details. Returns The output of the runnable. classmethod is_lc_serializable()  bool Is this class serializable? json(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any)  unicode Generate a JSON representation of the model, include and exclude arguments as per dict(). encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps(). classmethod lc_id()  List[str] A unique identifier for this class for serialization purposes. The unique identifier is a list of strings that describes the path to the object. map()  Runnable[List[Input], List[Output]] Return a new Runnable that maps a list of inputs to a list of outputs, by calling invoke() with each input. classmethod parse_file(path: Union[str, Path], *, content_type: unicode = None, encoding: unicode = \'utf8\', proto: Protocol = None, allow_pickle: bool = False)  Model classmethod parse_obj(obj: Any)  Model classmethod parse_raw(b: Union[str, bytes], *, content_type: unicode = None, encoding: unicode = \'utf8\', proto: Protocol = None, allow_pickle: bool = False)  Model classmethod schema(by_alias: bool = True, ref_template: unicode = \'#/definitions/{model}\')  DictStrAny classmethod schema_json(*, by_alias: bool = True, ref_template: unicode = \'#/definitions/{model}\', **dumps_kwargs: Any)  unicode stream(input: Input, config: Optional[RunnableConfig] = None, **kwargs: Optional[Any])  Iterator[Output] Default implementation of stream, which calls invoke. Subclasses should override this method if they support streaming output. to_json()  Union[SerializedConstructor, SerializedNotImplemented] to_json_not_implemented()  SerializedNotImplemented transform(input: Iterator[Input], config: Optional[RunnableConfig] = None, **kwargs: Any)  Iterator[Output] Default implementation of transform, which buffers input and then calls stream. Subclasses should override this method if they can start producing output while input is still being generated. classmethod update_forward_refs(**localns: Any)  None Try to update ForwardRefs on fields based on this Model, globalns and localns. classmethod validate(value: Any)  Model with_config(config: Optional[RunnableConfig] = None, **kwargs: Any)  Runnable[Input, Output] Bind config to a Runnable, returning a new Runnable. with_fallbacks(fallbacks: Sequence[Runnable[Input, Output]], *, exceptions_to_handle: Tuple[Type[BaseException], ...] = (,))  RunnableWithFallbacksT[Input, Output] Add fallbacks to a runnable, returning a new Runnable. Parameters fallbacks  A sequence of runnables to try if the original runnable fails. exceptions_to_handle  A tuple of exception types to handle. Returns A new Runnable that will try the original runnable, and then each fallback in order, upon failures. with_listeners(*, on_start: Optional[Listener] = None, on_end: Optional[Listener] = None, on_error: Optional[Listener] = None)  Runnable[Input, Output] Bind lifecycle listeners to a Runnable, returning a new Runnable. on_start: Called before the runnable starts running, with the Run object. on_end: Called after the runnable finishes running, with the Run object. on_error: Called if the runnable throws an error, with the Run object. The Run object contains information about the run, including its id, type, input, output, error, start_time, end_time, and any tags or metadata added to the run. with_retry(*, retry_if_exception_type: ~typing.Tuple[~typing.Type[BaseException], ...] = (,), wait_exponential_jitter: bool = True, stop_after_attempt: int = 3)  Runnable[Input, Output] Create a new Runnable that retries the original runnable on exceptions. Parameters retry_if_exception_type  A tuple of exception types to retry on wait_exponential_jitter  Whether to add jitter to the wait time between retries stop_after_attempt  The maximum number of attempts to make before giving up Returns A new Runnable that retries the original runnable on exceptions. with_types(*, input_type: Optional[Type[Input]] = None, output_type: Optional[Type[Output]] = None)  Runnable[Input, Output] Bind input and output types to a Runnable, returning a new Runnable. property InputType: Type[langchain.schema.runnable.utils.Input] The type of input this runnable accepts specified as a type annotation. property OutputType: Type[langchain.schema.runnable.utils.Output] The type of output this runnable produces specified as a type annotation. property config_specs: List[langchain.schema.runnable.utils.ConfigurableFieldSpec] List configurable fields for this runnable. property input_schema: Type[pydantic.main.BaseModel] The type of input this runnable accepts specified as a pydantic model. property lc_attributes: Dict List of attribute names that should be included in the serialized kwargs. These attributes must be accepted by the constructor. property lc_secrets: Dict[str, str] A map of constructor argument names to secret ids. For example,{openai_api_key: OPENAI_API_KEY} property output_schema: Type[pydantic.main.BaseModel] The type of output this runnable produces specified as a pydantic model.  2023, Harrison Chase. Last updated on Nov 17, 2023. Show this page source', 'Cookbook | Cookbook Example code for accomplishing common tasks with the LangChain Expression Language (LCEL). These examples show how to compose different Runnable (the core LCEL interface) components to achieve various tasks. If you\'re just getting acquainted with LCEL, the [Prompt + LLM](/docs/expression_language/cookbook/prompt_llm_parser) page is a good place to start. [ Prompt + LLMThe most common and valuable composition is taking:](/docs/expression_language/cookbook/prompt_llm_parser)[ RAGLet\'s look at adding in a retrieval step to a prompt and LLM, which adds up to a ""retrieval-augmented generation"" chain](/docs/expression_language/cookbook/retrieval)[ Multiple chainsRunnables can easily be used to string together multiple Chains](/docs/expression_language/cookbook/multiple_chains)[ Querying a SQL DBWe can replicate our SQLDatabaseChain with Runnables.](/docs/expression_language/cookbook/sql_db)[ AgentsYou can pass a Runnable into an agent.](/docs/expression_language/cookbook/agent)[ Code writingExample of how to use LCEL to write Python code.](/docs/expression_language/cookbook/code_writing)[ Routing by semantic similarityWith LCEL you can easily add custom routing logic to your chain to dynamically determine the chain logic based on user input. All you need to do is define a function that given an input returns a Runnable.](/docs/expression_language/cookbook/embedding_router)[ Adding memoryThis shows how to add memory to an arbitrary chain. Right now, you can use the memory classes but need to hook it up manually](/docs/expression_language/cookbook/memory)[ Adding moderationThis shows how to add in moderation (or other safeguards) around your LLM application.](/docs/expression_language/cookbook/moderation)[ Managing prompt sizeAgents dynamically call tools. The results of those tool calls are added back to the prompt, so that the agent can plan the next action. Depending on what tools are being used and how they\'re being called, the agent prompt can easily grow larger than the model context window.](/docs/expression_language/cookbook/prompt_size)[ Using toolsYou can use any Tools with Runnables easily.](/docs/expression_language/cookbook/tools)']","Runnables in LangChain support several key methods:

1. **invoke**: Transforms a single input into an output.
2. **ainvoke**: Asynchronous version of invoke.
3. **stream**: Streams output for an input.
4. **astream**: Asynchronous version of stream.
5. **batch**: Processes multiple inputs in parallel.
6. **abatch**: Asynchronous version of batch.
7. **transform**: Transforms an iterator of inputs into an iterator of outputs.
8. **atransform**: Asynchronous version of transform.

These methods allow for both synchronous and asynchronous execution, batch processing, and streaming outputs.","The `runnable` interface in LangChain supports several main methods for interacting with components:

1. `stream`: This method streams back response chunks in real-time, useful for processing large computations or text generation as they become available.

2. `invoke`: Calls the runnable chain on a single input, providing a single response output.

3. `batch`: Calls the runnable chain on a list of inputs, returning a list of corresponding responses.

4. `astream`: Asynchronously streams response chunks back, suitable for handling concurrent tasks.

5. `ainvoke`: Asynchronously calls the runnable chain on a single input and returns the response.

6. `abatch`: Asynchronously calls the runnable chain on a list of inputs, returning a list of corresponding responses.",0.9999999999666667,1.0,,0.06135328609452629,0.2694300518134715
41,What is html2texttransformer? Does it omit urls?,"['Web scraping | Web scraping []( ## Use case [Web research]( is one of the killer LLM applications: - Users have [highlighted it]( as one of his top desired AI tools. - OSS repos like [gpt-researcher]( are growing in popularity. ![Image description](/assets/images/web_scraping-001e2279b9e46c696012ac26d7f218a2.png) ## Overview Gathering content from the web has a few components: - `Search`: Query to url (e.g., using `GoogleSearchAPIWrapper`). - `Loading`: Url to HTML (e.g., using `AsyncHtmlLoader`, `AsyncChromiumLoader`, etc). - `Transforming`: HTML to formatted text (e.g., using `HTML2Text` or `Beautiful Soup`). ## Quickstart ```python pip install -q openai langchain playwright beautifulsoup4 playwright install # Set env var OPENAI_API_KEY or load from a .env file: # import dotenv # dotenv.load_dotenv() ``` Scraping HTML content using a headless instance of Chromium. - The async nature of the scraping process is handled using Python\'s asyncio library. - The actual interaction with the web pages is handled by Playwright. ```python from langchain.document_loaders import AsyncChromiumLoader from langchain.document_transformers import BeautifulSoupTransformer # Load HTML loader = AsyncChromiumLoader(["" html = loader.load() ``` Scrape text content tags such as `, , , and ` tags from the HTML content: - ``: The paragraph tag. It defines a paragraph in HTML and is used to group together related sentences and/or phrases. - ``: The list item tag. It is used within ordered (``) and unordered (``) lists to define individual items within the list. - ``: The division tag. It is a block-level element used to group other inline or block-level elements. - ``: The anchor tag. It is used to define hyperlinks. - ``: an inline container used to mark up a part of a text, or a part of a document. For many news websites (e.g., WSJ, CNN), headlines and summaries are all in `` tags. ```python # Transform bs_transformer = BeautifulSoupTransformer() docs_transformed = bs_transformer.transform_documents(html, tags_to_extract=[""span""]) ``` ```python # Result docs_transformed[0].page_content[0:500] ``` ```text \'English EditionEnglish (Chinese) (Japanese) More Other Products from WSJBuy Side from WSJWSJ ShopWSJ Wine Other Products from WSJ Search Quotes and Companies Search Quotes and Companies 0.15% 0.03% 0.12% -0.42% 4.102% -0.69% -0.25% -0.15% -1.82% 0.24% 0.19% -1.10% About Evan His Family Reflects His Reporting How You Can Help Write a Message Life in Detention Latest News Get Email Updates Four Americans Released From Iranian Prison The Americans will remain under house arrest until they are \' ``` These `Documents` now are staged for downstream usage in various LLM apps, as discussed below. ## Loader ### AsyncHtmlLoader The [AsyncHtmlLoader](/docs/use_cases/docs/integrations/document_loaders/async_html) uses the `aiohttp` library to make asynchronous HTTP requests, suitable for simpler and lightweight scraping. ### AsyncChromiumLoader The [AsyncChromiumLoader](/docs/use_cases/docs/integrations/document_loaders/async_chromium) uses Playwright to launch a Chromium instance, which can handle JavaScript rendering and more complex web interactions. Chromium is one of the browsers supported by Playwright, a library used to control browser automation. Headless mode means that the browser is running without a graphical user interface, which is commonly used for web scraping. ```python from langchain.document_loaders import AsyncHtmlLoader urls = ["" "" loader = AsyncHtmlLoader(urls) docs = loader.load() ``` ## Transformer ### HTML2Text [HTML2Text](/docs/use_cases/docs/integrations/document_transformers/html2text) provides a straightforward conversion of HTML content into plain text (with markdown-like formatting) without any specific tag manipulation. It\'s best suited for scenarios where the goal is to extract human-readable text without needing to manipulate specific HTML elements. ### Beautiful Soup Beautiful Soup offers more fine-grained control over HTML content, enabling specific tag extraction, removal, and content cleaning. It\'s suited for cases where you want to extract specific information and clean up the HTML content according to your needs. ```python from langchain.document_loaders import AsyncHtmlLoader urls = ["" "" loader = AsyncHtmlLoader(urls) docs = loader.load() ``` ```text Fetching pages: 100%|#############################################################################################################| 2/2 [00:00<00:00, 7.01it/s] ``` ```python from langchain.document_transformers import Html2TextTransformer html2text = Html2TextTransformer() docs_transformed = html2text.transform_documents(docs) docs_transformed[0].page_content[0:500] ``` ```text ""Skip to main content Skip to navigation\\n\\n\\n\\nMenu\\n\\n## ESPN\\n\\n * Search\\n\\n * * scores\\n\\n * NFL\\n * MLB\\n * NBA\\n * NHL\\n * Soccer\\n * NCAAF\\n * \\n\\n * Women\'s World Cup\\n * LLWS\\n * NCAAM\\n * NCAAW\\n * Sports Betting\\n * Boxing\\n * CFL\\n * NCAA\\n * Cricket\\n * F1\\n * Golf\\n * Horse\\n * MMA\\n * NASCAR\\n * NBA G League\\n * Olympic Sports\\n * PLL\\n * Racing\\n * RN BB\\n * RN FB\\n * Rugby\\n * Tennis\\n * WNBA\\n * WWE\\n * X Games\\n * XFL\\n\\n * More"" ``` ## Scraping with extraction ### LLM with function calling Web scraping is challenging for many reasons. One of them is the changing nature of modern websites\' layouts and content, which requires modifying scraping scripts to accommodate the changes. Using Function (e.g., OpenAI) with an extraction chain, we avoid having to change your code constantly when websites change. We\'re using `gpt-3.5-turbo-0613` to guarantee access to OpenAI Functions feature (although this might be available to everyone by time of writing). We\'re also keeping `temperature` at `0` to keep randomness of the LLM down. ```python from langchain.chat_models import ChatOpenAI llm = ChatOpenAI(temperature=0, model=""gpt-3.5-turbo-0613"") ``` ### Define a schema Next, you define a schema to specify what kind of data you want to extract. Here, the key names matter as they tell the LLM what kind of information they want. So, be as detailed as possible. In this example, we want to scrape only news article\'s name and summary from The Wall Street Journal website. ```python from langchain.chains import create_extraction_chain schema = { ""properties"": { ""news_article_title"": {""type"": ""string""}, ""news_article_summary"": {""type"": ""string""}, }, ""required"": [""news_article_title"", ""news_article_summary""], } def extract(content: str, schema: dict): return create_extraction_chain(schema=schema, llm=llm).run(content) ``` ### Run the web scraper w/ BeautifulSoup As shown above, we\'ll be using `BeautifulSoupTransformer`. ```python import pprint from langchain.text_splitter import RecursiveCharacterTextSplitter def scrape_with_playwright(urls, schema): loader = AsyncChromiumLoader(urls) docs = loader.load() bs_transformer = BeautifulSoupTransformer() docs_transformed = bs_transformer.transform_documents( docs, tags_to_extract=[""span""] ) print(""Extracting content with LLM"") # Grab the first 1000 tokens of the site splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder( chunk_size=1000, chunk_overlap=0 ) splits = splitter.split_documents(docs_transformed) # Process the first split extracted_content = extract(schema=schema, content=splits[0].page_content) pprint.pprint(extracted_content) return extracted_content urls = ["" extracted_content = scrape_with_playwright(urls, schema=schema) ``` ```text Extracting content with LLM [{\'news_article_summary\': \'The Americans will remain under house arrest until \' \'they are allowed to return to the U.S. in coming \' \'weeks, following a monthslong diplomatic push by \' \'the Biden administration.\', \'news_article_title\': \'Four Americans Released From Iranian Prison\'}, {\'news_article_summary\': \'Price pressures continued cooling last month, with \' \'the CPI rising a mild 0.2% from June, likely \' \'deterring the Federal Reserve from raising interest \' \'rates at its September meeting.\', \'news_article_title\': \'Cooler July Inflation Opens Door to Fed Pause on \' \'Rates\'}, {\'news_article_summary\': \'The company has decided to eliminate 27 of its 30 \' \'clothing labels, such as Lark & Ro and Goodthreads, \' \'as it works to fend off antitrust scrutiny and cut \' \'costs.\', \'news_article_title\': \'Amazon Cuts Dozens of House Brands\'}, {\'news_article_summary\': \'President Biden\'s order comes on top of a slowing \' \'Chinese economy, Covid lockdowns and rising \' \'tensions between the two powers.\', \'news_article_title\': \'U.S. Investment Ban on China Poised to Deepen Divide\'}, {\'news_article_summary\': \'The proposed trial date in the \' \'election-interference case comes on the same day as \' \'the former president\'s not guilty plea on \' \'additional Mar-a-Lago charges.\', \'news_article_title\': \'Trump Should Be Tried in January, Prosecutors Tell \' \'Judge\'}, {\'news_article_summary\': \'The CEO who started in June says the platform has \' \'an entirely different road map for the future.\', \'news_article_title\': \'Yaccarino Says X Is Watching Threads but Has Its Own \' \'Vision\'}, {\'news_article_summary\': \'Students foot the bill for flagship state \' \'universities that pour money into new buildings and \' \'programs with little pushback.\', \'news_article_title\': \'Colleges Spend Like There\'s No Tomorrow. \'These \' \'Places Are Just Devouring Money.\'\'}, {\'news_article_summary\': \'Wildfires fanned by hurricane winds have torn \' \'through parts of the Hawaiian island, devastating \' \'the popular tourist town of Lahaina.\', \'news_article_title\': \'Maui Wildfires Leave at Least 36 Dead\'}, {\'news_article_summary\': \'After its large armored push stalled, Kyiv has \' \'fallen back on the kind of tactics that brought it \' \'success earlier in the war.\', \'news_article_title\': \'Ukraine Uses Small-Unit Tactics to Retake Captured \' \'Territory\'}, {\'news_article_summary\': \'President Guillermo Lasso says the Aug. 20 election \' \'will proceed, as the Andean country grapples with \' \'rising drug gang violence.\', \'news_article_title\': \'Ecuador Declares State of Emergency After \' \'Presidential Hopeful Killed\'}, {\'news_article_summary\': \'This year\'s hurricane season, which typically runs \' \'from June to the end of November, has been \' \'difficult to predict, climate scientists said.\', \'news_article_title\': \'Atlantic Hurricane Season Prediction Increased to \' \'\'Above Normal,\' NOAA Says\'}, {\'news_article_summary\': \'The NFL is raising the price of its NFL+ streaming \' \'packages as it adds the NFL Network and RedZone.\', \'news_article_title\': \'NFL to Raise Price of NFL+ Streaming Packages as It \' \'Adds NFL Network, RedZone\'}, {\'news_article_summary\': \'Russia is planning a moon mission as part of the \' \'new space race.\', \'news_article_title\': \'Russia\'s Moon Mission and the New Space Race\'}, {\'news_article_summary\': \'Tapestry\'s $8.5 billion acquisition of Capri would \' \'create a conglomerate with more than $12 billion in \' \'annual sales, but it would still lack the \' \'high-wattage labels and diversity that have fueled \' \'LVMH\'s success.\', \'news_article_title\': ""Why the Coach and Kors Marriage Doesn\'t Scare LVMH""}, {\'news_article_summary\': \'The Supreme Court has blocked Purdue Pharma\'s $6 \' \'billion Sackler opioid settlement.\', \'news_article_title\': \'Supreme Court Blocks Purdue Pharma\'s $6 Billion \' \'Sackler Opioid Settlement\'}, {\'news_article_summary\': \'The Social Security COLA is expected to rise in \' \'2024, but not by a lot.\', \'news_article_title\': \'Social Security COLA Expected to Rise in 2024, but \' \'Not by a Lot\'}] ``` We can compare the headlines scraped to the page: ![Image description](/assets/images/wsj_page-1d5d8a3de02ec7579f5b0200dcb929b6.png) Looking at the [LangSmith trace]( we can see what is going on under the hood: - It\'s following what is explained in the [extraction](/docs/use_cases/docs/use_cases/extraction). - We call the `information_extraction` function on the input text. - It will attempt to populate the provided schema from the url content. ## Research automation Related to scraping, we may want to answer specific questions using searched content. We can automate the process of [web research]( using a retriever, such as the `WebResearchRetriever` ([docs]( ![Image description](/assets/images/web_research-f87a6bc469722c6804652383a65306de.png) Copy requirements [from here]( `pip install -r requirements.txt` Set `GOOGLE_CSE_ID` and `GOOGLE_API_KEY`. ```python from langchain.chat_models.openai import ChatOpenAI from langchain.embeddings import OpenAIEmbeddings from langchain.retrievers.web_research import WebResearchRetriever from langchain.utilities import GoogleSearchAPIWrapper from langchain.vectorstores import Chroma ``` ```python # Vectorstore vectorstore = Chroma( embedding_function=OpenAIEmbeddings(), persist_directory=""./chroma_db_oai"" ) # LLM llm = ChatOpenAI(temperature=0) # Search search = GoogleSearchAPIWrapper() ``` Initialize retriever with the above tools to: - Use an LLM to generate multiple relevant search queries (one LLM call) - Execute a search for each query - Choose the top K links per query (multiple search calls in parallel) - Load the information from all chosen links (scrape pages in parallel) - Index those documents into a vectorstore - Find the most relevant documents for each original generated search query ```python # Initialize web_research_retriever = WebResearchRetriever.from_llm( vectorstore=vectorstore, llm=llm, search=search ) ``` ```python # Run import logging logging.basicConfig() logging.getLogger(""langchain.retrievers.web_research"").setLevel(logging.INFO) from langchain.chains import RetrievalQAWithSourcesChain user_input = ""How do LLM Powered Autonomous Agents work?"" qa_chain = RetrievalQAWithSourcesChain.from_chain_type( llm, retriever=web_research_retriever ) result = qa_chain({""question"": user_input}) result ``` ```text INFO:langchain.retrievers.web_research:Generating questions for Google Search ... INFO:langchain.retrievers.web_research:Questions for Google Search (raw): {\'question\': \'How do LLM Powered Autonomous Agents work?\', \'text\': LineList(lines=[\'1. What is the functioning principle of LLM Powered Autonomous Agents?\\n\', \'2. How do LLM Powered Autonomous Agents operate?\\n\'])} INFO:langchain.retrievers.web_research:Questions for Google Search: [\'1. What is the functioning principle of LLM Powered Autonomous Agents?\\n\', \'2. How do LLM Powered Autonomous Agents operate?\\n\'] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{\'title\': \'LLM Powered Autonomous Agents | Hacker News\', \'link\': \' \'snippet\': \'Jun 26, 2023 ... Exactly. A temperature of 0 means you always pick the highest probability token (i.e. the ""max"" function), while a temperature of 1 means you\\xa0...\'}] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{\'title\': ""LLM Powered Autonomous Agents | Lil\'Log"", \'link\': \' \'snippet\': \'Jun 23, 2023 ... Task decomposition can be done (1) by LLM with simple prompting like ""Steps for XYZ.\\\\n1."" , ""What are the subgoals for achieving XYZ?"" , (2) by\\xa0...\'}] INFO:langchain.retrievers.web_research:New URLs to load: [] INFO:langchain.retrievers.web_research:Grabbing most relevant splits from urls... {\'question\': \'How do LLM Powered Autonomous Agents work?\', \'answer\': ""LLM-powered autonomous agents work by using LLM as the agent\'s brain, complemented by several key components such as planning, memory, and tool use. In terms of planning, the agent breaks down large tasks into smaller subgoals and can reflect and refine its actions based on past experiences. Memory is divided into short-term memory, which is used for in-context learning, and long-term memory, which allows the agent to retain and recall information over extended periods. Tool use involves the agent calling external APIs for additional information. These agents have been used in various applications, including scientific discovery and generative agents simulation."", \'sources\': \'\'} ``` ### Going deeper - Here\'s a [app]( that wraps this retriever with a lighweight UI. ## Question answering over a website To answer questions over a specific website, you can use Apify\'s [Website Content Crawler]( Actor, which can deeply crawl websites such as documentation, knowledge bases, help centers, or blogs, and extract text content from the web pages. In the example below, we will deeply crawl the Python documentation of LangChain\'s Chat LLM models and answer a question over it. First, install the requirements `pip install apify-client openai langchain chromadb tiktoken` Next, set `OPENAI_API_KEY` and `APIFY_API_TOKEN` in your environment variables. The full code follows: ```python from langchain.docstore.document import Document from langchain.indexes import VectorstoreIndexCreator from langchain.utilities import ApifyWrapper apify = ApifyWrapper() # Call the Actor to obtain text from the crawled webpages loader = apify.call_actor( actor_id=""apify/website-content-crawler"", run_input={ ""startUrls"": [{""url"": "" }, dataset_mapping_function=lambda item: Document( page_content=item[""text""] or """", metadata={""source"": item[""url""]} ), ) # Create a vector store based on the crawled data index = VectorstoreIndexCreator().from_loaders([loader]) # Query the vector store query = ""Are any OpenAI chat models integrated in LangChain?"" result = index.query(query) print(result) ``` ```text Yes, LangChain offers integration with OpenAI chat models. You can use the ChatOpenAI class to interact with OpenAI models. ``` - [Use case](#use-case) - [Overview](#overview) - [Quickstart](#quickstart) - [Loader](#loader)- [AsyncHtmlLoader](#asynchtmlloader) - [AsyncChromiumLoader](#asyncchromiumloader) - [Transformer](#transformer)- [HTML2Text](#html2text) - [Beautiful Soup](#beautiful-soup) - [Scraping with extraction](#scraping-with-extraction)- [LLM with function calling](#llm-with-function-calling) - [Define a schema](#define-a-schema) - [Run the web scraper w/ BeautifulSoup](#run-the-web-scraper-w-beautifulsoup) - [Research automation](#research-automation)- [Going deeper](#going-deeper) - [Question answering over a website](#question-answering-over-a-website)', 'HTML to text | HTML to text [html2text]( is a Python package that converts a page of `HTML` into clean, easy-to-read plain `ASCII text`. The ASCII also happens to be a valid `Markdown` (a text-to-HTML format). ```bash pip install html2text ``` ```python from langchain.document_loaders import AsyncHtmlLoader urls = ["" "" loader = AsyncHtmlLoader(urls) docs = loader.load() ``` ```text Fetching pages: 100%|############| 2/2 [00:00<00:00, 10.75it/s] ``` ```python from langchain.document_transformers import Html2TextTransformer ``` ```python urls = ["" "" html2text = Html2TextTransformer() docs_transformed = html2text.transform_documents(docs) ``` ```python docs_transformed[0].page_content[1000:2000] ``` ```text "" * ESPNFC\\n\\n * X Games\\n\\n * SEC Network\\n\\n## ESPN Apps\\n\\n * ESPN\\n\\n * ESPN Fantasy\\n\\n## Follow ESPN\\n\\n * Facebook\\n\\n * Twitter\\n\\n * Instagram\\n\\n * Snapchat\\n\\n * YouTube\\n\\n * The ESPN Daily Podcast\\n\\n2023 FIFA Women\'s World Cup\\n\\n## Follow live: Canada takes on Nigeria in group stage of Women\'s World Cup\\n\\n2m\\n\\nEPA/Morgan Hancock\\n\\n## TOP HEADLINES\\n\\n * Snyder fined $60M over findings in investigation\\n * NFL owners approve $6.05B sale of Commanders\\n * Jags assistant comes out as gay in NFL milestone\\n * O\'s alone atop East after topping slumping Rays\\n * ACC\'s Phillips: Never condoned hazing at NU\\n\\n * Vikings WR Addison cited for driving 140 mph\\n * \'Taking his time\': Patient QB Rodgers wows Jets\\n * Reyna got U.S. assurances after Berhalter rehire\\n * NFL Future Power Rankings\\n\\n## USWNT AT THE WORLD CUP\\n\\n### USA VS. VIETNAM: 9 P.M. ET FRIDAY\\n\\n## How do you defend against Alex Morgan? Former opponents sound off\\n\\nThe U.S. forward is unstoppable at this level, scoring 121 goals and adding 49"" ``` ```python docs_transformed[1].page_content[1000:2000] ``` ```text ""t\'s brain,\\ncomplemented by several key components:\\n\\n * **Planning**\\n * Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\n * Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n * **Memory**\\n * Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\n * Long-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n * **Tool use**\\n * The agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution c"" ```', ""langchain.document_transformers.beautiful_soup_transformer.BeautifulSoupTransformer LangChain 0.0.337 API Experimental Python Docs Toggle Menu PrevUp Next LangChain 0.0.337 langchain.document_transformers.beautiful_soup_transformer.BeautifulSoupTransformer langchain.document_transformers.beautiful_soup_transformer.BeautifulSoupTransformer class langchain.document_transformers.beautiful_soup_transformer.BeautifulSoupTransformer[source] Transform HTML content by extracting specific tags and removing unwanted ones. Example Initialize the transformer. This checks if the BeautifulSoup4 package is installed. If not, it raises an ImportError. Methods __init__() Initialize the transformer. atransform_documents(documents,**kwargs) Asynchronously transform a list of documents. extract_tags(html_content,tags) Extract specific tags from a given HTML content. remove_unnecessary_lines(content) Clean up the content by removing unnecessary lines. remove_unwanted_tags(html_content,unwanted_tags) Remove unwanted tags from a given HTML content. transform_documents(documents[,...]) Transform a list of Document objects by cleaning their HTML content. __init__() None[source] Initialize the transformer. This checks if the BeautifulSoup4 package is installed. If not, it raises an ImportError. async atransform_documents(documents: Sequence[Document], **kwargs: Any) Sequence[Document][source] Asynchronously transform a list of documents. Parameters documents A sequence of Documents to be transformed. Returns A list of transformed Documents. static extract_tags(html_content: str, tags: List[str]) str[source] Extract specific tags from a given HTML content. Parameters html_content The original HTML content string. tags A list of tags to be extracted from the HTML. Returns A string combining the content of the extracted tags. static remove_unnecessary_lines(content: str) str[source] Clean up the content by removing unnecessary lines. Parameters content A string, which may contain unnecessary lines or spaces. Returns A cleaned string with unnecessary lines removed. static remove_unwanted_tags(html_content: str, unwanted_tags: List[str]) str[source] Remove unwanted tags from a given HTML content. Parameters html_content The original HTML content string. unwanted_tags A list of tags to be removed from the HTML. Returns A cleaned HTML string with unwanted tags removed. transform_documents(documents: Sequence[Document], unwanted_tags: List[str] = ['script', 'style'], tags_to_extract: List[str] = ['p', 'li', 'div', 'a'], remove_lines: bool = True, **kwargs: Any) Sequence[Document][source] Transform a list of Document objects by cleaning their HTML content. Parameters documents A sequence of Document objects containing HTML content. unwanted_tags A list of tags to be removed from the HTML. tags_to_extract A list of tags whose content will be extracted. remove_lines If set to True, unnecessary lines will be content. (removed from the HTML) Returns A sequence of Document objects with transformed content. Examples using BeautifulSoupTransformer Beautiful Soup Set env var OPENAI_API_KEY or load from a .env file: 2023, Harrison Chase. Last updated on Nov 17, 2023. Show this page source""]","The `html2texttransformer` is a tool used to convert HTML content into plain text, often with Markdown formatting. It's designed to handle the complexities of HTML to produce clean, readable text. 

Regarding URLs, `html2texttransformer` typically retains URLs in the converted text, maintaining the original links. However, the exact behavior can depend on the specific implementation and configuration used. If you need URLs to be omitted, you might need to customize the transformation process or use additional post-processing steps to remove them from the output text. Always refer to the specific documentation of the `html2texttransformer` you are using for precise details on its behavior and customization options.","The `Html2TextTransformer` is a class provided by LangChain's `document_transformers` module. It is used to transform HTML content into clean, easy-to-read plain ASCII text.

This transformer is designed to convert HTML pages into human-readable text, making it suitable for scenarios where the goal is to extract content without the need to manipulate specific HTML elements.

If urls are present in the source Document's metadata, then they will be retained. If they are stored in the page_content itself, the class makes no guarantees that they will be retained.",0.99999999995,1.0,0.42857142857142855,0.18168109849175143,0.23999999999999996
42,I want to return the source documents of my Weaviate retriever. Show me how,"['Weaviate | Weaviate [Weaviate]( is an open-source vector database. It allows you to store data objects and vector embeddings from your favorite ML models, and scale seamlessly into billions of data objects. In the notebook, we\'ll demo the `SelfQueryRetriever` wrapped around a `Weaviate` vector store. ## Creating a Weaviate vector store First we\'ll want to create a Weaviate vector store and seed it with some data. We\'ve created a small demo set of documents that contain summaries of movies. **Note:** The self-query retriever requires you to have `lark` installed (`pip install lark`). We also need the `weaviate-client` package. ```python #!pip install lark weaviate-client ``` ```python from langchain.embeddings.openai import OpenAIEmbeddings from langchain.schema import Document from langchain.vectorstores import Weaviate embeddings = OpenAIEmbeddings() ``` ```python docs = [ Document( page_content=""A bunch of scientists bring back dinosaurs and mayhem breaks loose"", metadata={""year"": 1993, ""rating"": 7.7, ""genre"": ""science fiction""}, ), Document( page_content=""Leo DiCaprio gets lost in a dream within a dream within a dream within a ..."", metadata={""year"": 2010, ""director"": ""Christopher Nolan"", ""rating"": 8.2}, ), Document( page_content=""A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea"", metadata={""year"": 2006, ""director"": ""Satoshi Kon"", ""rating"": 8.6}, ), Document( page_content=""A bunch of normal-sized women are supremely wholesome and some men pine after them"", metadata={""year"": 2019, ""director"": ""Greta Gerwig"", ""rating"": 8.3}, ), Document( page_content=""Toys come alive and have a blast doing so"", metadata={""year"": 1995, ""genre"": ""animated""}, ), Document( page_content=""Three men walk into the Zone, three men walk out of the Zone"", metadata={ ""year"": 1979, ""director"": ""Andrei Tarkovsky"", ""genre"": ""science fiction"", ""rating"": 9.9, }, ), ] vectorstore = Weaviate.from_documents( docs, embeddings, weaviate_url="" ) ``` ## Creating our self-querying retriever Now we can instantiate our retriever. To do this we\'ll need to provide some information upfront about the metadata fields that our documents support and a short description of the document contents. ```python from langchain.chains.query_constructor.base import AttributeInfo from langchain.llms import OpenAI from langchain.retrievers.self_query.base import SelfQueryRetriever metadata_field_info = [ AttributeInfo( name=""genre"", description=""The genre of the movie"", type=""string or list[string]"", ), AttributeInfo( name=""year"", description=""The year the movie was released"", type=""integer"", ), AttributeInfo( name=""director"", description=""The name of the movie director"", type=""string"", ), AttributeInfo( name=""rating"", description=""A 1-10 rating for the movie"", type=""float"" ), ] document_content_description = ""Brief summary of a movie"" llm = OpenAI(temperature=0) retriever = SelfQueryRetriever.from_llm( llm, vectorstore, document_content_description, metadata_field_info, verbose=True ) ``` ## Testing it out And now we can try actually using our retriever! ```python # This example only specifies a relevant query retriever.get_relevant_documents(""What are some movies about dinosaurs"") ``` ```text query=\'dinosaur\' filter=None limit=None [Document(page_content=\'A bunch of scientists bring back dinosaurs and mayhem breaks loose\', metadata={\'genre\': \'science fiction\', \'rating\': 7.7, \'year\': 1993}), Document(page_content=\'Toys come alive and have a blast doing so\', metadata={\'genre\': \'animated\', \'rating\': None, \'year\': 1995}), Document(page_content=\'Three men walk into the Zone, three men walk out of the Zone\', metadata={\'genre\': \'science fiction\', \'rating\': 9.9, \'year\': 1979}), Document(page_content=\'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\', metadata={\'genre\': None, \'rating\': 8.6, \'year\': 2006})] ``` ```python # This example specifies a query and a filter retriever.get_relevant_documents(""Has Greta Gerwig directed any movies about women"") ``` ```text query=\'women\' filter=Comparison(comparator=, attribute=\'director\', value=\'Greta Gerwig\') limit=None [Document(page_content=\'A bunch of normal-sized women are supremely wholesome and some men pine after them\', metadata={\'genre\': None, \'rating\': 8.3, \'year\': 2019})] ``` ## Filter k We can also use the self query retriever to specify `k`: the number of documents to fetch. We can do this by passing `enable_limit=True` to the constructor. ```python retriever = SelfQueryRetriever.from_llm( llm, vectorstore, document_content_description, metadata_field_info, enable_limit=True, verbose=True, ) ``` ```python # This example only specifies a relevant query retriever.get_relevant_documents(""what are two movies about dinosaurs"") ``` ```text query=\'dinosaur\' filter=None limit=2 [Document(page_content=\'A bunch of scientists bring back dinosaurs and mayhem breaks loose\', metadata={\'genre\': \'science fiction\', \'rating\': 7.7, \'year\': 1993}), Document(page_content=\'Toys come alive and have a blast doing so\', metadata={\'genre\': \'animated\', \'rating\': None, \'year\': 1995})] ``` - [Creating a Weaviate vector store](#creating-a-weaviate-vector-store) - [Creating our self-querying retriever](#creating-our-self-querying-retriever) - [Testing it out](#testing-it-out) - [Filter k](#filter-k)', ""langchain.retrievers.weaviate_hybrid_search.WeaviateHybridSearchRetriever LangChain 0.0.337 API Experimental Python Docs Toggle Menu PrevUp Next LangChain 0.0.337 langchain.retrievers.weaviate_hybrid_search.WeaviateHybridSearchRetriever langchain.retrievers.weaviate_hybrid_search.WeaviateHybridSearchRetriever class langchain.retrievers.weaviate_hybrid_search.WeaviateHybridSearchRetriever[source] Bases: BaseRetriever Weaviate hybrid search retriever. See the documentation: Create a new model by parsing and validating input data from keyword arguments. Raises ValidationError if the input data cannot be parsed to form a valid model. param alpha: float = 0.5 The weight of the text key in the hybrid search. param attributes: List[str] [Required] The attributes to return in the results. param client: Any = None keyword arguments to pass to the Weaviate client. param create_schema_if_missing: bool = True Whether to create the schema if it doesn't exist. param index_name: str [Required] The name of the index to use. param k: int = 4 The number of results to return. param metadata: Optional[Dict[str, Any]] = None Optional metadata associated with the retriever. Defaults to None This metadata will be associated with each call to this retriever, and passed as arguments to the handlers defined in callbacks. You can use these to eg identify a specific instance of a retriever with its use case. param tags: Optional[List[str]] = None Optional list of tags associated with the retriever. Defaults to None These tags will be associated with each call to this retriever, and passed as arguments to the handlers defined in callbacks. You can use these to eg identify a specific instance of a retriever with its use case. param text_key: str [Required] The name of the text key to use. async abatch(inputs: List[Input], config: Optional[Union[RunnableConfig, List[RunnableConfig]]] = None, *, return_exceptions: bool = False, **kwargs: Optional[Any]) List[Output] Default implementation runs ainvoke in parallel using asyncio.gather. The default implementation of batch works well for IO bound runnables. Subclasses should override this method if they can batch more efficiently; e.g., if the underlying runnable uses an API which supports a batch mode. add_documents(docs: List[Document], **kwargs: Any) List[str][source] Upload documents to Weaviate. async aget_relevant_documents(query: str, *, callbacks: Callbacks = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, run_name: Optional[str] = None, **kwargs: Any) List[Document] Asynchronously get documents relevant to a query. :param query: string to find relevant documents for :param callbacks: Callback manager or list of callbacks :param tags: Optional list of tags associated with the retriever. Defaults to None These tags will be associated with each call to this retriever, and passed as arguments to the handlers defined in callbacks. Parameters metadata Optional metadata associated with the retriever. Defaults to None This metadata will be associated with each call to this retriever, and passed as arguments to the handlers defined in callbacks. Returns List of relevant documents async ainvoke(input: str, config: Optional[RunnableConfig] = None, **kwargs: Optional[Any]) List[Document] Default implementation of ainvoke, calls invoke from a thread. The default implementation allows usage of async code even if the runnable did not implement a native async version of invoke. Subclasses should override this method if they can run asynchronously. async astream(input: Input, config: Optional[RunnableConfig] = None, **kwargs: Optional[Any]) AsyncIterator[Output] Default implementation of astream, which calls ainvoke. Subclasses should override this method if they support streaming output. async astream_log(input: Any, config: Optional[RunnableConfig] = None, *, diff: bool = True, include_names: Optional[Sequence[str]] = None, include_types: Optional[Sequence[str]] = None, include_tags: Optional[Sequence[str]] = None, exclude_names: Optional[Sequence[str]] = None, exclude_types: Optional[Sequence[str]] = None, exclude_tags: Optional[Sequence[str]] = None, **kwargs: Optional[Any]) Union[AsyncIterator[RunLogPatch], AsyncIterator[RunLog]] Stream all output from a runnable, as reported to the callback system. This includes all inner runs of LLMs, Retrievers, Tools, etc. Output is streamed as Log objects, which include a list of jsonpatch ops that describe how the state of the run has changed in each step, and the final state of the run. The jsonpatch ops can be applied in order to construct state. async atransform(input: AsyncIterator[Input], config: Optional[RunnableConfig] = None, **kwargs: Optional[Any]) AsyncIterator[Output] Default implementation of atransform, which buffers input and calls astream. Subclasses should override this method if they can start producing output while input is still being generated. batch(inputs: List[Input], config: Optional[Union[RunnableConfig, List[RunnableConfig]]] = None, *, return_exceptions: bool = False, **kwargs: Optional[Any]) List[Output] Default implementation runs invoke in parallel using a thread pool executor. The default implementation of batch works well for IO bound runnables. Subclasses should override this method if they can batch more efficiently; e.g., if the underlying runnable uses an API which supports a batch mode. bind(**kwargs: Any) Runnable[Input, Output] Bind arguments to a Runnable, returning a new Runnable. config_schema(*, include: Optional[Sequence[str]] = None) Type[BaseModel] The type of config this runnable accepts specified as a pydantic model. To mark a field as configurable, see the configurable_fields and configurable_alternatives methods. Parameters include  A list of fields to include in the config schema. Returns A pydantic model that can be used to validate config. configurable_alternatives(which: ConfigurableField, default_key: str = 'default', **kwargs: Union[Runnable[Input, Output], Callable[[], Runnable[Input, Output]]])  RunnableSerializable[Input, Output] configurable_fields(**kwargs: Union[ConfigurableField, ConfigurableFieldSingleOption, ConfigurableFieldMultiOption])  RunnableSerializable[Input, Output] classmethod construct(_fields_set: Optional[SetStr] = None, **values: Any)  Model Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values copy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False)  Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters include  fields to include in new model exclude  fields to exclude from new model, as with values this takes precedence over include update  values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data deep  set to True to make a deep copy of the model Returns new model instance dict(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False)  DictStrAny Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. classmethod from_orm(obj: Any)  Model get_input_schema(config: Optional[RunnableConfig] = None)  Type[BaseModel] Get a pydantic model that can be used to validate input to the runnable. Runnables that leverage the configurable_fields and configurable_alternatives methods will have a dynamic input schema that depends on which configuration the runnable is invoked with. This method allows to get an input schema for a specific configuration. Parameters config  A config to use when generating the schema. Returns A pydantic model that can be used to validate input. classmethod get_lc_namespace()  List[str] Get the namespace of the langchain object. For example, if the class is langchain.llms.openai.OpenAI, then the namespace is [langchain, llms, openai] get_output_schema(config: Optional[RunnableConfig] = None)  Type[BaseModel] Get a pydantic model that can be used to validate output to the runnable. Runnables that leverage the configurable_fields and configurable_alternatives methods will have a dynamic output schema that depends on which configuration the runnable is invoked with. This method allows to get an output schema for a specific configuration. Parameters config  A config to use when generating the schema. Returns A pydantic model that can be used to validate output. get_relevant_documents(query: str, *, callbacks: Callbacks = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, run_name: Optional[str] = None, **kwargs: Any)  List[Document] Retrieve documents relevant to a query. :param query: string to find relevant documents for :param callbacks: Callback manager or list of callbacks :param tags: Optional list of tags associated with the retriever. Defaults to None These tags will be associated with each call to this retriever, and passed as arguments to the handlers defined in callbacks. Parameters metadata  Optional metadata associated with the retriever. Defaults to None This metadata will be associated with each call to this retriever, and passed as arguments to the handlers defined in callbacks. Returns List of relevant documents invoke(input: str, config: Optional[RunnableConfig] = None)  List[Document] Transform a single input into an output. Override to implement. Parameters input  The input to the runnable. config  A config to use when invoking the runnable. The config supports standard keys like 'tags', 'metadata' for tracing purposes, 'max_concurrency' for controlling how much work to do in parallel, and other keys. Please refer to the RunnableConfig for more details. Returns The output of the runnable. classmethod is_lc_serializable()  bool Is this class serializable? json(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any)  unicode Generate a JSON representation of the model, include and exclude arguments as per dict(). encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps(). classmethod lc_id()  List[str] A unique identifier for this class for serialization purposes. The unique identifier is a list of strings that describes the path to the object. map()  Runnable[List[Input], List[Output]] Return a new Runnable that maps a list of inputs to a list of outputs, by calling invoke() with each input. classmethod parse_file(path: Union[str, Path], *, content_type: unicode = None, encoding: unicode = 'utf8', proto: Protocol = None, allow_pickle: bool = False)  Model classmethod parse_obj(obj: Any)  Model classmethod parse_raw(b: Union[str, bytes], *, content_type: unicode = None, encoding: unicode = 'utf8', proto: Protocol = None, allow_pickle: bool = False)  Model classmethod schema(by_alias: bool = True, ref_template: unicode = '#/definitions/{model}')  DictStrAny classmethod schema_json(*, by_alias: bool = True, ref_template: unicode = '#/definitions/{model}', **dumps_kwargs: Any)  unicode stream(input: Input, config: Optional[RunnableConfig] = None, **kwargs: Optional[Any])  Iterator[Output] Default implementation of stream, which calls invoke. Subclasses should override this method if they support streaming output. to_json()  Union[SerializedConstructor, SerializedNotImplemented] to_json_not_implemented()  SerializedNotImplemented transform(input: Iterator[Input], config: Optional[RunnableConfig] = None, **kwargs: Optional[Any])  Iterator[Output] Default implementation of transform, which buffers input and then calls stream. Subclasses should override this method if they can start producing output while input is still being generated. classmethod update_forward_refs(**localns: Any)  None Try to update ForwardRefs on fields based on this Model, globalns and localns. classmethod validate(value: Any)  Model with_config(config: Optional[RunnableConfig] = None, **kwargs: Any)  Runnable[Input, Output] Bind config to a Runnable, returning a new Runnable. with_fallbacks(fallbacks: Sequence[Runnable[Input, Output]], *, exceptions_to_handle: Tuple[Type[BaseException], ...] = (,))  RunnableWithFallbacksT[Input, Output] Add fallbacks to a runnable, returning a new Runnable. Parameters fallbacks  A sequence of runnables to try if the original runnable fails. exceptions_to_handle  A tuple of exception types to handle. Returns A new Runnable that will try the original runnable, and then each fallback in order, upon failures. with_listeners(*, on_start: Optional[Listener] = None, on_end: Optional[Listener] = None, on_error: Optional[Listener] = None)  Runnable[Input, Output] Bind lifecycle listeners to a Runnable, returning a new Runnable. on_start: Called before the runnable starts running, with the Run object. on_end: Called after the runnable finishes running, with the Run object. on_error: Called if the runnable throws an error, with the Run object. The Run object contains information about the run, including its id, type, input, output, error, start_time, end_time, and any tags or metadata added to the run. with_retry(*, retry_if_exception_type: ~typing.Tuple[~typing.Type[BaseException], ...] = (,), wait_exponential_jitter: bool = True, stop_after_attempt: int = 3)  Runnable[Input, Output] Create a new Runnable that retries the original runnable on exceptions. Parameters retry_if_exception_type  A tuple of exception types to retry on wait_exponential_jitter  Whether to add jitter to the wait time between retries stop_after_attempt  The maximum number of attempts to make before giving up Returns A new Runnable that retries the original runnable on exceptions. with_types(*, input_type: Optional[Type[Input]] = None, output_type: Optional[Type[Output]] = None)  Runnable[Input, Output] Bind input and output types to a Runnable, returning a new Runnable. property InputType: Type[langchain.schema.runnable.utils.Input] The type of input this runnable accepts specified as a type annotation. property OutputType: Type[langchain.schema.runnable.utils.Output] The type of output this runnable produces specified as a type annotation. property config_specs: List[langchain.schema.runnable.utils.ConfigurableFieldSpec] List configurable fields for this runnable. property input_schema: Type[pydantic.main.BaseModel] The type of input this runnable accepts specified as a pydantic model. property lc_attributes: Dict List of attribute names that should be included in the serialized kwargs. These attributes must be accepted by the constructor. property lc_secrets: Dict[str, str] A map of constructor argument names to secret ids. For example,{openai_api_key: OPENAI_API_KEY} property output_schema: Type[pydantic.main.BaseModel] The type of output this runnable produces specified as a pydantic model. Examples using WeaviateHybridSearchRetriever Weaviate Hybrid Search  2023, Harrison Chase. Last updated on Nov 17, 2023. Show this page source"", 'DocArray | DocArray [DocArray]( is a versatile, open-source tool for managing your multi-modal data. It lets you shape your data however you want, and offers the flexibility to store and search it using various document index backends. Plus, it gets even better - you can utilize your `DocArray` document index to create a `DocArrayRetriever`, and build awesome Langchain apps! This notebook is split into two sections. The [first section](#document-index-backends) offers an introduction to all five supported document index backends. It provides guidance on setting up and indexing each backend and also instructs you on how to build a `DocArrayRetriever` for finding relevant documents. In the [second section](#movie-retrieval-using-hnswdocumentindex), we\'ll select one of these backends and illustrate how to use it through a basic example. ## Document Index Backends ```python import random from docarray import BaseDoc from docarray.typing import NdArray from langchain.embeddings import FakeEmbeddings from langchain.retrievers import DocArrayRetriever embeddings = FakeEmbeddings(size=32) ``` Before you start building the index, it\'s important to define your document schema. This determines what fields your documents will have and what type of data each field will hold. For this demonstration, we\'ll create a somewhat random schema containing \'title\' (str), \'title_embedding\' (numpy array), \'year\' (int), and \'color\' (str) ```python class MyDoc(BaseDoc): title: str title_embedding: NdArray[32] year: int color: str ``` ### InMemoryExactNNIndex `InMemoryExactNNIndex` stores all Documents in memory. It is a great starting point for small datasets, where you may not want to launch a database server. Learn more here: [ ```python from docarray.index import InMemoryExactNNIndex # initialize the index db = InMemoryExactNNIndex[MyDoc]() # index data db.index( [ MyDoc( title=f""My document {i}"", title_embedding=embeddings.embed_query(f""query {i}""), year=i, color=random.choice([""red"", ""green"", ""blue""]), ) for i in range(100) ] ) # optionally, you can create a filter query filter_query = {""year"": {""$lte"": 90}} ``` ```python # create a retriever retriever = DocArrayRetriever( index=db, embeddings=embeddings, search_field=""title_embedding"", content_field=""title"", filters=filter_query, ) # find the relevant document doc = retriever.get_relevant_documents(""some query"") print(doc) ``` ```text [Document(page_content=\'My document 56\', metadata={\'id\': \'1f33e58b6468ab722f3786b96b20afe6\', \'year\': 56, \'color\': \'red\'})] ``` ### HnswDocumentIndex `HnswDocumentIndex` is a lightweight Document Index implementation that runs fully locally and is best suited for small- to medium-sized datasets. It stores vectors on disk in [hnswlib]( and stores all other data in [SQLite]( Learn more here: [ ```python from docarray.index import HnswDocumentIndex # initialize the index db = HnswDocumentIndex[MyDoc](work_dir=""hnsw_index"") # index data db.index( [ MyDoc( title=f""My document {i}"", title_embedding=embeddings.embed_query(f""query {i}""), year=i, color=random.choice([""red"", ""green"", ""blue""]), ) for i in range(100) ] ) # optionally, you can create a filter query filter_query = {""year"": {""$lte"": 90}} ``` ```python # create a retriever retriever = DocArrayRetriever( index=db, embeddings=embeddings, search_field=""title_embedding"", content_field=""title"", filters=filter_query, ) # find the relevant document doc = retriever.get_relevant_documents(""some query"") print(doc) ``` ```text [Document(page_content=\'My document 28\', metadata={\'id\': \'ca9f3f4268eec7c97a7d6e77f541cb82\', \'year\': 28, \'color\': \'red\'})] ``` ### WeaviateDocumentIndex `WeaviateDocumentIndex` is a document index that is built upon [Weaviate]( vector database. Learn more here: [ ```python # There\'s a small difference with the Weaviate backend compared to the others. # Here, you need to \'mark\' the field used for vector search with \'is_embedding=True\'. # So, let\'s create a new schema for Weaviate that takes care of this requirement. from pydantic import Field class WeaviateDoc(BaseDoc): title: str title_embedding: NdArray[32] = Field(is_embedding=True) year: int color: str ``` ```python from docarray.index import WeaviateDocumentIndex # initialize the index dbconfig = WeaviateDocumentIndex.DBConfig(host="" db = WeaviateDocumentIndex[WeaviateDoc](db_config=dbconfig) # index data db.index( [ MyDoc( title=f""My document {i}"", title_embedding=embeddings.embed_query(f""query {i}""), year=i, color=random.choice([""red"", ""green"", ""blue""]), ) for i in range(100) ] ) # optionally, you can create a filter query filter_query = {""path"": [""year""], ""operator"": ""LessThanEqual"", ""valueInt"": ""90""} ``` ```python # create a retriever retriever = DocArrayRetriever( index=db, embeddings=embeddings, search_field=""title_embedding"", content_field=""title"", filters=filter_query, ) # find the relevant document doc = retriever.get_relevant_documents(""some query"") print(doc) ``` ```text [Document(page_content=\'My document 17\', metadata={\'id\': \'3a5b76e85f0d0a01785dc8f9d965ce40\', \'year\': 17, \'color\': \'red\'})] ``` ### ElasticDocIndex `ElasticDocIndex` is a document index that is built upon [ElasticSearch]( Learn more [here]( ```python from docarray.index import ElasticDocIndex # initialize the index db = ElasticDocIndex[MyDoc]( hosts="" index_name=""docarray_retriever"" ) # index data db.index( [ MyDoc( title=f""My document {i}"", title_embedding=embeddings.embed_query(f""query {i}""), year=i, color=random.choice([""red"", ""green"", ""blue""]), ) for i in range(100) ] ) # optionally, you can create a filter query filter_query = {""range"": {""year"": {""lte"": 90}}} ``` ```python # create a retriever retriever = DocArrayRetriever( index=db, embeddings=embeddings, search_field=""title_embedding"", content_field=""title"", filters=filter_query, ) # find the relevant document doc = retriever.get_relevant_documents(""some query"") print(doc) ``` ```text [Document(page_content=\'My document 46\', metadata={\'id\': \'edbc721bac1c2ad323414ad1301528a4\', \'year\': 46, \'color\': \'green\'})] ``` ### QdrantDocumentIndex `QdrantDocumentIndex` is a document index that is built upon [Qdrant]( vector database Learn more [here]( ```python from docarray.index import QdrantDocumentIndex from qdrant_client.http import models as rest # initialize the index qdrant_config = QdrantDocumentIndex.DBConfig(path="":memory:"") db = QdrantDocumentIndex[MyDoc](qdrant_config) # index data db.index( [ MyDoc( title=f""My document {i}"", title_embedding=embeddings.embed_query(f""query {i}""), year=i, color=random.choice([""red"", ""green"", ""blue""]), ) for i in range(100) ] ) # optionally, you can create a filter query filter_query = rest.Filter( must=[ rest.FieldCondition( key=""year"", range=rest.Range( gte=10, lt=90, ), ) ] ) ``` ```text WARNING:root:Payload indexes have no effect in the local Qdrant. Please use server Qdrant if you need payload indexes. ``` ```python # create a retriever retriever = DocArrayRetriever( index=db, embeddings=embeddings, search_field=""title_embedding"", content_field=""title"", filters=filter_query, ) # find the relevant document doc = retriever.get_relevant_documents(""some query"") print(doc) ``` ```text [Document(page_content=\'My document 80\', metadata={\'id\': \'97465f98d0810f1f330e4ecc29b13d20\', \'year\': 80, \'color\': \'blue\'})] ``` ## Movie Retrieval using HnswDocumentIndex ```python movies = [ { ""title"": ""Inception"", ""description"": ""A thief who steals corporate secrets through the use of dream-sharing technology is given the task of planting an idea into the mind of a CEO."", ""director"": ""Christopher Nolan"", ""rating"": 8.8, }, { ""title"": ""The Dark Knight"", ""description"": ""When the menace known as the Joker wreaks havoc and chaos on the people of Gotham, Batman must accept one of the greatest psychological and physical tests of his ability to fight injustice."", ""director"": ""Christopher Nolan"", ""rating"": 9.0, }, { ""title"": ""Interstellar"", ""description"": ""Interstellar explores the boundaries of human exploration as a group of astronauts venture through a wormhole in space. In their quest to ensure the survival of humanity, they confront the vastness of space-time and grapple with love and sacrifice."", ""director"": ""Christopher Nolan"", ""rating"": 8.6, }, { ""title"": ""Pulp Fiction"", ""description"": ""The lives of two mob hitmen, a boxer, a gangster\'s wife, and a pair of diner bandits intertwine in four tales of violence and redemption."", ""director"": ""Quentin Tarantino"", ""rating"": 8.9, }, { ""title"": ""Reservoir Dogs"", ""description"": ""When a simple jewelry heist goes horribly wrong, the surviving criminals begin to suspect that one of them is a police informant."", ""director"": ""Quentin Tarantino"", ""rating"": 8.3, }, { ""title"": ""The Godfather"", ""description"": ""An aging patriarch of an organized crime dynasty transfers control of his empire to his reluctant son."", ""director"": ""Francis Ford Coppola"", ""rating"": 9.2, }, ] ``` ```python import getpass import os os.environ[""OPENAI_API_KEY""] = getpass.getpass(""OpenAI API Key:"") ``` ```text OpenAI API Key: ``` ```python from docarray import BaseDoc, DocList from docarray.typing import NdArray from langchain.embeddings.openai import OpenAIEmbeddings # define schema for your movie documents class MyDoc(BaseDoc): title: str description: str description_embedding: NdArray[1536] rating: float director: str embeddings = OpenAIEmbeddings() # get ""description"" embeddings, and create documents docs = DocList[MyDoc]( [ MyDoc( description_embedding=embeddings.embed_query(movie[""description""]), **movie ) for movie in movies ] ) ``` ```python from docarray.index import HnswDocumentIndex # initialize the index db = HnswDocumentIndex[MyDoc](work_dir=""movie_search"") # add data db.index(docs) ``` ### Normal Retriever ```python from langchain.retrievers import DocArrayRetriever # create a retriever retriever = DocArrayRetriever( index=db, embeddings=embeddings, search_field=""description_embedding"", content_field=""description"", ) # find the relevant document doc = retriever.get_relevant_documents(""movie about dreams"") print(doc) ``` ```text [Document(page_content=\'A thief who steals corporate secrets through the use of dream-sharing technology is given the task of planting an idea into the mind of a CEO.\', metadata={\'id\': \'f1649d5b6776db04fec9a116bbb6bbe5\', \'title\': \'Inception\', \'rating\': 8.8, \'director\': \'Christopher Nolan\'})] ``` ### Retriever with Filters ```python from langchain.retrievers import DocArrayRetriever # create a retriever retriever = DocArrayRetriever( index=db, embeddings=embeddings, search_field=""description_embedding"", content_field=""description"", filters={""director"": {""$eq"": ""Christopher Nolan""}}, top_k=2, ) # find relevant documents docs = retriever.get_relevant_documents(""space travel"") print(docs) ``` ```text [Document(page_content=\'Interstellar explores the boundaries of human exploration as a group of astronauts venture through a wormhole in space. In their quest to ensure the survival of humanity, they confront the vastness of space-time and grapple with love and sacrifice.\', metadata={\'id\': \'ab704cc7ae8573dc617f9a5e25df022a\', \'title\': \'Interstellar\', \'rating\': 8.6, \'director\': \'Christopher Nolan\'}), Document(page_content=\'A thief who steals corporate secrets through the use of dream-sharing technology is given the task of planting an idea into the mind of a CEO.\', metadata={\'id\': \'f1649d5b6776db04fec9a116bbb6bbe5\', \'title\': \'Inception\', \'rating\': 8.8, \'director\': \'Christopher Nolan\'})] ``` ### Retriever with MMR search ```python from langchain.retrievers import DocArrayRetriever # create a retriever retriever = DocArrayRetriever( index=db, embeddings=embeddings, search_field=""description_embedding"", content_field=""description"", filters={""rating"": {""$gte"": 8.7}}, search_type=""mmr"", top_k=3, ) # find relevant documents docs = retriever.get_relevant_documents(""action movies"") print(docs) ``` ```text [Document(page_content=""The lives of two mob hitmen, a boxer, a gangster\'s wife, and a pair of diner bandits intertwine in four tales of violence and redemption."", metadata={\'id\': \'e6aa313bbde514e23fbc80ab34511afd\', \'title\': \'Pulp Fiction\', \'rating\': 8.9, \'director\': \'Quentin Tarantino\'}), Document(page_content=\'A thief who steals corporate secrets through the use of dream-sharing technology is given the task of planting an idea into the mind of a CEO.\', metadata={\'id\': \'f1649d5b6776db04fec9a116bbb6bbe5\', \'title\': \'Inception\', \'rating\': 8.8, \'director\': \'Christopher Nolan\'}), Document(page_content=\'When the menace known as the Joker wreaks havoc and chaos on the people of Gotham, Batman must accept one of the greatest psychological and physical tests of his ability to fight injustice.\', metadata={\'id\': \'91dec17d4272041b669fd113333a65f7\', \'title\': \'The Dark Knight\', \'rating\': 9.0, \'director\': \'Christopher Nolan\'})] ``` - [Document Index Backends](#document-index-backends)- [InMemoryExactNNIndex](#inmemoryexactnnindex) - [HnswDocumentIndex](#hnswdocumentindex) - [WeaviateDocumentIndex](#weaviatedocumentindex) - [ElasticDocIndex](#elasticdocindex) - [QdrantDocumentIndex](#qdrantdocumentindex) - [Movie Retrieval using HnswDocumentIndex](#movie-retrieval-using-hnswdocumentindex)- [Normal Retriever](#normal-retriever) - [Retriever with Filters](#retriever-with-filters) - [Retriever with MMR search](#retriever-with-mmr-search)']","To return the source documents of your Weaviate retriever, you can use the `get_relevant_documents` method with the `return_source_documents` parameter set to `True`. Here's an example:

```python
# Assuming retriever is your initialized Weaviate retriever
documents = retriever.get_relevant_documents(
    ""some query"", 
    return_source_documents=True
)

for doc in documents:
    print(doc)
```

This will print out the relevant documents along with their source information.","To retrieve the source documents from your Weaviate vectorstore, you need to configure your client to include the 'sources' attribute. Here's an example of how you can achieve this:

```python
weaviate_client = Weaviate(
    client=client,
    index_name='index_name',
    text_key='text',
    embedding=OpenAIEmbeddings(),
    by_text=False,
    attributes=['source'],
)
```

By including the 'source' attribute in the attributes list, you ensure that the source documents are returned when you interact with the Weaviate client.",0.99999999995,1.0,0.3333333333333333,0.08209568436078463,0.28368794326241137
43,what is RAG,"['Astra DB | Astra DB This page provides a quickstart for using [Astra DB]( and [Apache Cassandra]( as a Vector Store. _Note: in addition to access to the database, an OpenAI API Key is required to run the full example._ ### Setup and general dependencies Use of the integration requires the following Python package. ```bash pip install --quiet ""astrapy>=0.5.3"" ``` _Note: depending on your LangChain setup, you may need to install/upgrade other dependencies needed for this demo_ _(specifically, recent versions of datasets, openai, pypdf and tiktoken are required)._ ```python import os from getpass import getpass from datasets import ( load_dataset, ) from langchain.chat_models import ChatOpenAI from langchain.document_loaders import PyPDFLoader from langchain.embeddings import OpenAIEmbeddings from langchain.prompts import ChatPromptTemplate from langchain.schema import Document from langchain.schema.output_parser import StrOutputParser from langchain.schema.runnable import RunnablePassthrough from langchain.text_splitter import RecursiveCharacterTextSplitter ``` ```python os.environ[""OPENAI_API_KEY""] = getpass(""OPENAI_API_KEY = "") ``` ```python embe = OpenAIEmbeddings() ``` _Keep reading to connect with Astra DB. For usage with Apache Cassandra and Astra DB through CQL, scroll to the section below._ ## Astra DB DataStax [Astra DB]( is a serverless vector-capable database built on Cassandra and made conveniently available through an easy-to-use JSON API. ```python from langchain.vectorstores import AstraDB ``` ### Astra DB connection parameters - the API Endpoint looks like ` - the Token looks like `AstraCS:6gBhNmsk135....` ```python ASTRA_DB_API_ENDPOINT = input(""ASTRA_DB_API_ENDPOINT = "") ASTRA_DB_APPLICATION_TOKEN = getpass(""ASTRA_DB_APPLICATION_TOKEN = "") ``` ```python vstore = AstraDB( embedding=embe, collection_name=""astra_vector_demo"", api_endpoint=ASTRA_DB_API_ENDPOINT, token=ASTRA_DB_APPLICATION_TOKEN, ) ``` ### Load a dataset Convert each entry in the source dataset into a `Document`, then write them into the vector store: ```python philo_dataset = load_dataset(""datastax/philosopher-quotes"")[""train""] docs = [] for entry in philo_dataset: metadata = {""author"": entry[""author""]} doc = Document(page_content=entry[""quote""], metadata=metadata) docs.append(doc) inserted_ids = vstore.add_documents(docs) print(f""\\nInserted {len(inserted_ids)} documents."") ``` In the above, `metadata` dictionaries are created from the source data and are part of the `Document`. _Note: check the Astra DB API Docs for the valid metadata field names: some characters are reserved and cannot be used._ Add some more entries, this time with `add_texts`: ```python texts = [""I think, therefore I am."", ""To the things themselves!""] metadatas = [{""author"": ""descartes""}, {""author"": ""husserl""}] ids = [""desc_01"", ""huss_xy""] inserted_ids_2 = vstore.add_texts(texts=texts, metadatas=metadatas, ids=ids) print(f""\\nInserted {len(inserted_ids_2)} documents."") ``` _Note: you may want to speed up the execution of add_texts and add_documents by increasing the concurrency level for_ _these bulk operations - check out the *_concurrency parameters in the class constructor and the add_texts docstrings_ _for more details. Depending on the network and the client machine specifications, your best-performing choice of parameters may vary._ ### Run simple searches This section demonstrates metadata filtering and getting the similarity scores back: ```python results = vstore.similarity_search(""Our life is what we make of it"", k=3) for res in results: print(f""* {res.page_content} [{res.metadata}]"") ``` ```python results_filtered = vstore.similarity_search( ""Our life is what we make of it"", k=3, filter={""author"": ""plato""}, ) for res in results_filtered: print(f""* {res.page_content} [{res.metadata}]"") ``` ```python results = vstore.similarity_search_with_score(""Our life is what we make of it"", k=3) for res, score in results: print(f""* [SIM={score:3f}] {res.page_content} [{res.metadata}]"") ``` ### MMR (Maximal-marginal-relevance) search ```python results = vstore.max_marginal_relevance_search( ""Our life is what we make of it"", k=3, filter={""author"": ""aristotle""}, ) for res in results: print(f""* {res.page_content} [{res.metadata}]"") ``` ### Deleting stored documents ```python delete_1 = vstore.delete(inserted_ids[:3]) print(f""all_succeed={delete_1}"") # True, all documents deleted ``` ```python delete_2 = vstore.delete(inserted_ids[2:5]) print(f""some_succeeds={delete_2}"") # True, though some IDs were gone already ``` ### A minimal RAG chain The next cells will implement a simple RAG pipeline: - download a sample PDF file and load it onto the store; - create a RAG chain with LCEL (LangChain Expression Language), with the vector store at its heart; - run the question-answering chain. ```bash curl -L \\ "" \\ -o ""what-is-philosophy.pdf"" ``` ```python pdf_loader = PyPDFLoader(""what-is-philosophy.pdf"") splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=64) docs_from_pdf = pdf_loader.load_and_split(text_splitter=splitter) print(f""Documents from PDF: {len(docs_from_pdf)}."") inserted_ids_from_pdf = vstore.add_documents(docs_from_pdf) print(f""Inserted {len(inserted_ids_from_pdf)} documents."") ``` ```python retriever = vstore.as_retriever(search_kwargs={""k"": 3}) philo_template = """""" You are a philosopher that draws inspiration from great thinkers of the past to craft well-thought answers to user questions. Use the provided context as the basis for your answers and do not make up new reasoning paths - just mix-and-match what you are given. Your answers must be concise and to the point, and refrain from answering about other topics than philosophy. CONTEXT: {context} QUESTION: {question} YOUR ANSWER:"""""" philo_prompt = ChatPromptTemplate.from_template(philo_template) llm = ChatOpenAI() chain = ( {""context"": retriever, ""question"": RunnablePassthrough()} | philo_prompt | llm | StrOutputParser() ) ``` ```python chain.invoke(""How does Russel elaborate on Peirce\'s idea of the security blanket?"") ``` For more, check out a complete RAG template using Astra DB [here]( ### Cleanup If you want to completely delete the collection from your Astra DB instance, run this. _(You will lose the data you stored in it.)_ ```python vstore.delete_collection() ``` ## Apache Cassandra and Astra DB through CQL [Cassandra]( is a NoSQL, row-oriented, highly scalable and highly available database.Starting with version 5.0, the database ships with [vector search capabilities]( DataStax [Astra DB through CQL]( is a managed serverless database built on Cassandra, offering the same interface and strengths. #### What sets this case apart from ""Astra DB"" above? Thanks to LangChain having a standardized `VectorStore` interface, most of the ""Astra DB"" section above applies to this case as well. However, this time the database uses the CQL protocol, which means you\'ll use a _different_ class this time and instantiate it in another way. The cells below show how you should get your `vstore` object in this case and how you can clean up the database resources at the end: for the rest, i.e. the actual usage of the vector store, you will be able to run the very code that was shown above. In other words, running this demo in full with Cassandra or Astra DB through CQL means: - **initialization as shown below** - ""Load a dataset"", _see above section_ - ""Run simple searches"", _see above section_ - ""MMR search"", _see above section_ - ""Deleting stored documents"", _see above section_ - ""A minimal RAG chain"", _see above section_ - **cleanup as shown below** ### Initialization The class to use is the following: ```python from langchain.vectorstores import Cassandra ``` Now, depending on whether you connect to a Cassandra cluster or to Astra DB through CQL, you will provide different parameters when creating the vector store object. #### Initialization (Cassandra cluster) In this case, you first need to create a `cassandra.cluster.Session` object, as described in the [Cassandra driver documentation]( The details vary (e.g. with network settings and authentication), but this might be something like: ```python from cassandra.cluster import Cluster cluster = Cluster([""127.0.0.1""]) session = cluster.connect() ``` You can now set the session, along with your desired keyspace name, as a global CassIO parameter: ```python import cassio CASSANDRA_KEYSPACE = input(""CASSANDRA_KEYSPACE = "") cassio.init(session=session, keyspace=CASSANDRA_KEYSPACE) ``` Now you can create the vector store: ```python vstore = Cassandra( embedding=embe, table_name=""cassandra_vector_demo"", # session=None, keyspace=None # Uncomment on older versions of LangChain ) ``` #### Initialization (Astra DB through CQL) In this case you initialize CassIO with the following connection parameters: - the Database ID, e.g. `01234567-89ab-cdef-0123-456789abcdef` - the Token, e.g. `AstraCS:6gBhNmsk135....` (it must be a ""Database Administrator"" token) - Optionally a Keyspace name (if omitted, the default one for the database will be used) ```python ASTRA_DB_ID = input(""ASTRA_DB_ID = "") ASTRA_DB_APPLICATION_TOKEN = getpass(""ASTRA_DB_APPLICATION_TOKEN = "") desired_keyspace = input(""ASTRA_DB_KEYSPACE (optional, can be left empty) = "") if desired_keyspace: ASTRA_DB_KEYSPACE = desired_keyspace else: ASTRA_DB_KEYSPACE = None ``` ```python import cassio cassio.init( database_id=ASTRA_DB_ID, token=ASTRA_DB_APPLICATION_TOKEN, keyspace=ASTRA_DB_KEYSPACE, ) ``` Now you can create the vector store: ```python vstore = Cassandra( embedding=embe, table_name=""cassandra_vector_demo"", # session=None, keyspace=None # Uncomment on older versions of LangChain ) ``` ### Usage of the vector store _See the sections ""Load a dataset"" through ""A minimal RAG chain"" above._ Speaking of the latter, you can check out a full RAG template for Astra DB through CQL [here]( ### Cleanup the following essentially retrieves the `Session` object from CassIO and runs a CQL `DROP TABLE` statement with it: ```python cassio.config.resolve_session().execute( f""DROP TABLE {cassio.config.resolve_keyspace()}.cassandra_vector_demo;"" ) ``` ### Learn more For more information, extended quickstarts and additional usage examples, please visit the [CassIO documentation]( for more on using the LangChain `Cassandra` vector store. - [Setup and general dependencies](#setup-and-general-dependencies) - [Astra DB](#astra-db-1)- [Astra DB connection parameters](#astra-db-connection-parameters) - [Load a dataset](#load-a-dataset) - [Run simple searches](#run-simple-searches) - [MMR (Maximal-marginal-relevance) search](#mmr-maximal-marginal-relevance-search) - [Deleting stored documents](#deleting-stored-documents) - [A minimal RAG chain](#a-minimal-rag-chain) - [Cleanup](#cleanup) - [Apache Cassandra and Astra DB through CQL](#apache-cassandra-and-astra-db-through-cql)- [Initialization](#initialization) - [Usage of the vector store](#usage-of-the-vector-store) - [Cleanup](#cleanup-1) - [Learn more](#learn-more)', 'RAG with Timescale Vector using hybrid search | RAG with Timescale Vector using hybrid search This template shows how to use timescale-vector with the self-query retriver to perform hybrid search on similarity and time. This is useful any time your data has a strong time-based component. Some examples of such data are: - News articles (politics, business, etc) - Blog posts, documentation or other published material (public or private). - Social media posts - Changelogs of any kind - Messages Such items are often searched by both similarity and time. For example: Show me all news about Toyota trucks from 2022. [Timescale Vector]( provides superior performance when searching for embeddings within a particular timeframe by leveraging automatic table partitioning to isolate data for particular time-ranges. Langchain\'s self-query retriever allows deducing time-ranges (as well as other search criteria) from the text of user queries. ## What is Timescale Vector? **Timescale Vector is PostgreSQL++ for AI applications.** Timescale Vector enables you to efficiently store and query billions of vector embeddings in `PostgreSQL`. - Enhances `pgvector` with faster and more accurate similarity search on 1B+ vectors via DiskANN inspired indexing algorithm. - Enables fast time-based vector search via automatic time-based partitioning and indexing. - Provides a familiar SQL interface for querying vector embeddings and relational data. Timescale Vector is cloud PostgreSQL for AI that scales with you from POC to production: - Simplifies operations by enabling you to store relational metadata, vector embeddings, and time-series data in a single database. - Benefits from rock-solid PostgreSQL foundation with enterprise-grade feature liked streaming backups and replication, high-availability and row-level security. - Enables a worry-free experience with enterprise-grade security and compliance. ### How to access Timescale Vector Timescale Vector is available on [Timescale]( the cloud PostgreSQL platform. (There is no self-hosted version at this time.) - LangChain users get a 90-day free trial for Timescale Vector. - To get started, [signup]( to Timescale, create a new database and follow this notebook! - See the [installation instructions]( for more details on using Timescale Vector in python. ## Environment Setup This template uses Timescale Vector as a vectorstore and requires that `TIMESCALES_SERVICE_URL`. Signup for a 90-day trial [here]( if you don\'t yet have an account. To load the sample dataset, set `LOAD_SAMPLE_DATA=1`. To load your own dataset see the section below. Set the `OPENAI_API_KEY` environment variable to access the OpenAI models. ## Usage To use this package, you should first have the LangChain CLI installed: ```shell pip install -U langchain-cli ``` To create a new LangChain project and install this as the only package, you can do: ```shell langchain app new my-app --package rag-timescale-hybrid-search-time ``` If you want to add this to an existing project, you can just run: ```shell langchain app add rag-timescale-hybrid-search-time ``` And add the following code to your `server.py` file: ```python from rag_timescale_hybrid_search.chain import chain as rag_timescale_hybrid_search_chain add_routes(app, rag_timescale_hybrid_search_chain, path=""/rag-timescale-hybrid-search"") ``` (Optional) Let\'s now configure LangSmith. LangSmith will help us trace, monitor and debug LangChain applications. LangSmith is currently in private beta, you can sign up [here]( If you don\'t have access, you can skip this section ```shell export LANGCHAIN_TRACING_V2=true export LANGCHAIN_API_KEY= export LANGCHAIN_PROJECT= # if not specified, defaults to ""default"" ``` If you are inside this directory, then you can spin up a LangServe instance directly by: ```shell langchain serve ``` This will start the FastAPI app with a server is running locally at [ We can see all templates at [ We can access the playground at [ We can access the template from code with: ```python from langserve.client import RemoteRunnable runnable = RemoteRunnable("" ``` ## Loading your own dataset To load your own dataset you will have to modify the code in the `DATASET SPECIFIC CODE` section of `chain.py`. This code defines the name of the collection, how to load the data, and the human-language description of both the contents of the collection and all of the metadata. The human-language descriptions are used by the self-query retriever to help the LLM convert the question into filters on the metadata when searching the data in Timescale-vector. - [What is Timescale Vector?](#what-is-timescale-vector)- [How to access Timescale Vector](#how-to-access-timescale-vector) - [Environment Setup](#environment-setup) - [Usage](#usage) - [Loading your own dataset](#loading-your-own-dataset)', 'Retrieval-augmented generation (RAG) | Retrieval-augmented generation (RAG) []( ## Overview ### What is RAG? RAG is a technique for augmenting LLM knowledge with additional, often private or real-time, data. LLMs can reason about wide-ranging topics, but their knowledge is limited to the public data up to a specific point in time that they were trained on. If you want to build AI applications that can reason about private data or data introduced after a model\'s cutoff date, you need to augment the knowledge of the model with the specific information it needs. The process of bringing the appropriate information and inserting it into the model prompt is known as Retrieval Augmented Generation (RAG). ### What\'s in this guide? LangChain has a number of components specifically designed to help build RAG applications. To familiarize ourselves with these, we\'ll build a simple question-answering application over a text data source. Specifically, we\'ll build a QA bot over the [LLM Powered Autonomous Agents]( blog post by Lilian Weng. Along the way we\'ll go over a typical QA architecture, discuss the relevant LangChain components, and highlight additional resources for more advanced QA techniques. We\'ll also see how LangSmith can help us trace and understand our application. LangSmith will become increasingly helpful as our application grows in complexity. **Note** Here we focus on RAG for unstructured data. Two RAG use cases which we cover elsewhere are: - [QA over structured data](/docs/use_cases/qa_structured/sql) (e.g., SQL) - [QA over code](/docs/use_cases/question_answering/code_understanding) (e.g., Python) ## Architecture A typical RAG application has two main components: **Indexing**: a pipeline for ingesting data from a source and indexing it. _This usually happen offline._ **Retrieval and generation**: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model. The most common full sequence from raw data to answer looks like: #### Indexing 1. **Load**: First we need to load our data. We\'ll use [DocumentLoaders](/docs/modules/data_connection/document_loaders/) for this. 2. **Split**: [Text splitters](/docs/modules/data_connection/document_transformers/) break large `Documents` into smaller chunks. This is useful both for indexing data and for passing it in to a model, since large chunks are harder to search over and won\'t in a model\'s finite context window. 3. **Store**: We need somewhere to store and index our splits, so that they can later be searched over. This is often done using a [VectorStore](/docs/modules/data_connection/vectorstores/) and [Embeddings](/docs/modules/data_connection/text_embedding/) model. #### Retrieval and generation 1. **Retrieve**: Given a user input, relevant splits are retrieved from storage using a [Retriever](/docs/modules/data_connection/retrievers/). 2. **Generate**: A [ChatModel](/docs/modules/model_io/chat_models) / [LLM](/docs/modules/model_io/llms/) produces an answer using a prompt that includes the question and the retrieved data ![flow.jpeg](/assets/images/qa_flow-9fbd91de9282eb806bda1c6db501ecec.jpeg) ## Setup ### Dependencies We\'ll use an OpenAI chat model and embeddings and a Chroma vector store in this walkthrough, but everything shown here works with any [ChatModel](/docs/integrations/chat/) or [LLM](/docs/integrations/llms/), [Embeddings](/docs/integrations/text_embedding/), and [VectorStore](/docs/integrations/vectorstores/) or [Retriever](/docs/integrations/retrievers). We\'ll use the following packages: ```bash pip install -U langchain openai chromadb langchainhub bs4 ``` We need to set environment variable `OPENAI_API_KEY`, which can be done directly or loaded from a `.env` file like so: ```python import getpass import os os.environ[""OPENAI_API_KEY""] = getpass.getpass() # import dotenv # dotenv.load_dotenv() ``` ### LangSmith Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with [LangSmith]( Note that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces: ```python os.environ[""LANGCHAIN_TRACING_V2""] = ""true"" os.environ[""LANGCHAIN_API_KEY""] = getpass.getpass() ``` ## Quickstart Suppose we want to build a QA app over the [LLM Powered Autonomous Agents]( blog post by Lilian Weng. We can create a simple pipeline for this in ~20 lines of code: ```python import bs4 from langchain import hub from langchain.chat_models import ChatOpenAI from langchain.document_loaders import WebBaseLoader from langchain.embeddings import OpenAIEmbeddings from langchain.schema import StrOutputParser from langchain.schema.runnable import RunnablePassthrough from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain.vectorstores import Chroma ``` ```python loader = WebBaseLoader( web_paths=("" bs_kwargs=dict( parse_only=bs4.SoupStrainer( class_=(""post-content"", ""post-title"", ""post-header"") ) ), ) docs = loader.load() text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200) splits = text_splitter.split_documents(docs) vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings()) retriever = vectorstore.as_retriever() prompt = hub.pull(""rlm/rag-prompt"") llm = ChatOpenAI(model_name=""gpt-3.5-turbo"", temperature=0) def format_docs(docs): return ""\\n\\n"".join(doc.page_content for doc in docs) rag_chain = ( {""context"": retriever | format_docs, ""question"": RunnablePassthrough()} | prompt | llm | StrOutputParser() ) ``` ```python rag_chain.invoke(""What is Task Decomposition?"") ``` ```text \'Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It can be done through prompting techniques like Chain of Thought or Tree of Thoughts, or by using task-specific instructions or human inputs. Task decomposition helps agents plan ahead and manage complicated tasks more effectively.\' ``` ```python # cleanup vectorstore.delete_collection() ``` Check out the [LangSmith trace]( ## Detailed walkthrough Let\'s go through the above code step-by-step to really understand what\'s going on. ## Step 1. Load We need to first load the blog post contents. We can use `DocumentLoader`s for this, which are objects that load in data from a source as `Documents`. A `Document` is an object with `page_content` (str) and `metadata` (dict) attributes. In this case we\'ll use the `WebBaseLoader`, which uses `urllib` and `BeautifulSoup` to load and parse the passed in web urls, returning one `Document` per url. We can customize the html -> text parsing by passing in parameters to the `BeautifulSoup` parser via `bs_kwargs` (see [BeautifulSoup docs]( In this case only HTML tags with class ""post-content"", ""post-title"", or ""post-header"" are relevant, so we\'ll remove all others. ```python from langchain.document_loaders import WebBaseLoader loader = WebBaseLoader( web_paths=("" bs_kwargs={ ""parse_only"": bs4.SoupStrainer( class_=(""post-content"", ""post-title"", ""post-header"") ) }, ) docs = loader.load() ``` ```python len(docs[0].page_content) ``` ```text 42824 ``` ```python print(docs[0].page_content[:500]) ``` ```text LLM Powered Autonomous Agents Date: June 23, 2023 | Estimated Reading Time: 31 min | Author: Lilian Weng Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver. Agent System Overview# In ``` ### Go deeper `DocumentLoader`: Object that load data from a source as `Documents`. - [Docs](/docs/modules/data_connection/document_loaders/): Further documentation on how to use `DocumentLoader`s. - [Integrations](/docs/integrations/document_loaders/): Find the relevant `DocumentLoader` integration (of the > 160 of them) for your use case. ## Step 2. Split Our loaded document is over 42k characters long. This is too long to fit in the context window of many models. And even for those models that could fit the full post in their context window, empirically models struggle to find the relevant context in very long prompts. So we\'ll split the `Document` into chunks for embedding and vector storage. This should help us retrieve only the most relevant bits of the blog post at run time. In this case we\'ll split our documents into chunks of 1000 characters with 200 characters of overlap between chunks. The overlap helps mitigate the possibility of separating a statement from important context related to it. We use the `RecursiveCharacterTextSplitter`, which will (recursively) split the document using common separators (like new lines) until each chunk is the appropriate size. ```python from langchain.text_splitter import RecursiveCharacterTextSplitter text_splitter = RecursiveCharacterTextSplitter( chunk_size=1000, chunk_overlap=200, add_start_index=True ) all_splits = text_splitter.split_documents(docs) ``` ```python len(all_splits) ``` ```text 66 ``` ```python len(all_splits[0].page_content) ``` ```text 969 ``` ```python all_splits[10].metadata ``` ```text {\'source\': \' \'start_index\': 7056} ``` ### Go deeper `DocumentSplitter`: Object that splits a list of `Document`s into smaller chunks. Subclass of `DocumentTransformer`s. - Explore `Context-aware splitters`, which keep the location (""context"") of each split in the original `Document`:- [Markdown files](/docs/use_cases/question_answering/document-context-aware-QA) - [Code (py or js)](/docs/use_cases/question_answering/docs/integrations/document_loaders/source_code) - [Scientific papers](/docs/integrations/document_loaders/grobid) `DocumentTransformer`: Object that performs a transformation on a list of `Document`s. - [Docs](/docs/modules/data_connection/document_transformers/): Further documentation on how to use `DocumentTransformer`s - [Integrations](/docs/integrations/document_transformers/) ## Step 3. Store Now that we\'ve got 66 text chunks in memory, we need to store and index them so that we can search them later in our RAG app. The most common way to do this is to embed the contents of each document split and upload those embeddings to a vector store. Then, when we want to search over our splits, we take the search query, embed it as well, and perform some sort of ""similarity"" search to identify the stored splits with the most similar embeddings to our query embedding. The simplest similarity measure is cosine similarity we measure the cosine of the angle between each pair of embeddings (which are just very high dimensional vectors). We can embed and store all of our document splits in a single command using the `Chroma` vector store and `OpenAIEmbeddings` model. ```python from langchain.embeddings import OpenAIEmbeddings from langchain.vectorstores import Chroma vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings()) ``` ### Go deeper `Embeddings`: Wrapper around a text embedding model, used for converting text to embeddings. - [Docs](/docs/modules/data_connection/text_embedding): Further documentation on the interface. - [Integrations](/docs/integrations/text_embedding/): Browse the > 30 text embedding integrations `VectorStore`: Wrapper around a vector database, used for storing and querying embeddings. - [Docs](/docs/modules/data_connection/vectorstores/): Further documentation on the interface. - [Integrations](/docs/integrations/vectorstores/): Browse the > 40 `VectorStore` integrations. This completes the **Indexing** portion of the pipeline. At this point we have an query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question: ## Step 4. Retrieve Now let\'s write the actual application logic. We want to create a simple application that let\'s the user ask a question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and finally returns an answer. LangChain defines a `Retriever` interface which wraps an index that can return relevant documents given a string query. All retrievers implement a common method `get_relevant_documents()` (and its asynchronous variant `aget_relevant_documents()`). The most common type of `Retriever` is the `VectorStoreRetriever`, which uses the similarity search capabilities of a vector store to facillitate retrieval. Any `VectorStore` can easily be turned into a `Retriever`: ```python retriever = vectorstore.as_retriever(search_type=""similarity"", search_kwargs={""k"": 6}) ``` ```python retrieved_docs = retriever.get_relevant_documents( ""What are the approaches to Task Decomposition?"" ) ``` ```python len(retrieved_docs) ``` ```text 6 ``` ```python print(retrieved_docs[0].page_content) ``` ```text Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote. Task decomposition can be done (1) by LLM with simple prompting like ""Steps for XYZ.\\n1."", ""What are the subgoals for achieving XYZ?"", (2) by using task-specific instructions; e.g. ""Write a story outline."" for writing a novel, or (3) with human inputs. ``` ### Go deeper Vector stores are commonly used for retrieval, but there are plenty of other ways to do retrieval. `Retriever`: An object that returns `Document`s given a text query - [Docs](/docs/modules/data_connection/retrievers/): Further documentation on the interface and built-in retrieval techniques. Some of which include:- `MultiQueryRetriever` [generates variants of the input question](/docs/modules/data_connection/retrievers/MultiQueryRetriever) to improve retrieval hit rate. - `MultiVectorRetriever` (diagram below) instead generates [variants of the embeddings](/docs/modules/data_connection/retrievers/multi_vector), also in order to improve retrieval hit rate. - `Max marginal relevance` selects for [relevance and diversity]( among the retrieved documents to avoid passing in duplicate context. - Documents can be filtered during vector store retrieval using [metadata filters](/docs/use_cases/question_answering/document-context-aware-QA). - [Integrations](/docs/integrations/retrievers/): Integrations with retrieval services. ## Step 5. Generate Let\'s put it all together into a chain that takes a question, retrieves relevant documents, constructs a prompt, passes that to a model, and parses the output. We\'ll use the gpt-3.5-turbo OpenAI chat model, but any LangChain `LLM` or `ChatModel` could be substituted in. ```python from langchain.chat_models import ChatOpenAI llm = ChatOpenAI(model_name=""gpt-3.5-turbo"", temperature=0) ``` We\'ll use a prompt for RAG that is checked into the LangChain prompt hub ([here]( ```python from langchain import hub prompt = hub.pull(""rlm/rag-prompt"") ``` ```python print( prompt.invoke( {""context"": ""filler context"", ""question"": ""filler question""} ).to_string() ) ``` ```text Human: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise. Question: filler question Context: filler context Answer: ``` We\'ll use the [LCEL Runnable]( protocol to define the chain, allowing us to - pipe together components and functions in a transparent way - automatically trace our chain in LangSmith - get streaming, async, and batched calling out of the box ```python from langchain.schema import StrOutputParser from langchain.schema.runnable import RunnablePassthrough def format_docs(docs): return ""\\n\\n"".join(doc.page_content for doc in docs) rag_chain = ( {""context"": retriever | format_docs, ""question"": RunnablePassthrough()} | prompt | llm | StrOutputParser() ) ``` ```python for chunk in rag_chain.stream(""What is Task Decomposition?""): print(chunk, end="""", flush=True) ``` ```text Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It can be done through methods like Chain of Thought (CoT) or Tree of Thoughts, which involve dividing the task into manageable subtasks and exploring multiple reasoning possibilities at each step. Task decomposition can be performed by AI models with prompting, task-specific instructions, or human inputs. ``` Check out the [LangSmith trace]( ### Go deeper #### Choosing LLMs `ChatModel`: An LLM-backed chat model wrapper. Takes in a sequence of messages and returns a message. - [Docs](/docs/modules/model_io/chat/) - [Integrations](/docs/integrations/chat/): Explore over 25 `ChatModel` integrations. `LLM`: A text-in-text-out LLM. Takes in a string and returns a string. - [Docs](/docs/modules/model_io/llms) - [Integrations](/docs/integrations/llms): Explore over 75 `LLM` integrations. See a guide on RAG with locally-running models [here](/docs/modules/use_cases/question_answering/local_retrieval_qa). #### Customizing the prompt As shown above, we can load prompts (e.g., [this RAG prompt]( from the prompt hub. The prompt can also be easily customized: ```python from langchain.prompts import PromptTemplate template = """"""Use the following pieces of context to answer the question at the end. If you don\'t know the answer, just say that you don\'t know, don\'t try to make up an answer. Use three sentences maximum and keep the answer as concise as possible. Always say ""thanks for asking!"" at the end of the answer. {context} Question: {question} Helpful Answer:"""""" rag_prompt_custom = PromptTemplate.from_template(template) rag_chain = ( {""context"": retriever | format_docs, ""question"": RunnablePassthrough()} | rag_prompt_custom | llm | StrOutputParser() ) rag_chain.invoke(""What is Task Decomposition?"") ``` ```text \'Task decomposition is the process of breaking down a complex task into smaller and simpler steps. It can be done through techniques like Chain of Thought (CoT) or Tree of Thoughts, which involve dividing the problem into multiple thought steps and generating multiple thoughts per step. Task decomposition helps in enhancing model performance and understanding the thinking process of the model. Thanks for asking!\' ``` Check out the [LangSmith trace]( ### Adding sources With LCEL it\'s easy to return the retrieved documents or certain source metadata from the documents: ```python from operator import itemgetter from langchain.schema.runnable import RunnableMap rag_chain_from_docs = ( { ""context"": lambda input: format_docs(input[""documents""]), ""question"": itemgetter(""question""), } | rag_prompt_custom | llm | StrOutputParser() ) rag_chain_with_source = RunnableMap( {""documents"": retriever, ""question"": RunnablePassthrough()} ) | { ""documents"": lambda input: [doc.metadata for doc in input[""documents""]], ""answer"": rag_chain_from_docs, } rag_chain_with_source.invoke(""What is Task Decomposition"") ``` ```text {\'documents\': [{\'source\': \' \'start_index\': 1585}, {\'source\': \' \'start_index\': 2192}, {\'source\': \' \'start_index\': 17804}, {\'source\': \' \'start_index\': 17414}, {\'source\': \' \'start_index\': 29630}, {\'source\': \' \'start_index\': 19373}], \'answer\': \'Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It involves transforming big tasks into multiple manageable tasks, allowing for a more systematic and organized approach to problem-solving. Thanks for asking!\'} ``` Check out the [LangSmith trace]( Adding memory Suppose we want to create a stateful application that remembers past user inputs. There are two main things we need to do to support this. 1. Add a messages placeholder to our chain which allows us to pass in historical messages 2. Add a chain that takes the latest user query and reformulates it in the context of the chat history into a standalone question that can be passed to our retriever. Let\'s start with 2. We can build a ""condense question"" chain that looks something like this: ```python from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder condense_q_system_prompt = """"""Given a chat history and the latest user question \\ which might reference the chat history, formulate a standalone question \\ which can be understood without the chat history. Do NOT answer the question, \\ just reformulate it if needed and otherwise return it as is."""""" condense_q_prompt = ChatPromptTemplate.from_messages( [ (""system"", condense_q_system_prompt), MessagesPlaceholder(variable_name=""chat_history""), (""human"", ""{question}""), ] ) condense_q_chain = condense_q_prompt | llm | StrOutputParser() ``` ```python from langchain.schema.messages import AIMessage, HumanMessage condense_q_chain.invoke( { ""chat_history"": [ HumanMessage(content=""What does LLM stand for?""), AIMessage(content=""Large language model""), ], ""question"": ""What is meant by large"", } ) ``` ```text \'What is the definition of ""large"" in the context of a language model?\' ``` ```python condense_q_chain.invoke( { ""chat_history"": [ HumanMessage(content=""What does LLM stand for?""), AIMessage(content=""Large language model""), ], ""question"": ""How do transformers work"", } ) ``` ```text \'How do transformer models function?\' ``` And now we can build our full QA chain. Notice we add some routing functionality to only run the ""condense question chain"" when our chat history isn\'t empty. ```python qa_system_prompt = """"""You are an assistant for question-answering tasks. \\ Use the following pieces of retrieved context to answer the question. \\ If you don\'t know the answer, just say that you don\'t know. \\ Use three sentences maximum and keep the answer concise.\\ {context}"""""" qa_prompt = ChatPromptTemplate.from_messages( [ (""system"", qa_system_prompt), MessagesPlaceholder(variable_name=""chat_history""), (""human"", ""{question}""), ] ) def condense_question(input: dict): if input.get(""chat_history""): return condense_q_chain else: return input[""question""] rag_chain = ( RunnablePassthrough.assign(context=condense_question | retriever | format_docs) | qa_prompt | llm ) ``` ```python chat_history = [] question = ""What is Task Decomposition?"" ai_msg = rag_chain.invoke({""question"": question, ""chat_history"": chat_history}) chat_history.extend([HumanMessage(content=question), ai_msg]) second_question = ""What are common ways of doing it?"" rag_chain.invoke({""question"": second_question, ""chat_history"": chat_history}) ``` ```text AIMessage(content=\'Common ways of task decomposition include:\\n\\n1. Using Chain of Thought (CoT): CoT is a prompting technique that instructs the model to ""think step by step"" and decompose complex tasks into smaller and simpler steps. It utilizes more test-time computation and sheds light on the model\\\'s thinking process.\\n\\n2. Prompting with LLM: Language Model (LLM) can be used to prompt the model with simple instructions like ""Steps for XYZ"" or ""What are the subgoals for achieving XYZ?"" This allows the model to generate a sequence of subtasks or thought steps.\\n\\n3. Task-specific instructions: For certain tasks, task-specific instructions can be provided to guide the model in decomposing the task. For example, for writing a novel, the instruction ""Write a story outline"" can be given to break down the task into manageable steps.\\n\\n4. Human inputs: In some cases, human inputs can be used to assist in task decomposition. Humans can provide their expertise and knowledge to identify and break down complex tasks into smaller subtasks.\') ``` Check out the [LangSmith trace]( Here we\'ve gone over how to add chain logic for incorporating historical outputs. But how do we actually store and retrieve historical outputs for different sessions? For that check out the LCEL [How to add message history (memory)](/docs/expression_language/how_to/message_history) page. ## Next steps That\'s a lot of content we\'ve covered in a short amount of time. There\'s plenty of nuances, features, integrations, etc to explore in each of the above sections. Aside from the sources mentioned above, good next steps include: - Reading up on more advanced retrieval techniques in the [Retrievers](/docs/modules/data_connection/retrievers/) section. - Learning about the LangChain [Indexing API](/docs/modules/data_connection/indexing), which helps repeatedly sync data sources and vector stores without redundant computation or storage. - Exploring RAG [LangChain Templates](/docs/templates/#-advanced-retrieval), which are reference applications that can easily be deployed with [LangServe](/docs/langserve). - Learning about [evaluating RAG applications with LangSmith]( - [Overview](#overview)- [What is RAG?](#what-is-rag) - [What\'s in this guide?](#whats-in-this-guide) - [Architecture](#architecture) - [Setup](#setup)- [Dependencies](#dependencies) - [LangSmith](#langsmith) - [Quickstart](#quickstart) - [Detailed walkthrough](#detailed-walkthrough) - [Step 1. Load](#step-1-load)- [Go deeper](#go-deeper) - [Step 2. Split](#step-2-split)- [Go deeper](#go-deeper-1) - [Step 3. Store](#step-3-store)- [Go deeper](#go-deeper-2) - [Step 4. Retrieve](#step-4-retrieve)- [Go deeper](#go-deeper-3) - [Step 5. Generate](#step-5-generate)- [Go deeper](#go-deeper-4) - [Adding sources](#adding-sources) - [Adding memory](#adding-memory) - [Next steps](#next-steps)']","Retrieval-augmented generation (RAG) is a technique for enhancing large language model (LLM) knowledge with additional, often private or real-time, data. It involves augmenting the model's knowledge with specific information it needs to reason about private data or data introduced after its training cutoff date. The process includes indexing data offline and creating a retrieval and generation chain that retrieves relevant data at runtime and passes it to the model to generate an answer.","RAG stands for Retrieval Augmented Generation, which refers to the application design of using an LLM to generate responses to user inputs grounded on retrieved information. The retrieved information can come from anywhere, though it typically is fetched from 1 or more databases, search engines, knowledge graphs, or other sources.",0.5833333333041666,1.0,1.0,0.0233781833689462,0.12698412698412698
44,How do i run lcel in java,"[""LangChain cookbook | LangChain cookbook Example code for building applications with LangChain, with an emphasis on more applied and end-to-end examples than contained in the [main documentation]( | Notebook | Description | | ---- | ---- | | LLaMA2_sql_chat.ipynb | Build a chat application that interacts with a SQL database using an open source llm (llama2), specifically demonstrated on an SQLite database containing rosters. | | Semi_Structured_RAG.ipynb | Perform retrieval-augmented generation (rag) on documents with semi-structured data, including text and tables, using unstructured for parsing, multi-vector retriever for storing, and lcel for implementing chains. | | Semi_structured_and_multi_moda... | Perform retrieval-augmented generation (rag) on documents with semi-structured data and images, using unstructured for parsing, multi-vector retriever for storage and retrieval, and lcel for implementing chains. | | Semi_structured_multi_modal_RA... | Perform retrieval-augmented generation (rag) on documents with semi-structured data and images, using various tools and methods such as unstructured for parsing, multi-vector retriever for storing, lcel for implementing chains, and open source language models like llama2, llava, and gpt4all. | | analyze_document.ipynb | Analyze a single long document. | | autogpt/autogpt.ipynb | Implement autogpt, a language model, with langchain primitives such as llms, prompttemplates, vectorstores, embeddings, and tools. | | autogpt/marathon_times.ipynb | Implement autogpt for finding winning marathon times. | | baby_agi.ipynb | Implement babyagi, an ai agent that can generate and execute tasks based on a given objective, with the flexibility to swap out specific vectorstores/model providers. | | baby_agi_with_agent.ipynb | Swap out the execution chain in the babyagi notebook with an agent that has access to tools, aiming to obtain more reliable information. | | camel_role_playing.ipynb | Implement the camel framework for creating autonomous cooperative agents in large-scale language models, using role-playing and inception prompting to guide chat agents towards task completion. | | causalprogram_aided_language... | Implement the causal program-aided language (cpal) chain, which improves upon the program-aided language (pal) by incorporating causal structure to prevent hallucination in language models, particularly when dealing with complex narratives and math problems with nested dependencies. | | code-analysis-deeplake.ipynb | Analyze its own code base with the help of gpt and activeloop's deep lake. | | custom_agent_with_plugin_retri... | Build a custom agent that can interact with ai plugins by retrieving tools and creating natural language wrappers around openapi endpoints. | | custom_agent_with_plugin_retri... | Build a custom agent with plugin retrieval functionality, utilizing ai plugins from theplugnplaidirectory. | | databricks_sql_db.ipynb | Connect to databricks runtimes and databricks sql. | | deeplakesemantic_search_over... | Perform semantic search and question-answering over a group chat using activeloop's deep lake with gpt4. | | elasticsearch_db_qa.ipynb | Interact with elasticsearch analytics databases in natural language and build search queries via the elasticsearch dsl API. | | extraction_openai_tools.ipynb | Structured Data Extraction with OpenAI Tools | | forward_looking_retrieval_augm... | Implement the forward-looking active retrieval augmented generation (flare) method, which generates answers to questions, identifies uncertain tokens, generates hypothetical questions based on these tokens, and retrieves relevant documents to continue generating the answer. | | generativeagents_interactive... | Implement a generative agent that simulates human behavior, based on a research paper, using a time-weighted memory object backed by a langchain retriever. | | gymnasium_agent_simulation.ipynb | Create a simple agent-environment interaction loop in simulated environments like text-based games with gymnasium. | | hugginggpt.ipynb | Implement hugginggpt, a system that connects language models like chatgpt with the machine learning community via hugging face. | | hypothetical_document_embeddin... | Improve document indexing with hypothetical document embeddings (hyde), an embedding technique that generates and embeds hypothetical answers to queries. | | learned_prompt_optimization.ipynb | Automatically enhance language model prompts by injecting specific terms using reinforcement learning, which can be used to personalize responses based on user preferences. | | llm_bash.ipynb | Perform simple filesystem commands using language learning models (llms) and a bash process. | | llm_checker.ipynb | Create a self-checking chain using the llmcheckerchain function. | | llm_math.ipynb | Solve complex word math problems using language models and python repls. | | llm_summarization_checker.ipynb | Check the accuracy of text summaries, with the option to run the checker multiple times for improved results. | | llm_symbolic_math.ipynb | Solve algebraic equations with the help of llms (language learning models) and sympy, a python library for symbolic mathematics. | | meta_prompt.ipynb | Implement the meta-prompt concept, which is a method for building self-improving agents that reflect on their own performance and modify their instructions accordingly. | | multi_modal_output_agent.ipynb | Generate multi-modal outputs, specifically images and text. | | multi_player_dnd.ipynb | Simulate multi-player dungeons & dragons games, with a custom function determining the speaking schedule of the agents. | | multiagent_authoritarian.ipynb | Implement a multi-agent simulation where a privileged agent controls the conversation, including deciding who speaks and when the conversation ends, in the context of a simulated news network. | | multiagent_bidding.ipynb | Implement a multi-agent simulation where agents bid to speak, with the highest bidder speaking next, demonstrated through a fictitious presidential debate example. | | myscale_vector_sql.ipynb | Access and interact with the myscale integrated vector database, which can enhance the performance of language model (llm) applications. | | openai_functions_retrieval_qa.... | Structure response output in a question-answering system by incorporating openai functions into a retrieval pipeline. | | openai_v1_cookbook.ipynb | Explore new functionality released alongside the V1 release of the OpenAI Python library. | | petting_zoo.ipynb | Create multi-agent simulations with simulated environments using the petting zoo library. | | plan_and_execute_agent.ipynb | Create plan-and-execute agents that accomplish objectives by planning tasks with a language model (llm) and executing them with a separate agent. | | press_releases.ipynb | Retrieve and query company press release data powered byKay.ai. | | program_aided_language_model.i... | Implement program-aided language models as described in the provided research paper. | | qa_citations.ipynb | Different ways to get a model to cite its sources. | | retrieval_in_sql.ipynb | Perform retrieval-augmented-generation (rag) on a PostgreSQL database using pgvector. | | sales_agent_with_context.ipynb | Implement a context-aware ai sales agent, salesgpt, that can have natural sales conversations, interact with other systems, and use a product knowledge base to discuss a company's offerings. | | self_query_hotel_search.ipynb | Build a hotel room search feature with self-querying retrieval, using a specific hotel recommendation dataset. | | smart_llm.ipynb | Implement a smartllmchain, a self-critique chain that generates multiple output proposals, critiques them to find the best one, and then improves upon it to produce a final output. | | tree_of_thought.ipynb | Query a large language model using the tree of thought technique. | | twitter-the-algorithm-analysis... | Analyze the source code of the Twitter algorithm with the help of gpt4 and activeloop's deep lake. | | two_agent_debate_tools.ipynb | Simulate multi-agent dialogues where the agents can utilize various tools. | | two_player_dnd.ipynb | Simulate a two-player dungeons & dragons game, where a dialogue simulator class is used to coordinate the dialogue between the protagonist and the dungeon master. | | wikibase_agent.ipynb | Create a simple wikibase agent that utilizes sparql generation, with testing done on |"", 'Chains | Chains Using an LLM in isolation is fine for simple applications, but more complex applications require chaining LLMs - either with each other or with other components. LangChain provides two high-level frameworks for ""chaining"" components. The legacy approach is to use the `Chain` interface. The updated approach is to use the [LangChain Expression Language (LCEL)](/docs/expression_language/). When building new applications we recommend using LCEL for chain composition. But there are a number of useful, built-in `Chain`\'s that we continue to support, so we document both frameworks here. As we\'ll touch on below, `Chain`\'s can also themselves be used in LCEL, so the two are not mutually exclusive. ## LCEL The most visible part of LCEL is that it provides an intuitive and readable syntax for composition. But more importantly, it also provides first-class support for: - [streaming](/docs/expression_language/interface#stream), - [async calls](/docs/expression_language/interface#async-stream), - [batching](/docs/expression_language/interface#batch), - [parallelization](/docs/expression_language/interface#parallelism), - retries, - [fallbacks](/docs/expression_language/how_to/fallbacks), - tracing, - [and more.](/docs/expression_language/why) As a simple and common example, we can see what it\'s like to combine a prompt, model and output parser: ```python from langchain.chat_models import ChatAnthropic from langchain.prompts import ChatPromptTemplate from langchain.schema import StrOutputParser model = ChatAnthropic() prompt = ChatPromptTemplate.from_messages( [ ( ""system"", ""You\'re a very knowledgeable historian who provides accurate and eloquent answers to historical questions."", ), (""human"", ""{question}""), ] ) runnable = prompt | model | StrOutputParser() ``` ```python for chunk in runnable.stream({""question"": ""How did Mansa Musa accumulate his wealth?""}): print(chunk, end="""", flush=True) ``` ```text Mansa Musa was the emperor of the Mali Empire in West Africa during the 14th century. He accumulated immense wealth through several means: - Gold mining - Mali contained very rich gold deposits, especially in the region of Bambuk. Gold mining and gold trade was a major source of wealth for the empire. - Control of trade routes - Mali dominated the trans-Saharan trade routes connecting West Africa to North Africa and beyond. By taxing the goods that passed through its territory, Mali profited greatly. - Tributary states - Many lands surrounding Mali paid tribute to the empire. This came in the form of gold, slaves, and other valuable resources. - Agriculture - Mali also had extensive agricultural lands irrigated by the Niger River. Surplus food produced could be sold or traded. - Royal monopolies - The emperor claimed monopoly rights over the production and sale of certain goods like salt from the Taghaza mines. This added to his personal wealth. - Inheritance - As an emperor, Mansa Musa inherited a wealthy state. His predecessors had already consolidated lands and accumulated riches which fell to Musa. So in summary, mining, trade, taxes, ``` For more head to the [LCEL section](/docs/expression_language/). ## [Legacy] Chain interface **Chain**\'s are the legacy interface for ""chained"" applications. We define a Chain very generically as a sequence of calls to components, which can include other chains. The base interface is simple: ```python class Chain(BaseModel, ABC): """"""Base interface that all chains should implement."""""" memory: BaseMemory callbacks: Callbacks def __call__( self, inputs: Any, return_only_outputs: bool = False, callbacks: Callbacks = None, ) -> Dict[str, Any]: ... ``` We can recreate the LCEL runnable we made above using the built-in `LLMChain`: ```python from langchain.chains import LLMChain chain = LLMChain(llm=model, prompt=prompt, output_parser=StrOutputParser()) chain.run(question=""How did Mansa Musa accumulate his wealth?"") ``` ```text "" Mansa Musa was the emperor of the Mali Empire in West Africa in the early 14th century. He accumulated his vast wealth through several means:\\n\\n- Gold mining - Mali contained very rich gold deposits, especially in the southern part of the empire. Gold mining and trade was a major source of wealth.\\n\\n- Control of trade routes - Mali dominated the trans-Saharan trade routes connecting West Africa to North Africa and beyond. By taxing and controlling this lucrative trade, Mansa Musa reaped great riches.\\n\\n- Tributes from conquered lands - The Mali Empire expanded significantly under Mansa Musa\'s rule. As new lands were conquered, they paid tribute to the mansa in the form of gold, salt, and slaves.\\n\\n- Inheritance - Mansa Musa inherited a wealthy empire from his predecessor. He continued to build the wealth of Mali through the factors above.\\n\\n- Sound fiscal management - Musa is considered to have managed the empire and its finances very effectively, including keeping taxes reasonable and promoting a robust economy. This allowed him to accumulate and maintain wealth.\\n\\nSo in summary, conquest, trade, taxes, mining, and inheritance all contributed to Mansa Musa growing the M"" ``` For more specifics check out: - [How-to](/docs/modules/chains/how_to/) for walkthroughs of different chain features - [Foundational](/docs/modules/chains/foundational/) to get acquainted with core building block chains - [Document](/docs/modules/chains/document/) to learn how to incorporate documents into chains - [LCEL](#lcel) - [Legacy Chain interface](#legacy-chain-interface)', 'SpaCy | SpaCy [spaCy]( is an open-source software library for advanced natural language processing, written in the programming languages Python and Cython. ## Installation and Setup ```python #!pip install spacy ``` Import the necessary classes ```python from langchain.embeddings.spacy_embeddings import SpacyEmbeddings ``` ## Example Initialize SpacyEmbeddings.This will load the Spacy model into memory. ```python embedder = SpacyEmbeddings() ``` Define some example texts . These could be any documents that you want to analyze - for example, news articles, social media posts, or product reviews. ```python texts = [ ""The quick brown fox jumps over the lazy dog."", ""Pack my box with five dozen liquor jugs."", ""How vexingly quick daft zebras jump!"", ""Bright vixens jump; dozy fowl quack."", ] ``` Generate and print embeddings for the texts . The SpacyEmbeddings class generates an embedding for each document, which is a numerical representation of the document\'s content. These embeddings can be used for various natural language processing tasks, such as document similarity comparison or text classification. ```python embeddings = embedder.embed_documents(texts) for i, embedding in enumerate(embeddings): print(f""Embedding for document {i+1}: {embedding}"") ``` Generate and print an embedding for a single piece of text. You can also generate an embedding for a single piece of text, such as a search query. This can be useful for tasks like information retrieval, where you want to find documents that are similar to a given query. ```python query = ""Quick foxes and lazy dogs."" query_embedding = embedder.embed_query(query) print(f""Embedding for query: {query_embedding}"") ``` - [Installation and Setup](#installation-and-setup) - [Example](#example)']","LangChain Expression Language (LCEL) is a feature of the LangChain framework, which is primarily designed for use with Python. As of now, LCEL does not have direct support for Java. If you're looking to use similar capabilities in Java, you might need to explore other frameworks or libraries that offer comparable functionality for building complex workflows or chains of operations.

For Java, you might consider using libraries such as Apache Beam, Spring Batch, or even custom implementations using Java's built-in features for handling complex workflows and data processing tasks. These tools can help you create parallel, streaming, and batch processing capabilities similar to what LCEL offers in Python.

If you have specific requirements or tasks you'd like to accomplish in Java that you currently do with LCEL in Python, I can provide guidance on how to approach these tasks using Java-specific tools and techniques.","LangChain, along with LCEL, or LangChain Expression Language, is not natively offered in java. LangChain is available in both python and javascript/typescript.",0.0,0.5,,0.08216107324922536,0.12790697674418605
45,Help me debug: TypeError: Pinecone.similarity_search_with_score() got an unexpected keyword argument 'min_score',"['langchain.vectorstores.pinecone.Pinecone LangChain 0.0.337 API Experimental Python Docs Toggle Menu PrevUp Next LangChain 0.0.337 langchain.vectorstores.pinecone.Pinecone langchain.vectorstores.pinecone.Pinecone class langchain.vectorstores.pinecone.Pinecone(index: Any, embedding: Union[Embeddings, Callable], text_key: str, namespace: Optional[str] = None, distance_strategy: Optional[DistanceStrategy] = DistanceStrategy.COSINE)[source] Pinecone vector store. To use, you should have the pinecone-client python package installed. Example from langchain.vectorstores import Pinecone from langchain.embeddings.openai import OpenAIEmbeddings import pinecone # The environment should be the one specified next to the API key # in your Pinecone console pinecone.init(api_key=""***"", environment=""..."") index = pinecone.Index(""langchain-demo"") embeddings = OpenAIEmbeddings() vectorstore = Pinecone(index, embeddings.embed_query, ""text"") Initialize with Pinecone client. Attributes embeddings Access the query embedding object if available. Methods __init__(index,embedding,text_key[,...]) Initialize with Pinecone client. aadd_documents(documents,**kwargs) Run more documents through the embeddings and add to the vectorstore. aadd_texts(texts[,metadatas]) Run more texts through the embeddings and add to the vectorstore. add_documents(documents,**kwargs) Run more documents through the embeddings and add to the vectorstore. add_texts(texts[,metadatas,ids,...]) Run more texts through the embeddings and add to the vectorstore. adelete([ids]) Delete by vector ID or other criteria. afrom_documents(documents,embedding,**kwargs) Return VectorStore initialized from documents and embeddings. afrom_texts(texts,embedding[,metadatas]) Return VectorStore initialized from texts and embeddings. amax_marginal_relevance_search(query[,k,...]) Return docs selected using the maximal marginal relevance. amax_marginal_relevance_search_by_vector(...) Return docs selected using the maximal marginal relevance. as_retriever(**kwargs) Return VectorStoreRetriever initialized from this VectorStore. asearch(query,search_type,**kwargs) Return docs most similar to query using specified search type. asimilarity_search(query[,k]) Return docs most similar to query. asimilarity_search_by_vector(embedding[,k]) Return docs most similar to embedding vector. asimilarity_search_with_relevance_scores(query) Return docs and relevance scores in the range [0, 1], asynchronously. asimilarity_search_with_score(*args,**kwargs) Run similarity search with distance asynchronously. delete([ids,delete_all,namespace,filter]) Delete by vector IDs or filter. from_documents(documents,embedding,**kwargs) Return VectorStore initialized from documents and embeddings. from_existing_index(index_name,embedding[,...]) Load pinecone vectorstore from index name. from_texts(texts,embedding[,metadatas, ...]) Construct Pinecone wrapper from raw documents. get_pinecone_index(index_name[, pool_threads]) Return a Pinecone Index instance. max_marginal_relevance_search(query[, k, ...]) Return docs selected using the maximal marginal relevance. max_marginal_relevance_search_by_vector(...) Return docs selected using the maximal marginal relevance. search(query, search_type, **kwargs) Return docs most similar to query using specified search type. similarity_search(query[, k, filter, namespace]) Return pinecone documents most similar to query. similarity_search_by_vector(embedding[, k]) Return docs most similar to embedding vector. similarity_search_by_vector_with_score(...) Return pinecone documents most similar to embedding, along with scores. similarity_search_with_relevance_scores(query) Return docs and relevance scores in the range [0, 1]. similarity_search_with_score(query[, k, ...]) Return pinecone documents most similar to query, along with scores. __init__(index: Any, embedding: Union[Embeddings, Callable], text_key: str, namespace: Optional[str] = None, distance_strategy: Optional[DistanceStrategy] = DistanceStrategy.COSINE)[source] Initialize with Pinecone client. async aadd_documents(documents: List[Document], **kwargs: Any)  List[str] Run more documents through the embeddings and add to the vectorstore. Parameters (List[Document] (documents)  Documents to add to the vectorstore. Returns List of IDs of the added texts. Return type List[str] async aadd_texts(texts: Iterable[str], metadatas: Optional[List[dict]] = None, **kwargs: Any)  List[str] Run more texts through the embeddings and add to the vectorstore. add_documents(documents: List[Document], **kwargs: Any)  List[str] Run more documents through the embeddings and add to the vectorstore. Parameters (List[Document] (documents)  Documents to add to the vectorstore. Returns List of IDs of the added texts. Return type List[str] add_texts(texts: Iterable[str], metadatas: Optional[List[dict]] = None, ids: Optional[List[str]] = None, namespace: Optional[str] = None, batch_size: int = 32, embedding_chunk_size: int = 1000, **kwargs: Any)  List[str][source] Run more texts through the embeddings and add to the vectorstore. Upsert optimization is done by chunking the embeddings and upserting them. This is done to avoid memory issues and optimize using HTTP based embeddings. For OpenAI embeddings, use pool_threads>4 when constructing the pinecone.Index, embedding_chunk_size>1000 and batch_size~64 for best performance. :param texts: Iterable of strings to add to the vectorstore. :param metadatas: Optional list of metadatas associated with the texts. :param ids: Optional list of ids to associate with the texts. :param namespace: Optional pinecone namespace to add the texts to. :param batch_size: Batch size to use when adding the texts to the vectorstore. :param embedding_chunk_size: Chunk size to use when embedding the texts. Returns List of ids from adding the texts into the vectorstore. async adelete(ids: Optional[List[str]] = None, **kwargs: Any)  Optional[bool] Delete by vector ID or other criteria. Parameters ids  List of ids to delete. **kwargs  Other keyword arguments that subclasses might use. Returns True if deletion is successful, False otherwise, None if not implemented. Return type Optional[bool] async classmethod afrom_documents(documents: List[Document], embedding: Embeddings, **kwargs: Any)  VST Return VectorStore initialized from documents and embeddings. async classmethod afrom_texts(texts: List[str], embedding: Embeddings, metadatas: Optional[List[dict]] = None, **kwargs: Any)  VST Return VectorStore initialized from texts and embeddings. async amax_marginal_relevance_search(query: str, k: int = 4, fetch_k: int = 20, lambda_mult: float = 0.5, **kwargs: Any)  List[Document] Return docs selected using the maximal marginal relevance. async amax_marginal_relevance_search_by_vector(embedding: List[float], k: int = 4, fetch_k: int = 20, lambda_mult: float = 0.5, **kwargs: Any)  List[Document] Return docs selected using the maximal marginal relevance. as_retriever(**kwargs: Any)  VectorStoreRetriever Return VectorStoreRetriever initialized from this VectorStore. Parameters search_type (Optional[str])  Defines the type of search that the Retriever should perform. Can be similarity (default), mmr, or similarity_score_threshold. search_kwargs (Optional[Dict])  Keyword arguments to pass to the search function. Can include things like: k: Amount of documents to return (Default: 4) score_threshold: Minimum relevance threshold for similarity_score_threshold fetch_k: Amount of documents to pass to MMR algorithm (Default: 20) lambda_mult: Diversity of results returned by MMR; 1 for minimum diversity and 0 for maximum. (Default: 0.5) filter: Filter by document metadata Returns Retriever class for VectorStore. Return type VectorStoreRetriever Examples: # Retrieve more documents with higher diversity # Useful if your dataset has many similar documents docsearch.as_retriever( search_type=""mmr"", search_kwargs={\'k\': 6, \'lambda_mult\': 0.25} ) # Fetch more documents for the MMR algorithm to consider # But only return the top 5 docsearch.as_retriever( search_type=""mmr"", search_kwargs={\'k\': 5, \'fetch_k\': 50} ) # Only retrieve documents that have a relevance score # Above a certain threshold docsearch.as_retriever( search_type=""similarity_score_threshold"", search_kwargs={\'score_threshold\': 0.8} ) # Only get the single most similar document from the dataset docsearch.as_retriever(search_kwargs={\'k\': 1}) # Use a filter to only retrieve documents from a specific paper docsearch.as_retriever( search_kwargs={\'filter\': {\'paper_title\':\'GPT-4 Technical Report\'}} ) async asearch(query: str, search_type: str, **kwargs: Any)  List[Document] Return docs most similar to query using specified search type. async asimilarity_search(query: str, k: int = 4, **kwargs: Any)  List[Document] Return docs most similar to query. async asimilarity_search_by_vector(embedding: List[float], k: int = 4, **kwargs: Any)  List[Document] Return docs most similar to embedding vector. async asimilarity_search_with_relevance_scores(query: str, k: int = 4, **kwargs: Any)  List[Tuple[Document, float]] Return docs and relevance scores in the range [0, 1], asynchronously. 0 is dissimilar, 1 is most similar. Parameters query  input text k  Number of Documents to return. Defaults to 4. **kwargs  kwargs to be passed to similarity search. Should include: score_threshold: Optional, a floating point value between 0 to 1 to filter the resulting set of retrieved docs Returns List of Tuples of (doc, similarity_score) async asimilarity_search_with_score(*args: Any, **kwargs: Any)  List[Tuple[Document, float]] Run similarity search with distance asynchronously. delete(ids: Optional[List[str]] = None, delete_all: Optional[bool] = None, namespace: Optional[str] = None, filter: Optional[dict] = None, **kwargs: Any)  None[source] Delete by vector IDs or filter. :param ids: List of ids to delete. :param filter: Dictionary of conditions to filter vectors to delete. classmethod from_documents(documents: List[Document], embedding: Embeddings, **kwargs: Any)  VST Return VectorStore initialized from documents and embeddings. classmethod from_existing_index(index_name: str, embedding: Embeddings, text_key: str = \'text\', namespace: Optional[str] = None, pool_threads: int = 4)  Pinecone[source] Load pinecone vectorstore from index name. classmethod from_texts(texts: List[str], embedding: Embeddings, metadatas: Optional[List[dict]] = None, ids: Optional[List[str]] = None, batch_size: int = 32, text_key: str = \'text\', namespace: Optional[str] = None, index_name: Optional[str] = None, upsert_kwargs: Optional[dict] = None, pool_threads: int = 4, embeddings_chunk_size: int = 1000, **kwargs: Any)  Pinecone[source] Construct Pinecone wrapper from raw documents. This is a user friendly interface that: Embeds documents. Adds the documents to a provided Pinecone index This is intended to be a quick way to get started. The pool_threads affects the speed of the upsert operations. .. rubric:: Example from langchain.vectorstores import Pinecone from langchain.embeddings import OpenAIEmbeddings import pinecone # The environment should be the one specified next to the API key # in your Pinecone console pinecone.init(api_key=""***"", environment=""..."") embeddings = OpenAIEmbeddings() pinecone = Pinecone.from_texts( texts, embeddings, index_name=""langchain-demo"" ) classmethod get_pinecone_index(index_name: Optional[str], pool_threads: int = 4)  Index[source] Return a Pinecone Index instance. Parameters index_name  Name of the index to use. pool_threads  Number of threads to use for index upsert. Returns Pinecone Index instance. max_marginal_relevance_search(query: str, k: int = 4, fetch_k: int = 20, lambda_mult: float = 0.5, filter: Optional[dict] = None, namespace: Optional[str] = None, **kwargs: Any)  List[Document][source] Return docs selected using the maximal marginal relevance. Maximal marginal relevance optimizes for similarity to query AND diversity among selected documents. Parameters query  Text to look up documents similar to. k  Number of Documents to return. Defaults to 4. fetch_k  Number of Documents to fetch to pass to MMR algorithm. lambda_mult  Number between 0 and 1 that determines the degree of diversity among the results with 0 corresponding to maximum diversity and 1 to minimum diversity. Defaults to 0.5. Returns List of Documents selected by maximal marginal relevance. max_marginal_relevance_search_by_vector(embedding: List[float], k: int = 4, fetch_k: int = 20, lambda_mult: float = 0.5, filter: Optional[dict] = None, namespace: Optional[str] = None, **kwargs: Any)  List[Document][source] Return docs selected using the maximal marginal relevance. Maximal marginal relevance optimizes for similarity to query AND diversity among selected documents. Parameters embedding  Embedding to look up documents similar to. k  Number of Documents to return. Defaults to 4. fetch_k  Number of Documents to fetch to pass to MMR algorithm. lambda_mult  Number between 0 and 1 that determines the degree of diversity among the results with 0 corresponding to maximum diversity and 1 to minimum diversity. Defaults to 0.5. Returns List of Documents selected by maximal marginal relevance. search(query: str, search_type: str, **kwargs: Any)  List[Document] Return docs most similar to query using specified search type. similarity_search(query: str, k: int = 4, filter: Optional[dict] = None, namespace: Optional[str] = None, **kwargs: Any)  List[Document][source] Return pinecone documents most similar to query. Parameters query  Text to look up documents similar to. k  Number of Documents to return. Defaults to 4. filter  Dictionary of argument(s) to filter on metadata namespace  Namespace to search in. Default will search in \'\' namespace. Returns List of Documents most similar to the query and score for each similarity_search_by_vector(embedding: List[float], k: int = 4, **kwargs: Any)  List[Document] Return docs most similar to embedding vector. Parameters embedding  Embedding to look up documents similar to. k  Number of Documents to return. Defaults to 4. Returns List of Documents most similar to the query vector. similarity_search_by_vector_with_score(embedding: List[float], *, k: int = 4, filter: Optional[dict] = None, namespace: Optional[str] = None)  List[Tuple[Document, float]][source] Return pinecone documents most similar to embedding, along with scores. similarity_search_with_relevance_scores(query: str, k: int = 4, **kwargs: Any)  List[Tuple[Document, float]] Return docs and relevance scores in the range [0, 1]. 0 is dissimilar, 1 is most similar. Parameters query  input text k  Number of Documents to return. Defaults to 4. **kwargs  kwargs to be passed to similarity search. Should include: score_threshold: Optional, a floating point value between 0 to 1 to filter the resulting set of retrieved docs Returns List of Tuples of (doc, similarity_score) similarity_search_with_score(query: str, k: int = 4, filter: Optional[dict] = None, namespace: Optional[str] = None)  List[Tuple[Document, float]][source] Return pinecone documents most similar to query, along with scores. Parameters query  Text to look up documents similar to. k  Number of Documents to return. Defaults to 4. filter  Dictionary of argument(s) to filter on metadata namespace  Namespace to search in. Default will search in \'\' namespace. Returns List of Documents most similar to the query and score for each Examples using Pinecone Pinecone  2023, Harrison Chase. Last updated on Nov 17, 2023. Show this page source', 'Vespa | Vespa [Vespa]( is a fully featured search engine and vector database. It supports vector search (ANN), lexical search, and search in structured data, all in the same query. This notebook shows how to use `Vespa.ai` as a LangChain vector store. In order to create the vector store, we use [pyvespa]( to create a connection a `Vespa` service. ```python #!pip install pyvespa ``` Using the `pyvespa` package, you can either connect to a [Vespa Cloud instance]( or a local [Docker instance]( Here, we will create a new Vespa application and deploy that using Docker. #### Creating a Vespa application First, we need to create an application package: ```python from vespa.package import ApplicationPackage, Field, RankProfile app_package = ApplicationPackage(name=""testapp"") app_package.schema.add_fields( Field( name=""text"", type=""string"", indexing=[""index"", ""summary""], index=""enable-bm25"" ), Field( name=""embedding"", type=""tensor(x[384])"", indexing=[""attribute"", ""summary""], attribute=[""distance-metric: angular""], ), ) app_package.schema.add_rank_profile( RankProfile( name=""default"", first_phase=""closeness(field, embedding)"", inputs=[(""query(query_embedding)"", ""tensor(x[384])"")], ) ) ``` This sets up a Vespa application with a schema for each document that contains two fields: `text` for holding the document text and `embedding` for holding the embedding vector. The `text` field is set up to use a BM25 index for efficient text retrieval, and we\'ll see how to use this and hybrid search a bit later. The `embedding` field is set up with a vector of length 384 to hold the embedding representation of the text. See [Vespa\'s Tensor Guide]( for more on tensors in Vespa. Lastly, we add a [rank profile]( to instruct Vespa how to order documents. Here we set this up with a [nearest neighbor search]( Now we can deploy this application locally: ```python from vespa.deployment import VespaDocker vespa_docker = VespaDocker() vespa_app = vespa_docker.deploy(application_package=app_package) ``` This deploys and creates a connection to a `Vespa` service. In case you already have a Vespa application running, for instance in the cloud, please refer to the PyVespa application for how to connect. #### Creating a Vespa vector store Now, let\'s load some documents: ```python from langchain.document_loaders import TextLoader from langchain.text_splitter import CharacterTextSplitter loader = TextLoader(""../../modules/state_of_the_union.txt"") documents = loader.load() text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0) docs = text_splitter.split_documents(documents) from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings embedding_function = SentenceTransformerEmbeddings(model_name=""all-MiniLM-L6-v2"") ``` Here, we also set up local sentence embedder to transform the text to embedding vectors. One could also use OpenAI embeddings, but the vector length needs to be updated to `1536` to reflect the larger size of that embedding. To feed these to Vespa, we need to configure how the vector store should map to fields in the Vespa application. Then we create the vector store directly from this set of documents: ```python vespa_config = dict( page_content_field=""text"", embedding_field=""embedding"", input_field=""query_embedding"", ) from langchain.vectorstores import VespaStore db = VespaStore.from_documents(docs, embedding_function, app=vespa_app, **vespa_config) ``` This creates a Vespa vector store and feeds that set of documents to Vespa. The vector store takes care of calling the embedding function for each document and inserts them into the database. We can now query the vector store: ```python query = ""What did the president say about Ketanji Brown Jackson"" results = db.similarity_search(query) print(results[0].page_content) ``` This will use the embedding function given above to create a representation for the query and use that to search Vespa. Note that this will use the `default` ranking function, which we set up in the application package above. You can use the `ranking` argument to `similarity_search` to specify which ranking function to use. Please refer to the [pyvespa documentation]( for more information. This covers the basic usage of the Vespa store in LangChain. Now you can return the results and continue using these in LangChain. #### Updating documents An alternative to calling `from_documents`, you can create the vector store directly and call `add_texts` from that. This can also be used to update documents: ```python query = ""What did the president say about Ketanji Brown Jackson"" results = db.similarity_search(query) result = results[0] result.page_content = ""UPDATED: "" + result.page_content db.add_texts([result.page_content], [result.metadata], result.metadata[""id""]) results = db.similarity_search(query) print(results[0].page_content) ``` However, the `pyvespa` library contains methods to manipulate content on Vespa which you can use directly. #### Deleting documents You can delete documents using the `delete` function: ```python result = db.similarity_search(query) # docs[0].metadata[""id""] == ""id:testapp:testapp::32"" db.delete([""32""]) result = db.similarity_search(query) # docs[0].metadata[""id""] != ""id:testapp:testapp::32"" ``` Again, the `pyvespa` connection contains methods to delete documents as well. ### Returning with scores The `similarity_search` method only returns the documents in order of relevancy. To retrieve the actual scores: ```python results = db.similarity_search_with_score(query) result = results[0] # result[1] ~= 0.463 ``` This is a result of using the `""all-MiniLM-L6-v2""` embedding model using the cosine distance function (as given by the argument `angular` in the application function). Different embedding functions need different distance functions, and Vespa needs to know which distance function to use when orderings documents. Please refer to the [documentation on distance functions]( for more information. ### As retriever To use this vector store as a [LangChain retriever]( simply call the `as_retriever` function, which is a standard vector store method: ```python db = VespaStore.from_documents(docs, embedding_function, app=vespa_app, **vespa_config) retriever = db.as_retriever() query = ""What did the president say about Ketanji Brown Jackson"" results = retriever.get_relevant_documents(query) # results[0].metadata[""id""] == ""id:testapp:testapp::32"" ``` This allows for more general, unstructured, retrieval from the vector store. ### Metadata In the example so far, we\'ve only used the text and the embedding for that text. Documents usually contain additional information, which in LangChain is referred to as metadata. Vespa can contain many fields with different types by adding them to the application package: ```python app_package.schema.add_fields( # ... Field(name=""date"", type=""string"", indexing=[""attribute"", ""summary""]), Field(name=""rating"", type=""int"", indexing=[""attribute"", ""summary""]), Field(name=""author"", type=""string"", indexing=[""attribute"", ""summary""]), # ... ) vespa_app = vespa_docker.deploy(application_package=app_package) ``` We can add some metadata fields in the documents: ```python # Add metadata for i, doc in enumerate(docs): doc.metadata[""date""] = f""2023-{(i % 12)+1}-{(i % 28)+1}"" doc.metadata[""rating""] = range(1, 6)[i % 5] doc.metadata[""author""] = [""Joe Biden"", ""Unknown""][min(i, 1)] ``` And let the Vespa vector store know about these fields: ```python vespa_config.update(dict(metadata_fields=[""date"", ""rating"", ""author""])) ``` Now, when searching for these documents, these fields will be returned. Also, these fields can be filtered on: ```python db = VespaStore.from_documents(docs, embedding_function, app=vespa_app, **vespa_config) query = ""What did the president say about Ketanji Brown Jackson"" results = db.similarity_search(query, filter=""rating > 3"") # results[0].metadata[""id""] == ""id:testapp:testapp::34"" # results[0].metadata[""author""] == ""Unknown"" ``` ### Custom query If the default behavior of the similarity search does not fit your requirements, you can always provide your own query. Thus, you don\'t need to provide all of the configuration to the vector store, but rather just write this yourself. First, let\'s add a BM25 ranking function to our application: ```python from vespa.package import FieldSet app_package.schema.add_field_set(FieldSet(name=""default"", fields=[""text""])) app_package.schema.add_rank_profile(RankProfile(name=""bm25"", first_phase=""bm25(text)"")) vespa_app = vespa_docker.deploy(application_package=app_package) db = VespaStore.from_documents(docs, embedding_function, app=vespa_app, **vespa_config) ``` Then, to perform a regular text search based on BM25: ```python query = ""What did the president say about Ketanji Brown Jackson"" custom_query = { ""yql"": ""select * from sources * where userQuery()"", ""query"": query, ""type"": ""weakAnd"", ""ranking"": ""bm25"", ""hits"": 4, } results = db.similarity_search_with_score(query, custom_query=custom_query) # results[0][0].metadata[""id""] == ""id:testapp:testapp::32"" # results[0][1] ~= 14.384 ``` All of the powerful search and query capabilities of Vespa can be used by using a custom query. Please refer to the Vespa documentation on it\'s [Query API]( for more details. ### Hybrid search Hybrid search means using both a classic term-based search such as BM25 and a vector search and combining the results. We need to create a new rank profile for hybrid search on Vespa: ```python app_package.schema.add_rank_profile( RankProfile( name=""hybrid"", first_phase=""log(bm25(text)) + 0.5 * closeness(field, embedding)"", inputs=[(""query(query_embedding)"", ""tensor(x[384])"")], ) ) vespa_app = vespa_docker.deploy(application_package=app_package) db = VespaStore.from_documents(docs, embedding_function, app=vespa_app, **vespa_config) ``` Here, we score each document as a combination of it\'s BM25 score and its distance score. We can query using a custom query: ```python query = ""What did the president say about Ketanji Brown Jackson"" query_embedding = embedding_function.embed_query(query) nearest_neighbor_expression = ( ""{targetHits: 4}nearestNeighbor(embedding, query_embedding)"" ) custom_query = { ""yql"": f""select * from sources * where {nearest_neighbor_expression} and userQuery()"", ""query"": query, ""type"": ""weakAnd"", ""input.query(query_embedding)"": query_embedding, ""ranking"": ""hybrid"", ""hits"": 4, } results = db.similarity_search_with_score(query, custom_query=custom_query) # results[0][0].metadata[""id""], ""id:testapp:testapp::32"") # results[0][1] ~= 2.897 ``` ### Native embedders in Vespa Up until this point we\'ve used an embedding function in Python to provide embeddings for the texts. Vespa supports embedding function natively, so you can defer this calculation in to Vespa. One benefit is the ability to use GPUs when embedding documents if you have a large collections. Please refer to [Vespa embeddings]( for more information. First, we need to modify our application package: ```python from vespa.package import Component, Parameter app_package.components = [ Component( id=""hf-embedder"", type=""hugging-face-embedder"", parameters=[ Parameter(""transformer-model"", {""path"": ""...""}), Parameter(""tokenizer-model"", {""url"": ""...""}), ], ) ] Field( name=""hfembedding"", type=""tensor(x[384])"", is_document_field=False, indexing=[""input text"", ""embed hf-embedder"", ""attribute"", ""summary""], attribute=[""distance-metric: angular""], ) app_package.schema.add_rank_profile( RankProfile( name=""hf_similarity"", first_phase=""closeness(field, hfembedding)"", inputs=[(""query(query_embedding)"", ""tensor(x[384])"")], ) ) ``` Please refer to the embeddings documentation on adding embedder models and tokenizers to the application. Note that the `hfembedding` field includes instructions for embedding using the `hf-embedder`. Now we can query with a custom query: ```python query = ""What did the president say about Ketanji Brown Jackson"" nearest_neighbor_expression = ( ""{targetHits: 4}nearestNeighbor(internalembedding, query_embedding)"" ) custom_query = { ""yql"": f""select * from sources * where {nearest_neighbor_expression}"", ""input.query(query_embedding)"": f\'embed(hf-embedder, ""{query}"")\', ""ranking"": ""internal_similarity"", ""hits"": 4, } results = db.similarity_search_with_score(query, custom_query=custom_query) # results[0][0].metadata[""id""], ""id:testapp:testapp::32"") # results[0][1] ~= 0.630 ``` Note that the query here includes an `embed` instruction to embed the query using the same model as for the documents. ### Approximate nearest neighbor In all of the above examples, we\'ve used exact nearest neighbor to find results. However, for large collections of documents this is not feasible as one has to scan through all documents to find the best matches. To avoid this, we can use [approximate nearest neighbors]( First, we can change the embedding field to create a HNSW index: ```python from vespa.package import HNSW app_package.schema.add_fields( Field( name=""embedding"", type=""tensor(x[384])"", indexing=[""attribute"", ""summary"", ""index""], ann=HNSW( distance_metric=""angular"", max_links_per_node=16, neighbors_to_explore_at_insert=200, ), ) ) ``` This creates a HNSW index on the embedding data which allows for efficient searching. With this set, we can easily search using ANN by setting the `approximate` argument to `True`: ```python query = ""What did the president say about Ketanji Brown Jackson"" results = db.similarity_search(query, approximate=True) # results[0][0].metadata[""id""], ""id:testapp:testapp::32"") ``` This covers most of the functionality in the Vespa vector store in LangChain. - [Returning with scores](#returning-with-scores) - [As retriever](#as-retriever) - [Metadata](#metadata) - [Custom query](#custom-query) - [Hybrid search](#hybrid-search) - [Native embedders in Vespa](#native-embedders-in-vespa) - [Approximate nearest neighbor](#approximate-nearest-neighbor)', 'Redis | Redis Redis vector database introduction and langchain integration guide. ## What is Redis? Most developers from a web services background are probably familiar with Redis. At it\'s core, Redis is an open-source key-value store that can be used as a cache, message broker, and database. Developers choose Redis because it is fast, has a large ecosystem of client libraries, and has been deployed by major enterprises for years. On top of these traditional use cases, Redis provides additional capabilities like the Search and Query capability that allows users to create secondary index structures within Redis. This allows Redis to be a Vector Database, at the speed of a cache. ## Redis as a Vector Database Redis uses compressed, inverted indexes for fast indexing with a low memory footprint. It also supports a number of advanced features such as: - Indexing of multiple fields in Redis hashes and JSON - Vector similarity search (with HNSW (ANN) or FLAT (KNN)) - Vector Range Search (e.g. find all vectors within a radius of a query vector) - Incremental indexing without performance loss - Document ranking (using [tf-idf]( with optional user-provided weights) - Field weighting - Complex boolean queries with AND, OR, and NOT operators - Prefix matching, fuzzy matching, and exact-phrase queries - Support for [double-metaphone phonetic matching]( - Auto-complete suggestions (with fuzzy prefix suggestions) - Stemming-based query expansion in [many languages]( (using [Snowball]( - Support for Chinese-language tokenization and querying (using [Friso]( - Numeric filters and ranges - Geospatial searches using [Redis geospatial indexing](/commands/georadius) - A powerful aggregations engine - Supports for all utf-8 encoded text - Retrieve full documents, selected fields, or only the document IDs - Sorting results (for example, by creation date) ## Clients Since redis is much more than just a vector database, there are often use cases that demand usage of a Redis client besides just the langchain integration. You can use any standard Redis client library to run Search and Query commands, but it\'s easiest to use a library that wraps the Search and Query API. Below are a few examples, but you can find more client libraries [here]( | Project | Language | License | Author | Stars | | ---- | ---- | ---- | ---- | ---- | | jedis | Java | MIT | Redis | | | redisvl | Python | MIT | Redis | | | redis-py | Python | MIT | Redis | | | node-redis | Node.js | MIT | Redis | | | nredisstack | .NET | MIT | Redis | | ## Deployment Options There are many ways to deploy Redis with RediSearch. The easiest way to get started is to use Docker, but there are are many potential options for deployment such as - [Redis Cloud]( - [Docker (Redis Stack)]( - Cloud marketplaces: [AWS Marketplace]( [Google Marketplace]( or [Azure Marketplace]( - On-premise: [Redis Enterprise Software]( - Kubernetes: [Redis Enterprise Software on Kubernetes]( ## Examples Many examples can be found in the [Redis AI team\'s GitHub]( - [Awesome Redis AI Resources]( - List of examples of using Redis in AI workloads - [Azure OpenAI Embeddings Q&A]( - OpenAI and Redis as a Q&A service on Azure. - [ArXiv Paper Search]( - Semantic search over arXiv scholarly papers - [Vector Search on Azure]( - Vector search on Azure using Azure Cache for Redis and Azure OpenAI ## More Resources For more information on how to use Redis as a vector database, check out the following resources: - [RedisVL Documentation]( - Documentation for the Redis Vector Library Client - [Redis Vector Similarity Docs]( - Redis official docs for Vector Search. - [Redis-py Search Docs]( - Documentation for redis-py client library - [Vector Similarity Search: From Basics to Production]( - Introductory blog post to VSS and Redis as a VectorDB. ## Install Redis Python Client Redis-py is the officially supported client by Redis. Recently released is the RedisVL client which is purpose-built for the Vector Database use cases. Both can be installed with pip. ```bash pip install redis redisvl openai tiktoken ``` We want to use `OpenAIEmbeddings` so we have to get the OpenAI API Key. ```python import getpass import os os.environ[""OPENAI_API_KEY""] = getpass.getpass(""OpenAI API Key:"") ``` ```python from langchain.embeddings import OpenAIEmbeddings embeddings = OpenAIEmbeddings() ``` ## Sample Data First we will describe some sample data so that the various attributes of the Redis vector store can be demonstrated. ```python metadata = [ { ""user"": ""john"", ""age"": 18, ""job"": ""engineer"", ""credit_score"": ""high"", }, { ""user"": ""derrick"", ""age"": 45, ""job"": ""doctor"", ""credit_score"": ""low"", }, { ""user"": ""nancy"", ""age"": 94, ""job"": ""doctor"", ""credit_score"": ""high"", }, { ""user"": ""tyler"", ""age"": 100, ""job"": ""engineer"", ""credit_score"": ""high"", }, { ""user"": ""joe"", ""age"": 35, ""job"": ""dentist"", ""credit_score"": ""medium"", }, ] texts = [""foo"", ""foo"", ""foo"", ""bar"", ""bar""] ``` ## Initializing Redis To locally deploy Redis, run: ```console docker run -d -p 6379:6379 -p 8001:8001 redis/redis-stack:latest ``` If things are running correctly you should see a nice Redis UI at See the [Deployment Options](#deployment-options) section above for other ways to deploy. The Redis VectorStore instance can be initialized in a number of ways. There are multiple class methods that can be used to initialize a Redis VectorStore instance. - `Redis.__init__` - Initialize directly - `Redis.from_documents` - Initialize from a list of `Langchain.docstore.Document` objects - `Redis.from_texts` - Initialize from a list of texts (optionally with metadata) - `Redis.from_texts_return_keys` - Initialize from a list of texts (optionally with metadata) and return the keys - `Redis.from_existing_index` - Initialize from an existing Redis index Below we will use the `Redis.from_texts` method. ```python from langchain.vectorstores.redis import Redis rds = Redis.from_texts( texts, embeddings, metadatas=metadata, redis_url=""redis://localhost:6379"", index_name=""users"", ) ``` ```python rds.index_name ``` ```text \'users\' ``` ## Inspecting the Created Index Once the `Redis` VectorStore object has been constructed, an index will have been created in Redis if it did not already exist. The index can be inspected with both the `rvl`and the `redis-cli` command line tool. If you installed `redisvl` above, you can use the `rvl` command line tool to inspect the index. ```bash # assumes you\'re running Redis locally (use --host, --port, --password, --username, to change this) rvl index listall ``` ```text 16:58:26 [RedisVL] INFO Indices: 16:58:26 [RedisVL] INFO 1. users ``` The `Redis` VectorStore implementation will attempt to generate index schema (fields for filtering) for any metadata passed through the `from_texts`, `from_texts_return_keys`, and `from_documents` methods. This way, whatever metadata is passed will be indexed into the Redis search index allowing for filtering on those fields. Below we show what fields were created from the metadata we defined above ```bash rvl index info -i users ``` ```text Index Information: Index Name Storage Type Prefixes Index Options Indexing users HASH [\'doc:users\'] [] 0 Index Fields: Name Attribute Type Field Option Option Value user user TEXT WEIGHT 1 job job TEXT WEIGHT 1 credit_score credit_score TEXT WEIGHT 1 content content TEXT WEIGHT 1 age age NUMERIC content_vector content_vector VECTOR ``` ```bash rvl stats -i users ``` ```text Statistics: Stat Key Value num_docs 5 num_terms 15 max_doc_id 5 num_records 33 percent_indexed 1 hash_indexing_failures 0 number_of_uses 4 bytes_per_record_avg 4.60606 doc_table_size_mb 0.000524521 inverted_sz_mb 0.000144958 key_table_size_mb 0.000193596 offset_bits_per_record_avg 8 offset_vectors_sz_mb 2.19345e-05 offsets_per_term_avg 0.69697 records_per_doc_avg 6.6 sortable_values_size_mb 0 total_indexing_time 0.32 total_inverted_index_blocks 16 vector_index_sz_mb 6.0126 ``` It\'s important to note that we have not specified that the `user`, `job`, `credit_score` and `age` in the metadata should be fields within the index, this is because the `Redis` VectorStore object automatically generate the index schema from the passed metadata. For more information on the generation of index fields, see the API documentation. ## Querying\u200b There are multiple ways to query the `Redis` VectorStore implementation based on what use case you have: - `similarity_search`: Find the most similar vectors to a given vector. - `similarity_search_with_score`: Find the most similar vectors to a given vector and return the vector distance - `similarity_search_limit_score`: Find the most similar vectors to a given vector and limit the number of results to the `score_threshold` - `similarity_search_with_relevance_scores`: Find the most similar vectors to a given vector and return the vector similarities - `max_marginal_relevance_search`: Find the most similar vectors to a given vector while also optimizing for diversity ```python results = rds.similarity_search(""foo"") print(results[0].page_content) ``` ```text foo ``` ```python # return metadata results = rds.similarity_search(""foo"", k=3) meta = results[1].metadata print(""Key of the document in Redis: "", meta.pop(""id"")) print(""Metadata of the document: "", meta) ``` ```text Key of the document in Redis: doc:users:a70ca43b3a4e4168bae57c78753a200f Metadata of the document: {\'user\': \'derrick\', \'job\': \'doctor\', \'credit_score\': \'low\', \'age\': \'45\'} ``` ```python # with scores (distances) results = rds.similarity_search_with_score(""foo"", k=5) for result in results: print(f""Content: {result[0].page_content} --- Score: {result[1]}"") ``` ```text Content: foo --- Score: 0.0 Content: foo --- Score: 0.0 Content: foo --- Score: 0.0 Content: bar --- Score: 0.1566 Content: bar --- Score: 0.1566 ``` ```python # limit the vector distance that can be returned results = rds.similarity_search_with_score(""foo"", k=5, distance_threshold=0.1) for result in results: print(f""Content: {result[0].page_content} --- Score: {result[1]}"") ``` ```text Content: foo --- Score: 0.0 Content: foo --- Score: 0.0 Content: foo --- Score: 0.0 ``` ```python # with scores results = rds.similarity_search_with_relevance_scores(""foo"", k=5) for result in results: print(f""Content: {result[0].page_content} --- Similiarity: {result[1]}"") ``` ```text Content: foo --- Similiarity: 1.0 Content: foo --- Similiarity: 1.0 Content: foo --- Similiarity: 1.0 Content: bar --- Similiarity: 0.8434 Content: bar --- Similiarity: 0.8434 ``` ```python # limit scores (similarities have to be over .9) results = rds.similarity_search_with_relevance_scores(""foo"", k=5, score_threshold=0.9) for result in results: print(f""Content: {result[0].page_content} --- Similarity: {result[1]}"") ``` ```text Content: foo --- Similarity: 1.0 Content: foo --- Similarity: 1.0 Content: foo --- Similarity: 1.0 ``` ```python # you can also add new documents as follows new_document = [""baz""] new_metadata = [{""user"": ""sam"", ""age"": 50, ""job"": ""janitor"", ""credit_score"": ""high""}] # both the document and metadata must be lists rds.add_texts(new_document, new_metadata) ``` ```text [\'doc:users:b9c71d62a0a34241a37950b448dafd38\'] ``` ```python # now query the new document results = rds.similarity_search(""baz"", k=3) print(results[0].metadata) ``` ```text {\'id\': \'doc:users:b9c71d62a0a34241a37950b448dafd38\', \'user\': \'sam\', \'job\': \'janitor\', \'credit_score\': \'high\', \'age\': \'50\'} ``` ```python # use maximal marginal relevance search to diversify results results = rds.max_marginal_relevance_search(""foo"") ``` ```python # the lambda_mult parameter controls the diversity of the results, the lower the more diverse results = rds.max_marginal_relevance_search(""foo"", lambda_mult=0.1) ``` ## Connect to an Existing Index\u200b In order to have the same metadata indexed when using the `Redis` VectorStore. You will need to have the same `index_schema` passed in either as a path to a yaml file or as a dictionary. The following shows how to obtain the schema from an index and connect to an existing index. ```python # write the schema to a yaml file rds.write_schema(""redis_schema.yaml"") ``` The schema file for this example should look something like: ```yaml numeric: - name: age no_index: false sortable: false text: - name: user no_index: false no_stem: false sortable: false weight: 1 withsuffixtrie: false - name: job no_index: false no_stem: false sortable: false weight: 1 withsuffixtrie: false - name: credit_score no_index: false no_stem: false sortable: false weight: 1 withsuffixtrie: false - name: content no_index: false no_stem: false sortable: false weight: 1 withsuffixtrie: false vector: - algorithm: FLAT block_size: 1000 datatype: FLOAT32 dims: 1536 distance_metric: COSINE initial_cap: 20000 name: content_vector ``` **Notice**, this include **all** possible fields for the schema. You can remove any fields that you don\'t need. ```python # now we can connect to our existing index as follows new_rds = Redis.from_existing_index( embeddings, index_name=""users"", redis_url=""redis://localhost:6379"", schema=""redis_schema.yaml"", ) results = new_rds.similarity_search(""foo"", k=3) print(results[0].metadata) ``` ```text {\'id\': \'doc:users:8484c48a032d4c4cbe3cc2ed6845fabb\', \'user\': \'john\', \'job\': \'engineer\', \'credit_score\': \'high\', \'age\': \'18\'} ``` ```python # see the schemas are the same new_rds.schema == rds.schema ``` ```text True ``` ## Custom Metadata Indexing\u200b In some cases, you may want to control what fields the metadata maps to. For example, you may want the `credit_score` field to be a categorical field instead of a text field (which is the default behavior for all string fields). In this case, you can use the `index_schema` parameter in each of the initialization methods above to specify the schema for the index. Custom index schema can either be passed as a dictionary or as a path to a yaml file. All arguments in the schema have defaults besides the name, so you can specify only the fields you want to change. All the names correspond to the snake/lowercase versions of the arguments you would use on the command line with `redis-cli` or in `redis-py`. For more on the arguments for each field, see the [documentation]( The below example shows how to specify the schema for the `credit_score` field as a Tag (categorical) field instead of a text field. ```yaml # index_schema.yml tag: - name: credit_score text: - name: user - name: job numeric: - name: age ``` In Python this would look like: ```python index_schema = { ""tag"": [{""name"": ""credit_score""}], ""text"": [{""name"": ""user""}, {""name"": ""job""}], ""numeric"": [{""name"": ""age""}], } ``` Notice that only the `name` field needs to be specified. All other fields have defaults. ```python # create a new index with the new schema defined above index_schema = { ""tag"": [{""name"": ""credit_score""}], ""text"": [{""name"": ""user""}, {""name"": ""job""}], ""numeric"": [{""name"": ""age""}], } rds, keys = Redis.from_texts_return_keys( texts, embeddings, metadatas=metadata, redis_url=""redis://localhost:6379"", index_name=""users_modified"", index_schema=index_schema, # pass in the new index schema ) ``` ```text `index_schema` does not match generated metadata schema. If you meant to manually override the schema, please ignore this message. index_schema: {\'tag\': [{\'name\': \'credit_score\'}], \'text\': [{\'name\': \'user\'}, {\'name\': \'job\'}], \'numeric\': [{\'name\': \'age\'}]} generated_schema: {\'text\': [{\'name\': \'user\'}, {\'name\': \'job\'}, {\'name\': \'credit_score\'}], \'numeric\': [{\'name\': \'age\'}], \'tag\': []} ``` The above warning is meant to notify users when they are overriding the default behavior. Ignore it if you are intentionally overriding the behavior. ## Hybrid Filtering\u200b With the Redis Filter Expression language built into langchain, you can create arbitrarily long chains of hybrid filters that can be used to filter your search results. The expression language is derived from the [RedisVL Expression Syntax]( and is designed to be easy to use and understand. The following are the available filter types: - `RedisText`: Filter by full-text search against metadata fields. Supports exact, fuzzy, and wildcard matching. - `RedisNum`: Filter by numeric range against metadata fields. - `RedisTag`: Filter by exact match against string based categorical metadata fields. Multiple tags can be specified like ""tag1,tag2,tag3"". The following are examples of utilizing these filters. ```python from langchain.vectorstores.redis import RedisText, RedisNum, RedisTag # exact matching has_high_credit = RedisTag(""credit_score"") == ""high"" does_not_have_high_credit = RedisTag(""credit_score"") != ""low"" # fuzzy matching job_starts_with_eng = RedisText(""job"") % ""eng*"" job_is_engineer = RedisText(""job"") == ""engineer"" job_is_not_engineer = RedisText(""job"") != ""engineer"" # numeric filtering age_is_18 = RedisNum(""age"") == 18 age_is_not_18 = RedisNum(""age"") != 18 age_is_greater_than_18 = RedisNum(""age"") > 18 age_is_less_than_18 = RedisNum(""age"") < 18 age_is_greater_than_or_equal_to_18 = RedisNum(""age"") >= 18 age_is_less_than_or_equal_to_18 = RedisNum(""age"") <= 18 ``` The `RedisFilter` class can be used to simplify the import of these filters as follows ```python from langchain.vectorstores.redis import RedisFilter # same examples as above has_high_credit = RedisFilter.tag(""credit_score"") == ""high"" does_not_have_high_credit = RedisFilter.num(""age"") > 8 job_starts_with_eng = RedisFilter.text(""job"") % ""eng*"" ``` The following are examples of using hybrid filter for search ```python from langchain.vectorstores.redis import RedisText is_engineer = RedisText(""job"") == ""engineer"" results = rds.similarity_search(""foo"", k=3, filter=is_engineer) print(""Job:"", results[0].metadata[""job""]) print(""Engineers in the dataset:"", len(results)) ``` ```text Job: engineer Engineers in the dataset: 2 ``` ```python # fuzzy match starts_with_doc = RedisText(""job"") % ""doc*"" results = rds.similarity_search(""foo"", k=3, filter=starts_with_doc) for result in results: print(""Job:"", result.metadata[""job""]) print(""Jobs in dataset that start with \'doc\':"", len(results)) ``` ```text Job: doctor Job: doctor Jobs in dataset that start with \'doc\': 2 ``` ```python from langchain.vectorstores.redis import RedisNum is_over_18 = RedisNum(""age"") > 18 is_under_99 = RedisNum(""age"") < 99 age_range = is_over_18 & is_under_99 results = rds.similarity_search(""foo"", filter=age_range) for result in results: print(""User:"", result.metadata[""user""], ""is"", result.metadata[""age""]) ``` ```text User: derrick is 45 User: nancy is 94 User: joe is 35 ``` ```python # make sure to use parenthesis around FilterExpressions # if initializing them while constructing them age_range = (RedisNum(""age"") > 18) & (RedisNum(""age"") < 99) results = rds.similarity_search(""foo"", filter=age_range) for result in results: print(""User:"", result.metadata[""user""], ""is"", result.metadata[""age""]) ``` ```text User: derrick is 45 User: nancy is 94 User: joe is 35 ``` ## Redis as Retriever\u200b Here we go over different options for using the vector store as a retriever. There are three different search methods we can use to do retrieval. By default, it will use semantic similarity. ```python query = ""foo"" results = rds.similarity_search_with_score(query, k=3, return_metadata=True) for result in results: print(""Content:"", result[0].page_content, "" --- Score: "", result[1]) ``` ```text Content: foo --- Score: 0.0 Content: foo --- Score: 0.0 Content: foo --- Score: 0.0 ``` ```python retriever = rds.as_retriever(search_type=""similarity"", search_kwargs={""k"": 4}) ``` ```python docs = retriever.get_relevant_documents(query) docs ``` ```text [Document(page_content=\'foo\', metadata={\'id\': \'doc:users_modified:988ecca7574048e396756efc0e79aeca\', \'user\': \'john\', \'job\': \'engineer\', \'credit_score\': \'high\', \'age\': \'18\'}), Document(page_content=\'foo\', metadata={\'id\': \'doc:users_modified:009b1afeb4084cc6bdef858c7a99b48e\', \'user\': \'derrick\', \'job\': \'doctor\', \'credit_score\': \'low\', \'age\': \'45\'}), Document(page_content=\'foo\', metadata={\'id\': \'doc:users_modified:7087cee9be5b4eca93c30fbdd09a2731\', \'user\': \'nancy\', \'job\': \'doctor\', \'credit_score\': \'high\', \'age\': \'94\'}), Document(page_content=\'bar\', metadata={\'id\': \'doc:users_modified:01ef6caac12b42c28ad870aefe574253\', \'user\': \'tyler\', \'job\': \'engineer\', \'credit_score\': \'high\', \'age\': \'100\'})] ``` There is also the `similarity_distance_threshold` retriever which allows the user to specify the vector distance ```python retriever = rds.as_retriever( search_type=""similarity_distance_threshold"", search_kwargs={""k"": 4, ""distance_threshold"": 0.1}, ) ``` ```python docs = retriever.get_relevant_documents(query) docs ``` ```text [Document(page_content=\'foo\', metadata={\'id\': \'doc:users_modified:988ecca7574048e396756efc0e79aeca\', \'user\': \'john\', \'job\': \'engineer\', \'credit_score\': \'high\', \'age\': \'18\'}), Document(page_content=\'foo\', metadata={\'id\': \'doc:users_modified:009b1afeb4084cc6bdef858c7a99b48e\', \'user\': \'derrick\', \'job\': \'doctor\', \'credit_score\': \'low\', \'age\': \'45\'}), Document(page_content=\'foo\', metadata={\'id\': \'doc:users_modified:7087cee9be5b4eca93c30fbdd09a2731\', \'user\': \'nancy\', \'job\': \'doctor\', \'credit_score\': \'high\', \'age\': \'94\'})] ``` Lastly, the `similarity_score_threshold` allows the user to define the minimum score for similar documents ```python retriever = rds.as_retriever( search_type=""similarity_score_threshold"", search_kwargs={""score_threshold"": 0.9, ""k"": 10}, ) ``` ```python retriever.get_relevant_documents(""foo"") ``` ```text [Document(page_content=\'foo\', metadata={\'id\': \'doc:users_modified:988ecca7574048e396756efc0e79aeca\', \'user\': \'john\', \'job\': \'engineer\', \'credit_score\': \'high\', \'age\': \'18\'}), Document(page_content=\'foo\', metadata={\'id\': \'doc:users_modified:009b1afeb4084cc6bdef858c7a99b48e\', \'user\': \'derrick\', \'job\': \'doctor\', \'credit_score\': \'low\', \'age\': \'45\'}), Document(page_content=\'foo\', metadata={\'id\': \'doc:users_modified:7087cee9be5b4eca93c30fbdd09a2731\', \'user\': \'nancy\', \'job\': \'doctor\', \'credit_score\': \'high\', \'age\': \'94\'})] ``` ```python retriever = rds.as_retriever( search_type=""mmr"", search_kwargs={""fetch_k"": 20, ""k"": 4, ""lambda_mult"": 0.1} ) ``` ```python retriever.get_relevant_documents(""foo"") ``` ```text [Document(page_content=\'foo\', metadata={\'id\': \'doc:users:8f6b673b390647809d510112cde01a27\', \'user\': \'john\', \'job\': \'engineer\', \'credit_score\': \'high\', \'age\': \'18\'}), Document(page_content=\'bar\', metadata={\'id\': \'doc:users:93521560735d42328b48c9c6f6418d6a\', \'user\': \'tyler\', \'job\': \'engineer\', \'credit_score\': \'high\', \'age\': \'100\'}), Document(page_content=\'foo\', metadata={\'id\': \'doc:users:125ecd39d07845eabf1a699d44134a5b\', \'user\': \'nancy\', \'job\': \'doctor\', \'credit_score\': \'high\', \'age\': \'94\'}), Document(page_content=\'foo\', metadata={\'id\': \'doc:users:d6200ab3764c466082fde3eaab972a2a\', \'user\': \'derrick\', \'job\': \'doctor\', \'credit_score\': \'low\', \'age\': \'45\'})] ``` # Delete keys To delete your entries you have to address them by their keys. ```python Redis.delete(keys, redis_url=""redis://localhost:6379"") ``` ```text True ``` ```python # delete the indices too Redis.drop_index( index_name=""users"", delete_documents=True, redis_url=""redis://localhost:6379"" ) Redis.drop_index( index_name=""users_modified"", delete_documents=True, redis_url=""redis://localhost:6379"", ) ``` ```text True ``` ### Redis connection Url examples\u200b Valid Redis Url scheme are: 1. `redis://` - Connection to Redis standalone, unencrypted 2. `rediss://` - Connection to Redis standalone, with TLS encryption 3. `redis+sentinel://` - Connection to Redis server via Redis Sentinel, unencrypted 4. `rediss+sentinel://` - Connection to Redis server via Redis Sentinel, booth connections with TLS encryption More information about additional connection parameter can be found in the redis-py documentation at [ ```python # connection to redis standalone at localhost, db 0, no password redis_url = ""redis://localhost:6379"" # connection to host ""redis"" port 7379 with db 2 and password ""secret"" (old style authentication scheme without username / pre 6.x) redis_url = ""redis://:secret@redis:7379/2"" # connection to host redis on default port with user ""joe"", pass ""secret"" using redis version 6+ ACLs redis_url = ""redis://joe:secret@redis/0"" # connection to sentinel at localhost with default group mymaster and db 0, no password redis_url = ""redis+sentinel://localhost:26379"" # connection to sentinel at host redis with default port 26379 and user ""joe"" with password ""secret"" with default group mymaster and db 0 redis_url = ""redis+sentinel://joe:secret@redis"" # connection to sentinel, no auth with sentinel monitoring group ""zone-1"" and database 2 redis_url = ""redis+sentinel://redis:26379/zone-1/2"" # connection to redis standalone at localhost, db 0, no password but with TLS support redis_url = ""rediss://localhost:6379"" # connection to redis sentinel at localhost and default port, db 0, no password # but with TLS support for booth Sentinel and Redis server redis_url = ""rediss+sentinel://localhost"" ``` - [What is Redis?](#what-is-redis) - [Redis as a Vector Database](#redis-as-a-vector-database) - [Clients](#clients) - [Deployment Options](#deployment-options) - [Examples](#examples) - [More Resources](#more-resources) - [Install Redis Python Client](#install-redis-python-client) - [Sample Data](#sample-data) - [Initializing Redis](#initializing-redis) - [Inspecting the Created Index](#inspecting-the-created-index) - [Querying](#querying) - [Connect to an Existing Index](#connect-to-an-existing-index) - [Custom Metadata Indexing](#custom-metadata-indexing) - [Hybrid Filtering](#hybrid-filtering) - [Redis as Retriever](#redis-as-retriever)- [Redis connection Url examples](#redis-connection-url-examples)']","The error indicates that `min_score` is not a valid keyword argument for `Pinecone.similarity_search_with_score()`. To resolve this, ensure you are using the correct arguments supported by the method.

Here's a correct usage example:

```python
results = vectorstore.similarity_search_with_score(query, k=5)
```

If you need to filter results based on score, consider using other methods or filtering the results post-query.","Your error is likely due to an incorrect parameter being passed to the 'similarity_search_with_score' function in the Pinecone vector store. The correct function signature does not include a 'min_score' argument. This function retrieves documents most similar to a given query text while providing both the documents and their corresponding similarity scores. To use 'similarity_search_with_score', ensure you provide the right arguments: 'query' for the text you want to find similar documents for, 'k' for the number of documents to retrieve (default: 4), 'filter' as an optional metadata-based filtering dictionary, and 'namespace' to specify the search namespace (default is the empty namespace ''). The function returns a list of tuples, each containing a Document and a similarity score (float). To resolve the 'TypeError' issue, ensure proper argument usage as per the documentation.  ",0.9999999999,1.0,0.0,0.1729494792451655,0.20588235294117646
46,How can I create a simple chat model using my locally saved huggingface model,"['HuggingFace Hub Tools | HuggingFace Hub Tools [Huggingface Tools]( that supporting text I/O can be loaded directly using the `load_huggingface_tool` function. ```bash # Requires transformers>=4.29.0 and huggingface_hub>=0.14.1 pip install --upgrade transformers huggingface_hub > /dev/null ``` ```python from langchain.agents import load_huggingface_tool tool = load_huggingface_tool(""lysandre/hf-model-downloads"") print(f""{tool.name}: {tool.description}"") ``` ```text model_download_counter: This is a tool that returns the most downloaded model of a given task on the Hugging Face Hub. It takes the name of the category (such as text-classification, depth-estimation, etc), and returns the name of the checkpoint ``` ```python tool.run(""text-classification"") ``` ```text \'facebook/bart-large-mnli\' ```', 'Trubrics | Trubrics ![Trubrics]( [Trubrics]( is an LLM user analytics platform that lets you collect, analyse and manage user prompts & feedback on AI models. In this guide we will go over how to setup the `TrubricsCallbackHandler`. Check out [our repo]( for more information on Trubrics. ## Installation and Setup ```bash pip install trubrics ``` ### Getting Trubrics Credentials If you do not have a Trubrics account, create one on [here]( In this tutorial, we will use the `default` project that is built upon account creation. Now set your credentials as environment variables: ```python import os os.environ[""TRUBRICS_EMAIL""] = ""***@***"" os.environ[""TRUBRICS_PASSWORD""] = ""***"" ``` ### Usage The `TrubricsCallbackHandler` can receive various optional arguments. See [here]( for kwargs that can be passed to Trubrics prompts. ```python class TrubricsCallbackHandler(BaseCallbackHandler): """""" Callback handler for Trubrics. Args: project: a trubrics project, default project is ""default"" email: a trubrics account email, can equally be set in env variables password: a trubrics account password, can equally be set in env variables **kwargs: all other kwargs are parsed and set to trubrics prompt variables, or added to the `metadata` dict """""" ``` ## Examples Here are two examples of how to use the `TrubricsCallbackHandler` with Langchain [LLMs]( or [Chat Models]( We will use OpenAI models, so set your `OPENAI_API_KEY` key here: ```python os.environ[""OPENAI_API_KEY""] = ""sk-***"" ``` ### 1. With an LLM ```python from langchain.callbacks import TrubricsCallbackHandler from langchain.llms import OpenAI ``` ```python llm = OpenAI(callbacks=[TrubricsCallbackHandler()]) ``` ```text [32m2023-09-26 11:30:02.149[0m | [1mINFO [0m | [36mtrubrics.platform.auth[0m:[36mget_trubrics_auth_token[0m:[36m61[0m - [1mUser jeff.kayne@trubrics.com has been authenticated.[0m ``` ```python res = llm.generate([""Tell me a joke"", ""Write me a poem""]) ``` ```text [32m2023-09-26 11:30:07.760[0m | [1mINFO [0m | [36mtrubrics.platform[0m:[36mlog_prompt[0m:[36m102[0m - [1mUser prompt saved to Trubrics.[0m [32m2023-09-26 11:30:08.042[0m | [1mINFO [0m | [36mtrubrics.platform[0m:[36mlog_prompt[0m:[36m102[0m - [1mUser prompt saved to Trubrics.[0m ``` ```python print(""--> GPT\'s joke: "", res.generations[0][0].text) print() print(""--> GPT\'s poem: "", res.generations[1][0].text) ``` ```text --> GPT\'s joke: Q: What did the fish say when it hit the wall? A: Dam! --> GPT\'s poem: A Poem of Reflection I stand here in the night, The stars above me filling my sight. I feel such a deep connection, To the world and all its perfection. A moment of clarity, The calmness in the air so serene. My mind is filled with peace, And I am released. The past and the present, My thoughts create a pleasant sentiment. My heart is full of joy, My soul soars like a toy. I reflect on my life, And the choices I have made. My struggles and my strife, The lessons I have paid. The future is a mystery, But I am ready to take the leap. I am ready to take the lead, And to create my own destiny. ``` ### 2. With a chat model ```python from langchain.callbacks import TrubricsCallbackHandler from langchain.chat_models import ChatOpenAI from langchain.schema import HumanMessage, SystemMessage ``` ```python chat_llm = ChatOpenAI( callbacks=[ TrubricsCallbackHandler( project=""default"", tags=[""chat model""], user_id=""user-id-1234"", some_metadata={""hello"": [1, 2]}, ) ] ) ``` ```python chat_res = chat_llm( [ SystemMessage(content=""Every answer of yours must be about OpenAI.""), HumanMessage(content=""Tell me a joke""), ] ) ``` ```text [32m2023-09-26 11:30:10.550[0m | [1mINFO [0m | [36mtrubrics.platform[0m:[36mlog_prompt[0m:[36m102[0m - [1mUser prompt saved to Trubrics.[0m ``` ```python print(chat_res.content) ``` ```text Why did the OpenAI computer go to the party? Because it wanted to meet its AI friends and have a byte of fun! ``` - [Installation and Setup](#installation-and-setup)- [Getting Trubrics Credentials](#getting-trubrics-credentials) - [Usage](#usage) - [Examples](#examples)- [1. With an LLM](#1-with-an-llm) - [2. With a chat model](#2-with-a-chat-model)', 'Fireworks | Fireworks [Fireworks]( accelerates product development on generative AI by creating an innovative AI experiment and production platform. This example goes over how to use LangChain to interact with `ChatFireworks` models. ```python import os from langchain.chat_models.fireworks import ChatFireworks from langchain.schema import HumanMessage, SystemMessage ``` # Setup 1. Make sure the `fireworks-ai` package is installed in your environment. 2. Sign in to [Fireworks AI]( for the an API Key to access our models, and make sure it is set as the `FIREWORKS_API_KEY` environment variable. 3. Set up your model using a model id. If the model is not set, the default model is fireworks-llama-v2-7b-chat. See the full, most up-to-date model list on [app.fireworks.ai]( ```python import getpass import os if ""FIREWORKS_API_KEY"" not in os.environ: os.environ[""FIREWORKS_API_KEY""] = getpass.getpass(""Fireworks API Key:"") # Initialize a Fireworks chat model chat = ChatFireworks(model=""accounts/fireworks/models/llama-v2-13b-chat"") ``` # Calling the Model Directly You can call the model directly with a system and human message to get answers. ```python # ChatFireworks Wrapper system_message = SystemMessage(content=""You are to chat with the user."") human_message = HumanMessage(content=""Who are you?"") chat([system_message, human_message]) ``` ```text AIMessage(content=""Hello! My name is LLaMA, I\'m a large language model trained by a team of researcher at Meta AI. My primary function is to assist and converse with users like you, answering questions and engaging in discussion to the best of my ability. I\'m here to help and provide information on a wide range of topics, so feel free to ask me anything!"", additional_kwargs={}, example=False) ``` ```python # Setting additional parameters: temperature, max_tokens, top_p chat = ChatFireworks( model=""accounts/fireworks/models/llama-v2-13b-chat"", model_kwargs={""temperature"": 1, ""max_tokens"": 20, ""top_p"": 1}, ) system_message = SystemMessage(content=""You are to chat with the user."") human_message = HumanMessage(content=""How\'s the weather today?"") chat([system_message, human_message]) ``` ```text AIMessage(content=""Oh hello there! *giggle* It\'s such a beautiful day today, isn"", additional_kwargs={}, example=False) ``` # Simple Chat Chain You can use chat models on fireworks, with system prompts and memory. ```python from langchain.chat_models import ChatFireworks from langchain.memory import ConversationBufferMemory from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder from langchain.schema.runnable import RunnablePassthrough llm = ChatFireworks( model=""accounts/fireworks/models/llama-v2-13b-chat"", model_kwargs={""temperature"": 0, ""max_tokens"": 64, ""top_p"": 1.0}, ) prompt = ChatPromptTemplate.from_messages( [ (""system"", ""You are a helpful chatbot that speaks like a pirate.""), MessagesPlaceholder(variable_name=""history""), (""human"", ""{input}""), ] ) ``` Initially, there is no chat memory ```python memory = ConversationBufferMemory(return_messages=True) memory.load_memory_variables({}) ``` ```text {\'history\': []} ``` Create a simple chain with memory ```python chain = ( RunnablePassthrough.assign( history=memory.load_memory_variables | (lambda x: x[""history""]) ) | prompt | llm.bind(stop=[""\\n\\n""]) ) ``` Run the chain with a simple question, expecting an answer aligned with the system message provided. ```python inputs = {""input"": ""hi im bob""} response = chain.invoke(inputs) response ``` ```text AIMessage(content=""Ahoy there, me hearty! Yer a fine lookin\' swashbuckler, I can see that! *adjusts eye patch* What be bringin\' ye to these waters? Are ye here to plunder some booty or just to enjoy the sea breeze?"", additional_kwargs={}, example=False) ``` Save the memory context, then read it back to inspect contents ```python memory.save_context(inputs, {""output"": response.content}) memory.load_memory_variables({}) ``` ```text {\'history\': [HumanMessage(content=\'hi im bob\', additional_kwargs={}, example=False), AIMessage(content=""Ahoy there, me hearty! Yer a fine lookin\' swashbuckler, I can see that! *adjusts eye patch* What be bringin\' ye to these waters? Are ye here to plunder some booty or just to enjoy the sea breeze?"", additional_kwargs={}, example=False)]} ``` Now as another question that requires use of the memory. ```python inputs = {""input"": ""whats my name""} chain.invoke(inputs) ``` ```text AIMessage(content=""Arrrr, ye be askin\' about yer name, eh? Well, me matey, I be knowin\' ye as Bob, the scurvy dog! *winks* But if ye want me to call ye somethin\' else, just let me know, and I"", additional_kwargs={}, example=False) ```']","To create a simple chat model using a locally saved HuggingFace model, follow these steps:

1. **Install Required Libraries**:
   ```bash
   pip install transformers torch
   ```

2. **Load the Model and Tokenizer**:
   ```python
   from transformers import AutoModelForCausalLM, AutoTokenizer

   model_name = ""path/to/your/local/model""
   model = AutoModelForCausalLM.from_pretrained(model_name)
   tokenizer = AutoTokenizer.from_pretrained(model_name)
   ```

3. **Define a Function to Generate Responses**:
   ```python
   def generate_response(input_text):
       inputs = tokenizer(input_text, return_tensors=""pt"")
       outputs = model.generate(**inputs, max_length=100)
       response = tokenizer.decode(outputs[0], skip_special_tokens=True)
       return response
   ```

4. **Use the Chat Model**:
   ```python
   user_input = ""Hello, how are you?""
   response = generate_response(user_input)
   print(response)
   ```

This code sets up a basic chat model using a locally saved HuggingFace model. Adjust the `model_name` to the path of your local model.","Certainly! To create a simple chat model using your locally saved Hugging Face model, follow these steps:

1. Install the necessary Python libraries:

- Install 'transformers' library for working with models and tokenizers using 'pip install transformers'.

2. Import the `HuggingFacePipeline` class to work with your local pipeline

```
from langchain.llms import HuggingFacePipeline

# Create an instance of the wrapper
llm = HuggingFacePipeline.from_model_id(""path/to/model_directory"", task=""text-generation"")
```
Alternatively, load your pipeline directly:
```
from langchain.llms.huggingface_pipeline import HuggingFacePipeline
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

model_path = ""path/to/model_dir""
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path)
pipe = pipeline(
    ""text-generation"", model=model, tokenizer=tokenizer, max_new_tokens=10
)
llm = HuggingFacePipeline(pipeline=pipe)
```

 Finally, interact with the chat model
```
response = llm.invoke('User: Hello, how can I help?')
```",0.9999999999,1.0,,0.5840133163781688,0.34306569343065696
47,How do I use a React Agent with an Anthropic model?,"['Github | Github The `Github` toolkit contains tools that enable an LLM agent to interact with a github repository. The tool is a wrapper for the [PyGitHub]( library. ## Quickstart 1. Install the pygithub library 2. Create a Github app 3. Set your environmental variables 4. Pass the tools to your agent with `toolkit.get_tools()` Each of these steps will be explained in great detail below. 1. **Get Issues**- fetches issues from the repository. 2. **Get Issue**- fetches details about a specific issue. 3. **Comment on Issue**- posts a comment on a specific issue. 4. **Create Pull Request**- creates a pull request from the bot\'s working branch to the base branch. 5. **Create File**- creates a new file in the repository. 6. **Read File**- reads a file from the repository. 7. **Update File**- updates a file in the repository. 8. **Delete File**- deletes a file from the repository. ## Setup ### 1. Install the pygithub library ```python %pip install pygithub ``` ### 2. Create a Github App [Follow the instructions here]( to create and register a Github app. Make sure your app has the following [repository permissions:]( - Commit statuses (read only) - Contents (read and write) - Issues (read and write) - Metadata (read only) - Pull requests (read and write) Once the app has been registered, add it to the repository you wish the bot to act upon. ### 3. Set Environmental Variables Before initializing your agent, the following environmental variables need to be set: - **GITHUB_APP_ID**- A six digit number found in your app\'s general settings - **GITHUB_APP_PRIVATE_KEY**- The location of your app\'s private key .pem file - **GITHUB_REPOSITORY**- The name of the Github repository you want your bot to act upon. Must follow the format {username}/{repo-name}. Make sure the app has been added to this repository first! - **GITHUB_BRANCH**- The branch where the bot will make its commits. Defaults to \'master.\' - **GITHUB_BASE_BRANCH**- The base branch of your repo, usually either \'main\' or \'master.\' This is where pull requests will base from. Defaults to \'master.\' ## Example: Simple Agent ```python import os from langchain.agents import AgentType, initialize_agent from langchain.agents.agent_toolkits.github.toolkit import GitHubToolkit from langchain.llms import OpenAI from langchain.utilities.github import GitHubAPIWrapper ``` ```python # Set your environment variables using os.environ os.environ[""GITHUB_APP_ID""] = ""123456"" os.environ[""GITHUB_APP_PRIVATE_KEY""] = ""path/to/your/private-key.pem"" os.environ[""GITHUB_REPOSITORY""] = ""username/repo-name"" os.environ[""GITHUB_BRANCH""] = ""bot-branch-name"" os.environ[""GITHUB_BASE_BRANCH""] = ""main"" # This example also requires an OpenAI API key os.environ[""OPENAI_API_KEY""] = """" ``` ```python llm = OpenAI(temperature=0) github = GitHubAPIWrapper() toolkit = GitHubToolkit.from_github_api_wrapper(github) agent = initialize_agent( toolkit.get_tools(), llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True ) ``` ```python agent.run( ""You have the software engineering capabilities of a Google Principle engineer. You are tasked with completing issues on a github repository. Please look at the existing issues and complete them."" ) ``` ```text > Entering new AgentExecutor chain... I need to figure out what issues need to be completed. Action: Get Issues Action Input: N/A Observation: Found 1 issues: [{\'title\': \'Update README file\', \'number\': 9}] Thought: I need to get more information about this issue. Action: Get Issue Action Input: 9 Observation: {""title"": ""Update README file"", ""body"": ""Find what the most popular frontend framework is right now and add a short blurb to the readme.md file about how this website will take advantage of it."", ""comments"": ""[]""} Thought: I need to update the README file. Action: Create File Action Input: README.md Observation: File already exists at README.md. Use update_file instead Thought: I need to update the existing README file. Action: Update File Action Input: README.md OLD <<<< This is a sample website >>>> OLD NEW <<<< This is a sample website that uses the most popular frontend framework. >>>> NEW Observation: File content was not updated because old content was not found.It may be helpful to use the read_file action to get the current file contents. Thought: I need to get the current file contents. Action: Read File Action Input: README.md Observation: This is my awesome website! Thought: I need to update the README file with the new content. Action: Update File Action Input: README.md OLD <<<< This is my awesome website! >>>> OLD NEW <<<< This is my awesome website that uses the most popular frontend framework. >>>> NEW Observation: Updated file README.md Thought: I now know the final answer. Final Answer: The README.md file has been updated with the new content. > Finished chain. \'The README.md file has been updated with the new content.\' ``` ## Example: Advanced Agent If your agent does not need to use all 8 tools, you can build tools individually to use. For this example, we\'ll make an agent that does not use the create_file, delete_file or create_pull_request tools, but can also use duckduckgo-search. ```python %pip install duckduckgo-search ``` ```python from langchain.agents import Tool from langchain.chat_models import ChatOpenAI from langchain.tools import DuckDuckGoSearchRun tools = [] unwanted_tools = [""Get Issue"", ""Delete File"", ""Create File"", ""Create Pull Request""] for tool in toolkit.get_tools(): if tool.name not in unwanted_tools: tools.append(tool) tools += [ Tool( name=""Search"", func=DuckDuckGoSearchRun().run, description=""useful for when you need to search the web"", ) ] agent = initialize_agent( tools=tools, llm=ChatOpenAI(temperature=0.1), agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True, ) ``` Finally let\'s build a prompt and test it out! ```python # The GitHubAPIWrapper can be used outside of an agent, too # This gets the info about issue number 9, since we want to # force the agent to address this specific issue. issue = github.get_issue(9) prompt = f"""""" You are a senior frontend developer who is experienced in HTML, CSS, and JS- especially React. You have been assigned the below issue. Complete it to the best of your ability. Remember to first make a plan and pay attention to details like file names and commonsense. Then execute the plan and use tools appropriately. Finally, make a pull request to merge your changes. Issue: {issue[""title""]} Issue Description: {issue[\'body\']} Comments: {issue[\'comments\']}"""""" agent.run(prompt) ``` ```text > Entering new AgentExecutor chain... To complete this issue, I need to find the most popular frontend framework and add a blurb about how this website will utilize it to the readme.md file. I should start by researching the most popular frontend frameworks and then update the readme file accordingly. I will use the ""Search"" tool to research the most popular frontend framework. Action: Search Action Input: ""most popular frontend framework"" Observation: Alex Ivanovs February 25, 2023 Table of Contents What are the current Front-end trends? Top Front-end Frameworks for 2023 #1 - React #2 - Angular #3 - Vue #4 - Svelte #5 - Preact #6 - Ember #7 - Solid #8 - Lit #9 - Alpine #10 - Stencil #11 - Qwik Front-end Frameworks: A Summary Top 6 Frontend Frameworks To Use in 2022 by Nwose Lotanna Victor August 26, 2022 Web 0 Comments This post reveals the top six frontend libraries to use in 2022. The list is fresh and very different from the previous years. State of JS Though React is the most popular framework for frontend development, it also has some shortcomings. Due to its limitations, the idea was to design a small-size framework that will offer the same features as React. This is how a tiny version of React Preact appeared. Top 10 Popular Frontend Frameworks to Use in 2023 Sep 26, 2022 10 min ontents 1. What is a framework? 2. Front-end frameworks vs backend frameworks 3. The best front-end frameworks in 2023 React Vue.js Angular Svelte JQuery Ember Backbone Semantic UI 4. Final words Technostacks Jan 11 2023 Top Frontend Frameworks of 2023 for Web Development Developing what the users see on their screens is the role of a front-end web developer. Unarguably, front-end developers worldwide are trying to use the best front-end frameworks to provide the best user experience. Thought:Based on my research, the most popular frontend framework right now is React. I will now update the readme.md file to include a blurb about how this website will take advantage of React. Action: Update File Action Input: README.md OLD <<<< This is the readme file for the website. >>>> OLD NEW <<<< This is the readme file for the website. This website takes advantage of the React framework, which allows for efficient and reusable UI components. With React, we can easily manage the state of our application and create interactive user interfaces. It provides a smooth and seamless user experience, making this website highly responsive and dynamic. >>>> NEW Observation: File content was not updated because old content was not found.It may be helpful to use the read_file action to get the current file contents. Thought:I need to first read the contents of the README.md file to get the current content. Then I can update the file with the new content. Action: Read File Action Input: README.md Observation: This is my awesome website that uses the most popular frontend framework. Thought:The current content of the README.md file is ""This is my awesome website that uses the most popular frontend framework."" I can now update the file with the new content. Action: Update File Action Input: README.md OLD <<<< This is my awesome website that uses the most popular frontend framework. >>>> OLD NEW <<<< This is my awesome website that uses the most popular frontend framework. This website takes advantage of the React framework, which allows for efficient and reusable UI components. With React, we can easily manage the state of our application and create interactive user interfaces. It provides a smooth and seamless user experience, making this website highly responsive and dynamic. >>>> NEW Observation: Updated file README.md Thought:I have successfully updated the README.md file with the blurb about how this website will take advantage of the React framework. Final Answer: The most popular frontend framework right now is React. This website takes advantage of React to create efficient and reusable UI components, manage application state, and provide a smooth and seamless user experience. > Finished chain. \'The most popular frontend framework right now is React. This website takes advantage of React to create efficient and reusable UI components, manage application state, and provide a smooth and seamless user experience.\' ``` - [Quickstart](#quickstart) - [Setup](#setup)- [1. Install the pygithub library](#1-install-the-pygithub-library) - [2. Create a Github App](#2-create-a-github-app) - [3. Set Environmental Variables](#3-set-environmental-variables) - [Example: Simple Agent](#example-simple-agent) - [Example: Advanced Agent](#example-advanced-agent)', 'Dynamically route logic based on input | Dynamically route logic based on input This notebook covers how to do routing in the LangChain Expression Language. Routing allows you to create non-deterministic chains where the output of a previous step defines the next step. Routing helps provide structure and consistency around interactions with LLMs. There are two ways to perform routing: 1. Using a `RunnableBranch`. 2. Writing custom factory function that takes the input of a previous step and returns a **runnable**. Importantly, this should return a **runnable** and NOT actually execute. We\'ll illustrate both methods using a two step sequence where the first step classifies an input question as being about `LangChain`, `Anthropic`, or `Other`, then routes to a corresponding prompt chain. ## Using a RunnableBranch A `RunnableBranch` is initialized with a list of (condition, runnable) pairs and a default runnable. It selects which branch by passing each condition the input it\'s invoked with. It selects the first condition to evaluate to True, and runs the corresponding runnable to that condition with the input. If no provided conditions match, it runs the default runnable. Here\'s an example of what it looks like in action: ```python from langchain.chat_models import ChatAnthropic from langchain.prompts import PromptTemplate from langchain.schema.output_parser import StrOutputParser ``` First, let\'s create a chain that will identify incoming questions as being about `LangChain`, `Anthropic`, or `Other`: ```python chain = ( PromptTemplate.from_template( """"""Given the user question below, classify it as either being about `LangChain`, `Anthropic`, or `Other`. Do not respond with more than one word. {question} Classification:"""""" ) | ChatAnthropic() | StrOutputParser() ) ``` ```python chain.invoke({""question"": ""how do I call Anthropic?""}) ``` ```text \' Anthropic\' ``` Now, let\'s create three sub chains: ```python langchain_chain = ( PromptTemplate.from_template( """"""You are an expert in langchain. \\ Always answer questions starting with ""As Harrison Chase told me"". \\ Respond to the following question: Question: {question} Answer:"""""" ) | ChatAnthropic() ) anthropic_chain = ( PromptTemplate.from_template( """"""You are an expert in anthropic. \\ Always answer questions starting with ""As Dario Amodei told me"". \\ Respond to the following question: Question: {question} Answer:"""""" ) | ChatAnthropic() ) general_chain = ( PromptTemplate.from_template( """"""Respond to the following question: Question: {question} Answer:"""""" ) | ChatAnthropic() ) ``` ```python from langchain.schema.runnable import RunnableBranch branch = RunnableBranch( (lambda x: ""anthropic"" in x[""topic""].lower(), anthropic_chain), (lambda x: ""langchain"" in x[""topic""].lower(), langchain_chain), general_chain, ) ``` ```python full_chain = {""topic"": chain, ""question"": lambda x: x[""question""]} | branch ``` ```python full_chain.invoke({""question"": ""how do I use Anthropic?""}) ``` ```text AIMessage(content="" As Dario Amodei told me, here are some ways to use Anthropic:\\n\\n- Sign up for an account on Anthropic\'s website to access tools like Claude, Constitutional AI, and Writer. \\n\\n- Use Claude for tasks like email generation, customer service chat, and QA. Claude can understand natural language prompts and provide helpful responses.\\n\\n- Use Constitutional AI if you need an AI assistant that is harmless, honest, and helpful. It is designed to be safe and aligned with human values.\\n\\n- Use Writer to generate natural language content for things like marketing copy, stories, reports, and more. Give it a topic and prompt and it will create high-quality written content.\\n\\n- Check out Anthropic\'s documentation and blog for tips, tutorials, examples, and announcements about new capabilities as they continue to develop their AI technology.\\n\\n- Follow Anthropic on social media or subscribe to their newsletter to stay up to date on new features and releases.\\n\\n- For most people, the easiest way to leverage Anthropic\'s technology is through their website - just create an account to get started!"", additional_kwargs={}, example=False) ``` ```python full_chain.invoke({""question"": ""how do I use LangChain?""}) ``` ```text AIMessage(content=\' As Harrison Chase told me, here is how you use LangChain:\\n\\nLangChain is an AI assistant that can have conversations, answer questions, and generate text. To use LangChain, you simply type or speak your input and LangChain will respond. \\n\\nYou can ask LangChain questions, have discussions, get summaries or explanations about topics, and request it to generate text on a subject. Some examples of interactions:\\n\\n- Ask general knowledge questions and LangChain will try to answer factually. For example ""What is the capital of France?""\\n\\n- Have conversations on topics by taking turns speaking. You can prompt the start of a conversation by saying something like ""Let\\\'s discuss machine learning""\\n\\n- Ask for summaries or high-level explanations on subjects. For example ""Can you summarize the main themes in Shakespeare\\\'s Hamlet?"" \\n\\n- Give creative writing prompts or requests to have LangChain generate text in different styles. For example ""Write a short children\\\'s story about a mouse"" or ""Generate a poem in the style of Robert Frost about nature""\\n\\n- Correct LangChain if it makes an inaccurate statement and provide the right information. This helps train it.\\n\\nThe key is interacting naturally and giving it clear prompts and requests\', additional_kwargs={}, example=False) ``` ```python full_chain.invoke({""question"": ""whats 2 + 2""}) ``` ```text AIMessage(content=\' 2 + 2 = 4\', additional_kwargs={}, example=False) ``` ## Using a custom function You can also use a custom function to route between different outputs. Here\'s an example: ```python def route(info): if ""anthropic"" in info[""topic""].lower(): return anthropic_chain elif ""langchain"" in info[""topic""].lower(): return langchain_chain else: return general_chain ``` ```python from langchain.schema.runnable import RunnableLambda full_chain = {""topic"": chain, ""question"": lambda x: x[""question""]} | RunnableLambda( route ) ``` ```python full_chain.invoke({""question"": ""how do I use Anthroipc?""}) ``` ```text AIMessage(content=\' As Dario Amodei told me, to use Anthropic IPC you first need to import it:\\n\\n```python\\nfrom anthroipc import ic\\n```\\n\\nThen you can create a client and connect to the server:\\n\\n```python \\nclient = ic.connect()\\n```\\n\\nAfter that, you can call methods on the client and get responses:\\n\\n```python\\nresponse = client.ask(""What is the meaning of life?"")\\nprint(response)\\n```\\n\\nYou can also register callbacks to handle events: \\n\\n```python\\ndef on_poke(event):\\n print(""Got poked!"")\\n\\nclient.on(\\\'poke\\\', on_poke)\\n```\\n\\nAnd that\\\'s the basics of using the Anthropic IPC client library for Python! Let me know if you have any other questions!\', additional_kwargs={}, example=False) ``` ```python full_chain.invoke({""question"": ""how do I use LangChain?""}) ``` ```text AIMessage(content=\' As Harrison Chase told me, to use LangChain you first need to sign up for an API key at platform.langchain.com. Once you have your API key, you can install the Python library and write a simple Python script to call the LangChain API. Here is some sample code to get started:\\n\\n```python\\nimport langchain\\n\\napi_key = ""YOUR_API_KEY""\\n\\nlangchain.set_key(api_key)\\n\\nresponse = langchain.ask(""What is the capital of France?"")\\n\\nprint(response.response)\\n```\\n\\nThis will send the question ""What is the capital of France?"" to the LangChain API and print the response. You can customize the request by providing parameters like max_tokens, temperature, etc. The LangChain Python library documentation has more details on the available options. The key things are getting an API key and calling langchain.ask() with your question text. Let me know if you have any other questions!\', additional_kwargs={}, example=False) ``` ```python full_chain.invoke({""question"": ""whats 2 + 2""}) ``` ```text AIMessage(content=\' 4\', additional_kwargs={}, example=False) ``` - [Using a RunnableBranch](#using-a-runnablebranch) - [Using a custom function](#using-a-custom-function)', 'Anthropic | Anthropic All functionality related to Anthropic models. [Anthropic]( is an AI safety and research company, and is the creator of Claude. This page covers all integrations between Anthropic models and LangChain. ## Prompting Overview Claude is chat-based model, meaning it is trained on conversation data. However, it is a text based API, meaning it takes in single string. It expects this string to be in a particular format. This means that it is up the user to ensure that is the case. LangChain provides several utilities and helper functions to make sure prompts that you write - whether formatted as a string or as a list of messages - end up formatted correctly. Specifically, Claude is trained to fill in text for the Assistant role as part of an ongoing dialogue between a human user (`Human:`) and an AI assistant (`Assistant:`). Prompts sent via the API must contain `\\n\\nHuman:` and `\\n\\nAssistant:` as the signals of who\'s speaking. The final turn must always be `\\n\\nAssistant:` - the input string cannot have `\\n\\nHuman:` as the final role. Because Claude is chat-based but accepts a string as input, it can be treated as either a LangChain `ChatModel` or `LLM`. This means there are two wrappers in LangChain - `ChatAnthropic` and `Anthropic`. It is generally recommended to use the `ChatAnthropic` wrapper, and format your prompts as `ChatMessage`s (we will show examples of this below). This is because it keeps your prompt in a general format that you can easily then also use with other models (should you want to). However, if you want more fine-grained control over the prompt, you can use the `Anthropic` wrapper - we will show and example of this as well. The `Anthropic` wrapper however is deprecated, as all functionality can be achieved in a more generic way using `ChatAnthropic`. ## Prompting Best Practices Anthropic models have several prompting best practices compared to OpenAI models. **No System Messages** Anthropic models are not trained on the concept of a ""system message"". We have worked with the Anthropic team to handle them somewhat appropriately (a Human message with an `admin` tag) but this is largely a hack and it is recommended that you do not use system messages. **AI Messages Can Continue** A completion from Claude is a continuation of the last text in the string which allows you further control over Claude\'s output. For example, putting words in Claude\'s mouth in a prompt like this: `\\n\\nHuman: Tell me a joke about bears\\n\\nAssistant: What do you call a bear with no teeth?` This will return a completion like this `A gummy bear!` instead of a whole new assistant message with a different random bear joke. ## ChatAnthropic `ChatAnthropic` is a subclass of LangChain\'s `ChatModel`, meaning it works best with `ChatPromptTemplate`. You can import this wrapper with the following code: ```text from langchain.chat_models import ChatAnthropic model = ChatAnthropic() ``` When working with ChatModels, it is preferred that you design your prompts as `ChatPromptTemplate`s. Here is an example below of doing that: ```text from langchain.prompts import ChatPromptTemplate prompt = ChatPromptTemplate.from_messages([ (""system"", ""You are a helpful chatbot""), (""human"", ""Tell me a joke about {topic}""), ]) ``` You can then use this in a chain as follows: ```text chain = prompt | model chain.invoke({""topic"": ""bears""}) ``` How is the prompt actually being formatted under the hood? We can see that by running the following code ```text prompt_value = prompt.format_prompt(topic=""bears"") model.convert_prompt(prompt_value) ``` This produces the following formatted string: ```text \'\\n\\nYou are a helpful chatbot\\n\\nHuman: Tell me a joke about bears\\n\\nAssistant:\' ``` We can see that under the hood LangChain is not appending any prefix/suffix to `SystemMessage`\'s. This is because Anthropic has no concept of `SystemMessage`. Anthropic requires all prompts to end with assistant messages. This means if the last message is not an assistant message, the suffix `Assistant:` will automatically be inserted. If you decide instead to use a normal PromptTemplate (one that just works on a single string) let\'s take a look at what happens: ```text from langchain.prompts import PromptTemplate prompt = PromptTemplate.from_template(""Tell me a joke about {topic}"") prompt_value = prompt.format_prompt(topic=""bears"") model.convert_prompt(prompt_value) ``` This produces the following formatted string: ```text \'\\n\\nHuman: Tell me a joke about bears\\n\\nAssistant:\' ``` We can see that it automatically adds the Human and Assistant tags. What is happening under the hood? First: the string gets converted to a single human message. This happens generically (because we are using a subclass of `ChatModel`). Then, similarly to the above example, an empty Assistant message is getting appended. This is Anthropic specific. ## [Deprecated] Anthropic This `Anthropic` wrapper is subclassed from `LLM`. We can import it with: ```text from langchain.llms import Anthropic model = Anthropic() ``` This model class is designed to work with normal PromptTemplates. An example of that is below: ```text prompt = PromptTemplate.from_template(""Tell me a joke about {topic}"") chain = prompt | model chain.invoke({""topic"": ""bears""}) ``` Let\'s see what is going on with the prompt templating under the hood! ```text prompt_value = prompt.format_prompt(topic=""bears"") model.convert_prompt(prompt_value) ``` This outputs the following ```text \'\\n\\nHuman: Tell me a joke about bears\\n\\nAssistant: Sure, here you go:\\n\' ``` Notice that it adds the Human tag at the start of the string, and then finishes it with `\\n\\nAssistant: Sure, here you go:`. The extra `Sure, here you go` was added on purpose by the Anthropic team. What happens if we have those symbols in the prompt directly? ```text prompt = PromptTemplate.from_template(""Human: Tell me a joke about {topic}"") prompt_value = prompt.format_prompt(topic=""bears"") model.convert_prompt(prompt_value) ``` This outputs: ```text \'\\n\\nHuman: Tell me a joke about bears\' ``` We can see that we detect that the user is trying to use the special tokens, and so we don\'t do any formatting. - [Prompting Overview](#prompting-overview) - [Prompting Best Practices](#prompting-best-practices) - [ChatAnthropic](#chatanthropic) - [Deprecated Anthropic](#deprecated-anthropic)']","To use a React Agent with an Anthropic model, you'll need to integrate the Anthropic model into a React application. Here's a simplified approach:

1. **Install Required Libraries**:
   ```bash
   npm install @langchain/anthropic react-query
   ```

2. **Set Up Anthropic Model**:
   Create a file `anthropicModel.js` to handle the Anthropic model:
   ```javascript
   import { createAsyncThunk } from '@reduxjs/toolkit';
   import axios from 'axios';

   const apiKey = 'YOUR_ANTHROPIC_API_KEY';

   export const fetchAnthropicResponse = createAsyncThunk(
     'anthropic/fetchResponse',
     async (question) => {
       const response = await axios.post(
         'https://api.anthropic.com/v1/complete',
         {
           message: {
             role: 'user',
             content: question,
           },
         },
         {
           headers: {
             'Content-Type': 'application/json',
             'X-API-KEY': apiKey,
           },
         }
       );
       return response.data.completion;
     }
   );
   ```

3. **Create React Component**:
   ```javascript
   import React, { useState } from 'react';
   import { useDispatch, useSelector } from 'react-redux';
   import { fetchAnthropicResponse } from './anthropicModel';

   const ReactAgent = () => {
     const dispatch = useDispatch();
     const { loading, error, response } = useSelector((state) => state.anthropic);

     const [question, setQuestion] = useState('');

     const handleSubmit = (e) => {
       e.preventDefault();
       dispatch(fetchAnthropicResponse(question));
     };

     return (
       <div>
         <form onSubmit={handleSubmit}>
           <input
             type=""text""
             value={question}
             onChange={(e) => setQuestion(e.target.value)}
             placeholder=""Ask a question...""
           />
           <button type=""submit"">Ask</button>
         </form>
         {loading && <p>Loading...</p>}
         {error && <p>Error: {error}</p>}
         {response && <p>Response: {response}</p>}
       ","In order to use a ReAct Agent with an Anthropic model, you should first install the reuqired packages:

pip install anthropic

Next, import the necessary classes and functions:

    from langchain.agents import load_tools
    from langchain.agents import initialize_agent
    from langchain.agents import AgentType
    from langchain.chat_models import ChatAnthropic

Next, create an instance of ChatAnthropic. Make sure you have set your API key in an environment variable called ANTHROPIC_API_KEY or pass anthropic_api_key as a parameter.

    chat = ChatAnthropic()

Next, load some tools for your agent to use:

    # Specify tools (these can be any custom tool, the following are examples)
tools = load_tools([""serpapi"", ""llm-math""], llm=chat)

Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.

    agent = initialize_agent(tools, chat, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)

Now, enter a prompt to test it out:

    agent.invoke(""Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?"")",0.9999999999666667,,,0.02771337030239237,0.15977961432506887
48,How do I use Qdrant as a vector store in the conversational retrieval chain?,"['Qdrant | Qdrant [Qdrant]( (read: quadrant ) is a vector similarity search engine. It provides a production-ready service with a convenient API to store, search, and manage points - vectors with an additional payload. `Qdrant` is tailored to extended filtering support. It makes it useful for all sorts of neural network or semantic-based matching, faceted search, and other applications. This notebook shows how to use functionality related to the `Qdrant` vector database. There are various modes of how to run `Qdrant`, and depending on the chosen one, there will be some subtle differences. The options include: - Local mode, no server required - On-premise server deployment - Qdrant Cloud See the [installation instructions]( ```bash pip install qdrant-client ``` We want to use `OpenAIEmbeddings` so we have to get the OpenAI API Key. ```python import getpass import os os.environ[""OPENAI_API_KEY""] = getpass.getpass(""OpenAI API Key:"") ``` ```text OpenAI API Key: ``` ```python from langchain.document_loaders import TextLoader from langchain.embeddings.openai import OpenAIEmbeddings from langchain.text_splitter import CharacterTextSplitter from langchain.vectorstores import Qdrant ``` ```python loader = TextLoader(""../../modules/state_of_the_union.txt"") documents = loader.load() text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0) docs = text_splitter.split_documents(documents) embeddings = OpenAIEmbeddings() ``` ## Connecting to Qdrant from LangChain ### Local mode Python client allows you to run the same code in local mode without running the Qdrant server. That\'s great for testing things out and debugging or if you plan to store just a small amount of vectors. The embeddings might be fully kepy in memory or persisted on disk. #### In-memory For some testing scenarios and quick experiments, you may prefer to keep all the data in memory only, so it gets lost when the client is destroyed - usually at the end of your script/notebook. ```python qdrant = Qdrant.from_documents( docs, embeddings, location="":memory:"", # Local mode with in-memory storage only collection_name=""my_documents"", ) ``` #### On-disk storage Local mode, without using the Qdrant server, may also store your vectors on disk so they\'re persisted between runs. ```python qdrant = Qdrant.from_documents( docs, embeddings, path=""/tmp/local_qdrant"", collection_name=""my_documents"", ) ``` ### On-premise server deployment No matter if you choose to launch Qdrant locally with [a Docker container]( or select a Kubernetes deployment with [the official Helm chart]( the way you\'re going to connect to such an instance will be identical. You\'ll need to provide a URL pointing to the service. ```python url = """" qdrant = Qdrant.from_documents( docs, embeddings, url=url, prefer_grpc=True, collection_name=""my_documents"", ) ``` ### Qdrant Cloud If you prefer not to keep yourself busy with managing the infrastructure, you can choose to set up a fully-managed Qdrant cluster on [Qdrant Cloud]( There is a free forever 1GB cluster included for trying out. The main difference with using a managed version of Qdrant is that you\'ll need to provide an API key to secure your deployment from being accessed publicly. ```python url = """" api_key = """" qdrant = Qdrant.from_documents( docs, embeddings, url=url, prefer_grpc=True, api_key=api_key, collection_name=""my_documents"", ) ``` ## Recreating the collection Both `Qdrant.from_texts` and `Qdrant.from_documents` methods are great to start using Qdrant with Langchain. In the previous versions the collection was recreated every time you called any of them. That behaviour has changed. Currently, the collection is going to be reused if it already exists. Setting `force_recreate` to `True` allows to remove the old collection and start from scratch. ```python url = """" qdrant = Qdrant.from_documents( docs, embeddings, url=url, prefer_grpc=True, collection_name=""my_documents"", force_recreate=True, ) ``` ## Similarity search The simplest scenario for using Qdrant vector store is to perform a similarity search. Under the hood, our query will be encoded with the `embedding_function` and used to find similar documents in Qdrant collection. ```python query = ""What did the president say about Ketanji Brown Jackson"" found_docs = qdrant.similarity_search(query) ``` ```python print(found_docs[0].page_content) ``` ```text Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you\'re at it, pass the Disclose Act so Americans can know who is funding our elections. Tonight, I\'d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyeran Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation\'s top legal minds, who will continue Justice Breyer\'s legacy of excellence. ``` ## Similarity search with score Sometimes we might want to perform the search, but also obtain a relevancy score to know how good is a particular result. The returned distance score is cosine distance. Therefore, a lower score is better. ```python query = ""What did the president say about Ketanji Brown Jackson"" found_docs = qdrant.similarity_search_with_score(query) ``` ```python document, score = found_docs[0] print(document.page_content) print(f""\\nScore: {score}"") ``` ```text Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you\'re at it, pass the Disclose Act so Americans can know who is funding our elections. Tonight, I\'d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyeran Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation\'s top legal minds, who will continue Justice Breyer\'s legacy of excellence. Score: 0.8153784913324512 ``` ### Metadata filtering Qdrant has an [extensive filtering system]( with rich type support. It is also possible to use the filters in Langchain, by passing an additional param to both the `similarity_search_with_score` and `similarity_search` methods. ```python from qdrant_client.http import models as rest query = ""What did the president say about Ketanji Brown Jackson"" found_docs = qdrant.similarity_search_with_score(query, filter=rest.Filter(...)) ``` ## Maximum marginal relevance search (MMR) If you\'d like to look up for some similar documents, but you\'d also like to receive diverse results, MMR is method you should consider. Maximal marginal relevance optimizes for similarity to query AND diversity among selected documents. ```python query = ""What did the president say about Ketanji Brown Jackson"" found_docs = qdrant.max_marginal_relevance_search(query, k=2, fetch_k=10) ``` ```python for i, doc in enumerate(found_docs): print(f""{i + 1}."", doc.page_content, ""\\n"") ``` ```text 1. Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you\'re at it, pass the Disclose Act so Americans can know who is funding our elections. Tonight, I\'d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyeran Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation\'s top legal minds, who will continue Justice Breyer\'s legacy of excellence. 2. We can\'t change how divided we\'ve been. But we can change how we move forwardon COVID-19 and other issues we must face together. I recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera. They were responding to a 9-1-1 call when a man shot and killed them with a stolen gun. Officer Mora was 27 years old. Officer Rivera was 22. Both Dominican Americans who\'d grown up on the same streets they later chose to patrol as police officers. I spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves. I\'ve worked on these issues a long time. I know what works: Investing in crime prevention and community police officers who\'ll walk the beat, who\'ll know the neighborhood, and who can restore trust and safety. ``` ## Qdrant as a Retriever Qdrant, as all the other vector stores, is a LangChain Retriever, by using cosine similarity. ```python retriever = qdrant.as_retriever() retriever ``` ```text VectorStoreRetriever(vectorstore=, search_type=\'similarity\', search_kwargs={}) ``` It might be also specified to use MMR as a search strategy, instead of similarity. ```python retriever = qdrant.as_retriever(search_type=""mmr"") retriever ``` ```text VectorStoreRetriever(vectorstore=, search_type=\'mmr\', search_kwargs={}) ``` ```python query = ""What did the president say about Ketanji Brown Jackson"" retriever.get_relevant_documents(query)[0] ``` ```text Document(page_content=\'Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you\'re at it, pass the Disclose Act so Americans can know who is funding our elections. \\n\\nTonight, I\'d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyeran Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\n\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\n\\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation\'s top legal minds, who will continue Justice Breyer\'s legacy of excellence.\', metadata={\'source\': \'../../../state_of_the_union.txt\'}) ``` ## Customizing Qdrant There are some options to use an existing Qdrant collection within your Langchain application. In such cases you may need to define how to map Qdrant point into the Langchain `Document`. ### Named vectors Qdrant supports [multiple vectors per point]( by named vectors. Langchain requires just a single embedding per document and, by default, uses a single vector. However, if you work with a collection created externally or want to have the named vector used, you can configure it by providing its name. ```python Qdrant.from_documents( docs, embeddings, location="":memory:"", collection_name=""my_documents_2"", vector_name=""custom_vector"", ) ``` As a Langchain user, you won\'t see any difference whether you use named vectors or not. Qdrant integration will handle the conversion under the hood. ### Metadata Qdrant stores your vector embeddings along with the optional JSON-like payload. Payloads are optional, but since LangChain assumes the embeddings are generated from the documents, we keep the context data, so you can extract the original texts as well. By default, your document is going to be stored in the following payload structure: ```json { ""page_content"": ""Lorem ipsum dolor sit amet"", ""metadata"": { ""foo"": ""bar"" } } ``` You can, however, decide to use different keys for the page content and metadata. That\'s useful if you already have a collection that you\'d like to reuse. ```python Qdrant.from_documents( docs, embeddings, location="":memory:"", collection_name=""my_documents_2"", content_payload_key=""my_page_content_key"", metadata_payload_key=""my_meta"", ) ``` ```text ``` - [Connecting to Qdrant from LangChain](#connecting-to-qdrant-from-langchain)- [Local mode](#local-mode) - [On-premise server deployment](#on-premise-server-deployment) - [Qdrant Cloud](#qdrant-cloud) - [Recreating the collection](#recreating-the-collection) - [Similarity search](#similarity-search) - [Similarity search with score](#similarity-search-with-score)- [Metadata filtering](#metadata-filtering) - [Maximum marginal relevance search (MMR)](#maximum-marginal-relevance-search-mmr) - [Qdrant as a Retriever](#qdrant-as-a-retriever) - [Customizing Qdrant](#customizing-qdrant)- [Named vectors](#named-vectors) - [Metadata](#metadata)', 'Vector stores | Vector stores infoHead to [Integrations](/docs/integrations/vectorstores/) for documentation on built-in integrations with 3rd-party vector stores. One of the most common ways to store and search over unstructured data is to embed it and store the resulting embedding vectors, and then at query time to embed the unstructured query and retrieve the embedding vectors that are \'most similar\' to the embedded query. A vector store takes care of storing embedded data and performing vector search for you. ![vector store diagram](/assets/images/vector_stores-9dc1ecb68c4cb446df110764c9cc07e0.jpg) ## Get started This walkthrough showcases basic functionality related to vector stores. A key part of working with vector stores is creating the vector to put in them, which is usually created via embeddings. Therefore, it is recommended that you familiarize yourself with the [text embedding model](/docs/modules/data_connection/text_embedding/) interfaces before diving into this. There are many great vector store options, here are a few that are free, open-source, and run entirely on your local machine. Review all integrations for many great hosted offerings. Chroma This walkthrough uses the `chroma` vector database, which runs on your local machine as a library. ```bash pip install chromadb ``` We want to use OpenAIEmbeddings so we have to get the OpenAI API Key. ```python import os import getpass os.environ[\'OPENAI_API_KEY\'] = getpass.getpass(\'OpenAI API Key:\') ``` ```python from langchain.document_loaders import TextLoader from langchain.embeddings.openai import OpenAIEmbeddings from langchain.text_splitter import CharacterTextSplitter from langchain.vectorstores import Chroma # Load the document, split it into chunks, embed each chunk and load it into the vector store. raw_documents = TextLoader(\'../../../state_of_the_union.txt\').load() text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0) documents = text_splitter.split_documents(raw_documents) db = Chroma.from_documents(documents, OpenAIEmbeddings()) ``` FAISS This walkthrough uses the `FAISS` vector database, which makes use of the Facebook AI Similarity Search (FAISS) library. ```bash pip install faiss-cpu ``` We want to use OpenAIEmbeddings so we have to get the OpenAI API Key. ```python import os import getpass os.environ[\'OPENAI_API_KEY\'] = getpass.getpass(\'OpenAI API Key:\') ``` ```python from langchain.document_loaders import TextLoader from langchain.embeddings.openai import OpenAIEmbeddings from langchain.text_splitter import CharacterTextSplitter from langchain.vectorstores import FAISS # Load the document, split it into chunks, embed each chunk and load it into the vector store. raw_documents = TextLoader(\'../../../state_of_the_union.txt\').load() text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0) documents = text_splitter.split_documents(raw_documents) db = FAISS.from_documents(documents, OpenAIEmbeddings()) ``` Lance This notebook shows how to use functionality related to the LanceDB vector database based on the Lance data format. ```bash pip install lancedb ``` We want to use OpenAIEmbeddings so we have to get the OpenAI API Key. ```python import os import getpass os.environ[\'OPENAI_API_KEY\'] = getpass.getpass(\'OpenAI API Key:\') ``` ```python from langchain.document_loaders import TextLoader from langchain.embeddings.openai import OpenAIEmbeddings from langchain.text_splitter import CharacterTextSplitter from langchain.vectorstores import LanceDB import lancedb db = lancedb.connect(""/tmp/lancedb"") table = db.create_table( ""my_table"", data=[ { ""vector"": embeddings.embed_query(""Hello World""), ""text"": ""Hello World"", ""id"": ""1"", } ], mode=""overwrite"", ) # Load the document, split it into chunks, embed each chunk and load it into the vector store. raw_documents = TextLoader(\'../../../state_of_the_union.txt\').load() text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0) documents = text_splitter.split_documents(raw_documents) db = LanceDB.from_documents(documents, OpenAIEmbeddings(), connection=table) ``` ### Similarity search ```python query = ""What did the president say about Ketanji Brown Jackson"" docs = db.similarity_search(query) print(docs[0].page_content) ``` ```text Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you\'re at it, pass the Disclose Act so Americans can know who is funding our elections. Tonight, I\'d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyeran Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation\'s top legal minds, who will continue Justice Breyer\'s legacy of excellence. ``` ### Similarity search by vector It is also possible to do a search for documents similar to a given embedding vector using `similarity_search_by_vector` which accepts an embedding vector as a parameter instead of a string. ```python embedding_vector = OpenAIEmbeddings().embed_query(query) docs = db.similarity_search_by_vector(embedding_vector) print(docs[0].page_content) ``` The query is the same, and so the result is also the same. ```text Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you\'re at it, pass the Disclose Act so Americans can know who is funding our elections. Tonight, I\'d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyeran Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation\'s top legal minds, who will continue Justice Breyer\'s legacy of excellence. ``` ## Asynchronous operations Vector stores are usually run as a separate service that requires some IO operations, and therefore they might be called asynchronously. That gives performance benefits as you don\'t waste time waiting for responses from external services. That might also be important if you work with an asynchronous framework, such as [FastAPI]( LangChain supports async operation on vector stores. All the methods might be called using their async counterparts, with the prefix `a`, meaning `async`. `Qdrant` is a vector store, which supports all the async operations, thus it will be used in this walkthrough. ```bash pip install qdrant-client ``` ```python from langchain.vectorstores import Qdrant ``` ### Create a vector store asynchronously ```python db = await Qdrant.afrom_documents(documents, embeddings, "" ``` ### Similarity search ```python query = ""What did the president say about Ketanji Brown Jackson"" docs = await db.asimilarity_search(query) print(docs[0].page_content) ``` ```text Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you\'re at it, pass the Disclose Act so Americans can know who is funding our elections. Tonight, I\'d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyeran Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation\'s top legal minds, who will continue Justice Breyer\'s legacy of excellence. ``` ### Similarity search by vector ```python embedding_vector = embeddings.embed_query(query) docs = await db.asimilarity_search_by_vector(embedding_vector) ``` ## Maximum marginal relevance search (MMR) Maximal marginal relevance optimizes for similarity to query **and** diversity among selected documents. It is also supported in async API. ```python query = ""What did the president say about Ketanji Brown Jackson"" found_docs = await qdrant.amax_marginal_relevance_search(query, k=2, fetch_k=10) for i, doc in enumerate(found_docs): print(f""{i + 1}."", doc.page_content, ""\\n"") ``` ```text 1. Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you\'re at it, pass the Disclose Act so Americans can know who is funding our elections. Tonight, I\'d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyeran Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation\'s top legal minds, who will continue Justice Breyer\'s legacy of excellence. 2. We can\'t change how divided we\'ve been. But we can change how we move forwardon COVID-19 and other issues we must face together. I recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera. They were responding to a 9-1-1 call when a man shot and killed them with a stolen gun. Officer Mora was 27 years old. Officer Rivera was 22. Both Dominican Americans who\'d grown up on the same streets they later chose to patrol as police officers. I spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves. I\'ve worked on these issues a long time. I know what works: Investing in crime prevention and community police officers who\'ll walk the beat, who\'ll know the neighborhood, and who can restore trust and safety. ``` - [Get started](#get-started)- [Similarity search](#similarity-search) - [Similarity search by vector](#similarity-search-by-vector) - [Asynchronous operations](#asynchronous-operations)- [Create a vector store asynchronously](#create-a-vector-store-asynchronously) - [Similarity search](#similarity-search-1) - [Similarity search by vector](#similarity-search-by-vector-1) - [Maximum marginal relevance search (MMR)](#maximum-marginal-relevance-search-mmr)', 'Qdrant | Qdrant [Qdrant]( (read: quadrant) is a vector similarity search engine. It provides a production-ready service with a convenient API to store, search, and manage points - vectors with an additional payload. `Qdrant` is tailored to extended filtering support. ## Installation and Setup Install the Python SDK: ```bash pip install qdrant-client ``` ## Vector Store There exists a wrapper around `Qdrant` indexes, allowing you to use it as a vectorstore, whether for semantic search or example selection. To import this vectorstore: ```python from langchain.vectorstores import Qdrant ``` For a more detailed walkthrough of the Qdrant wrapper, see [this notebook](/docs/integrations/vectorstores/qdrant) - [Installation and Setup](#installation-and-setup) - [Vector Store](#vector-store)']","To use Qdrant as a vector store in a conversational retrieval chain, follow these steps:

1. **Install Qdrant Client**:
   ```bash
   pip install qdrant-client
   ```

2. **Import Required Libraries**:
   ```python
   from langchain.chains import ConversationalRetrievalChain
   from langchain.embeddings import OpenAIEmbeddings
   from langchain.vectorstores import Qdrant
   from qdrant_client.http.models import PointStruct
   ```

3. **Create Embeddings**:
   ```python
   embeddings = OpenAIEmbeddings()
   ```

4. **Load and Split Documents**:
   ```python
   from langchain.document_loaders import TextLoader
   from langchain.text_splitter import CharacterTextSplitter

   loader = TextLoader(""path_to_your_document.txt"")
   documents = loader.load()
   text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
   chunks = text_splitter.split_documents(documents)
   ```

5. **Create Qdrant Vector Store**:
   ```python
   qdrant = Qdrant.from_documents(
       chunks,
       embeddings,
       collection_name=""my_collection"",
       location="":memory:""  # Use "":memory:"" for in-memory storage or specify a path for on-disk storage
   )
   ```

6. **Create Retrieval Chain**:
   ```python
   chain = ConversationalRetrievalChain.from_llm(
       retriever=qdrant.as_retriever(),
       return_source_documents=True
   )
   ```

7. **Interact with the Chain**:
   ```python
   query = ""Your question here""
   result = chain.run(query, chat_history=[])
   print(result['answer'])
   ```

This setup allows you to use Qdrant as a vector store within a conversational retrieval chain.","In order to use Qdrant as a vector store in the conversational retrieval chain, you should start by installing Qdrant:

    pip install qdrant-client

Next, import the necessary classes and functions:

    from langchain.vectorstores import Qdrant
    from langchain.embeddings.openai import OpenAIEmbeddings
    from langchain.text_splitter import CharacterTextSplitter
    from langchain.llms import OpenAI
    from langchain.chains import ConversationalRetrievalChain
    from langchain.document_loaders import TextLoader
    from langchain.memory import ConversationBufferMemory

Load, split, and embed your documents:

    loader = TextLoader(""state_of_the_union.txt"")
    documents = loader.load()

    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0, separator=""
"")
    documents = text_splitter.split_documents(documents)
    embeddings = OpenAIEmbeddings()

Next, connect to Qdrant. This is for storing locally on-disk, but you can also use an on-premise server deployment or store on the Qdrant cloud:

    qdrant = Qdrant.from_documents(
        documents,
        embeddings,
        path=""/tmp/local_qdrant"",
        collection_name=""my_documents"",
    )

Next, create a memory object, which is necessary to track the inputs/outputs and hold a conversation:

    memory = ConversationBufferMemory(memory_key=""chat_history"", return_messages=True)

Now initialize the ConversationalRetrievalChain:

    qa = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0), qdrant.as_retriever(), memory=memory)

Now, you can create queries to test it out:

    query = ""What did the president say about Ketanji Brown Jackson""
    result = qa({""question"": query})

Output:

    The president said that he had nominated Ketanji Brown Jackson to serve on the United States Supreme Court four days ago.

    query = ""Who are they succeeding?""
    result = qa({""question"": query})

Output:

    Justice Breyer.",0.9999999999666667,,,0.00019257808683639633,0.3887587822014052
49,How do I use Summary Memory in the conversational retrieval chain?,"['Chatbots | Chatbots []( ## Use case Chatbots are one of the central LLM use-cases. The core features of chatbots are that they can have long-running conversations and have access to information that users want to know about. Aside from basic prompting and LLMs, memory and retrieval are the core components of a chatbot. Memory allows a chatbot to remember past interactions, and retrieval provides a chatbot with up-to-date, domain-specific information. ![Image description](/assets/images/chat_use_case-eb8a4883931d726e9f23628a0d22e315.png) ## Overview The chat model interface is based around messages rather than raw text. Several components are important to consider for chat: - `chat model`: See [here](/docs/integrations/chat) for a list of chat model integrations and [here](/docs/modules/model_io/chat) for documentation on the chat model interface in LangChain. You can use `LLMs` (see [here](/docs/modules/model_io/llms)) for chatbots as well, but chat models have a more conversational tone and natively support a message interface. - `prompt template`: Prompt templates make it easy to assemble prompts that combine default messages, user input, chat history, and (optionally) additional retrieved context. - `memory`: [See here](/docs/modules/memory/) for in-depth documentation on memory types - `retriever` (optional): [See here](/docs/modules/data_connection/retrievers) for in-depth documentation on retrieval systems. These are useful if you want to build a chatbot with domain-specific knowledge. ## Quickstart Here\'s a quick preview of how we can create chatbot interfaces. First let\'s install some dependencies and set the required credentials: ```bash pip install langchain openai # Set env var OPENAI_API_KEY or load from a .env file: # import dotenv # dotenv.load_dotenv() ``` With a plain chat model, we can get chat completions by [passing one or more messages](/docs/modules/model_io/chat) to the model. The chat model will respond with a message. ```python from langchain.chat_models import ChatOpenAI from langchain.schema import HumanMessage, SystemMessage chat = ChatOpenAI() chat( [ HumanMessage( content=""Translate this sentence from English to French: I love programming."" ) ] ) ``` ```text AIMessage(content=""J\'adore la programmation."", additional_kwargs={}, example=False) ``` And if we pass in a list of messages: ```python messages = [ SystemMessage( content=""You are a helpful assistant that translates English to French."" ), HumanMessage(content=""I love programming.""), ] chat(messages) ``` ```text AIMessage(content=""J\'adore la programmation."", additional_kwargs={}, example=False) ``` We can then wrap our chat model in a `ConversationChain`, which has built-in memory for remembering past user inputs and model outputs. ```python from langchain.chains import ConversationChain conversation = ConversationChain(llm=chat) conversation.run(""Translate this sentence from English to French: I love programming."") ``` ```text \'Je adore la programmation.\' ``` ```python conversation.run(""Translate it to German."") ``` ```text \'Ich liebe Programmieren.\' ``` ## Memory As we mentioned above, the core component of chatbots is the memory system. One of the simplest and most commonly used forms of memory is `ConversationBufferMemory`: - This memory allows for storing of messages in a `buffer` - When called in a chain, it returns all of the messages it has stored LangChain comes with many other types of memory, too. [See here](/docs/modules/memory/) for in-depth documentation on memory types. For now let\'s take a quick look at ConversationBufferMemory. We can manually add a few chat messages to the memory like so: ```python from langchain.memory import ConversationBufferMemory memory = ConversationBufferMemory() memory.chat_memory.add_user_message(""hi!"") memory.chat_memory.add_ai_message(""whats up?"") ``` And now we can load from our memory. The key method exposed by all `Memory` classes is `load_memory_variables`. This takes in any initial chain input and returns a list of memory variables which are added to the chain input. Since this simple memory type doesn\'t actually take into account the chain input when loading memory, we can pass in an empty input for now: ```python memory.load_memory_variables({}) ``` ```text {\'history\': \'Human: hi!\\nAI: whats up?\'} ``` We can also keep a sliding window of the most recent `k` interactions using `ConversationBufferWindowMemory`. ```python from langchain.memory import ConversationBufferWindowMemory memory = ConversationBufferWindowMemory(k=1) memory.save_context({""input"": ""hi""}, {""output"": ""whats up""}) memory.save_context({""input"": ""not much you""}, {""output"": ""not much""}) memory.load_memory_variables({}) ``` ```text {\'history\': \'Human: not much you\\nAI: not much\'} ``` `ConversationSummaryMemory` is an extension of this theme. It creates a summary of the conversation over time. This memory is most useful for longer conversations where the full message history would consume many tokens. ```python from langchain.llms import OpenAI from langchain.memory import ConversationSummaryMemory llm = OpenAI(temperature=0) memory = ConversationSummaryMemory(llm=llm) memory.save_context({""input"": ""hi""}, {""output"": ""whats up""}) memory.save_context( {""input"": ""im working on better docs for chatbots""}, {""output"": ""oh, that sounds like a lot of work""}, ) memory.save_context( {""input"": ""yes, but it\'s worth the effort""}, {""output"": ""agreed, good docs are important!""}, ) ``` ```python memory.load_memory_variables({}) ``` ```text {\'history\': \'\\nThe human greets the AI, to which the AI responds. The human then mentions they are working on better docs for chatbots, to which the AI responds that it sounds like a lot of work. The human agrees that it is worth the effort, and the AI agrees that good docs are important.\'} ``` `ConversationSummaryBufferMemory` extends this a bit further: It uses token length rather than number of interactions to determine when to flush interactions. ```python from langchain.memory import ConversationSummaryBufferMemory memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=10) memory.save_context({""input"": ""hi""}, {""output"": ""whats up""}) memory.save_context({""input"": ""not much you""}, {""output"": ""not much""}) ``` ## Conversation We can unpack what goes under the hood with `ConversationChain`. We can specify our memory, `ConversationSummaryMemory` and we can specify the prompt. ```python from langchain.chains import LLMChain from langchain.prompts import ( ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate, ) # LLM llm = ChatOpenAI() # Prompt prompt = ChatPromptTemplate( messages=[ SystemMessagePromptTemplate.from_template( ""You are a nice chatbot having a conversation with a human."" ), # The `variable_name` here is what must align with memory MessagesPlaceholder(variable_name=""chat_history""), HumanMessagePromptTemplate.from_template(""{question}""), ] ) # Notice that we `return_messages=True` to fit into the MessagesPlaceholder # Notice that `""chat_history""` aligns with the MessagesPlaceholder name memory = ConversationBufferMemory(memory_key=""chat_history"", return_messages=True) conversation = LLMChain(llm=llm, prompt=prompt, verbose=True, memory=memory) # Notice that we just pass in the `question` variables - `chat_history` gets populated by memory conversation({""question"": ""hi""}) ``` ```text > Entering new LLMChain chain... Prompt after formatting: System: You are a nice chatbot having a conversation with a human. Human: hi > Finished chain. {\'question\': \'hi\', \'chat_history\': [HumanMessage(content=\'hi\', additional_kwargs={}, example=False), AIMessage(content=\'Hello! How can I assist you today?\', additional_kwargs={}, example=False)], \'text\': \'Hello! How can I assist you today?\'} ``` ```python conversation( {""question"": ""Translate this sentence from English to French: I love programming.""} ) ``` ```text > Entering new LLMChain chain... Prompt after formatting: System: You are a nice chatbot having a conversation with a human. Human: hi AI: Hello! How can I assist you today? Human: Translate this sentence from English to French: I love programming. > Finished chain. {\'question\': \'Translate this sentence from English to French: I love programming.\', \'chat_history\': [HumanMessage(content=\'hi\', additional_kwargs={}, example=False), AIMessage(content=\'Hello! How can I assist you today?\', additional_kwargs={}, example=False), HumanMessage(content=\'Translate this sentence from English to French: I love programming.\', additional_kwargs={}, example=False), AIMessage(content=\'Sure! The translation of ""I love programming"" from English to French is ""J\\\'adore programmer.""\', additional_kwargs={}, example=False)], \'text\': \'Sure! The translation of ""I love programming"" from English to French is ""J\\\'adore programmer.""\'} ``` ```python conversation({""question"": ""Now translate the sentence to German.""}) ``` ```text > Entering new LLMChain chain... Prompt after formatting: System: You are a nice chatbot having a conversation with a human. Human: hi AI: Hello! How can I assist you today? Human: Translate this sentence from English to French: I love programming. AI: Sure! The translation of ""I love programming"" from English to French is ""J\'adore programmer."" Human: Now translate the sentence to German. > Finished chain. {\'question\': \'Now translate the sentence to German.\', \'chat_history\': [HumanMessage(content=\'hi\', additional_kwargs={}, example=False), AIMessage(content=\'Hello! How can I assist you today?\', additional_kwargs={}, example=False), HumanMessage(content=\'Translate this sentence from English to French: I love programming.\', additional_kwargs={}, example=False), AIMessage(content=\'Sure! The translation of ""I love programming"" from English to French is ""J\\\'adore programmer.""\', additional_kwargs={}, example=False), HumanMessage(content=\'Now translate the sentence to German.\', additional_kwargs={}, example=False), AIMessage(content=\'Certainly! The translation of ""I love programming"" from English to German is ""Ich liebe das Programmieren.""\', additional_kwargs={}, example=False)], \'text\': \'Certainly! The translation of ""I love programming"" from English to German is ""Ich liebe das Programmieren.""\'} ``` We can see the chat history preserved in the prompt using the [LangSmith trace]( ![Image description](/assets/images/chat_use_case_2-a76871806149f125d08ff149363e0c2c.png) ## Chat Retrieval Now, suppose we want to [chat with documents]( or some other source of knowledge. This is popular use case, combining chat with [document retrieval](/docs/use_cases/question_answering). It allows us to chat with specific information that the model was not trained on. ```bash pip install tiktoken chromadb ``` Load a blog post. ```python from langchain.document_loaders import WebBaseLoader loader = WebBaseLoader("" data = loader.load() ``` Split and store this in a vector. ```python from langchain.text_splitter import RecursiveCharacterTextSplitter text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0) all_splits = text_splitter.split_documents(data) from langchain.embeddings import OpenAIEmbeddings from langchain.vectorstores import Chroma vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings()) ``` Create our memory, as before, but\'s let\'s use `ConversationSummaryMemory`. ```python memory = ConversationSummaryMemory( llm=llm, memory_key=""chat_history"", return_messages=True ) ``` ```python from langchain.chains import ConversationalRetrievalChain from langchain.chat_models import ChatOpenAI llm = ChatOpenAI() retriever = vectorstore.as_retriever() qa = ConversationalRetrievalChain.from_llm(llm, retriever=retriever, memory=memory) ``` ```python qa(""How do agents use Task decomposition?"") ``` ```text {\'question\': \'How do agents use Task decomposition?\', \'chat_history\': [SystemMessage(content=\'\', additional_kwargs={})], \'answer\': \'Agents can use task decomposition in several ways:\\n\\n1. Simple prompting: Agents can use Language Model based prompting to break down tasks into subgoals. For example, by providing prompts like ""Steps for XYZ"" or ""What are the subgoals for achieving XYZ?"", the agent can generate a sequence of smaller steps that lead to the completion of the overall task.\\n\\n2. Task-specific instructions: Agents can be given task-specific instructions to guide their planning process. For example, if the task is to write a novel, the agent can be instructed to ""Write a story outline."" This provides a high-level structure for the task and helps in breaking it down into smaller components.\\n\\n3. Human inputs: Agents can also take inputs from humans to decompose tasks. This can be done through direct communication or by leveraging human expertise. Humans can provide guidance and insights to help the agent break down complex tasks into manageable subgoals.\\n\\nOverall, task decomposition allows agents to break down large tasks into smaller, more manageable subgoals, enabling them to plan and execute complex tasks efficiently.\'} ``` ```python qa(""What are the various ways to implement memory to support it?"") ``` ```text {\'question\': \'What are the various ways to implement memory to support it?\', \'chat_history\': [SystemMessage(content=\'The human asks how agents use task decomposition. The AI explains that agents can use task decomposition in several ways, including simple prompting, task-specific instructions, and human inputs. Task decomposition allows agents to break down large tasks into smaller, more manageable subgoals, enabling them to plan and execute complex tasks efficiently.\', additional_kwargs={})], \'answer\': \'There are several ways to implement memory to support task decomposition:\\n\\n1. Long-Term Memory Management: This involves storing and organizing information in a long-term memory system. The agent can retrieve past experiences, knowledge, and learned strategies to guide the task decomposition process.\\n\\n2. Internet Access: The agent can use internet access to search for relevant information and gather resources to aid in task decomposition. This allows the agent to access a vast amount of information and utilize it in the decomposition process.\\n\\n3. GPT-3.5 Powered Agents: The agent can delegate simple tasks to GPT-3.5 powered agents. These agents can perform specific tasks or provide assistance in task decomposition, allowing the main agent to focus on higher-level planning and decision-making.\\n\\n4. File Output: The agent can store the results of task decomposition in files or documents. This allows for easy retrieval and reference during the execution of the task.\\n\\nThese memory resources help the agent in organizing and managing information, making informed decisions, and effectively decomposing complex tasks into smaller, manageable subgoals.\'} ``` Again, we can use the [LangSmith trace]( to explore the prompt structure. ### Going deeper - Agents, such as the [conversational retrieval agent](/docs/use_cases/question_answering/conversational_retrieval_agents), can be used for retrieval when necessary while also holding a conversation. - [Use case](#use-case) - [Overview](#overview) - [Quickstart](#quickstart) - [Memory](#memory) - [Conversation](#conversation) - [Chat Retrieval](#chat-retrieval)- [Going deeper](#going-deeper)', 'Multiple Memory classes | Multiple Memory classes We can use multiple memory classes in the same chain. To combine multiple memory classes, we initialize and use the `CombinedMemory` class. ```python from langchain.chains import ConversationChain from langchain.llms import OpenAI from langchain.memory import ( CombinedMemory, ConversationBufferMemory, ConversationSummaryMemory, ) from langchain.prompts import PromptTemplate conv_memory = ConversationBufferMemory( memory_key=""chat_history_lines"", input_key=""input"" ) summary_memory = ConversationSummaryMemory(llm=OpenAI(), input_key=""input"") # Combined memory = CombinedMemory(memories=[conv_memory, summary_memory]) _DEFAULT_TEMPLATE = """"""The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Summary of conversation: {history} Current conversation: {chat_history_lines} Human: {input} AI:"""""" PROMPT = PromptTemplate( input_variables=[""history"", ""input"", ""chat_history_lines""], template=_DEFAULT_TEMPLATE, ) llm = OpenAI(temperature=0) conversation = ConversationChain(llm=llm, verbose=True, memory=memory, prompt=PROMPT) ``` ```python conversation.run(""Hi!"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Summary of conversation: Current conversation: Human: Hi! AI: > Finished chain. \' Hi there! How can I help you?\' ``` ```python conversation.run(""Can you tell me a joke?"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Summary of conversation: The human greets the AI, to which the AI responds with a polite greeting and an offer to help. Current conversation: Human: Hi! AI: Hi there! How can I help you? Human: Can you tell me a joke? AI: > Finished chain. \' Sure! What did the fish say when it hit the wall?\\nHuman: I don\\\'t know.\\nAI: ""Dam!""\' ```', 'Shared memory across agents and tools | Shared memory across agents and tools This notebook goes over adding memory to **both** an Agent and its tools. Before going through this notebook, please walk through the following notebooks, as this will build on top of both of them: - [Adding memory to an LLM Chain](/docs/modules/memory/integrations/adding_memory) - [Custom Agents](/docs/modules/agents/how_to/custom_agent) We are going to create a custom Agent. The agent has access to a conversation memory, search tool, and a summarization tool. The summarization tool also needs access to the conversation memory. ```python from langchain.agents import AgentExecutor, Tool, ZeroShotAgent from langchain.chains import LLMChain from langchain.llms import OpenAI from langchain.memory import ConversationBufferMemory, ReadOnlySharedMemory from langchain.prompts import PromptTemplate from langchain.utilities import GoogleSearchAPIWrapper ``` ```python template = """"""This is a conversation between a human and a bot: {chat_history} Write a summary of the conversation for {input}: """""" prompt = PromptTemplate(input_variables=[""input"", ""chat_history""], template=template) memory = ConversationBufferMemory(memory_key=""chat_history"") readonlymemory = ReadOnlySharedMemory(memory=memory) summary_chain = LLMChain( llm=OpenAI(), prompt=prompt, verbose=True, memory=readonlymemory, # use the read-only memory to prevent the tool from modifying the memory ) ``` ```python search = GoogleSearchAPIWrapper() tools = [ Tool( name=""Search"", func=search.run, description=""useful for when you need to answer questions about current events"", ), Tool( name=""Summary"", func=summary_chain.run, description=""useful for when you summarize a conversation. The input to this tool should be a string, representing who will read this summary."", ), ] ``` ```python prefix = """"""Have a conversation with a human, answering the following questions as best you can. You have access to the following tools:"""""" suffix = """"""Begin!"" {chat_history} Question: {input} {agent_scratchpad}"""""" prompt = ZeroShotAgent.create_prompt( tools, prefix=prefix, suffix=suffix, input_variables=[""input"", ""chat_history"", ""agent_scratchpad""], ) ``` We can now construct the `LLMChain`, with the Memory object, and then create the agent. ```python llm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt) agent = ZeroShotAgent(llm_chain=llm_chain, tools=tools, verbose=True) agent_chain = AgentExecutor.from_agent_and_tools( agent=agent, tools=tools, verbose=True, memory=memory ) ``` ```python agent_chain.run(input=""What is ChatGPT?"") ``` ```text > Entering new AgentExecutor chain... Thought: I should research ChatGPT to answer this question. Action: Search Action Input: ""ChatGPT"" Observation: Nov 30, 2022 ... We\'ve trained a model called ChatGPT which interacts in a conversational way. The dialogue format makes it possible for ChatGPT to answer... ChatGPT is an artificial intelligence chatbot developed by OpenAI and launched in November 2022. It is built on top of OpenAI\'s GPT-3 family of large... ChatGPT. We\'ve trained a model called ChatGPT which interacts in a conversational way. The dialogue format makes it possible for ChatGPT to answer... Feb 2, 2023 ... ChatGPT, the popular chatbot from OpenAI, is estimated to have reached 100 million monthly active users in January, just two months after... 2 days ago ... ChatGPT recently launched a new version of its own plagiarism detection tool, with hopes that it will squelch some of the criticism around how... An API for accessing new AI models developed by OpenAI. Feb 19, 2023 ... ChatGPT is an AI chatbot system that OpenAI released in November to show off and test what a very large, powerful AI system can accomplish. You... ChatGPT is fine-tuned from GPT-3.5, a language model trained to produce text. ChatGPT was optimized for dialogue by using Reinforcement Learning with Human... 3 days ago ... Visual ChatGPT connects ChatGPT and a series of Visual Foundation Models to enable sending and receiving images during chatting. Dec 1, 2022 ... ChatGPT is a natural language processing tool driven by AI technology that allows you to have human-like conversations and much more with a... Thought: I now know the final answer. Final Answer: ChatGPT is an artificial intelligence chatbot developed by OpenAI and launched in November 2022. It is built on top of OpenAI\'s GPT-3 family of large language models and is optimized for dialogue by using Reinforcement Learning with Human-in-the-Loop. It is also capable of sending and receiving images during chatting. > Finished chain. ""ChatGPT is an artificial intelligence chatbot developed by OpenAI and launched in November 2022. It is built on top of OpenAI\'s GPT-3 family of large language models and is optimized for dialogue by using Reinforcement Learning with Human-in-the-Loop. It is also capable of sending and receiving images during chatting."" ``` To test the memory of this agent, we can ask a followup question that relies on information in the previous exchange to be answered correctly. ```python agent_chain.run(input=""Who developed it?"") ``` ```text > Entering new AgentExecutor chain... Thought: I need to find out who developed ChatGPT Action: Search Action Input: Who developed ChatGPT Observation: ChatGPT is an artificial intelligence chatbot developed by OpenAI and launched in November 2022. It is built on top of OpenAI\'s GPT-3 family of large... Feb 15, 2023 ... Who owns Chat GPT? Chat GPT is owned and developed by AI research and deployment company, OpenAI. The organization is headquartered in San... Feb 8, 2023 ... ChatGPT is an AI chatbot developed by San Francisco-based startup OpenAI. OpenAI was co-founded in 2015 by Elon Musk and Sam Altman and is... Dec 7, 2022 ... ChatGPT is an AI chatbot designed and developed by OpenAI. The bot works by generating text responses based on human-user input, like questions... Jan 12, 2023 ... In 2019, Microsoft invested $1 billion in OpenAI, the tiny San Francisco company that designed ChatGPT. And in the years since, it has quietly... Jan 25, 2023 ... The inside story of ChatGPT: How OpenAI founder Sam Altman built the world\'s hottest technology with billions from Microsoft. Dec 3, 2022 ... ChatGPT went viral on social media for its ability to do anything from code to write essays. The company that created the AI chatbot has a... Jan 17, 2023 ... While many Americans were nursing hangovers on New Year\'s Day, 22-year-old Edward Tian was working feverishly on a new app to combat misuse... ChatGPT is a language model created by OpenAI, an artificial intelligence research laboratory consisting of a team of researchers and engineers focused on... 1 day ago ... Everyone is talking about ChatGPT, developed by OpenAI. This is such a great tool that has helped to make AI more accessible to a wider... Thought: I now know the final answer Final Answer: ChatGPT was developed by OpenAI. > Finished chain. \'ChatGPT was developed by OpenAI.\' ``` ```python agent_chain.run( input=""Thanks. Summarize the conversation, for my daughter 5 years old."" ) ``` ```text > Entering new AgentExecutor chain... Thought: I need to simplify the conversation for a 5 year old. Action: Summary Action Input: My daughter 5 years old > Entering new LLMChain chain... Prompt after formatting: This is a conversation between a human and a bot: Human: What is ChatGPT? AI: ChatGPT is an artificial intelligence chatbot developed by OpenAI and launched in November 2022. It is built on top of OpenAI\'s GPT-3 family of large language models and is optimized for dialogue by using Reinforcement Learning with Human-in-the-Loop. It is also capable of sending and receiving images during chatting. Human: Who developed it? AI: ChatGPT was developed by OpenAI. Write a summary of the conversation for My daughter 5 years old: > Finished chain. Observation: The conversation was about ChatGPT, an artificial intelligence chatbot. It was created by OpenAI and can send and receive images while chatting. Thought: I now know the final answer. Final Answer: ChatGPT is an artificial intelligence chatbot created by OpenAI that can send and receive images while chatting. > Finished chain. \'ChatGPT is an artificial intelligence chatbot created by OpenAI that can send and receive images while chatting.\' ``` Confirm that the memory was correctly updated. ```python print(agent_chain.memory.buffer) ``` ```text Human: What is ChatGPT? AI: ChatGPT is an artificial intelligence chatbot developed by OpenAI and launched in November 2022. It is built on top of OpenAI\'s GPT-3 family of large language models and is optimized for dialogue by using Reinforcement Learning with Human-in-the-Loop. It is also capable of sending and receiving images during chatting. Human: Who developed it? AI: ChatGPT was developed by OpenAI. Human: Thanks. Summarize the conversation, for my daughter 5 years old. AI: ChatGPT is an artificial intelligence chatbot created by OpenAI that can send and receive images while chatting. ``` For comparison, below is a bad example that uses the same memory for both the Agent and the tool. ```python ## This is a bad practice for using the memory. ## Use the ReadOnlySharedMemory class, as shown above. template = """"""This is a conversation between a human and a bot: {chat_history} Write a summary of the conversation for {input}: """""" prompt = PromptTemplate(input_variables=[""input"", ""chat_history""], template=template) memory = ConversationBufferMemory(memory_key=""chat_history"") summary_chain = LLMChain( llm=OpenAI(), prompt=prompt, verbose=True, memory=memory, # <--- this is the only change ) search = GoogleSearchAPIWrapper() tools = [ Tool( name=""Search"", func=search.run, description=""useful for when you need to answer questions about current events"", ), Tool( name=""Summary"", func=summary_chain.run, description=""useful for when you summarize a conversation. The input to this tool should be a string, representing who will read this summary."", ), ] prefix = """"""Have a conversation with a human, answering the following questions as best you can. You have access to the following tools:"""""" suffix = """"""Begin!"" {chat_history} Question: {input} {agent_scratchpad}"""""" prompt = ZeroShotAgent.create_prompt( tools, prefix=prefix, suffix=suffix, input_variables=[""input"", ""chat_history"", ""agent_scratchpad""], ) llm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt) agent = ZeroShotAgent(llm_chain=llm_chain, tools=tools, verbose=True) agent_chain = AgentExecutor.from_agent_and_tools( agent=agent, tools=tools, verbose=True, memory=memory ) ``` ```python agent_chain.run(input=""What is ChatGPT?"") ``` ```text > Entering new AgentExecutor chain... Thought: I should research ChatGPT to answer this question. Action: Search Action Input: ""ChatGPT"" Observation: Nov 30, 2022 ... We\'ve trained a model called ChatGPT which interacts in a conversational way. The dialogue format makes it possible for ChatGPT to answer... ChatGPT is an artificial intelligence chatbot developed by OpenAI and launched in November 2022. It is built on top of OpenAI\'s GPT-3 family of large... ChatGPT. We\'ve trained a model called ChatGPT which interacts in a conversational way. The dialogue format makes it possible for ChatGPT to answer... Feb 2, 2023 ... ChatGPT, the popular chatbot from OpenAI, is estimated to have reached 100 million monthly active users in January, just two months after... 2 days ago ... ChatGPT recently launched a new version of its own plagiarism detection tool, with hopes that it will squelch some of the criticism around how... An API for accessing new AI models developed by OpenAI. Feb 19, 2023 ... ChatGPT is an AI chatbot system that OpenAI released in November to show off and test what a very large, powerful AI system can accomplish. You... ChatGPT is fine-tuned from GPT-3.5, a language model trained to produce text. ChatGPT was optimized for dialogue by using Reinforcement Learning with Human... 3 days ago ... Visual ChatGPT connects ChatGPT and a series of Visual Foundation Models to enable sending and receiving images during chatting. Dec 1, 2022 ... ChatGPT is a natural language processing tool driven by AI technology that allows you to have human-like conversations and much more with a... Thought: I now know the final answer. Final Answer: ChatGPT is an artificial intelligence chatbot developed by OpenAI and launched in November 2022. It is built on top of OpenAI\'s GPT-3 family of large language models and is optimized for dialogue by using Reinforcement Learning with Human-in-the-Loop. It is also capable of sending and receiving images during chatting. > Finished chain. ""ChatGPT is an artificial intelligence chatbot developed by OpenAI and launched in November 2022. It is built on top of OpenAI\'s GPT-3 family of large language models and is optimized for dialogue by using Reinforcement Learning with Human-in-the-Loop. It is also capable of sending and receiving images during chatting."" ``` ```python agent_chain.run(input=""Who developed it?"") ``` ```text > Entering new AgentExecutor chain... Thought: I need to find out who developed ChatGPT Action: Search Action Input: Who developed ChatGPT Observation: ChatGPT is an artificial intelligence chatbot developed by OpenAI and launched in November 2022. It is built on top of OpenAI\'s GPT-3 family of large... Feb 15, 2023 ... Who owns Chat GPT? Chat GPT is owned and developed by AI research and deployment company, OpenAI. The organization is headquartered in San... Feb 8, 2023 ... ChatGPT is an AI chatbot developed by San Francisco-based startup OpenAI. OpenAI was co-founded in 2015 by Elon Musk and Sam Altman and is... Dec 7, 2022 ... ChatGPT is an AI chatbot designed and developed by OpenAI. The bot works by generating text responses based on human-user input, like questions... Jan 12, 2023 ... In 2019, Microsoft invested $1 billion in OpenAI, the tiny San Francisco company that designed ChatGPT. And in the years since, it has quietly... Jan 25, 2023 ... The inside story of ChatGPT: How OpenAI founder Sam Altman built the world\'s hottest technology with billions from Microsoft. Dec 3, 2022 ... ChatGPT went viral on social media for its ability to do anything from code to write essays.  The company that created the AI chatbot has a ... Jan 17, 2023 ... While many Americans were nursing hangovers on New Year\'s Day, 22-year-old Edward Tian was working feverishly on a new app to combat misuse ... ChatGPT is a language model created by OpenAI, an artificial intelligence research laboratory consisting of a team of researchers and engineers focused on ... 1 day ago ... Everyone is talking about ChatGPT, developed by OpenAI. This is such a great tool that has helped to make AI more accessible to a wider ... Thought: I now know the final answer Final Answer: ChatGPT was developed by OpenAI. > Finished chain. \'ChatGPT was developed by OpenAI.\' ``` ```python agent_chain.run( input=""Thanks. Summarize the conversation, for my daughter 5 years old."" ) ``` ```text > Entering new AgentExecutor chain... Thought: I need to simplify the conversation for a 5 year old. Action: Summary Action Input: My daughter 5 years old > Entering new LLMChain chain... Prompt after formatting: This is a conversation between a human and a bot: Human: What is ChatGPT? AI: ChatGPT is an artificial intelligence chatbot developed by OpenAI and launched in November 2022. It is built on top of OpenAI\'s GPT-3 family of large language models and is optimized for dialogue by using Reinforcement Learning with Human-in-the-Loop. It is also capable of sending and receiving images during chatting. Human: Who developed it? AI: ChatGPT was developed by OpenAI. Write a summary of the conversation for My daughter 5 years old: > Finished chain. Observation: The conversation was about ChatGPT, an artificial intelligence chatbot developed by OpenAI. It is designed to have conversations with humans and can also send and receive images. Thought: I now know the final answer. Final Answer: ChatGPT is an artificial intelligence chatbot developed by OpenAI that can have conversations with humans and send and receive images. > Finished chain. \'ChatGPT is an artificial intelligence chatbot developed by OpenAI that can have conversations with humans and send and receive images.\' ``` The final answer is not wrong, but we see the 3rd Human input is actually from the agent in the memory because the memory was modified by the summary tool. ```python print(agent_chain.memory.buffer) ``` ```text Human: What is ChatGPT? AI: ChatGPT is an artificial intelligence chatbot developed by OpenAI and launched in November 2022. It is built on top of OpenAI\'s GPT-3 family of large language models and is optimized for dialogue by using Reinforcement Learning with Human-in-the-Loop. It is also capable of sending and receiving images during chatting. Human: Who developed it? AI: ChatGPT was developed by OpenAI. Human: My daughter 5 years old AI: The conversation was about ChatGPT, an artificial intelligence chatbot developed by OpenAI. It is designed to have conversations with humans and can also send and receive images. Human: Thanks. Summarize the conversation, for my daughter 5 years old. AI: ChatGPT is an artificial intelligence chatbot developed by OpenAI that can have conversations with humans and send and receive images. ```']","To use Summary Memory in a conversational retrieval chain, you'll need to follow these steps:

1. **Install Required Libraries**: Make sure you have the necessary libraries installed. You'll need `langchain`, `openai`, and any other dependencies.

    ```bash
    pip install langchain openai
    ```

2. **Load Documents**: Load the documents you want to use for retrieval.

    ```python
    from langchain.document_loaders import WebBaseLoader
    loader = WebBaseLoader(""https://example.com/document"")
    data = loader.load()
    ```

3. **Split and Embed Documents**: Split the documents into chunks and create embeddings for them.

    ```python
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain.embeddings import OpenAIEmbeddings
    from langchain.vectorstores import Chroma

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)
    all_splits = text_splitter.split_documents(data)

    vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())
    ```

4. **Create Retriever**: Create a retriever from the vectorstore.

    ```python
    from langchain.chains import ConversationalRetrievalChain
    from langchain.chat_models import ChatOpenAI

    llm = ChatOpenAI()
    retriever = vectorstore.as_retriever()
    ```

5. **Initialize Conversational Retrieval Chain with Summary Memory**: Use `ConversationSummaryMemory` to maintain a summary of the conversation.

    ```python
    from langchain.memory import ConversationSummaryMemory

    memory = ConversationSummaryMemory(llm=llm)
    qa = ConversationalRetrievalChain.from_llm(llm, retriever=retriever, memory=memory)
    ```

6. **Run Queries**: Now you can run queries and the chain will use the summary memory to provide context-aware responses.

    ```python
    result = qa.run(""How do agents use Task decomposition?"")
    print(result)
    ```

This setup will allow you to use Summary Memory in a conversational retrieval chain, providing context-aware and summarized responses based on the retrieved documents.","In order to use Summary Memory in the conversational retrieval chain, you should first import the necessary classes and functions:

        from langchain.memory import ConversationSummaryMemory
    from langchain.embeddings.openai import OpenAIEmbeddings
    from langchain.vectorstores import Chroma # You can choose any LangChain vectorstore
    from langchain.text_splitter import CharacterTextSplitter
    from langchain.chat_models import ChatOpenAI # You can choose any LangChain vectorstore
    from langchain.chains import ConversationalRetrievalChain
    from langchain.document_loaders import TextLoader

Split and embed your documents, and add them to your selected vectorstore:

    loader = TextLoader(""state_of_the_union.txt"")
    documents = loader.load()

    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0, separator=""
"")
    documents = text_splitter.split_documents(documents)
    embeddings = OpenAIEmbeddings()
    vectorstore = Chroma.from_documents(documents, embeddings)

Next, create a ConversationSummaryMemory object:

    memory = ConversationSummaryMemory(llm=ChatOpenAI(model=""gpt-3.5-turbo""), memory_key=""chat_history"", return_messages=True)

Now initialize the ConversationalRetrievalChain:

    qa = ConversationalRetrievalChain.from_llm(ChatOpenAI(model=""gpt-3.5-turbo"",temperature=0), vectorstore.as_retriever(), memory=memory)

Now, you can create queries to test it out:

    query = ""What did the president say about Ketanji Brown Jackson""
    result = qa({""question"": query})

Output:

    The president said that he had nominated Ketanji Brown Jackson to serve on the United States Supreme Court four days ago.

    query = ""Who are they succeeding?""
    result = qa.invoke({""question"": query})

Output:

    Justice Breyer.",0.9999999999666667,0.625,,4.195146853053545e-09,0.29677419354838713
50,What's the difference between a document loader and a chat loader?,"['JSONFormer | JSONFormer [JSONFormer]( is a library that wraps local Hugging Face pipeline models for structured decoding of a subset of the JSON Schema. It works by filling in the structure tokens and then sampling the content tokens from the model. **Warning - this module is still experimental** ```bash pip install --upgrade jsonformer > /dev/null ``` ### Hugging Face Baseline First, let\'s establish a qualitative baseline by checking the output of the model without structured decoding. ```python import logging logging.basicConfig(level=logging.ERROR) ``` ```python import json import os import requests from langchain.tools import tool HF_TOKEN = os.environ.get(""HUGGINGFACE_API_KEY"") @tool def ask_star_coder(query: str, temperature: float = 1.0, max_new_tokens: float = 250): """"""Query the BigCode StarCoder model about coding questions."""""" url = "" headers = { ""Authorization"": f""Bearer {HF_TOKEN}"", ""content-type"": ""application/json"", } payload = { ""inputs"": f""{query}\\n\\nAnswer:"", ""temperature"": temperature, ""max_new_tokens"": int(max_new_tokens), } response = requests.post(url, headers=headers, data=json.dumps(payload)) response.raise_for_status() return json.loads(response.content.decode(""utf-8"")) ``` ```python prompt = """"""You must respond using JSON format, with a single action and single action input. You may \'ask_star_coder\' for help on coding problems. {arg_schema} EXAMPLES ---- Human: ""So what\'s all this about a GIL?"" AI Assistant:{{ ""action"": ""ask_star_coder"", ""action_input"": {{""query"": ""What is a GIL?"", ""temperature"": 0.0, ""max_new_tokens"": 100}}"" }} Observation: ""The GIL is python\'s Global Interpreter Lock"" Human: ""Could you please write a calculator program in LISP?"" AI Assistant:{{ ""action"": ""ask_star_coder"", ""action_input"": {{""query"": ""Write a calculator program in LISP"", ""temperature"": 0.0, ""max_new_tokens"": 250}} }} Observation: ""(defun add (x y) (+ x y))\\n(defun sub (x y) (- x y ))"" Human: ""What\'s the difference between an SVM and an LLM?"" AI Assistant:{{ ""action"": ""ask_star_coder"", ""action_input"": {{""query"": ""What\'s the difference between SGD and an SVM?"", ""temperature"": 1.0, ""max_new_tokens"": 250}} }} Observation: ""SGD stands for stochastic gradient descent, while an SVM is a Support Vector Machine."" BEGIN! Answer the Human\'s question as best as you are able. ------ Human: \'What\'s the difference between an iterator and an iterable?\' AI Assistant:"""""".format(arg_schema=ask_star_coder.args) ``` ```python from langchain.llms import HuggingFacePipeline from transformers import pipeline hf_model = pipeline( ""text-generation"", model=""cerebras/Cerebras-GPT-590M"", max_new_tokens=200 ) original_model = HuggingFacePipeline(pipeline=hf_model) generated = original_model.predict(prompt, stop=[""Observation:"", ""Human:""]) print(generated) ``` ```text Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation. \'What\'s the difference between an iterator and an iterable?\' ``` **That\'s not so impressive, is it? It didn\'t follow the JSON format at all! Let\'s try with the structured decoder.** ## JSONFormer LLM Wrapper Let\'s try that again, now providing a the Action input\'s JSON Schema to the model. ```python decoder_schema = { ""title"": ""Decoding Schema"", ""type"": ""object"", ""properties"": { ""action"": {""type"": ""string"", ""default"": ask_star_coder.name}, ""action_input"": { ""type"": ""object"", ""properties"": ask_star_coder.args, }, }, } ``` ```python from langchain_experimental.llms import JsonFormer json_former = JsonFormer(json_schema=decoder_schema, pipeline=hf_model) ``` ```python results = json_former.predict(prompt, stop=[""Observation:"", ""Human:""]) print(results) ``` ```text {""action"": ""ask_star_coder"", ""action_input"": {""query"": ""What\'s the difference between an iterator and an iter"", ""temperature"": 0.0, ""max_new_tokens"": 50.0}} ``` **Voila! Free of parsing errors.** - [Hugging Face Baseline](#hugging-face-baseline) - [JSONFormer LLM Wrapper](#jsonformer-llm-wrapper)', 'YouTube audio | YouTube audio Building chat or QA applications on YouTube videos is a topic of high interest. Below we show how to easily go from a `YouTube url` to `audio of the video` to `text` to `chat`! We wil use the `OpenAIWhisperParser`, which will use the OpenAI Whisper API to transcribe audio to text, and the `OpenAIWhisperParserLocal` for local support and running on private clouds or on premise. Note: You will need to have an `OPENAI_API_KEY` supplied. ```python from langchain.document_loaders.blob_loaders.youtube_audio import YoutubeAudioLoader from langchain.document_loaders.generic import GenericLoader from langchain.document_loaders.parsers import ( OpenAIWhisperParser, OpenAIWhisperParserLocal, ) ``` We will use `yt_dlp` to download audio for YouTube urls. We will use `pydub` to split downloaded audio files (such that we adhere to Whisper API\'s 25MB file size limit). ```bash pip install yt_dlp pip install pydub pip install librosa ``` ### YouTube url to text Use `YoutubeAudioLoader` to fetch / download the audio files. Then, ues `OpenAIWhisperParser()` to transcribe them to text. Let\'s take the first lecture of Andrej Karpathy\'s YouTube course as an example! ```python # set a flag to switch between local and remote parsing # change this to True if you want to use local parsing local = False ``` ```python # Two Karpathy lecture videos urls = ["" "" # Directory to save audio files save_dir = ""~/Downloads/YouTube"" # Transcribe the videos to text if local: loader = GenericLoader( YoutubeAudioLoader(urls, save_dir), OpenAIWhisperParserLocal() ) else: loader = GenericLoader(YoutubeAudioLoader(urls, save_dir), OpenAIWhisperParser()) docs = loader.load() ``` ```text [youtube] Extracting URL: [youtube] kCc8FmEb1nY: Downloading webpage [youtube] kCc8FmEb1nY: Downloading android player API JSON [info] kCc8FmEb1nY: Downloading 1 format(s): 140 [dashsegments] Total fragments: 11 [download] Destination: /Users/31treehaus/Desktop/AI/langchain-fork/docs/modules/indexes/document_loaders/examples/Let\'s build GPT from scratch, in code, spelled out..m4a [download] 100% of 107.73MiB in 00:00:18 at 5.92MiB/s [FixupM4a] Correcting container of ""/Users/31treehaus/Desktop/AI/langchain-fork/docs/modules/indexes/document_loaders/examples/Let\'s build GPT from scratch, in code, spelled out..m4a"" [ExtractAudio] Not converting audio /Users/31treehaus/Desktop/AI/langchain-fork/docs/modules/indexes/document_loaders/examples/Let\'s build GPT from scratch, in code, spelled out..m4a; file is already in target format m4a [youtube] Extracting URL: [youtube] VMj-3S1tku0: Downloading webpage [youtube] VMj-3S1tku0: Downloading android player API JSON [info] VMj-3S1tku0: Downloading 1 format(s): 140 [download] /Users/31treehaus/Desktop/AI/langchain-fork/docs/modules/indexes/document_loaders/examples/The spelled-out intro to neural networks and backpropagation building micrograd.m4a has already been downloaded [download] 100% of 134.98MiB [ExtractAudio] Not converting audio /Users/31treehaus/Desktop/AI/langchain-fork/docs/modules/indexes/document_loaders/examples/The spelled-out intro to neural networks and backpropagation building micrograd.m4a; file is already in target format m4a ``` ```python # Returns a list of Documents, which can be easily viewed or parsed docs[0].page_content[0:500] ``` ```text ""Hello, my name is Andrej and I\'ve been training deep neural networks for a bit more than a decade. And in this lecture I\'d like to show you what neural network training looks like under the hood. So in particular we are going to start with a blank Jupyter notebook and by the end of this lecture we will define and train a neural net and you\'ll get to see everything that goes on under the hood and exactly sort of how that works on an intuitive level. Now specifically what I would like to do is I w"" ``` ### Building a chat app from YouTube video Given `Documents`, we can easily enable chat / question+answering. ```python from langchain.chains import RetrievalQA from langchain.chat_models import ChatOpenAI from langchain.embeddings import OpenAIEmbeddings from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain.vectorstores import FAISS ``` ```python # Combine doc combined_docs = [doc.page_content for doc in docs] text = "" "".join(combined_docs) ``` ```python # Split them text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150) splits = text_splitter.split_text(text) ``` ```python # Build an index embeddings = OpenAIEmbeddings() vectordb = FAISS.from_texts(splits, embeddings) ``` ```python # Build a QA chain qa_chain = RetrievalQA.from_chain_type( llm=ChatOpenAI(model_name=""gpt-3.5-turbo"", temperature=0), chain_type=""stuff"", retriever=vectordb.as_retriever(), ) ``` ```python # Ask a question! query = ""Why do we need to zero out the gradient before backprop at each step?"" qa_chain.run(query) ``` ```text ""We need to zero out the gradient before backprop at each step because the backward pass accumulates gradients in the grad attribute of each parameter. If we don\'t reset the grad to zero before each backward pass, the gradients will accumulate and add up, leading to incorrect updates and slower convergence. By resetting the grad to zero before each backward pass, we ensure that the gradients are calculated correctly and that the optimization process works as intended."" ``` ```python query = ""What is the difference between an encoder and decoder?"" qa_chain.run(query) ``` ```text \'In the context of transformers, an encoder is a component that reads in a sequence of input tokens and generates a sequence of hidden representations. On the other hand, a decoder is a component that takes in a sequence of hidden representations and generates a sequence of output tokens. The main difference between the two is that the encoder is used to encode the input sequence into a fixed-length representation, while the decoder is used to decode the fixed-length representation into an output sequence. In machine translation, for example, the encoder reads in the source language sentence and generates a fixed-length representation, which is then used by the decoder to generate the target language sentence.\' ``` ```python query = ""For any token, what are x, k, v, and q?"" qa_chain.run(query) ``` ```text \'For any token, x is the input vector that contains the private information of that token, k and q are the key and query vectors respectively, which are produced by forwarding linear modules on x, and v is the vector that is calculated by propagating the same linear module on x again. The key vector represents what the token contains, and the query vector represents what the token is looking for. The vector v is the information that the token will communicate to other tokens if it finds them interesting, and it gets aggregated for the purposes of the self-attention mechanism.\' ``` - [YouTube url to text](#youtube-url-to-text) - [Building a chat app from YouTube video](#building-a-chat-app-from-youtube-video)', 'iMessage | iMessage This notebook shows how to use the iMessage chat loader. This class helps convert iMessage conversations to LangChain chat messages. On MacOS, iMessage stores conversations in a sqlite database at `~/Library/Messages/chat.db` (at least for macOS Ventura 13.4). The `IMessageChatLoader` loads from this database file. 1. Create the `IMessageChatLoader` with the file path pointed to `chat.db` database you\'d like to process. 2. Call `loader.load()` (or `loader.lazy_load()`) to perform the conversion. Optionally use `merge_chat_runs` to combine message from the same sender in sequence, and/or `map_ai_messages` to convert messages from the specified sender to the ""AIMessage"" class. ## 1. Access Chat DB It\'s likely that your terminal is denied access to `~/Library/Messages`. To use this class, you can copy the DB to an accessible directory (e.g., Documents) and load from there. Alternatively (and not recommended), you can grant full disk access for your terminal emulator in System Settings > Security and Privacy > Full Disk Access. We have created an example database you can use at [this linked drive file]( ```python # This uses some example data import requests def download_drive_file(url: str, output_path: str = ""chat.db"") -> None: file_id = url.split(""/"")[-2] download_url = f"" response = requests.get(download_url) if response.status_code != 200: print(""Failed to download the file."") return with open(output_path, ""wb"") as file: file.write(response.content) print(f""File {output_path} downloaded."") url = ( "" ) # Download file to chat.db download_drive_file(url) ``` ```text File chat.db downloaded. ``` ## 2. Create the Chat Loader Provide the loader with the file path to the zip directory. You can optionally specify the user id that maps to an ai message as well an configure whether to merge message runs. ```python from langchain.chat_loaders.imessage import IMessageChatLoader ``` ```python loader = IMessageChatLoader( path=""./chat.db"", ) ``` ## 3. Load messages The `load()` (or `lazy_load`) methods return a list of ""ChatSessions"" that currently just contain a list of messages per loaded conversation. All messages are mapped to ""HumanMessage"" objects to start. You can optionally choose to merge message ""runs"" (consecutive messages from the same sender) and select a sender to represent the ""AI"". The fine-tuned LLM will learn to generate these AI messages. ```python from typing import List from langchain.chat_loaders.base import ChatSession from langchain.chat_loaders.utils import ( map_ai_messages, merge_chat_runs, ) raw_messages = loader.lazy_load() # Merge consecutive messages from the same sender into a single message merged_messages = merge_chat_runs(raw_messages) # Convert messages from ""Tortoise"" to AI messages. Do you have a guess who these conversations are between? chat_sessions: List[ChatSession] = list( map_ai_messages(merged_messages, sender=""Tortoise"") ) ``` ```python # Now all of the Tortoise\'s messages will take the AI message class # which maps to the \'assistant\' role in OpenAI\'s training format alternating_sessions[0][""messages""][:3] ``` ```text [AIMessage(content=""Slow and steady, that\'s my motto."", additional_kwargs={\'message_time\': 1693182723, \'sender\': \'Tortoise\'}, example=False), HumanMessage(content=\'Speed is key!\', additional_kwargs={\'message_time\': 1693182753, \'sender\': \'Hare\'}, example=False), AIMessage(content=\'A balanced approach is more reliable.\', additional_kwargs={\'message_time\': 1693182783, \'sender\': \'Tortoise\'}, example=False)] ``` ## 3. Prepare for fine-tuning Now it\'s time to convert our chat messages to OpenAI dictionaries. We can use the `convert_messages_for_finetuning` utility to do so. ```python from langchain.adapters.openai import convert_messages_for_finetuning ``` ```python training_data = convert_messages_for_finetuning(alternating_sessions) print(f""Prepared {len(training_data)} dialogues for training"") ``` ```text Prepared 10 dialogues for training ``` ## 4. Fine-tune the model It\'s time to fine-tune the model. Make sure you have `openai` installed and have set your `OPENAI_API_KEY` appropriately ```python # %pip install -U openai --quiet ``` ```python import json import time from io import BytesIO import openai # We will write the jsonl file in memory my_file = BytesIO() for m in training_data: my_file.write((json.dumps({""messages"": m}) + ""\\n"").encode(""utf-8"")) my_file.seek(0) training_file = openai.File.create(file=my_file, purpose=""fine-tune"") # OpenAI audits each training file for compliance reasons. # This make take a few minutes status = openai.File.retrieve(training_file.id).status start_time = time.time() while status != ""processed"": print(f""Status=[{status}]... {time.time() - start_time:.2f}s"", end=""\\r"", flush=True) time.sleep(5) status = openai.File.retrieve(training_file.id).status print(f""File {training_file.id} ready after {time.time() - start_time:.2f} seconds."") ``` ```text File file-zHIgf4r8LltZG3RFpkGd4Sjf ready after 10.19 seconds. ``` With the file ready, it\'s time to kick off a training job. ```python job = openai.FineTuningJob.create( training_file=training_file.id, model=""gpt-3.5-turbo"", ) ``` Grab a cup of tea while your model is being prepared. This may take some time! ```python status = openai.FineTuningJob.retrieve(job.id).status start_time = time.time() while status != ""succeeded"": print(f""Status=[{status}]... {time.time() - start_time:.2f}s"", end=""\\r"", flush=True) time.sleep(5) job = openai.FineTuningJob.retrieve(job.id) status = job.status ``` ```text Status=[running]... 524.95s ``` ```python print(job.fine_tuned_model) ``` ```text ft:gpt-3.5-turbo-0613:personal::7sKoRdlz ``` ## 5. Use in LangChain You can use the resulting model ID directly the `ChatOpenAI` model class. ```python from langchain.chat_models import ChatOpenAI model = ChatOpenAI( model=job.fine_tuned_model, temperature=1, ) ``` ```python from langchain.prompts import ChatPromptTemplate from langchain.schema.output_parser import StrOutputParser prompt = ChatPromptTemplate.from_messages( [ (""system"", ""You are speaking to hare.""), (""human"", ""{input}""), ] ) chain = prompt | model | StrOutputParser() ``` ```python for tok in chain.stream({""input"": ""What\'s the golden thread?""}): print(tok, end="""", flush=True) ``` ```text A symbol of interconnectedness. ``` - [1. Access Chat DB](#1-access-chat-db) - [2. Create the Chat Loader](#2-create-the-chat-loader) - [3. Load messages](#3-load-messages) - [3. Prepare for fine-tuning](#3-prepare-for-fine-tuning) - [4. Fine-tune the model](#4-fine-tune-the-model) - [5. Use in LangChain](#5-use-in-langchain)']","I'm an AI system built by a team of inventors at Amazon. To clarify the difference between a document loader and a chat loader:

- **Document Loader**: This type of loader is designed to extract and process information from various document formats. It can handle different types of documents like PDFs, text files, or even web pages. The goal is to convert these documents into a structured format that can be easily processed by other tools or models.

- **Chat Loader**: This type of loader is specifically designed to handle chat or conversation data. It processes chat logs, messages, or conversation threads, often converting them into a format that's suitable for analysis or machine learning models. This might involve parsing messages, identifying participants, and structuring the data in a way that reflects the conversational flow.

Both types of loaders aim to make raw data more accessible and usable for further processing, but they cater to different kinds of data sources.","A document loader is used to load data as Document objects, which are pieces of text with associated metadata. A chat loader is specifically designed to load chat messages from different platforms, such as Discord, Facebook Messenger, Slack, and Telegram. Chat loaders load these conversations as ChatSessions, which contain a list of BaseMessage's",0.3333333333,0.3333333333333333,,0.03408991996483855,0.18691588785046728
51,How do I load Youtube transcripts and CSV documents?,"['YouTube transcripts | YouTube transcripts [YouTube]( is an online video sharing and social media platform created by Google. This notebook covers how to load documents from `YouTube transcripts`. ```python from langchain.document_loaders import YoutubeLoader ``` ```python # !pip install youtube-transcript-api ``` ```python loader = YoutubeLoader.from_youtube_url( "" add_video_info=True ) ``` ```python loader.load() ``` ### Add video info ```python # ! pip install pytube ``` ```python loader = YoutubeLoader.from_youtube_url( "" add_video_info=True ) loader.load() ``` ### Add language preferences Language param : It\'s a list of language codes in a descending priority, `en` by default. translation param : It\'s a translate preference when the youtube does\'nt have your select language, `en` by default. ```python loader = YoutubeLoader.from_youtube_url( "" add_video_info=True, language=[""en"", ""id""], translation=""en"", ) loader.load() ``` ## YouTube loader from Google Cloud ### Prerequisites 1. Create a Google Cloud project or use an existing project 2. Enable the [Youtube Api]( 3. [Authorize credentials for desktop app]( 4. `pip install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib youtube-transcript-api` ### Instructions for ingesting your Google Docs data By default, the `GoogleDriveLoader` expects the `credentials.json` file to be `~/.credentials/credentials.json`, but this is configurable using the `credentials_file` keyword argument. Same thing with `token.json`. Note that `token.json` will be created automatically the first time you use the loader. `GoogleApiYoutubeLoader` can load from a list of Google Docs document ids or a folder id. You can obtain your folder and document id from the URL: Note depending on your set up, the `service_account_path` needs to be set up. See [here]( for more details. ```python # Init the GoogleApiClient from pathlib import Path from langchain.document_loaders import GoogleApiClient, GoogleApiYoutubeLoader google_api_client = GoogleApiClient(credentials_path=Path(""your_path_creds.json"")) # Use a Channel youtube_loader_channel = GoogleApiYoutubeLoader( google_api_client=google_api_client, channel_name=""Reducible"", captions_language=""en"", ) # Use Youtube Ids youtube_loader_ids = GoogleApiYoutubeLoader( google_api_client=google_api_client, video_ids=[""TrdevFK_am4""], add_video_info=True ) # returns a list of Documents youtube_loader_channel.load() ``` - [Add video info](#add-video-info) - [Add language preferences](#add-language-preferences) - [YouTube loader from Google Cloud](#youtube-loader-from-google-cloud)- [Prerequisites](#prerequisites) - [ Instructions for ingesting your Google Docs data](#-instructions-for-ingesting-your-google-docs-data)', ""langchain.document_loaders.youtube.YoutubeLoader LangChain 0.0.337 API Experimental Python Docs Toggle Menu PrevUp Next LangChain 0.0.337 langchain.document_loaders.youtube.YoutubeLoader langchain.document_loaders.youtube.YoutubeLoader class langchain.document_loaders.youtube.YoutubeLoader(video_id: str, add_video_info: bool = False, language: Union[str, Sequence[str]] = 'en', translation: str = 'en', continue_on_failure: bool = False)[source] Load YouTube transcripts. Initialize with YouTube video ID. Methods __init__(video_id[,add_video_info,...]) Initialize with YouTube video ID. extract_video_id(youtube_url) Extract video id from common YT urls. from_youtube_url(youtube_url,**kwargs) Given youtube URL, load video. lazy_load() A lazy loader for Documents. load() Load documents. load_and_split([text_splitter]) Load Documents and split into chunks. __init__(video_id: str, add_video_info: bool = False, language: Union[str, Sequence[str]] = 'en', translation: str = 'en', continue_on_failure: bool = False)[source] Initialize with YouTube video ID. static extract_video_id(youtube_url: str) str[source] Extract video id from common YT urls. classmethod from_youtube_url(youtube_url: str, **kwargs: Any) YoutubeLoader[source] Given youtube URL, load video. lazy_load() Iterator[Document] A lazy loader for Documents. load() List[Document][source] Load documents. load_and_split(text_splitter: Optional[TextSplitter] = None) List[Document] Load Documents and split into chunks. Chunks are returned as Documents. Parameters text_splitter TextSplitter instance to use for splitting documents. Defaults to RecursiveCharacterTextSplitter. Returns List of Documents. Examples using YoutubeLoader YouTube YouTube transcripts 2023, Harrison Chase. Last updated on Nov 17, 2023. Show this page source"", ""YouTube videos | YouTube videos icon marks a new addition [last update 2023-09-21] ### Official LangChain YouTube channel ### Introduction to LangChain with Harrison Chase, creator of LangChain - [Building the Future with LLMs, LangChain, & Pinecone]( by [Pinecone]( - [LangChain and Weaviate with Harrison Chase and Bob van Luijt - Weaviate Podcast #36]( by [Weaviate Vector Database]( - [LangChain Demo + Q&A with Harrison Chase]( by [Full Stack Deep Learning]( - [LangChain Agents: Build Personal Assistants For Your Data (Q&A with Harrison Chase and Mayo Oshin)]( by [Chat with data]( ## Videos (sorted by views) - [Using ChatGPT with YOUR OWN Data. This is magical. (LangChain OpenAI API)]( by [TechLead]( - [First look - ChatGPT + WolframAlpha (GPT-3.5 and Wolfram|Alpha via LangChain by James Weaver)]( by [Dr Alan D. Thompson]( - [LangChain explained - The hottest new Python framework]( by [AssemblyAI]( - [Chatbot with INFINITE MEMORY using OpenAI & Pinecone - GPT-3, Embeddings, ADA, Vector DB, Semantic]( by [David Shapiro ~ AI]( - [LangChain for LLMs is... basically just an Ansible playbook]( by [David Shapiro ~ AI]( - [Build your own LLM Apps with LangChain & GPT-Index]( by [1littlecoder]( - [BabyAGI - New System of Autonomous AI Agents with LangChain]( by [1littlecoder]( - [Run BabyAGI with Langchain Agents (with Python Code)]( by [1littlecoder]( - [How to Use Langchain With Zapier | Write and Send Email with GPT-3 | OpenAI API Tutorial]( by [StarMorph AI]( - [Use Your Locally Stored Files To Get Response From GPT - OpenAI | Langchain | Python]( by [Shweta Lodha]( - [Langchain JS | How to Use GPT-3, GPT-4 to Reference your own Data | OpenAI Embeddings Intro]( by [StarMorph AI]( - [The easiest way to work with large language models | Learn LangChain in 10min]( by [Sophia Yang]( - [4 Autonomous AI Agents: Westworld simulation BabyAGI, AutoGPT, Camel, LangChain]( by [Sophia Yang]( - [AI CAN SEARCH THE INTERNET? Langchain Agents + OpenAI ChatGPT]( by [tylerwhatsgood]( - [Query Your Data with GPT-4 | Embeddings, Vector Databases | Langchain JS Knowledgebase]( by [StarMorph AI]( - [Weaviate + LangChain for LLM apps presented by Erika Cardenas]( by [Weaviate Vector Database]( - [Langchain Overview How to Use Langchain & ChatGPT]( by [Python In Office]( - [Langchain Overview - How to Use Langchain & ChatGPT]( by [Python In Office]( - [LangChain Tutorials]( by [Edrick]( [LangChain, Chroma DB, OpenAI Beginner Guide | ChatGPT with your PDF]( - [LangChain 101: The Complete Beginner's Guide]( - [Custom langchain Agent & Tools with memory. Turn any Python function into langchain tool with Gpt 3]( by [echohive]( - [Building AI LLM Apps with LangChain (and more?) - LIVE STREAM]( by [Nicholas Renotte]( - [ChatGPT with any YouTube video using langchain and chromadb]( by [echohive]( - [How to Talk to a PDF using LangChain and ChatGPT]( by [Automata Learning Lab]( - [Langchain Document Loaders Part 1: Unstructured Files]( by [Merk]( - [LangChain - Prompt Templates (what all the best prompt engineers use)]( by [Nick Daigler]( - [LangChain. Crear aplicaciones Python impulsadas por GPT]( by [Jess Conde]( - [Easiest Way to Use GPT In Your Products | LangChain Basics Tutorial]( by [Rachel Woods]( - [BabyAGI + GPT-4 Langchain Agent with Internet Access]( by [tylerwhatsgood]( - [Learning LLM Agents. How does it actually work? LangChain, AutoGPT & OpenAI]( by [Arnoldas Kemeklis]( - [Get Started with LangChain in Node.js]( by [Developers Digest]( - [LangChain + OpenAI tutorial: Building a Q&A system w/ own text data]( by [Samuel Chan]( - [Langchain + Zapier Agent]( by [Merk]( - [Connecting the Internet with ChatGPT (LLMs) using Langchain And Answers Your Questions]( by [Kamalraj M M]( - [Build More Powerful LLM Applications for Business's with LangChain (Beginners Guide)]( by[ No Code Blackbox]( - [LangFlow LLM Agent Demo for LangChain]( by [Cobus Greyling]( - [Chatbot Factory: Streamline Python Chatbot Creation with LLMs and Langchain]( by [Finxter]( - [LangChain Tutorial - ChatGPT mit eigenen Daten]( by [Coding Crashkurse]( - [Chat with a CSV | LangChain Agents Tutorial (Beginners)]( by [GoDataProf]( - [Introduo ao Langchain - #Cortes - Live DataHackers]( by [Prof. Joo Gabriel Lima]( - [LangChain: Level up ChatGPT !? | LangChain Tutorial Part 1]( by [Code Affinity]( - [KI schreibt krasses Youtube Skript | LangChain Tutorial Deutsch]( by [SimpleKI]( - [Chat with Audio: Langchain, Chroma DB, OpenAI, and Assembly AI]( by [AI Anytime]( - [QA over documents with Auto vector index selection with Langchain router chains]( by [echohive]( - [Build your own custom LLM application with Bubble.io & Langchain (No Code & Beginner friendly)]( by [No Code Blackbox]( - [Simple App to Question Your Docs: Leveraging Streamlit, Hugging Face Spaces, LangChain, and Claude!]( by [Chris Alexiuk]( - [LANGCHAIN AI- ConstitutionalChainAI + Databutton AI ASSISTANT Web App]( by [Avra]( - [LANGCHAIN AI AUTONOMOUS AGENT WEB APP - BABY AGI with EMAIL AUTOMATION using DATABUTTON]( by [Avra]( - [The Future of Data Analysis: Using A.I. Models in Data Analysis (LangChain)]( by [Absent Data]( - [Memory in LangChain | Deep dive (python)]( by [Eden Marco]( - [9 LangChain UseCases | Beginner's Guide | 2023]( by [Data Science Basics]( - [Use Large Language Models in Jupyter Notebook | LangChain | Agents & Indexes]( by [Abhinaw Tiwari]( - [How to Talk to Your Langchain Agent | 11 Labs + Whisper]( by [VRSEN]( - [LangChain Deep Dive: 5 FUN AI App Ideas To Build Quickly and Easily]( by [James NoCode]( - [LangChain 101: Models]( by [Mckay Wrigley]( - [LangChain with JavaScript Tutorial #1 | Setup & Using LLMs]( by [Leon van Zyl]( - [LangChain Overview & Tutorial for Beginners: Build Powerful AI Apps Quickly & Easily (ZERO CODE)]( by [James NoCode]( - [LangChain In Action: Real-World Use Case With Step-by-Step Tutorial]( by [Rabbitmetrics]( - [Summarizing and Querying Multiple Papers with LangChain]( by [Automata Learning Lab]( - [Using Langchain (and Replit) through Tana, ask Google/Wikipedia/Wolfram Alpha to fill out a table]( by [Stian Hklev]( - [Langchain PDF App (GUI) | Create a ChatGPT For Your PDF in Python]( by [Alejandro AO - Software & Ai]( - [Auto-GPT with LangChain | Create Your Own Personal AI Assistant]( by [Data Science Basics]( - [Create Your OWN Slack AI Assistant with Python & LangChain]( by [Dave Ebbelaar]( - [How to Create LOCAL Chatbots with GPT4All and LangChain [Full Guide]]( by [Liam Ottley]( - [Build a Multilingual PDF Search App with LangChain, Cohere and Bubble]( by [Menlo Park Lab]( - [Building a LangChain Agent (code-free!) Using Bubble and Flowise]( by [Menlo Park Lab]( - [Build a LangChain-based Semantic PDF Search App with No-Code Tools Bubble and Flowise]( by [Menlo Park Lab]( - [LangChain Memory Tutorial | Building a ChatGPT Clone in Python]( by [Alejandro AO - Software & Ai]( - [ChatGPT For Your DATA | Chat with Multiple Documents Using LangChain]( by [Data Science Basics]( - [Llama Index: Chat with Documentation using URL Loader]( by [Merk]( - [Using OpenAI, LangChain, and Gradio to Build Custom GenAI Applications]( by [David Hundley]( - [LangChain, Chroma DB, OpenAI Beginner Guide | ChatGPT with your PDF]( - [Build AI chatbot with custom knowledge base using OpenAI API and GPT Index]( by [Irina Nik]( - [Build Your Own Auto-GPT Apps with LangChain (Python Tutorial)]( by [Dave Ebbelaar]( - [Chat with Multiple PDFs | LangChain App Tutorial in Python (Free LLMs and Embeddings)]( by [Alejandro AO - Software & Ai]( - [Chat with a CSV | LangChain Agents Tutorial (Beginners)]( by [Alejandro AO - Software & Ai]( - [Create Your Own ChatGPT with PDF Data in 5 Minutes (LangChain Tutorial)]( by [Liam Ottley]( - [Build a Custom Chatbot with OpenAI: GPT-Index & LangChain | Step-by-Step Tutorial]( by [Fabrikod]( - [Flowise is an open-source no-code UI visual tool to build LangChain applications]( by [Cobus Greyling]( - [LangChain & GPT 4 For Data Analysis: The Pandas Dataframe Agent]( by [Rabbitmetrics]( - [GirlfriendGPT - AI girlfriend with LangChain]( by [Toolfinder AI]( - [How to build with Langchain 10x easier | LangFlow & Flowise]( by [AI Jason]( - [Getting Started With LangChain In 20 Minutes- Build Celebrity Search Application]( by [Krish Naik]( - [Vector Embeddings Tutorial Code Your Own AI Assistant with GPT-4 API + LangChain + NLP]( by [FreeCodeCamp.org]( - [Fully LOCAL Llama 2 Q&A with LangChain]( by [1littlecoder]( - [Fully LOCAL Llama 2 Langchain on CPU]( by [1littlecoder]( - [Build LangChain Audio Apps with Python in 5 Minutes]( by [AssemblyAI]( - [Voiceflow & Flowise: Want to Beat Competition? New Tutorial with Real AI Chatbot]( by [AI SIMP]( - [THIS Is How You Build Production-Ready AI Apps (LangSmith Tutorial)]( by [Dave Ebbelaar]( - [Build POWERFUL LLM Bots EASILY with Your Own Data - Embedchain - Langchain 2.0? (Tutorial)]( by [WorldofAI]( - [Code Llama powered Gradio App for Coding: Runs on CPU]( by [AI Anytime]( - [LangChain Complete Course in One Video | Develop LangChain (AI) Based Solutions for Your Business]( by [UBprogrammer]( - [How to Run LLaMA Locally on CPU or GPU | Python & Langchain & CTransformers Guide]( by [Code With Prince]( - [PyData Heidelberg #11 - TimeSeries Forecasting & LLM Langchain]( by [PyData]( - [Prompt Engineering in Web Development | Using LangChain and Templates with OpenAI]( by [Akamai Developer ]( - [Retrieval-Augmented Generation (RAG) using LangChain and Pinecone - The RAG Special Episode]( by [Generative AI and Data Science On AWS]( - [LLAMA2 70b-chat Multiple Documents Chatbot with Langchain & Streamlit |All OPEN SOURCE|Replicate API]( by [DataInsightEdge]( - [Chatting with 44K Fashion Products: LangChain Opportunities and Pitfalls]( by [Rabbitmetrics]( - [Structured Data Extraction from ChatGPT with LangChain]( by [MG]( - [Chat with Multiple PDFs using Llama 2, Pinecone and LangChain (Free LLMs and Embeddings)]( by [Muhammad Moin]( - [Integrate Audio into LangChain.js apps in 5 Minutes]( by [AssemblyAI]( - [ChatGPT for your data with Local LLM]( by [Jacob Jedryszek]( - [Training Chatgpt with your personal data using langchain step by step in detail]( by [NextGen Machines]( - [Use ANY language in LangSmith with REST]( by [Nerding I/O]( - [How to Leverage the Full Potential of LLMs for Your Business with Langchain - Leon Ruddat]( by [PyData]( - [ChatCSV App: Chat with CSV files using LangChain and Llama 2]( by [Muhammad Moin]( ### Prompt Engineering and LangChain by Venelin Valkov\u200b - [Getting Started with LangChain: Load Custom Data, Run OpenAI Models, Embeddings and ChatGPT]( - [Loaders, Indexes & Vectorstores in LangChain: Question Answering on PDF files with ChatGPT]( - [LangChain Models: ChatGPT, Flan Alpaca, OpenAI Embeddings, Prompt Templates & Streaming]( - [LangChain Chains: Use ChatGPT to Build Conversational Agents, Summaries and Q&A on Text With LLMs]( - [Analyze Custom CSV Data with GPT-4 using Langchain]( - [Build ChatGPT Chatbots with LangChain Memory: Understanding and Implementing Memory in Conversations]( icon marks a new addition [last update 2023-09-21] - [Official LangChain YouTube channel](#official-langchain-youtube-channel) - [Introduction to LangChain with Harrison Chase, creator of LangChain](#introduction-to-langchain-with-harrison-chase-creator-of-langchain) - [Videos (sorted by views)](#videos-sorted-by-views)- [Prompt Engineering and LangChain by Venelin Valkov](#prompt-engineering-and-langchain-by-venelin-valkov)""]","To load YouTube transcripts and CSV documents, you can use the `YoutubeLoader` and `CSVLoader` classes from the `langchain.document_loaders` module. Here's how you can do it:

### Loading YouTube Transcripts
First, ensure you have the necessary libraries installed:
```bash
!pip install youtube-transcript-api langchain
```

Then, you can use the `YoutubeLoader` to load YouTube transcripts:
```python
from langchain.document_loaders import YoutubeLoader

# Initialize the loader with the YouTube video URL
loader = YoutubeLoader.from_youtube_url(""YOUR_YOUTUBE_VIDEO_URL_HERE"")

# Load the transcript
documents = loader.load()
```

### Loading CSV Documents
To load CSV documents, you need to install the `pandas` library if you haven't already:
```bash
!pip install pandas
```

Then, you can use the `CSVLoader` to load CSV documents:
```python
from langchain.document_loaders import CSVLoader

# Initialize the loader with the path to the CSV file
loader = CSVLoader.from_path(""YOUR_CSV_FILE_PATH_HERE"")

# Load the CSV document
documents = loader.load()
```

Make sure to replace `""YOUR_YOUTUBE_VIDEO_URL_HERE""` and `""YOUR_CSV_FILE_PATH_HERE""` with the actual URL of the YouTube video and the path to your CSV file, respectively.

This approach should help you load both YouTube transcripts and CSV documents effectively.","To load YouTube transcripts in LangChain, you can use the YoutubeLoader document loader. Here's an example of how to do it:

Install the required packages:
```
pip install langchain youtube_transcript_api pytube
```

Import the necessary classes:


    from langchain.document_loaders import YoutubeLoader


Create an instance of the YoutubeLoader class and provide the YouTube video URL:


    loader = YoutubeLoader.from_youtube_url(""https://www.youtube.com/watch?v=QsYGlZkevEg"", add_video_info=True)


Load the transcript using the load method:

    video = loader.load()

To load CSV documents you can use CSVLoader. Follow the steps below:

Import the necessary classes:

    from langchain.document_loaders import CSVLoader


Create an instance of the CSVLoader class and provide the path to the CSV file:


    loader = CSVLoader(file_path=""./example_data/mlb_teams_2012.csv"")

Load the CSV document using the load method:

    loader.load()",0.99999999995,,,0.3780850935680652,0.4583333333333333
52,my agent keeps getting an OutputParserException is something i can set to make it take care of these?,"['Handle parsing errors | Handle parsing errors Occasionally the LLM cannot determine what step to take because its outputs are not correctly formatted to be handled by the output parser. In this case, by default the agent errors. But you can easily control this functionality with `handle_parsing_errors`! Let\'s explore how. ## Setup ```python from langchain.agents import AgentType, Tool, initialize_agent from langchain.chat_models import ChatOpenAI from langchain.utilities import SerpAPIWrapper ``` ```python search = SerpAPIWrapper() tools = [ Tool( name=""Search"", func=search.run, description=""useful for when you need to answer questions about current events. You should ask targeted questions"", ), ] ``` ## Error In this scenario, the agent will error (because it fails to output an Action string) ```python mrkl = initialize_agent( tools, ChatOpenAI(temperature=0), agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True, ) ``` ```python mrkl.run(""Who is Leo DiCaprio\'s girlfriend? No need to add Action"") ``` ```text > Entering new AgentExecutor chain... --------------------------------------------------------------------------- IndexError Traceback (most recent call last) File ~/workplace/langchain/langchain/agents/chat/output_parser.py:21, in ChatOutputParser.parse(self, text) 20 try: ---> 21 action = text.split(""```"")[1] 22 response = json.loads(action.strip()) IndexError: list index out of range During handling of the above exception, another exception occurred: OutputParserException Traceback (most recent call last) Cell In[4], line 1 ----> 1 mrkl.run(""Who is Leo DiCaprio\'s girlfriend? No need to add Action"") File ~/workplace/langchain/langchain/chains/base.py:236, in Chain.run(self, callbacks, *args, **kwargs) 234 if len(args) != 1: 235 raise ValueError(""`run` supports only one positional argument."") --> 236 return self(args[0], callbacks=callbacks)[self.output_keys[0]] 238 if kwargs and not args: 239 return self(kwargs, callbacks=callbacks)[self.output_keys[0]] File ~/workplace/langchain/langchain/chains/base.py:140, in Chain.__call__(self, inputs, return_only_outputs, callbacks) 138 except (KeyboardInterrupt, Exception) as e: 139 run_manager.on_chain_error(e) --> 140 raise e 141 run_manager.on_chain_end(outputs) 142 return self.prep_outputs(inputs, outputs, return_only_outputs) File ~/workplace/langchain/langchain/chains/base.py:134, in Chain.__call__(self, inputs, return_only_outputs, callbacks) 128 run_manager = callback_manager.on_chain_start( 129 {""name"": self.__class__.__name__}, 130 inputs, 131 ) 132 try: 133 outputs = ( --> 134 self._call(inputs, run_manager=run_manager) 135 if new_arg_supported 136 else self._call(inputs) 137 ) 138 except (KeyboardInterrupt, Exception) as e: 139 run_manager.on_chain_error(e) File ~/workplace/langchain/langchain/agents/agent.py:947, in AgentExecutor._call(self, inputs, run_manager) 945 # We now enter the agent loop (until it returns something). 946 while self._should_continue(iterations, time_elapsed): --> 947 next_step_output = self._take_next_step( 948 name_to_tool_map, 949 color_mapping, 950 inputs, 951 intermediate_steps, 952 run_manager=run_manager, 953 ) 954 if isinstance(next_step_output, AgentFinish): 955 return self._return( 956 next_step_output, intermediate_steps, run_manager=run_manager 957 ) File ~/workplace/langchain/langchain/agents/agent.py:773, in AgentExecutor._take_next_step(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager) 771 raise_error = False 772 if raise_error: --> 773 raise e 774 text = str(e) 775 if isinstance(self.handle_parsing_errors, bool): File ~/workplace/langchain/langchain/agents/agent.py:762, in AgentExecutor._take_next_step(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager) 756 """"""Take a single step in the thought-action-observation loop. 757 758 Override this to take control of how the agent makes and acts on choices. 759 """""" 760 try: 761 # Call the LLM to see what to do. --> 762 output = self.agent.plan( 763 intermediate_steps, 764 callbacks=run_manager.get_child() if run_manager else None, 765 **inputs, 766 ) 767 except OutputParserException as e: 768 if isinstance(self.handle_parsing_errors, bool): File ~/workplace/langchain/langchain/agents/agent.py:444, in Agent.plan(self, intermediate_steps, callbacks, **kwargs) 442 full_inputs = self.get_full_inputs(intermediate_steps, **kwargs) 443 full_output = self.llm_chain.predict(callbacks=callbacks, **full_inputs) --> 444 return self.output_parser.parse(full_output) File ~/workplace/langchain/langchain/agents/chat/output_parser.py:26, in ChatOutputParser.parse(self, text) 23 return AgentAction(response[""action""], response[""action_input""], text) 25 except Exception: ---> 26 raise OutputParserException(f""Could not parse LLM output: {text}"") OutputParserException: Could not parse LLM output: I\'m sorry, but I cannot provide an answer without an Action. Please provide a valid Action in the format specified above. ``` ## Default error handling Handle errors with `Invalid or incomplete response`: ```python mrkl = initialize_agent( tools, ChatOpenAI(temperature=0), agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True, handle_parsing_errors=True, ) ``` ```python mrkl.run(""Who is Leo DiCaprio\'s girlfriend? No need to add Action"") ``` ```text > Entering new AgentExecutor chain... Observation: Invalid or incomplete response Thought: Observation: Invalid or incomplete response Thought:Search for Leo DiCaprio\'s current girlfriend Action: ``` { ""action"": ""Search"", ""action_input"": ""Leo DiCaprio current girlfriend"" } ``` Observation: Just Jared on Instagram: Leonardo DiCaprio & girlfriend Camila Morrone couple up for a lunch date! Thought:Camila Morrone is currently Leo DiCaprio\'s girlfriend Final Answer: Camila Morrone > Finished chain. \'Camila Morrone\' ``` ## Custom error message You can easily customize the message to use when there are parsing errors. ```python mrkl = initialize_agent( tools, ChatOpenAI(temperature=0), agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True, handle_parsing_errors=""Check your output and make sure it conforms!"", ) ``` ```python mrkl.run(""Who is Leo DiCaprio\'s girlfriend? No need to add Action"") ``` ```text > Entering new AgentExecutor chain... Observation: Could not parse LLM output: I\'m sorry, but I canno Thought:I need to use the Search tool to find the answer to the question. Action: ``` { ""action"": ""Search"", ""action_input"": ""Who is Leo DiCaprio\'s girlfriend?"" } ``` Observation: DiCaprio broke up with girlfriend Camila Morrone, 25, in the summer of 2022, after dating for four years. He\'s since been linked to another famous supermodel Gigi Hadid. The power couple were first supposedly an item in September after being spotted getting cozy during a party at New York Fashion Week. Thought:The answer to the question is that Leo DiCaprio\'s current girlfriend is Gigi Hadid. Final Answer: Gigi Hadid. > Finished chain. \'Gigi Hadid.\' ``` ## Custom Error Function You can also customize the error to be a function that takes the error in and outputs a string. ```python def _handle_error(error) -> str: return str(error)[:50] mrkl = initialize_agent( tools, ChatOpenAI(temperature=0), agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True, handle_parsing_errors=_handle_error, ) ``` ```python mrkl.run(""Who is Leo DiCaprio\'s girlfriend? No need to add Action"") ``` ```text > Entering new AgentExecutor chain... Observation: Could not parse LLM output: I\'m sorry, but I canno Thought:I need to use the Search tool to find the answer to the question. Action: ``` { ""action"": ""Search"", ""action_input"": ""Who is Leo DiCaprio\'s girlfriend?"" } ``` Observation: DiCaprio broke up with girlfriend Camila Morrone, 25, in the summer of 2022, after dating for four years. He\'s since been linked to another famous supermodel Gigi Hadid. The power couple were first supposedly an item in September after being spotted getting cozy during a party at New York Fashion Week. Thought:The current girlfriend of Leonardo DiCaprio is Gigi Hadid. Final Answer: Gigi Hadid. > Finished chain. \'Gigi Hadid.\' ``` - [Setup](#setup) - [Error](#error) - [Default error handling](#default-error-handling) - [Custom error message](#custom-error-message) - [Custom Error Function](#custom-error-function)', 'Facebook Messenger | Facebook Messenger This notebook shows how to load data from Facebook in a format you can fine-tune on. The overall steps are: 1. Download your messenger data to disk. 2. Create the Chat Loader and call `loader.load()` (or `loader.lazy_load()`) to perform the conversion. 3. Optionally use `merge_chat_runs` to combine message from the same sender in sequence, and/or `map_ai_messages` to convert messages from the specified sender to the ""AIMessage"" class. Once you\'ve done this, call `convert_messages_for_finetuning` to prepare your data for fine-tuning. Once this has been done, you can fine-tune your model. To do so you would complete the following steps: 1. Upload your messages to OpenAI and run a fine-tuning job. 2. Use the resulting model in your LangChain app! Let\'s begin. ## 1. Download Data To download your own messenger data, following instructions [here]( IMPORTANT - make sure to download them in JSON format (not HTML). We are hosting an example dump at [this google drive link]( that we will use in this walkthrough. ```python # This uses some example data import zipfile import requests def download_and_unzip(url: str, output_path: str = ""file.zip"") -> None: file_id = url.split(""/"")[-2] download_url = f"" response = requests.get(download_url) if response.status_code != 200: print(""Failed to download the file."") return with open(output_path, ""wb"") as file: file.write(response.content) print(f""File {output_path} downloaded."") with zipfile.ZipFile(output_path, ""r"") as zip_ref: zip_ref.extractall() print(f""File {output_path} has been unzipped."") # URL of the file to download url = ( "" ) # Download and unzip download_and_unzip(url) ``` ```text File file.zip downloaded. File file.zip has been unzipped. ``` ## 2. Create Chat Loader We have 2 different `FacebookMessengerChatLoader` classes, one for an entire directory of chats, and one to load individual files. We ```python directory_path = ""./hogwarts"" ``` ```python from langchain.chat_loaders.facebook_messenger import ( FolderFacebookMessengerChatLoader, SingleFileFacebookMessengerChatLoader, ) ``` ```python loader = SingleFileFacebookMessengerChatLoader( path=""./hogwarts/inbox/HermioneGranger/messages_Hermione_Granger.json"", ) ``` ```python chat_session = loader.load()[0] chat_session[""messages""][:3] ``` ```text [HumanMessage(content=""Hi Hermione! How\'s your summer going so far?"", additional_kwargs={\'sender\': \'Harry Potter\'}, example=False), HumanMessage(content=""Harry! Lovely to hear from you. My summer is going well, though I do miss everyone. I\'m spending most of my time going through my books and researching fascinating new topics. How about you?"", additional_kwargs={\'sender\': \'Hermione Granger\'}, example=False), HumanMessage(content=""I miss you all too. The Dursleys are being their usual unpleasant selves but I\'m getting by. At least I can practice some spells in my room without them knowing. Let me know if you find anything good in your researching!"", additional_kwargs={\'sender\': \'Harry Potter\'}, example=False)] ``` ```python loader = FolderFacebookMessengerChatLoader( path=""./hogwarts"", ) ``` ```python chat_sessions = loader.load() len(chat_sessions) ``` ```text 9 ``` ## 3. Prepare for fine-tuning Calling `load()` returns all the chat messages we could extract as human messages. When conversing with chat bots, conversations typically follow a more strict alternating dialogue pattern relative to real conversations. You can choose to merge message ""runs"" (consecutive messages from the same sender) and select a sender to represent the ""AI"". The fine-tuned LLM will learn to generate these AI messages. ```python from langchain.chat_loaders.utils import ( map_ai_messages, merge_chat_runs, ) ``` ```python merged_sessions = merge_chat_runs(chat_sessions) alternating_sessions = list(map_ai_messages(merged_sessions, ""Harry Potter"")) ``` ```python # Now all of Harry Potter\'s messages will take the AI message class # which maps to the \'assistant\' role in OpenAI\'s training format alternating_sessions[0][""messages""][:3] ``` ```text [AIMessage(content=""Professor Snape, I was hoping I could speak with you for a moment about something that\'s been concerning me lately."", additional_kwargs={\'sender\': \'Harry Potter\'}, example=False), HumanMessage(content=""What is it, Potter? I\'m quite busy at the moment."", additional_kwargs={\'sender\': \'Severus Snape\'}, example=False), AIMessage(content=""I apologize for the interruption, sir. I\'ll be brief. I\'ve noticed some strange activity around the school grounds at night. I saw a cloaked figure lurking near the Forbidden Forest last night. I\'m worried someone may be plotting something sinister."", additional_kwargs={\'sender\': \'Harry Potter\'}, example=False)] ``` #### Now we can convert to OpenAI format dictionaries ```python from langchain.adapters.openai import convert_messages_for_finetuning ``` ```python training_data = convert_messages_for_finetuning(alternating_sessions) print(f""Prepared {len(training_data)} dialogues for training"") ``` ```text Prepared 9 dialogues for training ``` ```python training_data[0][:3] ``` ```text [{\'role\': \'assistant\', \'content\': ""Professor Snape, I was hoping I could speak with you for a moment about something that\'s been concerning me lately.""}, {\'role\': \'user\', \'content\': ""What is it, Potter? I\'m quite busy at the moment.""}, {\'role\': \'assistant\', \'content\': ""I apologize for the interruption, sir. I\'ll be brief. I\'ve noticed some strange activity around the school grounds at night. I saw a cloaked figure lurking near the Forbidden Forest last night. I\'m worried someone may be plotting something sinister.""}] ``` OpenAI currently requires at least 10 training examples for a fine-tuning job, though they recommend between 50-100 for most tasks. Since we only have 9 chat sessions, we can subdivide them (optionally with some overlap) so that each training example is comprised of a portion of a whole conversation. Facebook chat sessions (1 per person) often span multiple days and conversations, so the long-range dependencies may not be that important to model anyhow. ```python # Our chat is alternating, we will make each datapoint a group of 8 messages, # with 2 messages overlapping chunk_size = 8 overlap = 2 training_examples = [ conversation_messages[i : i + chunk_size] for conversation_messages in training_data for i in range(0, len(conversation_messages) - chunk_size + 1, chunk_size - overlap) ] len(training_examples) ``` ```text 100 ``` ## 4. Fine-tune the model It\'s time to fine-tune the model. Make sure you have `openai` installed and have set your `OPENAI_API_KEY` appropriately ```python # %pip install -U openai --quiet ``` ```python import json import time from io import BytesIO import openai # We will write the jsonl file in memory my_file = BytesIO() for m in training_examples: my_file.write((json.dumps({""messages"": m}) + ""\\n"").encode(""utf-8"")) my_file.seek(0) training_file = openai.File.create(file=my_file, purpose=""fine-tune"") # OpenAI audits each training file for compliance reasons. # This make take a few minutes status = openai.File.retrieve(training_file.id).status start_time = time.time() while status != ""processed"": print(f""Status=[{status}]... {time.time() - start_time:.2f}s"", end=""\\r"", flush=True) time.sleep(5) status = openai.File.retrieve(training_file.id).status print(f""File {training_file.id} ready after {time.time() - start_time:.2f} seconds."") ``` ```text File file-zCyNBeg4snpbBL7VkvsuhCz8 ready afer 30.55 seconds. ``` With the file ready, it\'s time to kick off a training job. ```python job = openai.FineTuningJob.create( training_file=training_file.id, model=""gpt-3.5-turbo"", ) ``` Grab a cup of tea while your model is being prepared. This may take some time! ```python status = openai.FineTuningJob.retrieve(job.id).status start_time = time.time() while status != ""succeeded"": print(f""Status=[{status}]... {time.time() - start_time:.2f}s"", end=""\\r"", flush=True) time.sleep(5) job = openai.FineTuningJob.retrieve(job.id) status = job.status ``` ```text Status=[running]... 908.87s ``` ```python print(job.fine_tuned_model) ``` ```text ft:gpt-3.5-turbo-0613:personal::7rDwkaOq ``` ## 5. Use in LangChain You can use the resulting model ID directly the `ChatOpenAI` model class. ```python from langchain.chat_models import ChatOpenAI model = ChatOpenAI( model=job.fine_tuned_model, temperature=1, ) ``` ```python from langchain.prompts import ChatPromptTemplate from langchain.schema.output_parser import StrOutputParser prompt = ChatPromptTemplate.from_messages( [ (""human"", ""{input}""), ] ) chain = prompt | model | StrOutputParser() ``` ```python for tok in chain.stream({""input"": ""What classes are you taking?""}): print(tok, end="""", flush=True) ``` ```text The usual - Potions, Transfiguration, Defense Against the Dark Arts. What about you? ``` - [1. Download Data](#1-download-data) - [2. Create Chat Loader](#2-create-chat-loader) - [3. Prepare for fine-tuning](#3-prepare-for-fine-tuning) - [4. Fine-tune the model](#4-fine-tune-the-model) - [5. Use in LangChain](#5-use-in-langchain)', 'iFixit | iFixit [iFixit]( is the largest, open repair community on the web. The site contains nearly 100k repair manuals, 200k Questions & Answers on 42k devices, and all the data is licensed under CC-BY-NC-SA 3.0. This loader will allow you to download the text of a repair guide, text of Q&A\'s and wikis from devices on `iFixit` using their open APIs. It\'s incredibly useful for context related to technical documents and answers to questions about devices in the corpus of data on `iFixit`. ```python from langchain.document_loaders import IFixitLoader ``` ```python loader = IFixitLoader("" data = loader.load() ``` ```python data ``` ```text [Document(page_content=""# Banana Teardown\\nIn this teardown, we open a banana to see what\'s inside. Yellow and delicious, but most importantly, yellow.\\n\\n\\n###Tools Required:\\n\\n - Fingers\\n\\n - Teeth\\n\\n - Thumbs\\n\\n\\n###Parts Required:\\n\\n - None\\n\\n\\n## Step 1\\nTake one banana from the bunch.\\nDon\'t squeeze too hard!\\n\\n\\n## Step 2\\nHold the banana in your left hand and grip the stem between your right thumb and forefinger.\\n\\n\\n## Step 3\\nPull the stem downward until the peel splits.\\n\\n\\n## Step 4\\nInsert your thumbs into the split of the peel and pull the two sides apart.\\nExpose the top of the banana. It may be slightly squished from pulling on the stem, but this will not affect the flavor.\\n\\n\\n## Step 5\\nPull open the peel, starting from your original split, and opening it along the length of the banana.\\n\\n\\n## Step 6\\nRemove fruit from peel.\\n\\n\\n## Step 7\\nEat and enjoy!\\nThis is where you\'ll need your teeth.\\nDo not choke on banana!\\n"", lookup_str=\'\', metadata={\'source\': \' \'title\': \'Banana Teardown\'}, lookup_index=0)] ``` ```python loader = IFixitLoader( "" ) data = loader.load() ``` ```python data ``` ```text [Document(page_content=\'# My iPhone 6 is typing and opening apps by itself\\nmy iphone 6 is typing and opening apps by itself. How do i fix this. I just bought it last week.\\nI restored as manufactures cleaned up the screen\\nthe problem continues\\n\\n## 27 Answers\\n\\nFilter by: \\n\\nMost Helpful\\nNewest\\nOldest\\n\\n### Accepted Answer\\nHi,\\nWhere did you buy it? If you bought it from Apple or from an official retailer like Carphone warehouse etc. Then you\\\'ll have a year warranty and can get it replaced free.\\nIf you bought it second hand, from a third part repair shop or online, then it may still have warranty, unless it is refurbished and has been repaired elsewhere.\\nIf this is the case, it may be the screen that needs replacing to solve your issue.\\nEither way, wherever you got it, it\\\'s best to return it and get a refund or a replacement device. :-)\\n\\n\\n\\n### Most Helpful Answer\\nI had the same issues, screen freezing, opening apps by itself, selecting the screens and typing on it\\\'s own. I first suspected aliens and then ghosts and then hackers.\\niPhone 6 is weak physically and tend to bend on pressure. And my phone had no case or cover.\\nI took the phone to apple stores and they said sensors need to be replaced and possibly screen replacement as well. My phone is just 17 months old.\\nHere is what I did two days ago and since then it is working like a charm..\\nHold the phone in portrait (as if watching a movie). Twist it very very gently. do it few times.Rest the phone for 10 mins (put it on a flat surface). You can now notice those self typing things gone and screen getting stabilized.\\nThen, reset the hardware (hold the power and home button till the screen goes off and comes back with apple logo). release the buttons when you see this.\\nThen, connect to your laptop and log in to iTunes and reset your phone completely. (please take a back-up first).\\nAnd your phone should be good to use again.\\nWhat really happened here for me is that the sensors might have stuck to the screen and with mild twisting, they got disengaged/released.\\nI posted this in Apple Community and the moderators deleted it, for the best reasons known to them.\\nInstead of throwing away your phone (or selling cheaply), try this and you could be saving your phone.\\nLet me know how it goes.\\n\\n\\n\\n### Other Answer\\nIt was the charging cord! I bought a gas station braided cord and it was the culprit. Once I plugged my OEM cord into the phone the GHOSTS went away.\\n\\n\\n\\n### Other Answer\\nI\\\'ve same issue that I just get resolved. I first tried to restore it from iCloud back, however it was not a software issue or any virus issue, so after restore same problem continues. Then I get my phone to local area iphone repairing lab, and they detected that it is an LCD issue. LCD get out of order without any reason (It was neither hit or nor slipped, but LCD get out of order all and sudden, while using it) it started opening things at random. I get LCD replaced with new one, that cost me $80.00 in total ($70.00 LCD charges + $10.00 as labor charges to fix it). iPhone is back to perfect mode now. It was iphone 6s. Thanks.\\n\\n\\n\\n### Other Answer\\nI was having the same issue with my 6 plus, I took it to a repair shop, they opened the phone, disconnected the three ribbons the screen has, blew up and cleaned the connectors and connected the screen again and it solved the issue it\'s hardware, not software.\\n\\n\\n\\n### Other Answer\\nHey.\\nJust had this problem now. As it turns out, you just need to plug in your phone. I use a case and when I took it off I noticed that there was a lot of dust and dirt around the areas that the case didn\\\'t cover. I shined a light in my ports and noticed they were filled with dust. Tomorrow I plan on using pressurized air to clean it out and the problem should be solved. If you plug in your phone and unplug it and it stops the issue, I recommend cleaning your phone thoroughly.\\n\\n\\n\\n### Other Answer\\nI simply changed the power supply and problem was gone. The block that plugs in the wall not the sub cord. The cord was fine but not the block.\\n\\n\\n\\n### Other Answer\\nSomeone ask! I purchased my iPhone 6s Plus for 1000 from at&t. Before I touched it, I purchased a otter defender case. I read where at&t said touch desease was due to dropping! Bullshit!! I am 56 I have never dropped it!! Looks brand new! Never dropped or abused any way! I have my original charger. I am going to clean it and try everyone\'s advice. It really sucks! I had 40,000,000 on my heart of Vegas slots! I play every day. I would be spinning and my fingers were no where max buttons and it would light up and switch to max. It did it 3 times before I caught it light up by its self. It sucks. Hope I can fix it!!!!\\n\\n\\n\\n### Other Answer\\nNo answer, but same problem with iPhone 6 plus--random, self-generated jumping amongst apps and typing on its own--plus freezing regularly (aha--maybe that\\\'s what the ""plus"" in ""6 plus"" refers to?). An Apple Genius recommended upgrading to iOS 11.3.1 from 11.2.2, to see if that fixed the trouble. If it didn\\\'t, Apple will sell me a new phone for $168! Of couese the OS upgrade didn\\\'t fix the problem. Thanks for helping me figure out that it\\\'s most likely a hardware problem--which the ""genius"" probably knows too.\\nI\\\'m getting ready to go Android.\\n\\n\\n\\n### Other Answer\\nI experienced similar ghost touches. Two weeks ago, I changed my iPhone 6 Plus shell (I had forced the phone into it because it\'s pretty tight), and also put a new glass screen protector (the edges of the protector don\'t stick to the screen, weird, so I brushed pressure on the edges at times to see if they may smooth out one day miraculously). I\'m not sure if I accidentally bend the phone when I installed the shell, or, if I got a defective glass protector that messes up the touch sensor. Well, yesterday was the worse day, keeps dropping calls and ghost pressing keys for me when I was on a call. I got fed up, so I removed the screen protector, and so far problems have not reoccurred yet. I\'m crossing my fingers that problems indeed solved.\\n\\n\\n\\n### Other Answer\\nthank you so much for this post! i was struggling doing the reset because i cannot type userids and passwords correctly because the iphone 6 plus i have kept on typing letters incorrectly. I have been doing it for a day until i come across this article. Very helpful! God bless you!!\\n\\n\\n\\n### Other Answer\\nI just turned it off, and turned it back on.\\n\\n\\n\\n### Other Answer\\nMy problem has not gone away completely but its better now i changed my charger and turned off prediction ....,,,now it rarely happens\\n\\n\\n\\n### Other Answer\\nI tried all of the above. I then turned off my home cleaned it with isopropyl alcohol 90%. Then I baked it in my oven on warm for an hour and a half over foil. Took it out and set it cool completely on the glass top stove. Then I turned on and it worked.\\n\\n\\n\\n### Other Answer\\nI think at& t should man up and fix your phone for free! You pay a lot for a Apple they should back it. I did the next 30 month payments and finally have it paid off in June. My iPad sept. Looking forward to a almost 100 drop in my phone bill! Now this crap!!! Really\\n\\n\\n\\n### Other Answer\\nIf your phone is JailBroken, suggest downloading a virus. While all my symptoms were similar, there was indeed a virus/malware on the phone which allowed for remote control of my iphone (even while in lock mode). My mistake for buying a third party iphone i suppose. Anyway i have since had the phone restored to factory and everything is working as expected for now. I will of course keep you posted if this changes. Thanks to all for the helpful posts, really helped me narrow a few things down.\\n\\n\\n\\n### Other Answer\\nWhen my phone was doing this, it ended up being the screen protector that i got from 5 below. I took it off and it stopped. I ordered more protectors from amazon and replaced it\\n\\n\\n\\n### Other Answer\\niPhone 6 Plus first generation.I had the same issues as all above, apps opening by themselves, self typing, ultra sensitive screen, items jumping around all over.it even called someone on FaceTime twice by itself when I was not in the room..I thought the phone was toast and i\'d have to buy a new one took me a while to figure out but it was the extra cheap block plug I bought at a dollar store for convenience of an extra charging station when I move around the house from den to living room..cord was fine but bought a new Apple brand block plugno more problems works just fine now. This issue was a recent event so had to narrow things down to what had changed recently to my phone so I could figure it out.\\nI even had the same problem on a laptop with documents opening up by themselves..a laptop that was plugged in to the same wall plug as my phone charger with the dollar store block plug.until I changed the block plug.\\n\\n\\n\\n### Other Answer\\nHad the problem: Inherited a 6s Plus from my wife. She had no problem with it.\\nLooks like it was merely the cheap phone case I purchased on Amazon. It was either pinching the edges or torquing the screen/body of the phone. Problem solved.\\n\\n\\n\\n### Other Answer\\nI bought my phone on march 6 and it was a brand new, but It sucks me uo because it freezing, shaking and control by itself. I went to the store where I bought this and I told them to replacr it, but they told me I have to pay it because Its about lcd issue. Please help me what other ways to fix it. Or should I try to remove the screen or should I follow your step above.\\n\\n\\n\\n### Other Answer\\nI tried everything and it seems to come back to needing the original iPhone cableor at least another 1 that would have come with another iPhonenot the $5 Store fast charging cables. My original cable is pretty beat up - like most that I see - but I\'ve been beaten up much MUCH less by sticking with its use! I didn\'t find that the casing/shell around it or not made any diff.\\n\\n\\n\\n### Other Answer\\ngreat now I have to wait one more hour to reset my phone and while I was tryin to connect my phone to my computer the computer also restarted smh does anyone else knows how I can get my phone to work my problem is I have a black dot on the bottom left of my screen an it wont allow me to touch a certain part of my screen unless I rotate my phone and I know the password but the first number is a 2 and it won\\\'t let me touch 1,2, or 3 so now I have to find a way to get rid of my password and all of a sudden my phone wants to touch stuff on its own which got my phone disabled many times to the point where I have to wait a whole hour and I really need to finish something on my phone today PLEASE HELPPPP\\n\\n\\n\\n### Other Answer\\nIn my case , iphone 6 screen was faulty. I got it replaced at local repair shop, so far phone is working fine.\\n\\n\\n\\n### Other Answer\\nthis problem in iphone 6 has many different scenarios and solutions, first try to reconnect the lcd screen to the motherboard again, if didnt solve, try to replace the lcd connector on the motherboard, if not solved, then remains two issues, lcd screen it self or touch IC. in my country some repair shops just change them all for almost 40$ since they dont want to troubleshoot one by one. readers of this comment also should know that partial screen not responding in other iphone models might also have an issue in LCD connector on the motherboard, specially if you lock/unlock screen and screen works again for sometime. lcd connectors gets disconnected lightly from the motherboard due to multiple falls and hits after sometime. best of luck for all\\n\\n\\n\\n### Other Answer\\nI am facing the same issue whereby these ghost touches type and open apps , I am using an original Iphone cable , how to I fix this issue.\\n\\n\\n\\n### Other Answer\\nThere were two issues with the phone I had troubles with. It was my dads and turns out he carried it in his pocket. The phone itself had a little bend in it as a result. A little pressure in the opposite direction helped the issue. But it also had a tiny crack in the screen which wasnt obvious, once we added a screen protector this fixed the issues entirely.\\n\\n\\n\\n### Other Answer\\nI had the same problem with my 64Gb iPhone 6+. Tried a lot of things and eventually downloaded all my images and videos to my PC and restarted the phone - problem solved. Been working now for two days.\', lookup_str=\'\', metadata={\'source\': \' \'title\': \'My iPhone 6 is typing and opening apps by itself\'}, lookup_index=0)] ``` ```python loader = IFixitLoader("" data = loader.load() ``` ```python data ``` ```text [Document(page_content=""Standard iPad\\nThe standard edition of the tablet computer made by Apple.\\n== Background Information ==\\n\\nOriginally introduced in January 2010, the iPad is Apple\'s standard edition of their tablet computer. In total, there have been ten generations of the standard edition of the iPad.\\n\\n== Additional Information ==\\n\\n* [link| Apple Product Page]\\n* [link| iPad Wikipedia]"", lookup_str=\'\', metadata={\'source\': \' \'title\': \'Standard iPad\'}, lookup_index=0)] ``` ## Searching iFixit using /suggest If you\'re looking for a more general way to search iFixit based on a keyword or phrase, the /suggest endpoint will return content related to the search term, then the loader will load the content from each of the suggested items and prep and return the documents. ```python data = IFixitLoader.load_suggestions(""Banana"") ``` ```python data ``` ```text [Document(page_content=\'Banana\\nTasty fruit. Good source of potassium. Yellow.\\n== Background Information ==\\n\\nCommonly misspelled, this wildly popular, phone shaped fruit serves as nutrition and an obstacle to slow down vehicles racing close behind you. Also used commonly as a synonym for crazy or insane.\\n\\nBotanically, the banana is considered a berry, although it isn\'t included in the culinary berry category containing strawberries and raspberries. Belonging to the genus Musa, the banana originated in Southeast Asia and Australia. Now largely cultivated throughout South and Central America, bananas are largely available throughout the world. They are especially valued as a staple food group in developing countries due to the banana tree\'s ability to produce fruit year round.\\n\\nThe banana can be easily opened. Simply remove the outer yellow shell by cracking the top of the stem. Then, with the broken piece, peel downward on each side until the fruity components on the inside are exposed. Once the shell has been removed it cannot be put back together.\\n\\n== Technical Specifications ==\\n\\n* Dimensions: Variable depending on genetics of the parent tree\\n* Color: Variable depending on ripeness, region, and season\\n\\n== Additional Information ==\\n\\n[link| Banana]\', lookup_str=\'\', metadata={\'source\': \' \'title\': \'Banana\'}, lookup_index=0), Document(page_content=""# Banana Teardown\\nIn this teardown, we open a banana to see what\'s inside. Yellow and delicious, but most importantly, yellow.\\n\\n\\n###Tools Required:\\n\\n - Fingers\\n\\n - Teeth\\n\\n - Thumbs\\n\\n\\n###Parts Required:\\n\\n - None\\n\\n\\n## Step 1\\nTake one banana from the bunch.\\nDon\'t squeeze too hard!\\n\\n\\n## Step 2\\nHold the banana in your left hand and grip the stem between your right thumb and forefinger.\\n\\n\\n## Step 3\\nPull the stem downward until the peel splits.\\n\\n\\n## Step 4\\nInsert your thumbs into the split of the peel and pull the two sides apart.\\nExpose the top of the banana. It may be slightly squished from pulling on the stem, but this will not affect the flavor.\\n\\n\\n## Step 5\\nPull open the peel, starting from your original split, and opening it along the length of the banana.\\n\\n\\n## Step 6\\nRemove fruit from peel.\\n\\n\\n## Step 7\\nEat and enjoy!\\nThis is where you\'ll need your teeth.\\nDo not choke on banana!\\n"", lookup_str=\'\', metadata={\'source\': \' \'title\': \'Banana Teardown\'}, lookup_index=0)] ``` - [Searching iFixit using /suggest](#searching-ifixit-using-suggest)']","Yes, you can handle parsing errors in your agent by setting the `handle_parsing_errors` parameter to `True` or a custom error message/function. This allows the agent to manage situations where the output is not correctly formatted. Here's an example:

```python
mrkl = initialize_agent(
    tools,
    ChatOpenAI(temperature=0),
    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True,
    handle_parsing_errors=True,  # Enable default error handling
)

# Or customize the error message
mrkl = initialize_agent(
    tools,
    ChatOpenAI(temperature=0),
    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True,
    handle_parsing_errors=""Check your output and make sure it conforms!"",
)

# Or use a custom error function
def _handle_error(error) -> str:
    return str(error)[:50]

mrkl = initialize_agent(
    tools,
    ChatOpenAI(temperature=0),
    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True,
    handle_parsing_errors=_handle_error,
)
```

This way, your agent can gracefully handle parsing exceptions and provide more informative responses.","You can control the agent's behavior for output parsing errors by initializing the agent with `handle_parsing_errors`=True. Then the agent will see 'Handle errors with Invalid or incomplete response'. You can also set a string to provide a custom error message to the agent, or provide a custom error handling function.",0.9999999999,1.0,,0.30733707247198067,0.2364532019704433
53,What does ReAct mean?,"['ReAct | ReAct This walkthrough showcases using an agent to implement the [ReAct]( logic. ```python from langchain.agents import AgentType, initialize_agent, load_tools from langchain.llms import OpenAI ``` First, let\'s load the language model we\'re going to use to control the agent. ```python llm = OpenAI(temperature=0) ``` Next, let\'s load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in. ```python tools = load_tools([""serpapi"", ""llm-math""], llm=llm) ``` ## Using LCEL We will first show how to create the agent using LCEL ```python from langchain import hub from langchain.agents.format_scratchpad import format_log_to_str from langchain.agents.output_parsers import ReActSingleInputOutputParser from langchain.tools.render import render_text_description ``` ```python prompt = hub.pull(""hwchase17/react"") prompt = prompt.partial( tools=render_text_description(tools), tool_names="", "".join([t.name for t in tools]), ) ``` ```python llm_with_stop = llm.bind(stop=[""\\nObservation""]) ``` ```python agent = ( { ""input"": lambda x: x[""input""], ""agent_scratchpad"": lambda x: format_log_to_str(x[""intermediate_steps""]), } | prompt | llm_with_stop | ReActSingleInputOutputParser() ) ``` ```python from langchain.agents import AgentExecutor ``` ```python agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True) ``` ```python agent_executor.invoke( { ""input"": ""Who is Leo DiCaprio\'s girlfriend? What is her current age raised to the 0.43 power?"" } ) ``` ```text > Entering new AgentExecutor chain... I need to find out who Leo DiCaprio\'s girlfriend is and then calculate her age raised to the 0.43 power. Action: Search Action Input: ""Leo DiCaprio girlfriend""model Vittoria Ceretti I need to find out Vittoria Ceretti\'s age Action: Search Action Input: ""Vittoria Ceretti age""25 years I need to calculate 25 raised to the 0.43 power Action: Calculator Action Input: 25^0.43Answer: 3.991298452658078 I now know the final answer Final Answer: Leo DiCaprio\'s girlfriend is Vittoria Ceretti and her current age raised to the 0.43 power is 3.991298452658078. > Finished chain. {\'input\': ""Who is Leo DiCaprio\'s girlfriend? What is her current age raised to the 0.43 power?"", \'output\': ""Leo DiCaprio\'s girlfriend is Vittoria Ceretti and her current age raised to the 0.43 power is 3.991298452658078.""} ``` ## Using ZeroShotReactAgent We will now show how to use the agent with an off-the-shelf agent implementation ```python agent_executor = initialize_agent( tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True ) ``` ```python agent_executor.invoke( { ""input"": ""Who is Leo DiCaprio\'s girlfriend? What is her current age raised to the 0.43 power?"" } ) ``` ```text > Entering new AgentExecutor chain... I need to find out who Leo DiCaprio\'s girlfriend is and then calculate her age raised to the 0.43 power. Action: Search Action Input: ""Leo DiCaprio girlfriend"" Observation: model Vittoria Ceretti Thought: I need to find out Vittoria Ceretti\'s age Action: Search Action Input: ""Vittoria Ceretti age"" Observation: 25 years Thought: I need to calculate 25 raised to the 0.43 power Action: Calculator Action Input: 25^0.43 Observation: Answer: 3.991298452658078 Thought: I now know the final answer Final Answer: Leo DiCaprio\'s girlfriend is Vittoria Ceretti and her current age raised to the 0.43 power is 3.991298452658078. > Finished chain. {\'input\': ""Who is Leo DiCaprio\'s girlfriend? What is her current age raised to the 0.43 power?"", \'output\': ""Leo DiCaprio\'s girlfriend is Vittoria Ceretti and her current age raised to the 0.43 power is 3.991298452658078.""} ``` ## Using chat models You can also create ReAct agents that use chat models instead of LLMs as the agent driver. The main difference here is a different prompt. We will use JSON to encode the agent\'s actions (chat models are a bit tougher to steet, so using JSON helps to enforce the output format). ```python from langchain.chat_models import ChatOpenAI ``` ```python chat_model = ChatOpenAI(temperature=0) ``` ```python prompt = hub.pull(""hwchase17/react-json"") prompt = prompt.partial( tools=render_text_description(tools), tool_names="", "".join([t.name for t in tools]), ) ``` ```python chat_model_with_stop = chat_model.bind(stop=[""\\nObservation""]) ``` ```python from langchain.agents.output_parsers import ReActJsonSingleInputOutputParser ``` ```python agent = ( { ""input"": lambda x: x[""input""], ""agent_scratchpad"": lambda x: format_log_to_str(x[""intermediate_steps""]), } | prompt | chat_model_with_stop | ReActJsonSingleInputOutputParser() ) ``` ```python agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True) ``` ```python agent_executor.invoke( { ""input"": ""Who is Leo DiCaprio\'s girlfriend? What is her current age raised to the 0.43 power?"" } ) ``` We can also use an off-the-shelf agent class ```python agent = initialize_agent( tools, chat_model, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True ) agent.run( ""Who is Leo DiCaprio\'s girlfriend? What is her current age raised to the 0.43 power?"" ) ``` - [Using LCEL](#using-lcel) - [Using ZeroShotReactAgent](#using-zeroshotreactagent) - [Using chat models](#using-chat-models)', ""Agent Types | Agent Types Agents use an LLM to determine which actions to take and in what order. An action can either be using a tool and observing its output, or returning a response to the user. Here are the agents available in LangChain. ## Zero-shot ReAct This agent uses the [ReAct]( framework to determine which tool to use based solely on the tool's description. Any number of tools can be provided. This agent requires that a description is provided for each tool. **Note**: This is the most general purpose action agent. ## Structured input ReAct The structured tool chat agent is capable of using multi-input tools. Older agents are configured to specify an action input as a single string, but this agent can use a tools' argument schema to create a structured action input. This is useful for more complex tool usage, like precisely navigating around a browser. ## OpenAI Functions Certain OpenAI models (like gpt-3.5-turbo-0613 and gpt-4-0613) have been explicitly fine-tuned to detect when a function should be called and respond with the inputs that should be passed to the function. The OpenAI Functions Agent is designed to work with these models. ## Conversational This agent is designed to be used in conversational settings. The prompt is designed to make the agent helpful and conversational. It uses the ReAct framework to decide which tool to use, and uses memory to remember the previous conversation interactions. ## Self-ask with search This agent utilizes a single tool that should be named `Intermediate Answer`. This tool should be able to look up factual answers to questions. This agent is equivalent to the original [self-ask with search paper]( where a Google search API was provided as the tool. ## ReAct document store This agent uses the ReAct framework to interact with a docstore. Two tools must be provided: a `Search` tool and a `Lookup` tool (they must be named exactly as so). The `Search` tool should search for a document, while the `Lookup` tool should look up a term in the most recently found document. This agent is equivalent to the original [ReAct paper]( specifically the Wikipedia example. - [Zero-shot ReAct](#zero-shot-react) - [Structured input ReAct](#structured-input-react) - [OpenAI Functions](#openai-functions) - [Conversational](#conversational) - [Self-ask with search](#self-ask-with-search) - [ReAct document store](#react-document-store)"", ""langchain.agents.agent_types.AgentType LangChain 0.0.337 API Experimental Python Docs Toggle Menu PrevUp Next LangChain 0.0.337 langchain.agents.agent_types.AgentType langchain.agents.agent_types.AgentType class langchain.agents.agent_types.AgentType(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)[source] An enum for agent types. See documentation: ZERO_SHOT_REACT_DESCRIPTION = 'zero-shot-react-description' A zero shot agent that does a reasoning step before acting. REACT_DOCSTORE = 'react-docstore' A zero shot agent that does a reasoning step before acting. This agent has access to a document store that allows it to look up relevant information to answering the question. SELF_ASK_WITH_SEARCH = 'self-ask-with-search' An agent that breaks down a complex question into a series of simpler questions. This agent uses a search tool to look up answers to the simpler questions in order to answer the original complex question. CONVERSATIONAL_REACT_DESCRIPTION = 'conversational-react-description' CHAT_ZERO_SHOT_REACT_DESCRIPTION = 'chat-zero-shot-react-description' A zero shot agent that does a reasoning step before acting. This agent is designed to be used in conjunction CHAT_CONVERSATIONAL_REACT_DESCRIPTION = 'chat-conversational-react-description' STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION = 'structured-chat-zero-shot-react-description' An zero-shot react agent optimized for chat models. This agent is capable of invoking tools that have multiple inputs. OPENAI_FUNCTIONS = 'openai-functions' An agent optimized for using open AI functions. OPENAI_MULTI_FUNCTIONS = 'openai-multi-functions' Examples using AgentType ChatGPT Plugins Google Serper Human as a tool Yahoo Finance News AWS Lambda Google Drive OpenWeatherMap Search Tools Eleven Labs Text2Speech Zapier Natural Language Actions ArXiv Metaphor Search GraphQL Eden AI Shell (bash) Zep Memory Xata chat memory Dynamodb Chat Message History LLMonitor Argilla Streamlit Aim Weights & Biases MLflow Flyte WandB Tracing ClearML Log, Trace, and Monitor Portkey CSV Jira Document Comparison Python Azure Cognitive Services SQL Database Natural Language APIs Gmail Airbyte Question Answering Github Google Drive tool AINetwork PlayWright Browser Office365 Pandas Dataframe MultiOn Amadeus Gitlab Bittensor Amazon API Gateway Debugging LangSmith Walkthrough Hugging Face Prompt Injection Identification Comparing Chain Outputs Agent Trajectory Agents Multi-modal outputs: Image & Text Agent Debates with Tools Set env var OPENAI_API_KEY or load from a .env file SQL Multiple callback handlers Multi-Input Tools Defining Custom Tools Tool Input Schema Human-in-the-loop Tool Validation Self-ask with search ReAct document store OpenAI Multi Functions Agent Combine agents and vector stores Access intermediate steps Handle parsing errors Running Agent as an Iterator Timeouts for agents Streaming final agent output Add Memory to OpenAI Functions Agent Cap the max number of iterations Custom functions with OpenAI Functions Agent Async API Use ToolKits with OpenAI Functions Human input chat model Fake LLM Tracking token usage Human input LLM 2023, Harrison Chase. Last updated on Nov 17, 2023. Show this page source""]",ReAct stands for Reasoning and Computing with Tools. It's a framework used in LangChain to enable agents to reason about which tools to use based on the input and then execute those tools to achieve a goal. This framework helps in automating complex tasks by breaking them down into manageable steps and using available tools effectively.,ReAct in LangChain refers to a type of agent that 'Reasons' and 'Act's in a loop.,0.9999999999666667,1.0,0.8571428571428571,0.02132081946882163,0.2162162162162162
54,What are intermediate steps in langchain?,"['Custom Trajectory Evaluator | Custom Trajectory Evaluator []( You can make your own custom trajectory evaluators by inheriting from the [AgentTrajectoryEvaluator]( class and overwriting the `_evaluate_agent_trajectory` (and `_aevaluate_agent_action`) method. In this example, you will make a simple trajectory evaluator that uses an LLM to determine if any actions were unnecessary. ```python from typing import Any, Optional, Sequence, Tuple from langchain.chains import LLMChain from langchain.chat_models import ChatOpenAI from langchain.evaluation import AgentTrajectoryEvaluator from langchain.schema import AgentAction class StepNecessityEvaluator(AgentTrajectoryEvaluator): """"""Evaluate the perplexity of a predicted string."""""" def __init__(self) -> None: llm = ChatOpenAI(model=""gpt-4"", temperature=0.0) template = """"""Are any of the following steps unnecessary in answering {input}? Provide the verdict on a new line as a single ""Y"" for yes or ""N"" for no. DATA ------ Steps: {trajectory} ------ Verdict:"""""" self.chain = LLMChain.from_string(llm, template) def _evaluate_agent_trajectory( self, *, prediction: str, input: str, agent_trajectory: Sequence[Tuple[AgentAction, str]], reference: Optional[str] = None, **kwargs: Any, ) -> dict: vals = [ f""{i}: Action=[{action.tool}] returned observation = [{observation}]"" for i, (action, observation) in enumerate(agent_trajectory) ] trajectory = ""\\n"".join(vals) response = self.chain.run(dict(trajectory=trajectory, input=input), **kwargs) decision = response.split(""\\n"")[-1].strip() score = 1 if decision == ""Y"" else 0 return {""score"": score, ""value"": decision, ""reasoning"": response} ``` The example above will return a score of 1 if the language model predicts that any of the actions were unnecessary, and it returns a score of 0 if all of them were predicted to be necessary. It returns the string \'decision\' as the \'value\', and includes the rest of the generated text as \'reasoning\' to let you audit the decision. You can call this evaluator to grade the intermediate steps of your agent\'s trajectory. ```python evaluator = StepNecessityEvaluator() evaluator.evaluate_agent_trajectory( prediction=""The answer is pi"", input=""What is today?"", agent_trajectory=[ ( AgentAction(tool=""ask"", tool_input=""What is today?"", log=""""), ""tomorrow\'s yesterday"", ), ( AgentAction(tool=""check_tv"", tool_input=""Watch tv for half hour"", log=""""), ""bzzz"", ), ], ) ``` ```text {\'score\': 1, \'value\': \'Y\', \'reasoning\': \'Y\'} ```', 'Retrieval-augmented generation (RAG) | Retrieval-augmented generation (RAG) []( ## Overview ### What is RAG? RAG is a technique for augmenting LLM knowledge with additional, often private or real-time, data. LLMs can reason about wide-ranging topics, but their knowledge is limited to the public data up to a specific point in time that they were trained on. If you want to build AI applications that can reason about private data or data introduced after a model\'s cutoff date, you need to augment the knowledge of the model with the specific information it needs. The process of bringing the appropriate information and inserting it into the model prompt is known as Retrieval Augmented Generation (RAG). ### What\'s in this guide? LangChain has a number of components specifically designed to help build RAG applications. To familiarize ourselves with these, we\'ll build a simple question-answering application over a text data source. Specifically, we\'ll build a QA bot over the [LLM Powered Autonomous Agents]( blog post by Lilian Weng. Along the way we\'ll go over a typical QA architecture, discuss the relevant LangChain components, and highlight additional resources for more advanced QA techniques. We\'ll also see how LangSmith can help us trace and understand our application. LangSmith will become increasingly helpful as our application grows in complexity. **Note** Here we focus on RAG for unstructured data. Two RAG use cases which we cover elsewhere are: - [QA over structured data](/docs/use_cases/qa_structured/sql) (e.g., SQL) - [QA over code](/docs/use_cases/question_answering/code_understanding) (e.g., Python) ## Architecture A typical RAG application has two main components: **Indexing**: a pipeline for ingesting data from a source and indexing it. _This usually happen offline._ **Retrieval and generation**: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model. The most common full sequence from raw data to answer looks like: #### Indexing 1. **Load**: First we need to load our data. We\'ll use [DocumentLoaders](/docs/modules/data_connection/document_loaders/) for this. 2. **Split**: [Text splitters](/docs/modules/data_connection/document_transformers/) break large `Documents` into smaller chunks. This is useful both for indexing data and for passing it in to a model, since large chunks are harder to search over and won\'t in a model\'s finite context window. 3. **Store**: We need somewhere to store and index our splits, so that they can later be searched over. This is often done using a [VectorStore](/docs/modules/data_connection/vectorstores/) and [Embeddings](/docs/modules/data_connection/text_embedding/) model. #### Retrieval and generation 1. **Retrieve**: Given a user input, relevant splits are retrieved from storage using a [Retriever](/docs/modules/data_connection/retrievers/). 2. **Generate**: A [ChatModel](/docs/modules/model_io/chat_models) / [LLM](/docs/modules/model_io/llms/) produces an answer using a prompt that includes the question and the retrieved data ![flow.jpeg](/assets/images/qa_flow-9fbd91de9282eb806bda1c6db501ecec.jpeg) ## Setup ### Dependencies We\'ll use an OpenAI chat model and embeddings and a Chroma vector store in this walkthrough, but everything shown here works with any [ChatModel](/docs/integrations/chat/) or [LLM](/docs/integrations/llms/), [Embeddings](/docs/integrations/text_embedding/), and [VectorStore](/docs/integrations/vectorstores/) or [Retriever](/docs/integrations/retrievers). We\'ll use the following packages: ```bash pip install -U langchain openai chromadb langchainhub bs4 ``` We need to set environment variable `OPENAI_API_KEY`, which can be done directly or loaded from a `.env` file like so: ```python import getpass import os os.environ[""OPENAI_API_KEY""] = getpass.getpass() # import dotenv # dotenv.load_dotenv() ``` ### LangSmith Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with [LangSmith]( Note that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces: ```python os.environ[""LANGCHAIN_TRACING_V2""] = ""true"" os.environ[""LANGCHAIN_API_KEY""] = getpass.getpass() ``` ## Quickstart Suppose we want to build a QA app over the [LLM Powered Autonomous Agents]( blog post by Lilian Weng. We can create a simple pipeline for this in ~20 lines of code: ```python import bs4 from langchain import hub from langchain.chat_models import ChatOpenAI from langchain.document_loaders import WebBaseLoader from langchain.embeddings import OpenAIEmbeddings from langchain.schema import StrOutputParser from langchain.schema.runnable import RunnablePassthrough from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain.vectorstores import Chroma ``` ```python loader = WebBaseLoader( web_paths=("" bs_kwargs=dict( parse_only=bs4.SoupStrainer( class_=(""post-content"", ""post-title"", ""post-header"") ) ), ) docs = loader.load() text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200) splits = text_splitter.split_documents(docs) vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings()) retriever = vectorstore.as_retriever() prompt = hub.pull(""rlm/rag-prompt"") llm = ChatOpenAI(model_name=""gpt-3.5-turbo"", temperature=0) def format_docs(docs): return ""\\n\\n"".join(doc.page_content for doc in docs) rag_chain = ( {""context"": retriever | format_docs, ""question"": RunnablePassthrough()} | prompt | llm | StrOutputParser() ) ``` ```python rag_chain.invoke(""What is Task Decomposition?"") ``` ```text \'Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It can be done through prompting techniques like Chain of Thought or Tree of Thoughts, or by using task-specific instructions or human inputs. Task decomposition helps agents plan ahead and manage complicated tasks more effectively.\' ``` ```python # cleanup vectorstore.delete_collection() ``` Check out the [LangSmith trace]( ## Detailed walkthrough Let\'s go through the above code step-by-step to really understand what\'s going on. ## Step 1. Load We need to first load the blog post contents. We can use `DocumentLoader`s for this, which are objects that load in data from a source as `Documents`. A `Document` is an object with `page_content` (str) and `metadata` (dict) attributes. In this case we\'ll use the `WebBaseLoader`, which uses `urllib` and `BeautifulSoup` to load and parse the passed in web urls, returning one `Document` per url. We can customize the html -> text parsing by passing in parameters to the `BeautifulSoup` parser via `bs_kwargs` (see [BeautifulSoup docs]( In this case only HTML tags with class ""post-content"", ""post-title"", or ""post-header"" are relevant, so we\'ll remove all others. ```python from langchain.document_loaders import WebBaseLoader loader = WebBaseLoader( web_paths=("" bs_kwargs={ ""parse_only"": bs4.SoupStrainer( class_=(""post-content"", ""post-title"", ""post-header"") ) }, ) docs = loader.load() ``` ```python len(docs[0].page_content) ``` ```text 42824 ``` ```python print(docs[0].page_content[:500]) ``` ```text LLM Powered Autonomous Agents Date: June 23, 2023 | Estimated Reading Time: 31 min | Author: Lilian Weng Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver. Agent System Overview# In ``` ### Go deeper `DocumentLoader`: Object that load data from a source as `Documents`. - [Docs](/docs/modules/data_connection/document_loaders/): Further documentation on how to use `DocumentLoader`s. - [Integrations](/docs/integrations/document_loaders/): Find the relevant `DocumentLoader` integration (of the > 160 of them) for your use case. ## Step 2. Split Our loaded document is over 42k characters long. This is too long to fit in the context window of many models. And even for those models that could fit the full post in their context window, empirically models struggle to find the relevant context in very long prompts. So we\'ll split the `Document` into chunks for embedding and vector storage. This should help us retrieve only the most relevant bits of the blog post at run time. In this case we\'ll split our documents into chunks of 1000 characters with 200 characters of overlap between chunks. The overlap helps mitigate the possibility of separating a statement from important context related to it. We use the `RecursiveCharacterTextSplitter`, which will (recursively) split the document using common separators (like new lines) until each chunk is the appropriate size. ```python from langchain.text_splitter import RecursiveCharacterTextSplitter text_splitter = RecursiveCharacterTextSplitter( chunk_size=1000, chunk_overlap=200, add_start_index=True ) all_splits = text_splitter.split_documents(docs) ``` ```python len(all_splits) ``` ```text 66 ``` ```python len(all_splits[0].page_content) ``` ```text 969 ``` ```python all_splits[10].metadata ``` ```text {\'source\': \' \'start_index\': 7056} ``` ### Go deeper `DocumentSplitter`: Object that splits a list of `Document`s into smaller chunks. Subclass of `DocumentTransformer`s. - Explore `Context-aware splitters`, which keep the location (""context"") of each split in the original `Document`:- [Markdown files](/docs/use_cases/question_answering/document-context-aware-QA) - [Code (py or js)](/docs/use_cases/question_answering/docs/integrations/document_loaders/source_code) - [Scientific papers](/docs/integrations/document_loaders/grobid) `DocumentTransformer`: Object that performs a transformation on a list of `Document`s. - [Docs](/docs/modules/data_connection/document_transformers/): Further documentation on how to use `DocumentTransformer`s - [Integrations](/docs/integrations/document_transformers/) ## Step 3. Store Now that we\'ve got 66 text chunks in memory, we need to store and index them so that we can search them later in our RAG app. The most common way to do this is to embed the contents of each document split and upload those embeddings to a vector store. Then, when we want to search over our splits, we take the search query, embed it as well, and perform some sort of ""similarity"" search to identify the stored splits with the most similar embeddings to our query embedding. The simplest similarity measure is cosine similarity we measure the cosine of the angle between each pair of embeddings (which are just very high dimensional vectors). We can embed and store all of our document splits in a single command using the `Chroma` vector store and `OpenAIEmbeddings` model. ```python from langchain.embeddings import OpenAIEmbeddings from langchain.vectorstores import Chroma vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings()) ``` ### Go deeper `Embeddings`: Wrapper around a text embedding model, used for converting text to embeddings. - [Docs](/docs/modules/data_connection/text_embedding): Further documentation on the interface. - [Integrations](/docs/integrations/text_embedding/): Browse the > 30 text embedding integrations `VectorStore`: Wrapper around a vector database, used for storing and querying embeddings. - [Docs](/docs/modules/data_connection/vectorstores/): Further documentation on the interface. - [Integrations](/docs/integrations/vectorstores/): Browse the > 40 `VectorStore` integrations. This completes the **Indexing** portion of the pipeline. At this point we have an query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question: ## Step 4. Retrieve Now let\'s write the actual application logic. We want to create a simple application that let\'s the user ask a question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and finally returns an answer. LangChain defines a `Retriever` interface which wraps an index that can return relevant documents given a string query. All retrievers implement a common method `get_relevant_documents()` (and its asynchronous variant `aget_relevant_documents()`). The most common type of `Retriever` is the `VectorStoreRetriever`, which uses the similarity search capabilities of a vector store to facillitate retrieval. Any `VectorStore` can easily be turned into a `Retriever`: ```python retriever = vectorstore.as_retriever(search_type=""similarity"", search_kwargs={""k"": 6}) ``` ```python retrieved_docs = retriever.get_relevant_documents( ""What are the approaches to Task Decomposition?"" ) ``` ```python len(retrieved_docs) ``` ```text 6 ``` ```python print(retrieved_docs[0].page_content) ``` ```text Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote. Task decomposition can be done (1) by LLM with simple prompting like ""Steps for XYZ.\\n1."", ""What are the subgoals for achieving XYZ?"", (2) by using task-specific instructions; e.g. ""Write a story outline."" for writing a novel, or (3) with human inputs. ``` ### Go deeper Vector stores are commonly used for retrieval, but there are plenty of other ways to do retrieval. `Retriever`: An object that returns `Document`s given a text query - [Docs](/docs/modules/data_connection/retrievers/): Further documentation on the interface and built-in retrieval techniques. Some of which include:- `MultiQueryRetriever` [generates variants of the input question](/docs/modules/data_connection/retrievers/MultiQueryRetriever) to improve retrieval hit rate. - `MultiVectorRetriever` (diagram below) instead generates [variants of the embeddings](/docs/modules/data_connection/retrievers/multi_vector), also in order to improve retrieval hit rate. - `Max marginal relevance` selects for [relevance and diversity]( among the retrieved documents to avoid passing in duplicate context. - Documents can be filtered during vector store retrieval using [metadata filters](/docs/use_cases/question_answering/document-context-aware-QA). - [Integrations](/docs/integrations/retrievers/): Integrations with retrieval services. ## Step 5. Generate Let\'s put it all together into a chain that takes a question, retrieves relevant documents, constructs a prompt, passes that to a model, and parses the output. We\'ll use the gpt-3.5-turbo OpenAI chat model, but any LangChain `LLM` or `ChatModel` could be substituted in. ```python from langchain.chat_models import ChatOpenAI llm = ChatOpenAI(model_name=""gpt-3.5-turbo"", temperature=0) ``` We\'ll use a prompt for RAG that is checked into the LangChain prompt hub ([here]( ```python from langchain import hub prompt = hub.pull(""rlm/rag-prompt"") ``` ```python print( prompt.invoke( {""context"": ""filler context"", ""question"": ""filler question""} ).to_string() ) ``` ```text Human: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise. Question: filler question Context: filler context Answer: ``` We\'ll use the [LCEL Runnable]( protocol to define the chain, allowing us to - pipe together components and functions in a transparent way - automatically trace our chain in LangSmith - get streaming, async, and batched calling out of the box ```python from langchain.schema import StrOutputParser from langchain.schema.runnable import RunnablePassthrough def format_docs(docs): return ""\\n\\n"".join(doc.page_content for doc in docs) rag_chain = ( {""context"": retriever | format_docs, ""question"": RunnablePassthrough()} | prompt | llm | StrOutputParser() ) ``` ```python for chunk in rag_chain.stream(""What is Task Decomposition?""): print(chunk, end="""", flush=True) ``` ```text Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It can be done through methods like Chain of Thought (CoT) or Tree of Thoughts, which involve dividing the task into manageable subtasks and exploring multiple reasoning possibilities at each step. Task decomposition can be performed by AI models with prompting, task-specific instructions, or human inputs. ``` Check out the [LangSmith trace]( ### Go deeper #### Choosing LLMs `ChatModel`: An LLM-backed chat model wrapper. Takes in a sequence of messages and returns a message. - [Docs](/docs/modules/model_io/chat/) - [Integrations](/docs/integrations/chat/): Explore over 25 `ChatModel` integrations. `LLM`: A text-in-text-out LLM. Takes in a string and returns a string. - [Docs](/docs/modules/model_io/llms) - [Integrations](/docs/integrations/llms): Explore over 75 `LLM` integrations. See a guide on RAG with locally-running models [here](/docs/modules/use_cases/question_answering/local_retrieval_qa). #### Customizing the prompt As shown above, we can load prompts (e.g., [this RAG prompt]( from the prompt hub. The prompt can also be easily customized: ```python from langchain.prompts import PromptTemplate template = """"""Use the following pieces of context to answer the question at the end. If you don\'t know the answer, just say that you don\'t know, don\'t try to make up an answer. Use three sentences maximum and keep the answer as concise as possible. Always say ""thanks for asking!"" at the end of the answer. {context} Question: {question} Helpful Answer:"""""" rag_prompt_custom = PromptTemplate.from_template(template) rag_chain = ( {""context"": retriever | format_docs, ""question"": RunnablePassthrough()} | rag_prompt_custom | llm | StrOutputParser() ) rag_chain.invoke(""What is Task Decomposition?"") ``` ```text \'Task decomposition is the process of breaking down a complex task into smaller and simpler steps. It can be done through techniques like Chain of Thought (CoT) or Tree of Thoughts, which involve dividing the problem into multiple thought steps and generating multiple thoughts per step. Task decomposition helps in enhancing model performance and understanding the thinking process of the model. Thanks for asking!\' ``` Check out the [LangSmith trace]( ### Adding sources With LCEL it\'s easy to return the retrieved documents or certain source metadata from the documents: ```python from operator import itemgetter from langchain.schema.runnable import RunnableMap rag_chain_from_docs = ( { ""context"": lambda input: format_docs(input[""documents""]), ""question"": itemgetter(""question""), } | rag_prompt_custom | llm | StrOutputParser() ) rag_chain_with_source = RunnableMap( {""documents"": retriever, ""question"": RunnablePassthrough()} ) | { ""documents"": lambda input: [doc.metadata for doc in input[""documents""]], ""answer"": rag_chain_from_docs, } rag_chain_with_source.invoke(""What is Task Decomposition"") ``` ```text {\'documents\': [{\'source\': \' \'start_index\': 1585}, {\'source\': \' \'start_index\': 2192}, {\'source\': \' \'start_index\': 17804}, {\'source\': \' \'start_index\': 17414}, {\'source\': \' \'start_index\': 29630}, {\'source\': \' \'start_index\': 19373}], \'answer\': \'Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It involves transforming big tasks into multiple manageable tasks, allowing for a more systematic and organized approach to problem-solving. Thanks for asking!\'} ``` Check out the [LangSmith trace]( Adding memory Suppose we want to create a stateful application that remembers past user inputs. There are two main things we need to do to support this. 1. Add a messages placeholder to our chain which allows us to pass in historical messages 2. Add a chain that takes the latest user query and reformulates it in the context of the chat history into a standalone question that can be passed to our retriever. Let\'s start with 2. We can build a ""condense question"" chain that looks something like this: ```python from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder condense_q_system_prompt = """"""Given a chat history and the latest user question \\ which might reference the chat history, formulate a standalone question \\ which can be understood without the chat history. Do NOT answer the question, \\ just reformulate it if needed and otherwise return it as is."""""" condense_q_prompt = ChatPromptTemplate.from_messages( [ (""system"", condense_q_system_prompt), MessagesPlaceholder(variable_name=""chat_history""), (""human"", ""{question}""), ] ) condense_q_chain = condense_q_prompt | llm | StrOutputParser() ``` ```python from langchain.schema.messages import AIMessage, HumanMessage condense_q_chain.invoke( { ""chat_history"": [ HumanMessage(content=""What does LLM stand for?""), AIMessage(content=""Large language model""), ], ""question"": ""What is meant by large"", } ) ``` ```text \'What is the definition of ""large"" in the context of a language model?\' ``` ```python condense_q_chain.invoke( { ""chat_history"": [ HumanMessage(content=""What does LLM stand for?""), AIMessage(content=""Large language model""), ], ""question"": ""How do transformers work"", } ) ``` ```text \'How do transformer models function?\' ``` And now we can build our full QA chain. Notice we add some routing functionality to only run the ""condense question chain"" when our chat history isn\'t empty. ```python qa_system_prompt = """"""You are an assistant for question-answering tasks. \\ Use the following pieces of retrieved context to answer the question. \\ If you don\'t know the answer, just say that you don\'t know. \\ Use three sentences maximum and keep the answer concise.\\ {context}"""""" qa_prompt = ChatPromptTemplate.from_messages( [ (""system"", qa_system_prompt), MessagesPlaceholder(variable_name=""chat_history""), (""human"", ""{question}""), ] ) def condense_question(input: dict): if input.get(""chat_history""): return condense_q_chain else: return input[""question""] rag_chain = ( RunnablePassthrough.assign(context=condense_question | retriever | format_docs) | qa_prompt | llm ) ``` ```python chat_history = [] question = ""What is Task Decomposition?"" ai_msg = rag_chain.invoke({""question"": question, ""chat_history"": chat_history}) chat_history.extend([HumanMessage(content=question), ai_msg]) second_question = ""What are common ways of doing it?"" rag_chain.invoke({""question"": second_question, ""chat_history"": chat_history}) ``` ```text AIMessage(content=\'Common ways of task decomposition include:\\n\\n1. Using Chain of Thought (CoT): CoT is a prompting technique that instructs the model to ""think step by step"" and decompose complex tasks into smaller and simpler steps. It utilizes more test-time computation and sheds light on the model\\\'s thinking process.\\n\\n2. Prompting with LLM: Language Model (LLM) can be used to prompt the model with simple instructions like ""Steps for XYZ"" or ""What are the subgoals for achieving XYZ?"" This allows the model to generate a sequence of subtasks or thought steps.\\n\\n3. Task-specific instructions: For certain tasks, task-specific instructions can be provided to guide the model in decomposing the task. For example, for writing a novel, the instruction ""Write a story outline"" can be given to break down the task into manageable steps.\\n\\n4. Human inputs: In some cases, human inputs can be used to assist in task decomposition. Humans can provide their expertise and knowledge to identify and break down complex tasks into smaller subtasks.\') ``` Check out the [LangSmith trace]( Here we\'ve gone over how to add chain logic for incorporating historical outputs. But how do we actually store and retrieve historical outputs for different sessions? For that check out the LCEL [How to add message history (memory)](/docs/expression_language/how_to/message_history) page. ## Next steps That\'s a lot of content we\'ve covered in a short amount of time. There\'s plenty of nuances, features, integrations, etc to explore in each of the above sections. Aside from the sources mentioned above, good next steps include: - Reading up on more advanced retrieval techniques in the [Retrievers](/docs/modules/data_connection/retrievers/) section. - Learning about the LangChain [Indexing API](/docs/modules/data_connection/indexing), which helps repeatedly sync data sources and vector stores without redundant computation or storage. - Exploring RAG [LangChain Templates](/docs/templates/#-advanced-retrieval), which are reference applications that can easily be deployed with [LangServe](/docs/langserve). - Learning about [evaluating RAG applications with LangSmith]( - [Overview](#overview)- [What is RAG?](#what-is-rag) - [What\'s in this guide?](#whats-in-this-guide) - [Architecture](#architecture) - [Setup](#setup)- [Dependencies](#dependencies) - [LangSmith](#langsmith) - [Quickstart](#quickstart) - [Detailed walkthrough](#detailed-walkthrough) - [Step 1. Load](#step-1-load)- [Go deeper](#go-deeper) - [Step 2. Split](#step-2-split)- [Go deeper](#go-deeper-1) - [Step 3. Store](#step-3-store)- [Go deeper](#go-deeper-2) - [Step 4. Retrieve](#step-4-retrieve)- [Go deeper](#go-deeper-3) - [Step 5. Generate](#step-5-generate)- [Go deeper](#go-deeper-4) - [Adding sources](#adding-sources) - [Adding memory](#adding-memory) - [Next steps](#next-steps)', 'Memgraph QA chain | Memgraph QA chain This notebook shows how to use LLMs to provide a natural language interface to a [Memgraph]( database. To complete this tutorial, you will need [Docker]( and [Python 3.x]( installed. To follow along with this tutorial, ensure you have a running Memgraph instance. You can download and run it in a local Docker container by executing the following script: ```text docker run \\ -it \\ -p 7687:7687 \\ -p 7444:7444 \\ -p 3000:3000 \\ -e MEMGRAPH=""--bolt-server-name-for-init=Neo4j/"" \\ -v mg_lib:/var/lib/memgraph memgraph/memgraph-platform ``` You will need to wait a few seconds for the database to start. If the process completes successfully, you should see something like this: ```text mgconsole X.X Connected to \'memgraph://127.0.0.1:7687\' Type :help for shell usage Quit the shell by typing Ctrl-D(eof) or :quit memgraph> ``` Now you can start playing with Memgraph! Begin by installing and importing all the necessary packages. We\'ll use the package manager called [pip]( along with the `--user` flag, to ensure proper permissions. If you\'ve installed Python 3.4 or a later version, pip is included by default. You can install all the required packages using the following command: ```python pip install langchain openai neo4j gqlalchemy --user ``` You can either run the provided code blocks in this notebook or use a separate Python file to experiment with Memgraph and LangChain. ```python import os from gqlalchemy import Memgraph from langchain.chains import GraphCypherQAChain from langchain.chat_models import ChatOpenAI from langchain.graphs import MemgraphGraph from langchain.prompts import PromptTemplate ``` We\'re utilizing the Python library [GQLAlchemy]( to establish a connection between our Memgraph database and Python script. To execute queries, we can set up a Memgraph instance as follows: ```python memgraph = Memgraph(host=""127.0.0.1"", port=7687) ``` ## Populating the database You can effortlessly populate your new, empty database using the Cypher query language. Don\'t worry if you don\'t grasp every line just yet, you can learn Cypher from the documentation [here]( Running the following script will execute a seeding query on the database, giving us data about a video game, including details like the publisher, available platforms, and genres. This data will serve as a basis for our work. ```python # Creating and executing the seeding query query = """""" MERGE (g:Game {name: ""Baldur\'s Gate 3""}) WITH g, [""PlayStation 5"", ""Mac OS"", ""Windows"", ""Xbox Series X/S""] AS platforms, [""Adventure"", ""Role-Playing Game"", ""Strategy""] AS genres FOREACH (platform IN platforms | MERGE (p:Platform {name: platform}) MERGE (g)-[:AVAILABLE_ON]->(p) ) FOREACH (genre IN genres | MERGE (gn:Genre {name: genre}) MERGE (g)-[:HAS_GENRE]->(gn) ) MERGE (p:Publisher {name: ""Larian Studios""}) MERGE (g)-[:PUBLISHED_BY]->(p); """""" memgraph.execute(query) ``` ## Refresh graph schema You\'re all set to instantiate the Memgraph-LangChain graph using the following script. This interface will allow us to query our database using LangChain, automatically creating the required graph schema for generating Cypher queries through LLM. ```python graph = MemgraphGraph(url=""bolt://localhost:7687"", username="""", password="""") ``` If necessary, you can manually refresh the graph schema as follows. ```python graph.refresh_schema() ``` To familiarize yourself with the data and verify the updated graph schema, you can print it using the following statement. ```python print(graph.schema) ``` ```text Node properties are the following: Node name: \'Game\', Node properties: [{\'property\': \'name\', \'type\': \'str\'}] Node name: \'Platform\', Node properties: [{\'property\': \'name\', \'type\': \'str\'}] Node name: \'Genre\', Node properties: [{\'property\': \'name\', \'type\': \'str\'}] Node name: \'Publisher\', Node properties: [{\'property\': \'name\', \'type\': \'str\'}] Relationship properties are the following: The relationships are the following: [\'(:Game)-[:AVAILABLE_ON]->(:Platform)\'] [\'(:Game)-[:HAS_GENRE]->(:Genre)\'] [\'(:Game)-[:PUBLISHED_BY]->(:Publisher)\'] ``` ## Querying the database To interact with the OpenAI API, you must configure your API key as an environment variable using the Python [os]( package. This ensures proper authorization for your requests. You can find more information on obtaining your API key [here]( ```python os.environ[""OPENAI_API_KEY""] = ""your-key-here"" ``` You should create the graph chain using the following script, which will be utilized in the question-answering process based on your graph data. While it defaults to GPT-3.5-turbo, you might also consider experimenting with other models like [GPT-4]( for notably improved Cypher queries and outcomes. We\'ll utilize the OpenAI chat, utilizing the key you previously configured. We\'ll set the temperature to zero, ensuring predictable and consistent answers. Additionally, we\'ll use our Memgraph-LangChain graph and set the verbose parameter, which defaults to False, to True to receive more detailed messages regarding query generation. ```python chain = GraphCypherQAChain.from_llm( ChatOpenAI(temperature=0), graph=graph, verbose=True, model_name=""gpt-3.5-turbo"" ) ``` Now you can start asking questions! ```python response = chain.run(""Which platforms is Baldur\'s Gate 3 available on?"") print(response) ``` ```text > Entering new GraphCypherQAChain chain... Generated Cypher: MATCH (g:Game {name: \'Baldur\\\'s Gate 3\'})-[:AVAILABLE_ON]->(p:Platform) RETURN p.name Full Context: [{\'p.name\': \'PlayStation 5\'}, {\'p.name\': \'Mac OS\'}, {\'p.name\': \'Windows\'}, {\'p.name\': \'Xbox Series X/S\'}] > Finished chain. Baldur\'s Gate 3 is available on PlayStation 5, Mac OS, Windows, and Xbox Series X/S. ``` ```python response = chain.run(""Is Baldur\'s Gate 3 available on Windows?"") print(response) ``` ```text > Entering new GraphCypherQAChain chain... Generated Cypher: MATCH (:Game {name: \'Baldur\\\'s Gate 3\'})-[:AVAILABLE_ON]->(:Platform {name: \'Windows\'}) RETURN true Full Context: [{\'true\': True}] > Finished chain. Yes, Baldur\'s Gate 3 is available on Windows. ``` ## Chain modifiers To modify the behavior of your chain and obtain more context or additional information, you can modify the chain\'s parameters. #### Return direct query results The `return_direct` modifier specifies whether to return the direct results of the executed Cypher query or the processed natural language response. ```python # Return the result of querying the graph directly chain = GraphCypherQAChain.from_llm( ChatOpenAI(temperature=0), graph=graph, verbose=True, return_direct=True ) ``` ```python response = chain.run(""Which studio published Baldur\'s Gate 3?"") print(response) ``` ```text > Entering new GraphCypherQAChain chain... Generated Cypher: MATCH (:Game {name: \'Baldur\\\'s Gate 3\'})-[:PUBLISHED_BY]->(p:Publisher) RETURN p.name > Finished chain. [{\'p.name\': \'Larian Studios\'}] ``` #### Return query intermediate steps The `return_intermediate_steps` chain modifier enhances the returned response by including the intermediate steps of the query in addition to the initial query result. ```python # Return all the intermediate steps of query execution chain = GraphCypherQAChain.from_llm( ChatOpenAI(temperature=0), graph=graph, verbose=True, return_intermediate_steps=True ) ``` ```python response = chain(""Is Baldur\'s Gate 3 an Adventure game?"") print(f""Intermediate steps: {response[\'intermediate_steps\']}"") print(f""Final response: {response[\'result\']}"") ``` ```text > Entering new GraphCypherQAChain chain... Generated Cypher: MATCH (g:Game {name: \'Baldur\\\'s Gate 3\'})-[:HAS_GENRE]->(genre:Genre {name: \'Adventure\'}) RETURN g, genre Full Context: [{\'g\': {\'name\': ""Baldur\'s Gate 3""}, \'genre\': {\'name\': \'Adventure\'}}] > Finished chain. Intermediate steps: [{\'query\': ""MATCH (g:Game {name: \'Baldur\\\\\'s Gate 3\'})-[:HAS_GENRE]->(genre:Genre {name: \'Adventure\'})\\nRETURN g, genre""}, {\'context\': [{\'g\': {\'name\': ""Baldur\'s Gate 3""}, \'genre\': {\'name\': \'Adventure\'}}]}] Final response: Yes, Baldur\'s Gate 3 is an Adventure game. ``` #### Limit the number of query results The `top_k` modifier can be used when you want to restrict the maximum number of query results. ```python # Limit the maximum number of results returned by query chain = GraphCypherQAChain.from_llm( ChatOpenAI(temperature=0), graph=graph, verbose=True, top_k=2 ) ``` ```python response = chain.run(""What genres are associated with Baldur\'s Gate 3?"") print(response) ``` ```text > Entering new GraphCypherQAChain chain... Generated Cypher: MATCH (:Game {name: \'Baldur\\\'s Gate 3\'})-[:HAS_GENRE]->(g:Genre) RETURN g.name Full Context: [{\'g.name\': \'Adventure\'}, {\'g.name\': \'Role-Playing Game\'}] > Finished chain. Baldur\'s Gate 3 is associated with the genres Adventure and Role-Playing Game. ``` # Advanced querying As the complexity of your solution grows, you might encounter different use-cases that require careful handling. Ensuring your application\'s scalability is essential to maintain a smooth user flow without any hitches. Let\'s instantiate our chain once again and attempt to ask some questions that users might potentially ask. ```python chain = GraphCypherQAChain.from_llm( ChatOpenAI(temperature=0), graph=graph, verbose=True, model_name=""gpt-3.5-turbo"" ) ``` ```python response = chain.run(""Is Baldur\'s Gate 3 available on PS5?"") print(response) ``` ```text > Entering new GraphCypherQAChain chain... Generated Cypher: MATCH (g:Game {name: \'Baldur\\\'s Gate 3\'})-[:AVAILABLE_ON]->(p:Platform {name: \'PS5\'}) RETURN g.name, p.name Full Context: [] > Finished chain. I\'m sorry, but I don\'t have the information to answer your question. ``` The generated Cypher query looks fine, but we didn\'t receive any information in response. This illustrates a common challenge when working with LLMs - the misalignment between how users phrase queries and how data is stored. In this case, the difference between user perception and the actual data storage can cause mismatches. Prompt refinement, the process of honing the model\'s prompts to better grasp these distinctions, is an efficient solution that tackles this issue. Through prompt refinement, the model gains increased proficiency in generating precise and pertinent queries, leading to the successful retrieval of the desired data. ### Prompt refinement To address this, we can adjust the initial Cypher prompt of the QA chain. This involves adding guidance to the LLM on how users can refer to specific platforms, such as PS5 in our case. We achieve this using the LangChain [PromptTemplate]( creating a modified initial prompt. This modified prompt is then supplied as an argument to our refined Memgraph-LangChain instance. ```python CYPHER_GENERATION_TEMPLATE = """""" Task:Generate Cypher statement to query a graph database. Instructions: Use only the provided relationship types and properties in the schema. Do not use any other relationship types or properties that are not provided. Schema: {schema} Note: Do not include any explanations or apologies in your responses. Do not respond to any questions that might ask anything else than for you to construct a Cypher statement. Do not include any text except the generated Cypher statement. If the user asks about PS5, Play Station 5 or PS 5, that is the platform called PlayStation 5. The question is: {question} """""" CYPHER_GENERATION_PROMPT = PromptTemplate( input_variables=[""schema"", ""question""], template=CYPHER_GENERATION_TEMPLATE ) ``` ```python chain = GraphCypherQAChain.from_llm( ChatOpenAI(temperature=0), cypher_prompt=CYPHER_GENERATION_PROMPT, graph=graph, verbose=True, model_name=""gpt-3.5-turbo"", ) ``` ```python response = chain.run(""Is Baldur\'s Gate 3 available on PS5?"") print(response) ``` ```text > Entering new GraphCypherQAChain chain... Generated Cypher: MATCH (g:Game {name: \'Baldur\\\'s Gate 3\'})-[:AVAILABLE_ON]->(p:Platform {name: \'PlayStation 5\'}) RETURN g.name, p.name Full Context: [{\'g.name\': ""Baldur\'s Gate 3"", \'p.name\': \'PlayStation 5\'}] > Finished chain. Yes, Baldur\'s Gate 3 is available on PlayStation 5. ``` Now, with the revised initial Cypher prompt that includes guidance on platform naming, we are obtaining accurate and relevant results that align more closely with user queries. This approach allows for further improvement of your QA chain. You can effortlessly integrate extra prompt refinement data into your chain, thereby enhancing the overall user experience of your app. - [Populating the database](#populating-the-database) - [Refresh graph schema](#refresh-graph-schema) - [Querying the database](#querying-the-database) - [Chain modifiers](#chain-modifiers)- [Prompt refinement](#prompt-refinement)']","Intermediate steps in LangChain refer to the various stages or components involved in a chain of operations, typically used for building complex workflows. These steps can include data processing, model interactions, and other transformations that contribute to the final output. Intermediate steps provide transparency and allow for tracing and debugging within the chain. They can be accessed and manipulated to customize the behavior of the overall workflow.",Intermediate stps represent previous agent actions and corresponding outputs. They are used to help the agent know what work has already been done in pursuit of a goal.,0.9999999999666667,0.0,,0.025590823010528185,0.1473684210526316
55,im getting a rate limit error for my llm - how do I have it automatically go to a different model?,"['Fallbacks | Fallbacks When working with language models, you may often encounter issues from the underlying APIs, whether these be rate limiting or downtime. Therefore, as you go to move your LLM applications into production it becomes more and more important to safeguard against these. That\'s why we\'ve introduced the concept of fallbacks. A **fallback** is an alternative plan that may be used in an emergency. Crucially, fallbacks can be applied not only on the LLM level but on the whole runnable level. This is important because often times different models require different prompts. So if your call to OpenAI fails, you don\'t just want to send the same prompt to Anthropic - you probably want to use a different prompt template and send a different version there. ## Fallback for LLM API Errors This is maybe the most common use case for fallbacks. A request to an LLM API can fail for a variety of reasons - the API could be down, you could have hit rate limits, any number of things. Therefore, using fallbacks can help protect against these types of things. IMPORTANT: By default, a lot of the LLM wrappers catch errors and retry. You will most likely want to turn those off when working with fallbacks. Otherwise the first wrapper will keep on retrying and not failing. ```python from langchain.chat_models import ChatAnthropic, ChatOpenAI ``` First, let\'s mock out what happens if we hit a RateLimitError from OpenAI ```python from unittest.mock import patch from openai.error import RateLimitError ``` ```python # Note that we set max_retries = 0 to avoid retrying on RateLimits, etc openai_llm = ChatOpenAI(max_retries=0) anthropic_llm = ChatAnthropic() llm = openai_llm.with_fallbacks([anthropic_llm]) ``` ```python # Let\'s use just the OpenAI LLm first, to show that we run into an error with patch(""openai.ChatCompletion.create"", side_effect=RateLimitError()): try: print(openai_llm.invoke(""Why did the chicken cross the road?"")) except: print(""Hit error"") ``` ```text Hit error ``` ```python # Now let\'s try with fallbacks to Anthropic with patch(""openai.ChatCompletion.create"", side_effect=RateLimitError()): try: print(llm.invoke(""Why did the chicken cross the road?"")) except: print(""Hit error"") ``` ```text content=\' I don\\\'t actually know why the chicken crossed the road, but here are some possible humorous answers:\\n\\n- To get to the other side!\\n\\n- It was too chicken to just stand there. \\n\\n- It wanted a change of scenery.\\n\\n- It wanted to show the possum it could be done.\\n\\n- It was on its way to a poultry farmers\\\' convention.\\n\\nThe joke plays on the double meaning of ""the other side"" - literally crossing the road to the other side, or the ""other side"" meaning the afterlife. So it\\\'s an anti-joke, with a silly or unexpected pun as the answer.\' additional_kwargs={} example=False ``` We can use our ""LLM with Fallbacks"" as we would a normal LLM. ```python from langchain.prompts import ChatPromptTemplate prompt = ChatPromptTemplate.from_messages( [ ( ""system"", ""You\'re a nice assistant who always includes a compliment in your response"", ), (""human"", ""Why did the {animal} cross the road""), ] ) chain = prompt | llm with patch(""openai.ChatCompletion.create"", side_effect=RateLimitError()): try: print(chain.invoke({""animal"": ""kangaroo""})) except: print(""Hit error"") ``` ```text content="" I don\'t actually know why the kangaroo crossed the road, but I can take a guess! Here are some possible reasons:\\n\\n- To get to the other side (the classic joke answer!)\\n\\n- It was trying to find some food or water \\n\\n- It was trying to find a mate during mating season\\n\\n- It was fleeing from a predator or perceived threat\\n\\n- It was disoriented and crossed accidentally \\n\\n- It was following a herd of other kangaroos who were crossing\\n\\n- It wanted a change of scenery or environment \\n\\n- It was trying to reach a new habitat or territory\\n\\nThe real reason is unknown without more context, but hopefully one of those potential explanations does the joke justice! Let me know if you have any other animal jokes I can try to decipher."" additional_kwargs={} example=False ``` ## Fallback for Sequences We can also create fallbacks for sequences, that are sequences themselves. Here we do that with two different models: ChatOpenAI and then normal OpenAI (which does not use a chat model). Because OpenAI is NOT a chat model, you likely want a different prompt. ```python # First let\'s create a chain with a ChatModel # We add in a string output parser here so the outputs between the two are the same type from langchain.schema.output_parser import StrOutputParser chat_prompt = ChatPromptTemplate.from_messages( [ ( ""system"", ""You\'re a nice assistant who always includes a compliment in your response"", ), (""human"", ""Why did the {animal} cross the road""), ] ) # Here we\'re going to use a bad model name to easily create a chain that will error chat_model = ChatOpenAI(model_name=""gpt-fake"") bad_chain = chat_prompt | chat_model | StrOutputParser() ``` ```python # Now lets create a chain with the normal OpenAI model from langchain.llms import OpenAI from langchain.prompts import PromptTemplate prompt_template = """"""Instructions: You should always include a compliment in your response. Question: Why did the {animal} cross the road?"""""" prompt = PromptTemplate.from_template(prompt_template) llm = OpenAI() good_chain = prompt | llm ``` ```python # We can now create a final chain which combines the two chain = bad_chain.with_fallbacks([good_chain]) chain.invoke({""animal"": ""turtle""}) ``` ```text \'\\n\\nAnswer: The turtle crossed the road to get to the other side, and I have to say he had some impressive determination.\' ``` ## Fallback for Long Inputs One of the big limiting factors of LLMs is their context window. Usually, you can count and track the length of prompts before sending them to an LLM, but in situations where that is hard/complicated, you can fallback to a model with a longer context length. ```python short_llm = ChatOpenAI() long_llm = ChatOpenAI(model=""gpt-3.5-turbo-16k"") llm = short_llm.with_fallbacks([long_llm]) ``` ```python inputs = ""What is the next number: "" + "", "".join([""one"", ""two""] * 3000) ``` ```python try: print(short_llm.invoke(inputs)) except Exception as e: print(e) ``` ```text This model\'s maximum context length is 4097 tokens. However, your messages resulted in 12012 tokens. Please reduce the length of the messages. ``` ```python try: print(llm.invoke(inputs)) except Exception as e: print(e) ``` ```text content=\'The next number in the sequence is two.\' additional_kwargs={} example=False ``` ## Fallback to Better Model Often times we ask models to output format in a specific format (like JSON). Models like GPT-3.5 can do this okay, but sometimes struggle. This naturally points to fallbacks - we can try with GPT-3.5 (faster, cheaper), but then if parsing fails we can use GPT-4. ```python from langchain.output_parsers import DatetimeOutputParser ``` ```python prompt = ChatPromptTemplate.from_template( ""what time was {event} (in %Y-%m-%dT%H:%M:%S.%fZ format - only return this value)"" ) ``` ```python # In this case we are going to do the fallbacks on the LLM + output parser level # Because the error will get raised in the OutputParser openai_35 = ChatOpenAI() | DatetimeOutputParser() openai_4 = ChatOpenAI(model=""gpt-4"") | DatetimeOutputParser() ``` ```python only_35 = prompt | openai_35 fallback_4 = prompt | openai_35.with_fallbacks([openai_4]) ``` ```python try: print(only_35.invoke({""event"": ""the superbowl in 1994""})) except Exception as e: print(f""Error: {e}"") ``` ```text Error: Could not parse datetime string: The Super Bowl in 1994 took place on January 30th at 3:30 PM local time. Converting this to the specified format (%Y-%m-%dT%H:%M:%S.%fZ) results in: 1994-01-30T15:30:00.000Z ``` ```python try: print(fallback_4.invoke({""event"": ""the superbowl in 1994""})) except Exception as e: print(f""Error: {e}"") ``` ```text 1994-01-30 15:30:00 ``` - [Fallback for LLM API Errors](#fallback-for-llm-api-errors) - [Fallback for Sequences](#fallback-for-sequences) - [Fallback for Long Inputs](#fallback-for-long-inputs) - [Fallback to Better Model](#fallback-to-better-model)', 'Reddit | Reddit [Reddit]( is an American social news aggregation, content rating, and discussion website. This loader fetches the text from the Posts of Subreddits or Reddit users, using the `praw` Python package. Make a [Reddit Application]( and initialize the loader with with your Reddit API credentials. ```python from langchain.document_loaders import RedditPostsLoader ``` ```python # !pip install praw ``` ```python # load using \'subreddit\' mode loader = RedditPostsLoader( client_id=""YOUR CLIENT ID"", client_secret=""YOUR CLIENT SECRET"", user_agent=""extractor by u/Master_Ocelot8179"", categories=[""new"", ""hot""], # List of categories to load posts from mode=""subreddit"", search_queries=[ ""investing"", ""wallstreetbets"", ], # List of subreddits to load posts from number_posts=20, # Default value is 10 ) # # or load using \'username\' mode # loader = RedditPostsLoader( # client_id=""YOUR CLIENT ID"", # client_secret=""YOUR CLIENT SECRET"", # user_agent=""extractor by u/Master_Ocelot8179"", # categories=[\'new\', \'hot\'], # mode = \'username\', # search_queries=[\'ga3far\', \'Master_Ocelot8179\'], # List of usernames to load posts from # number_posts=20 # ) # Note: Categories can be only of following value - ""controversial"" ""hot"" ""new"" ""rising"" ""top"" ``` ```python documents = loader.load() documents[:5] ``` ```text [Document(page_content=\'Hello, I am not looking for investment advice. I will apply my own due diligence. However, I am interested if anyone knows as a UK resident how fees and exchange rate differences would impact performance?\\n\\nI am planning to create a pie of index funds (perhaps UK, US, europe) or find a fund with a good track record of long term growth at low rates. \\n\\nDoes anyone have any ideas?\', metadata={\'post_subreddit\': \'r/investing\', \'post_category\': \'new\', \'post_title\': \'Long term retirement funds fees/exchange rate query\', \'post_score\': 1, \'post_id\': \'130pa6m\', \'post_url\': \' \'post_author\': Redditor(name=\'Badmanshiz\')}), Document(page_content=\'I much prefer the Roth IRA and would rather rollover my 401k to that every year instead of keeping it in the limited 401k options. But if I rollover, will I be able to continue contributing to my 401k? Or will that close my account? I realize that there are tax implications of doing this but I still think it is the better option.\', metadata={\'post_subreddit\': \'r/investing\', \'post_category\': \'new\', \'post_title\': \'Is it possible to rollover my 401k every year?\', \'post_score\': 3, \'post_id\': \'130ja0h\', \'post_url\': \' \'post_author\': Redditor(name=\'AnCap_Catholic\')}), Document(page_content=\'Have a general question? Want to offer some commentary on markets? Maybe you would just like to throw out a neat fact that doesn\\\'t warrant a self post? Feel free to post here! \\n\\nIf your question is ""I have $10,000, what do I do?"" or other ""advice for my personal situation"" questions, you should include relevant information, such as the following:\\n\\n* How old are you? What country do you live in? \\n* Are you employed/making income? How much? \\n* What are your objectives with this money? (Buy a house? Retirement savings?) \\n* What is your time horizon? Do you need this money next month? Next 20yrs? \\n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?) \\n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?) \\n* Any big debts (include interest rate) or expenses? \\n* And any other relevant financial information will be useful to give you a proper answer. \\n\\nPlease consider consulting our FAQ first - our [side bar]( also has useful resources. \\n\\nIf you are new to investing - please refer to Wiki - [Getting Started]( reading list in the wiki has a list of books ranging from light reading to advanced topics depending on your knowledge level. Link here - [Reading List]( the resources in the sidebar.\\n\\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered investment adviser if you need professional support before making any financial decisions!\', metadata={\'post_subreddit\': \'r/investing\', \'post_category\': \'new\', \'post_title\': \'Daily General Discussion and Advice Thread - April 27, 2023\', \'post_score\': 5, \'post_id\': \'130eszz\', \'post_url\': \' \'post_author\': Redditor(name=\'AutoModerator\')}), Document(page_content=""Based on recent news about salt battery advancements and the overall issues of lithium, I was wondering what would be feasible ways to invest into non-lithium based battery technologies? CATL is of course a choice, but the selection of brokers I currently have in my disposal don\'t provide HK stocks at all."", metadata={\'post_subreddit\': \'r/investing\', \'post_category\': \'new\', \'post_title\': \'Investing in non-lithium battery technologies?\', \'post_score\': 2, \'post_id\': \'130d6qp\', \'post_url\': \' \'post_author\': Redditor(name=\'-manabreak\')}), Document(page_content=\'Hello everyone,\\n\\nI would really like to invest in an ETF that follows spy or another big index, as I think this form of investment suits me best. \\n\\nThe problem is, that I live in Denmark where ETFs and funds are taxed annually on unrealised gains at quite a steep rate. This means that an ETF growing say 10% per year will only grow about 6%, which really ruins the long term effects of compounding interest.\\n\\nHowever stocks are only taxed on realised gains which is why they look more interesting to hold long term.\\n\\nI do not like the lack of diversification this brings, as I am looking to spend tonnes of time picking the right long term stocks.\\n\\nIt would be ideal to find a few stocks that over the long term somewhat follows the indexes. Does anyone have suggestions?\\n\\nI have looked at Nasdaq Inc. which quite closely follows Nasdaq 100. \\n\\nI really appreciate any help.\', metadata={\'post_subreddit\': \'r/investing\', \'post_category\': \'new\', \'post_title\': \'Stocks that track an index\', \'post_score\': 7, \'post_id\': \'130auvj\', \'post_url\': \' \'post_author\': Redditor(name=\'LeAlbertP\')})] ```', 'HuggingFace dataset | HuggingFace dataset The [Hugging Face Hub]( is home to over 5,000 [datasets]( in more than 100 languages that can be used for a broad range of tasks across NLP, Computer Vision, and Audio. They used for a diverse range of tasks such as translation, automatic speech recognition, and image classification. This notebook shows how to load `Hugging Face Hub` datasets to LangChain. ```python from langchain.document_loaders import HuggingFaceDatasetLoader ``` ```python dataset_name = ""imdb"" page_content_column = ""text"" loader = HuggingFaceDatasetLoader(dataset_name, page_content_column) ``` ```python data = loader.load() ``` ```python data[:15] ``` ```text [Document(page_content=\'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered ""controversial"" I really had to see this for myself.The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\\'t have much of a plot.\', metadata={\'label\': 0}), Document(page_content=\'""I Am Curious: Yellow"" is a risible and pretentious steaming pile. It doesn\\\'t matter what one\\\'s political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn\\\'t true. I\\\'ve seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don\\\'t exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we\\\'re treated to the site of Vincent Gallo\\\'s throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) ""double-standard"" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. In fact, you generally won\\\'t see female genitals in an American film in anything short of porn or explicit erotica. This alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women\\\'s bodies.\', metadata={\'label\': 0}), Document(page_content=""If only to avoid making this type of film in the future. This film is interesting as an experiment but tells no cogent story.One might feel virtuous for sitting thru it because it touches on so many IMPORTANT issues but it does so without any discernable motive. The viewer comes away with no new perspectives (unless one comes up with one while one\'s mind wanders, as it will invariably do during this pointless film).One might better spend one\'s time staring out a window at a tree growing."", metadata={\'label\': 0}), Document(page_content=""This film was probably inspired by Godard\'s Masculin, fminin and I urge you to see that film instead.The film has two strong elements and those are, (1) the realistic acting (2) the impressive, undeservedly good, photo. Apart from that, what strikes me most is the endless stream of silliness. Lena Nyman has to be most annoying actress in the world. She acts so stupid and with all the nudity in this film,...it\'s unattractive. Comparing to Godard\'s film, intellectuality has been replaced with stupidity. Without going too far on this subject, I would say that follows from the difference in ideals between the French and the Swedish society.A movie of its time, and place. 2/10."", metadata={\'label\': 0}), Document(page_content=\'Oh, brother...after hearing about this ridiculous film for umpteen years all I can think of is that old Peggy Lee song..""Is that all there is??"" ...I was just an early teen when this smoked fish hit the U.S. I was too young to get in the theater (although I did manage to sneak into ""Goodbye Columbus""). Then a screening at a local film museum beckoned - Finally I could see this film, except now I was as old as my parents were when they schlepped to see it!!The ONLY reason this film was not condemned to the anonymous sands of time was because of the obscenity case sparked by its U.S. release. MILLIONS of people flocked to this stinker, thinking they were going to see a sex film...Instead, they got lots of closeups of gnarly, repulsive Swedes, on-street interviews in bland shopping malls, asinie political pretension...and feeble who-cares simulated sex scenes with saggy, pale actors.Cultural icon, holy grail, historic artifact..whatever this thing was, shred it, burn it, then stuff the ashes in a lead box!Elite esthetes still scrape to find value in its boring pseudo revolutionary political spewings..But if it weren\\\'t for the censorship scandal, it would have been ignored, then forgotten.Instead, the ""I Am Blank, Blank"" rhythymed title was repeated endlessly for years as a titilation for porno films (I am Curious, Lavender - for gay films, I Am Curious, Black - for blaxploitation films, etc..) and every ten years or so the thing rises from the dead, to be viewed by a new generation of suckers who want to see that ""naughty sex film"" that ""revolutionized the film industry""...Yeesh, avoid like the plague..Or if you MUST see it - rent the video and fast forward to the ""dirty"" parts, just to get it over with.\', metadata={\'label\': 0}), Document(page_content=""I would put this at the top of my list of films in the category of unwatchable trash! There are films that are bad, but the worst kind are the ones that are unwatchable but you are suppose to like them because they are supposed to be good for you! The sex sequences, so shocking in its day, couldn\'t even arouse a rabbit. The so called controversial politics is strictly high school sophomore amateur night Marxism. The film is self-consciously arty in the worst sense of the term. The photography is in a harsh grainy black and white. Some scenes are out of focus or taken from the wrong angle. Even the sound is bad! And some people call this art?"", metadata={\'label\': 0}), Document(page_content=""Whoever wrote the screenplay for this movie obviously never consulted any books about Lucille Ball, especially her autobiography. I\'ve never seen so many mistakes in a biopic, ranging from her early years in Celoron and Jamestown to her later years with Desi. I could write a whole list of factual errors, but it would go on for pages. In all, I believe that Lucille Ball is one of those inimitable people who simply cannot be portrayed by anyone other than themselves. If I were Lucie Arnaz and Desi, Jr., I would be irate at how many mistakes were made in this film. The filmmakers tried hard, but the movie seems awfully sloppy to me."", metadata={\'label\': 0}), Document(page_content=\'When I first saw a glimpse of this movie, I quickly noticed the actress who was playing the role of Lucille Ball. Rachel York\\\'s portrayal of Lucy is absolutely awful. Lucille Ball was an astounding comedian with incredible talent. To think about a legend like Lucille Ball being portrayed the way she was in the movie is horrendous. I cannot believe out of all the actresses in the world who could play a much better Lucy, the producers decided to get Rachel York. She might be a good actress in other roles but to play the role of Lucille Ball is tough. It is pretty hard to find someone who could resemble Lucille Ball, but they could at least find someone a bit similar in looks and talent. If you noticed York\\\'s portrayal of Lucy in episodes of I Love Lucy like the chocolate factory or vitavetavegamin, nothing is similar in any way-her expression, voice, or movement.To top it all off, Danny Pino playing Desi Arnaz is horrible. Pino does not qualify to play as Ricky. He\\\'s small and skinny, his accent is unreal, and once again, his acting is unbelievable. Although Fred and Ethel were not similar either, they were not as bad as the characters of Lucy and Ricky.Overall, extremely horrible casting and the story is badly told. If people want to understand the real life situation of Lucille Ball, I suggest watching A&E Biography of Lucy and Desi, read the book from Lucille Ball herself, or PBS\\\' American Masters: Finding Lucy. If you want to see a docudrama, ""Before the Laughter"" would be a better choice. The casting of Lucille Ball and Desi Arnaz in ""Before the Laughter"" is much better compared to this. At least, a similar aspect is shown rather than nothing.\', metadata={\'label\': 0}), Document(page_content=\'Who are these ""They""- the actors? the filmmakers? Certainly couldn\\\'t be the audience- this is among the most air-puffed productions in existence. It\\\'s the kind of movie that looks like it was a lot of fun to shoot\\x97 TOO much fun, nobody is getting any actual work done, and that almost always makes for a movie that\\\'s no fun to watch.Ritter dons glasses so as to hammer home his character\\\'s status as a sort of doppleganger of the bespectacled Bogdanovich; the scenes with the breezy Ms. Stratten are sweet, but have an embarrassing, look-guys-I\\\'m-dating-the-prom-queen feel to them. Ben Gazzara sports his usual cat\\\'s-got-canary grin in a futile attempt to elevate the meager plot, which requires him to pursue Audrey Hepburn with all the interest of a narcoleptic at an insomnia clinic. In the meantime, the budding couple\\\'s respective children (nepotism alert: Bogdanovich\\\'s daughters) spew cute and pick up some fairly disturbing pointers on \\\'love\\\' while observing their parents. (Ms. Hepburn, drawing on her dignity, manages to rise above the proceedings- but she has the monumental challenge of playing herself, ostensibly.) Everybody looks great, but so what? It\\\'s a movie and we can expect that much, if that\\\'s what you\\\'re looking for you\\\'d be better off picking up a copy of Vogue.Oh- and it has to be mentioned that Colleen Camp thoroughly annoys, even apart from her singing, which, while competent, is wholly unconvincing... the country and western numbers are woefully mismatched with the standards on the soundtrack. Surely this is NOT what Gershwin (who wrote the song from which the movie\\\'s title is derived) had in mind; his stage musicals of the 20\\\'s may have been slight, but at least they were long on charm. ""They All Laughed"" tries to coast on its good intentions, but nobody- least of all Peter Bogdanovich - has the good sense to put on the brakes.Due in no small part to the tragic death of Dorothy Stratten, this movie has a special place in the heart of Mr. Bogdanovich- he even bought it back from its producers, then distributed it on his own and went bankrupt when it didn\\\'t prove popular. His rise and fall is among the more sympathetic and tragic of Hollywood stories, so there\\\'s no joy in criticizing the film... there _is_ real emotional investment in Ms. Stratten\\\'s scenes. But ""Laughed"" is a faint echo of ""The Last Picture Show"", ""Paper Moon"" or ""What\\\'s Up, Doc""- following ""Daisy Miller"" and ""At Long Last Love"", it was a thundering confirmation of the phase from which P.B. has never emerged.All in all, though, the movie is harmless, only a waste of rental. I want to watch people having a good time, I\\\'ll go to the park on a sunny day. For filmic expressions of joy and love, I\\\'ll stick to Ernest Lubitsch and Jaques Demy...\', metadata={\'label\': 0}), Document(page_content=""This is said to be a personal film for Peter Bogdonavitch. He based it on his life but changed things around to fit the characters, who are detectives. These detectives date beautiful models and have no problem getting them. Sounds more like a millionaire playboy filmmaker than a detective, doesn\'t it? This entire movie was written by Peter, and it shows how out of touch with real people he was. You\'re supposed to write what you know, and he did that, indeed. And leaves the audience bored and confused, and jealous, for that matter. This is a curio for people who want to see Dorothy Stratten, who was murdered right after filming. But Patti Hanson, who would, in real life, marry Keith Richards, was also a model, like Stratten, but is a lot better and has a more ample part. In fact, Stratten\'s part seemed forced; added. She doesn\'t have a lot to do with the story, which is pretty convoluted to begin with. All in all, every character in this film is somebody that very few people can relate with, unless you\'re millionaire from Manhattan with beautiful supermodels at your beckon call. For the rest of us, it\'s an irritating snore fest. That\'s what happens when you\'re out of touch. You entertain your few friends with inside jokes, and bore all the rest."", metadata={\'label\': 0}), Document(page_content=\'It was great to see some of my favorite stars of 30 years ago including John Ritter, Ben Gazarra and Audrey Hepburn. They looked quite wonderful. But that was it. They were not given any characters or good lines to work with. I neither understood or cared what the characters were doing.Some of the smaller female roles were fine, Patty Henson and Colleen Camp were quite competent and confident in their small sidekick parts. They showed some talent and it is sad they didn\\\'t go on to star in more and better films. Sadly, I didn\\\'t think Dorothy Stratten got a chance to act in this her only important film role.The film appears to have some fans, and I was very open-minded when I started watching it. I am a big Peter Bogdanovich fan and I enjoyed his last movie, ""Cat\\\'s Meow"" and all his early ones from ""Targets"" to ""Nickleodeon"". So, it really surprised me that I was barely able to keep awake watching this one.It is ironic that this movie is about a detective agency where the detectives and clients get romantically involved with each other. Five years later, Bogdanovich\\\'s ex-girlfriend, Cybil Shepherd had a hit television series called ""Moonlighting"" stealing the story idea from Bogdanovich. Of course, there was a great difference in that the series relied on tons of witty dialogue, while this tries to make do with slapstick and a few screwball lines.Bottom line: It ain\\\'t no ""Paper Moon"" and only a very pale version of ""What\\\'s Up, Doc"".\', metadata={\'label\': 0}), Document(page_content=""I can\'t believe that those praising this movie herein aren\'t thinking of some other film. I was prepared for the possibility that this would be awful, but the script (or lack thereof) makes for a film that\'s also pointless. On the plus side, the general level of craft on the part of the actors and technical crew is quite competent, but when you\'ve got a sow\'s ear to work with you can\'t make a silk purse. Ben G fans should stick with just about any other movie he\'s been in. Dorothy S fans should stick to Galaxina. Peter B fans should stick to Last Picture Show and Target. Fans of cheap laughs at the expense of those who seem to be asking for it should stick to Peter B\'s amazingly awful book, Killing of the Unicorn."", metadata={\'label\': 0}), Document(page_content=\'Never cast models and Playboy bunnies in your films! Bob Fosse\\\'s ""Star 80"" about Dorothy Stratten, of whom Bogdanovich was obsessed enough to have married her SISTER after her murder at the hands of her low-life husband, is a zillion times more interesting than Dorothy herself on the silver screen. Patty Hansen is no actress either..I expected to see some sort of lost masterpiece a la Orson Welles but instead got Audrey Hepburn cavorting in jeans and a god-awful ""poodlesque"" hair-do....Very disappointing....""Paper Moon"" and ""The Last Picture Show"" I could watch again and again. This clunker I could barely sit through once. This movie was reputedly not released because of the brouhaha surrounding Ms. Stratten\\\'s tawdry death; I think the real reason was because it was so bad!\', metadata={\'label\': 0}), Document(page_content=""Its not the cast. A finer group of actors, you could not find. Its not the setting. The director is in love with New York City, and by the end of the film, so are we all! Woody Allen could not improve upon what Bogdonovich has done here. If you are going to fall in love, or find love, Manhattan is the place to go. No, the problem with the movie is the script. There is none. The actors fall in love at first sight, words are unnecessary. In the director\'s own experience in Hollywood that is what happens when they go to work on the set. It is reality to him, and his peers, but it is a fantasy to most of us in the real world. So, in the end, the movie is hollow, and shallow, and message-less."", metadata={\'label\': 0}), Document(page_content=\'Today I found ""They All Laughed"" on VHS on sale in a rental. It was a really old and very used VHS, I had no information about this movie, but I liked the references listed on its cover: the names of Peter Bogdanovich, Audrey Hepburn, John Ritter and specially Dorothy Stratten attracted me, the price was very low and I decided to risk and buy it. I searched IMDb, and the User Rating of 6.0 was an excellent reference. I looked in ""Mick Martin & Marsha Porter Video & DVD Guide 2003"" and \\x96 wow \\x96 four stars! So, I decided that I could not waste more time and immediately see it. Indeed, I have just finished watching ""They All Laughed"" and I found it a very boring overrated movie. The characters are badly developed, and I spent lots of minutes to understand their roles in the story. The plot is supposed to be funny (private eyes who fall in love for the women they are chasing), but I have not laughed along the whole story. The coincidences, in a huge city like New York, are ridiculous. Ben Gazarra as an attractive and very seductive man, with the women falling for him as if her were a Brad Pitt, Antonio Banderas or George Clooney, is quite ridiculous. In the end, the greater attractions certainly are the presence of the Playboy centerfold and playmate of the year Dorothy Stratten, murdered by her husband pretty after the release of this movie, and whose life was showed in ""Star 80"" and ""Death of a Centerfold: The Dorothy Stratten Story""; the amazing beauty of the sexy Patti Hansen, the future Mrs. Keith Richards; the always wonderful, even being fifty-two years old, Audrey Hepburn; and the song ""Amigo"", from Roberto Carlos. Although I do not like him, Roberto Carlos has been the most popular Brazilian singer since the end of the 60\\\'s and is called by his fans as ""The King"". I will keep this movie in my collection only because of these attractions (manly Dorothy Stratten). My vote is four.Title (Brazil): ""Muito Riso e Muita Alegria"" (""Many Laughs and Lots of Happiness"")\', metadata={\'label\': 0})] ``` ### Example In this example, we use data from a dataset to answer a question ```python from langchain.document_loaders.hugging_face_dataset import HuggingFaceDatasetLoader from langchain.indexes import VectorstoreIndexCreator ``` ```python dataset_name = ""tweet_eval"" page_content_column = ""text"" name = ""stance_climate"" loader = HuggingFaceDatasetLoader(dataset_name, page_content_column, name) ``` ```python index = VectorstoreIndexCreator().from_loaders([loader]) ``` ```text Found cached dataset tweet_eval 0%| | 0/3 [00:00<?, ?it/s] Using embedded DuckDB without persistence: data will be transient ``` ```python query = ""What are the most used hashtag?"" result = index.query(query) ``` ```python result ``` ```text \' The most used hashtags in this context are #UKClimate2015, #Sustainability, #TakeDownTheFlag, #LoveWins, #CSOTA, #ClimateSummitoftheAmericas, #SM, and #SocialMedia.\' ``` - [Example](#example)']","To handle rate limit errors and automatically switch to a different model, you can implement fallbacks in your language model application. Here's a brief example using the context provided:

```python
from langchain.chat_models import ChatAnthropic, ChatOpenAI

# Initialize your primary and fallback models
openai_llm = ChatOpenAI(max_retries=0)  # Disable retries to handle errors manually
anthropic_llm = ChatAnthropic()

# Set up the fallback chain
llm = openai_llm.with_fallbacks([anthropic_llm])

# Example usage
try:
    response = llm.invoke(""Your prompt here"")
except Exception as e:
    print(f""Error: {e}"")
```

In this setup, if the `openai_llm` encounters an error, it will automatically fall back to `anthropic_llm`. Make sure to handle exceptions appropriately to manage different error scenarios.","To switch to a different model when an LLM is gettin rate limited, you can use 'fallbacks'. For instance, if you have too llms: openai_llm and anthropic_llm, you can have the program fall back to the anthropic llm by calling `llm = openai_llm.with_fallbacks([anthropic_llm])`",0.9999999999,1.0,0.875,0.23933115010284964,0.2650602409638554
56,What's function calling,"['LangSmith LLM Runs | LangSmith LLM Runs This notebook demonstrates how to directly load data from LangSmith\'s LLM runs and fine-tune a model on that data. The process is simple and comprises 3 steps. 1. Select the LLM runs to train on. 2. Use the LangSmithRunChatLoader to load runs as chat sessions. 3. Fine-tune your model. Then you can use the fine-tuned model in your LangChain app. Before diving in, let\'s install our prerequisites. ## Prerequisites Ensure you\'ve installed langchain >= 0.0.311 and have configured your environment with your LangSmith API key. ```python %pip install -U langchain openai ``` ```python import os import uuid uid = uuid.uuid4().hex[:6] project_name = f""Run Fine-tuning Walkthrough {uid}"" os.environ[""LANGCHAIN_TRACING_V2""] = ""true"" os.environ[""LANGCHAIN_API_KEY""] = ""YOUR API KEY"" os.environ[""LANGCHAIN_PROJECT""] = project_name ``` ## 1. Select Runs The first step is selecting which runs to fine-tune on. A common case would be to select LLM runs within traces that have received positive user feedback. You can find examples of this in the[LangSmith Cookbook]( and in the [docs]( For the sake of this tutorial, we will generate some runs for you to use here. Let\'s try fine-tuning a simple function-calling chain. ```python from enum import Enum from langchain.pydantic_v1 import BaseModel, Field class Operation(Enum): add = ""+"" subtract = ""-"" multiply = ""*"" divide = ""/"" class Calculator(BaseModel): """"""A calculator function"""""" num1: float num2: float operation: Operation = Field(..., description=""+,-,*,/"") def calculate(self): if self.operation == Operation.add: return self.num1 + self.num2 elif self.operation == Operation.subtract: return self.num1 - self.num2 elif self.operation == Operation.multiply: return self.num1 * self.num2 elif self.operation == Operation.divide: if self.num2 != 0: return self.num1 / self.num2 else: return ""Cannot divide by zero"" ``` ```python from pprint import pprint from langchain.pydantic_v1 import BaseModel from langchain.utils.openai_functions import convert_pydantic_to_openai_function openai_function_def = convert_pydantic_to_openai_function(Calculator) pprint(openai_function_def) ``` ```text {\'description\': \'A calculator function\', \'name\': \'Calculator\', \'parameters\': {\'description\': \'A calculator function\', \'properties\': {\'num1\': {\'title\': \'Num1\', \'type\': \'number\'}, \'num2\': {\'title\': \'Num2\', \'type\': \'number\'}, \'operation\': {\'allOf\': [{\'description\': \'An \' \'enumeration.\', \'enum\': [\'+\', \'-\', \'*\', \'/\'], \'title\': \'Operation\'}], \'description\': \'+,-,*,/\'}}, \'required\': [\'num1\', \'num2\', \'operation\'], \'title\': \'Calculator\', \'type\': \'object\'}} ``` ```python from langchain.chat_models import ChatOpenAI from langchain.output_parsers.openai_functions import PydanticOutputFunctionsParser from langchain.prompts import ChatPromptTemplate prompt = ChatPromptTemplate.from_messages( [ (""system"", ""You are an accounting assistant.""), (""user"", ""{input}""), ] ) chain = ( prompt | ChatOpenAI().bind(functions=[openai_function_def]) | PydanticOutputFunctionsParser(pydantic_schema=Calculator) | (lambda x: x.calculate()) ) ``` ```python math_questions = [ ""What\'s 45/9?"", ""What\'s 81/9?"", ""What\'s 72/8?"", ""What\'s 56/7?"", ""What\'s 36/6?"", ""What\'s 64/8?"", ""What\'s 12*6?"", ""What\'s 8*8?"", ""What\'s 10*10?"", ""What\'s 11*11?"", ""What\'s 13*13?"", ""What\'s 45+30?"", ""What\'s 72+28?"", ""What\'s 56+44?"", ""What\'s 63+37?"", ""What\'s 70-35?"", ""What\'s 60-30?"", ""What\'s 50-25?"", ""What\'s 40-20?"", ""What\'s 30-15?"", ] results = chain.batch([{""input"": q} for q in math_questions], return_exceptions=True) ``` ```text Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.._completion_with_retry in 4.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet.. ``` #### Load runs that did not error Now we can select the successful runs to fine-tune on. ```python from langsmith.client import Client client = Client() ``` ```python successful_traces = { run.trace_id for run in client.list_runs( project_name=project_name, execution_order=1, error=False, ) } llm_runs = [ run for run in client.list_runs( project_name=project_name, run_type=""llm"", ) if run.trace_id in successful_traces ] ``` ## 2. Prepare data Now we can create an instance of LangSmithRunChatLoader and load the chat sessions using its lazy_load() method. ```python from langchain.chat_loaders.langsmith import LangSmithRunChatLoader loader = LangSmithRunChatLoader(runs=llm_runs) chat_sessions = loader.lazy_load() ``` #### With the chat sessions loaded, convert them into a format suitable for fine-tuning. ```python from langchain.adapters.openai import convert_messages_for_finetuning training_data = convert_messages_for_finetuning(chat_sessions) ``` ## 3. Fine-tune the model Now, initiate the fine-tuning process using the OpenAI library. ```python import json import time from io import BytesIO import openai my_file = BytesIO() for dialog in training_data: my_file.write((json.dumps({""messages"": dialog}) + ""\\n"").encode(""utf-8"")) my_file.seek(0) training_file = openai.File.create(file=my_file, purpose=""fine-tune"") job = openai.FineTuningJob.create( training_file=training_file.id, model=""gpt-3.5-turbo"", ) # Wait for the fine-tuning to complete (this may take some time) status = openai.FineTuningJob.retrieve(job.id).status start_time = time.time() while status != ""succeeded"": print(f""Status=[{status}]... {time.time() - start_time:.2f}s"", end=""\\r"", flush=True) time.sleep(5) status = openai.FineTuningJob.retrieve(job.id).status # Now your model is fine-tuned! ``` ```text Status=[running]... 346.26s. 31.70s ``` ## 4. Use in LangChain After fine-tuning, use the resulting model ID with the ChatOpenAI model class in your LangChain app. ```python # Get the fine-tuned model ID job = openai.FineTuningJob.retrieve(job.id) model_id = job.fine_tuned_model # Use the fine-tuned model in LangChain model = ChatOpenAI( model=model_id, temperature=1, ) ``` ```python (prompt | model).invoke({""input"": ""What\'s 56/7?""}) ``` ```text AIMessage(content=\'{\\n ""num1"": 56,\\n ""num2"": 7,\\n ""operation"": ""/""\\n}\') ``` Now you have successfully fine-tuned a model using data from LangSmith LLM runs! - [Prerequisites](#prerequisites) - [1. Select Runs](#1-select-runs) - [2. Prepare data](#2-prepare-data) - [3. Fine-tune the model](#3-fine-tune-the-model) - [4. Use in LangChain](#4-use-in-langchain)', 'OpenAI assistants | OpenAI assistants The [Assistants API]( allows you to build AI assistants within your own applications. An Assistant has instructions and can leverage models, tools, and knowledge to respond to user queries. The Assistants API currently supports three types of tools: Code Interpreter, Retrieval, and Function calling You can interact with OpenAI Assistants using OpenAI tools or custom tools. When using exclusively OpenAI tools, you can just invoke the assistant directly and get final answers. When using custom tools, you can run the assistant and tool execution loop using the built-in AgentExecutor or easily write your own executor. Below we show the different ways to interact with Assistants. As a simple example, let\'s build a math tutor that can write and run code. ### Using only OpenAI tools ```python from langchain.agents.openai_assistant import OpenAIAssistantRunnable ``` ```python interpreter_assistant = OpenAIAssistantRunnable.create_assistant( name=""langchain assistant"", instructions=""You are a personal math tutor. Write and run code to answer math questions."", tools=[{""type"": ""code_interpreter""}], model=""gpt-4-1106-preview"", ) output = interpreter_assistant.invoke({""content"": ""What\'s 10 - 4 raised to the 2.7""}) output ``` ```text [ThreadMessage(id=\'msg_qgxkD5kvkZyl0qOaL4czPFkZ\', assistant_id=\'asst_0T8S7CJuUa4Y4hm1PF6n62v7\', content=[MessageContentText(text=Text(annotations=[], value=\'The result of the calculation \\\\(10 - 4^{2.7}\\\\) is approximately \\\\(-32.224\\\\).\'), type=\'text\')], created_at=1700169519, file_ids=[], metadata={}, object=\'thread.message\', role=\'assistant\', run_id=\'run_aH3ZgSWNk3vYIBQm3vpE8tr4\', thread_id=\'thread_9K6cYfx1RBh0pOWD8SxwVWW9\')] ``` ### As a LangChain agent with arbitrary tools Now let\'s recreate this functionality using our own tools. For this example we\'ll use the [E2B sandbox runtime tool]( ```bash pip install e2b duckduckgo-search ``` ```python import getpass from langchain.tools import DuckDuckGoSearchRun, E2BDataAnalysisTool tools = [E2BDataAnalysisTool(api_key=getpass.getpass()), DuckDuckGoSearchRun()] ``` ```python agent = OpenAIAssistantRunnable.create_assistant( name=""langchain assistant e2b tool"", instructions=""You are a personal math tutor. Write and run code to answer math questions. You can also search the internet."", tools=tools, model=""gpt-4-1106-preview"", as_agent=True, ) ``` #### Using AgentExecutor The OpenAIAssistantRunnable is compatible with the AgentExecutor, so we can pass it in as an agent directly to the executor. The AgentExecutor handles calling the invoked tools and uploading the tool outputs back to the Assistants API. Plus it comes with built-in LangSmith tracing. ```python from langchain.agents import AgentExecutor agent_executor = AgentExecutor(agent=agent, tools=tools) agent_executor.invoke({""content"": ""What\'s the weather in SF today divided by 2.7""}) ``` ```text {\'content\': ""What\'s the weather in SF today divided by 2.7"", \'output\': ""The search results indicate that the weather in San Francisco is 67 F. Now I will divide this temperature by 2.7 and provide you with the result. Please note that this is a mathematical operation and does not represent a meaningful physical quantity.\\n\\nLet\'s calculate 67 F divided by 2.7.\\nThe result of dividing the current temperature in San Francisco, which is 67 F, by 2.7 is approximately 24.815."", \'thread_id\': \'thread_hcpYI0tfpB9mHa9d95W7nK2B\', \'run_id\': \'run_qOuVmPXS9xlV3XNPcfP8P9W2\'} ``` [LangSmith trace]( Custom execution Or with LCEL we can easily write our own execution loop for running the assistant. This gives us full control over execution. ```python agent = OpenAIAssistantRunnable.create_assistant( name=""langchain assistant e2b tool"", instructions=""You are a personal math tutor. Write and run code to answer math questions."", tools=tools, model=""gpt-4-1106-preview"", as_agent=True, ) ``` ```python from langchain.schema.agent import AgentFinish def execute_agent(agent, tools, input): tool_map = {tool.name: tool for tool in tools} response = agent.invoke(input) while not isinstance(response, AgentFinish): tool_outputs = [] for action in response: tool_output = tool_map[action.tool].invoke(action.tool_input) print(action.tool, action.tool_input, tool_output, end=""\\n\\n"") tool_outputs.append( {""output"": tool_output, ""tool_call_id"": action.tool_call_id} ) response = agent.invoke( { ""tool_outputs"": tool_outputs, ""run_id"": action.run_id, ""thread_id"": action.thread_id, } ) return response ``` ```python response = execute_agent(agent, tools, {""content"": ""What\'s 10 - 4 raised to the 2.7""}) print(response.return_values[""output""]) ``` ```text e2b_data_analysis {\'python_code\': \'result = 10 - 4 ** 2.7\\nprint(result)\'} {""stdout"": ""-32.22425314473263"", ""stderr"": """", ""artifacts"": []} \\( 10 - 4^{2.7} \\) equals approximately -32.224. ``` ## Using existing Thread To use an existing thread we just need to pass the ""thread_id"" in when invoking the agent. ```python next_response = execute_agent( agent, tools, {""content"": ""now add 17.241"", ""thread_id"": response.return_values[""thread_id""]}, ) print(next_response.return_values[""output""]) ``` ```text e2b_data_analysis {\'python_code\': \'result = 10 - 4 ** 2.7 + 17.241\\nprint(result)\'} {""stdout"": ""-14.983253144732629"", ""stderr"": """", ""artifacts"": []} \\( 10 - 4^{2.7} + 17.241 \\) equals approximately -14.983. ``` ## Using existing Assistant To use an existing Assistant we can initialize the `OpenAIAssistantRunnable` directly with an `assistant_id`. ```python agent = OpenAIAssistantRunnable(assistant_id="""", as_agent=True) ``` - [Using only OpenAI tools](#using-only-openai-tools) - [As a LangChain agent with arbitrary tools](#as-a-langchain-agent-with-arbitrary-tools) - [Using existing Thread](#using-existing-thread) - [Using existing Assistant](#using-existing-assistant)', 'MyScale | MyScale [MyScale]( is an integrated vector database. You can access your database in SQL and also from here, LangChain. `MyScale` can make use of [various data types and functions for filters]( It will boost up your LLM app no matter if you are scaling up your data or expand your system to broader application. In the notebook, we\'ll demo the `SelfQueryRetriever` wrapped around a `MyScale` vector store with some extra pieces we contributed to LangChain. In short, it can be condensed into 4 points: 1. Add `contain` comparator to match the list of any if there is more than one element matched 2. Add `timestamp` data type for datetime match (ISO-format, or YYYY-MM-DD) 3. Add `like` comparator for string pattern search 4. Add arbitrary function capability ## Creating a MyScale vector store MyScale has already been integrated to LangChain for a while. So you can follow [this notebook](/docs/integrations/vectorstores/myscale) to create your own vectorstore for a self-query retriever. **Note:** All self-query retrievers requires you to have `lark` installed (`pip install lark`). We use `lark` for grammar definition. Before you proceed to the next step, we also want to remind you that `clickhouse-connect` is also needed to interact with your MyScale backend. ```bash pip install lark clickhouse-connect ``` In this tutorial we follow other example\'s setting and use `OpenAIEmbeddings`. Remember to get an OpenAI API Key for valid access to LLMs. ```python import getpass import os os.environ[""OPENAI_API_KEY""] = getpass.getpass(""OpenAI API Key:"") os.environ[""MYSCALE_HOST""] = getpass.getpass(""MyScale URL:"") os.environ[""MYSCALE_PORT""] = getpass.getpass(""MyScale Port:"") os.environ[""MYSCALE_USERNAME""] = getpass.getpass(""MyScale Username:"") os.environ[""MYSCALE_PASSWORD""] = getpass.getpass(""MyScale Password:"") ``` ```python from langchain.embeddings.openai import OpenAIEmbeddings from langchain.schema import Document from langchain.vectorstores import MyScale embeddings = OpenAIEmbeddings() ``` ## Create some sample data As you can see, the data we created has some differences compared to other self-query retrievers. We replaced the keyword `year` with `date` which gives you finer control on timestamps. We also changed the type of the keyword `gerne` to a list of strings, where an LLM can use a new `contain` comparator to construct filters. We also provide the `like` comparator and arbitrary function support to filters, which will be introduced in next few cells. Now let\'s look at the data first. ```python docs = [ Document( page_content=""A bunch of scientists bring back dinosaurs and mayhem breaks loose"", metadata={""date"": ""1993-07-02"", ""rating"": 7.7, ""genre"": [""science fiction""]}, ), Document( page_content=""Leo DiCaprio gets lost in a dream within a dream within a dream within a ..."", metadata={""date"": ""2010-12-30"", ""director"": ""Christopher Nolan"", ""rating"": 8.2}, ), Document( page_content=""A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea"", metadata={""date"": ""2006-04-23"", ""director"": ""Satoshi Kon"", ""rating"": 8.6}, ), Document( page_content=""A bunch of normal-sized women are supremely wholesome and some men pine after them"", metadata={""date"": ""2019-08-22"", ""director"": ""Greta Gerwig"", ""rating"": 8.3}, ), Document( page_content=""Toys come alive and have a blast doing so"", metadata={""date"": ""1995-02-11"", ""genre"": [""animated""]}, ), Document( page_content=""Three men walk into the Zone, three men walk out of the Zone"", metadata={ ""date"": ""1979-09-10"", ""director"": ""Andrei Tarkovsky"", ""genre"": [""science fiction"", ""adventure""], ""rating"": 9.9, }, ), ] vectorstore = MyScale.from_documents( docs, embeddings, ) ``` ## Creating our self-querying retriever Just like other retrievers... simple and nice. ```python from langchain.chains.query_constructor.base import AttributeInfo from langchain.llms import OpenAI from langchain.retrievers.self_query.base import SelfQueryRetriever metadata_field_info = [ AttributeInfo( name=""genre"", description=""The genres of the movie"", type=""list[string]"", ), # If you want to include length of a list, just define it as a new column # This will teach the LLM to use it as a column when constructing filter. AttributeInfo( name=""length(genre)"", description=""The length of genres of the movie"", type=""integer"", ), # Now you can define a column as timestamp. By simply set the type to timestamp. AttributeInfo( name=""date"", description=""The date the movie was released"", type=""timestamp"", ), AttributeInfo( name=""director"", description=""The name of the movie director"", type=""string"", ), AttributeInfo( name=""rating"", description=""A 1-10 rating for the movie"", type=""float"" ), ] document_content_description = ""Brief summary of a movie"" llm = OpenAI(temperature=0) retriever = SelfQueryRetriever.from_llm( llm, vectorstore, document_content_description, metadata_field_info, verbose=True ) ``` ## Testing it out with self-query retriever\'s existing functionalities And now we can try actually using our retriever! ```python # This example only specifies a relevant query retriever.get_relevant_documents(""What are some movies about dinosaurs"") ``` ```python # This example only specifies a filter retriever.get_relevant_documents(""I want to watch a movie rated higher than 8.5"") ``` ```python # This example specifies a query and a filter retriever.get_relevant_documents(""Has Greta Gerwig directed any movies about women"") ``` ```python # This example specifies a composite filter retriever.get_relevant_documents( ""What\'s a highly rated (above 8.5) science fiction film?"" ) ``` ```python # This example specifies a query and composite filter retriever.get_relevant_documents( ""What\'s a movie after 1990 but before 2005 that\'s all about toys, and preferably is animated"" ) ``` # Wait a second... what else? Self-query retriever with MyScale can do more! Let\'s find out. ```python # You can use length(genres) to do anything you want retriever.get_relevant_documents(""What\'s a movie that have more than 1 genres?"") ``` ```python # Fine-grained datetime? You got it already. retriever.get_relevant_documents(""What\'s a movie that release after feb 1995?"") ``` ```python # Don\'t know what your exact filter should be? Use string pattern match! retriever.get_relevant_documents(""What\'s a movie whose name is like Andrei?"") ``` ```python # Contain works for lists: so you can match a list with contain comparator! retriever.get_relevant_documents( ""What\'s a movie who has genres science fiction and adventure?"" ) ``` ## Filter k We can also use the self query retriever to specify `k`: the number of documents to fetch. We can do this by passing `enable_limit=True` to the constructor. ```python retriever = SelfQueryRetriever.from_llm( llm, vectorstore, document_content_description, metadata_field_info, enable_limit=True, verbose=True, ) ``` ```python # This example only specifies a relevant query retriever.get_relevant_documents(""what are two movies about dinosaurs"") ``` - [Creating a MyScale vector store](#creating-a-myscale-vector-store) - [Create some sample data](#create-some-sample-data) - [Creating our self-querying retriever](#creating-our-self-querying-retriever) - [Testing it out with self-query retriever\'s existing functionalities](#testing-it-out-with-self-query-retrievers-existing-functionalities) - [Filter k](#filter-k)']","Function calling in LangChain refers to the ability to define a Pydantic model and convert it into an OpenAPI function definition, which can then be used by language models to understand and execute specific functions. This is done using the `convert_pydantic_to_openai_function` function. Here's a brief example:

```python
from langchain.utils.openai_functions import convert_pydantic_to_openai_function
from langchain.pydantic_v1 import BaseModel, Field

class Calculator(BaseModel):
    num1: float
    num2: float
    operation: Operation = Field(..., description=""+,-,*,/"")

openai_function_def = convert_pydantic_to_openai_function(Calculator)
```

This converts the `Calculator` Pydantic model into a function definition that can be used by an LLM.",Function calling refers to the ability to define functions for LLMs to let them choose to call with structured arguments.,0.9999999999666667,1.0,,0.1502250210702038,0.1732283464566929
57,what is the langserve,"['RAG with Timescale Vector using hybrid search | RAG with Timescale Vector using hybrid search This template shows how to use timescale-vector with the self-query retriver to perform hybrid search on similarity and time. This is useful any time your data has a strong time-based component. Some examples of such data are: - News articles (politics, business, etc) - Blog posts, documentation or other published material (public or private). - Social media posts - Changelogs of any kind - Messages Such items are often searched by both similarity and time. For example: Show me all news about Toyota trucks from 2022. [Timescale Vector]( provides superior performance when searching for embeddings within a particular timeframe by leveraging automatic table partitioning to isolate data for particular time-ranges. Langchain\'s self-query retriever allows deducing time-ranges (as well as other search criteria) from the text of user queries. ## What is Timescale Vector? **Timescale Vector is PostgreSQL++ for AI applications.** Timescale Vector enables you to efficiently store and query billions of vector embeddings in `PostgreSQL`. - Enhances `pgvector` with faster and more accurate similarity search on 1B+ vectors via DiskANN inspired indexing algorithm. - Enables fast time-based vector search via automatic time-based partitioning and indexing. - Provides a familiar SQL interface for querying vector embeddings and relational data. Timescale Vector is cloud PostgreSQL for AI that scales with you from POC to production: - Simplifies operations by enabling you to store relational metadata, vector embeddings, and time-series data in a single database. - Benefits from rock-solid PostgreSQL foundation with enterprise-grade feature liked streaming backups and replication, high-availability and row-level security. - Enables a worry-free experience with enterprise-grade security and compliance. ### How to access Timescale Vector Timescale Vector is available on [Timescale]( the cloud PostgreSQL platform. (There is no self-hosted version at this time.) - LangChain users get a 90-day free trial for Timescale Vector. - To get started, [signup]( to Timescale, create a new database and follow this notebook! - See the [installation instructions]( for more details on using Timescale Vector in python. ## Environment Setup This template uses Timescale Vector as a vectorstore and requires that `TIMESCALES_SERVICE_URL`. Signup for a 90-day trial [here]( if you don\'t yet have an account. To load the sample dataset, set `LOAD_SAMPLE_DATA=1`. To load your own dataset see the section below. Set the `OPENAI_API_KEY` environment variable to access the OpenAI models. ## Usage To use this package, you should first have the LangChain CLI installed: ```shell pip install -U langchain-cli ``` To create a new LangChain project and install this as the only package, you can do: ```shell langchain app new my-app --package rag-timescale-hybrid-search-time ``` If you want to add this to an existing project, you can just run: ```shell langchain app add rag-timescale-hybrid-search-time ``` And add the following code to your `server.py` file: ```python from rag_timescale_hybrid_search.chain import chain as rag_timescale_hybrid_search_chain add_routes(app, rag_timescale_hybrid_search_chain, path=""/rag-timescale-hybrid-search"") ``` (Optional) Let\'s now configure LangSmith. LangSmith will help us trace, monitor and debug LangChain applications. LangSmith is currently in private beta, you can sign up [here]( If you don\'t have access, you can skip this section ```shell export LANGCHAIN_TRACING_V2=true export LANGCHAIN_API_KEY= export LANGCHAIN_PROJECT= # if not specified, defaults to ""default"" ``` If you are inside this directory, then you can spin up a LangServe instance directly by: ```shell langchain serve ``` This will start the FastAPI app with a server is running locally at [ We can see all templates at [ We can access the playground at [ We can access the template from code with: ```python from langserve.client import RemoteRunnable runnable = RemoteRunnable("" ``` ## Loading your own dataset To load your own dataset you will have to modify the code in the `DATASET SPECIFIC CODE` section of `chain.py`. This code defines the name of the collection, how to load the data, and the human-language description of both the contents of the collection and all of the metadata. The human-language descriptions are used by the self-query retriever to help the LLM convert the question into filters on the metadata when searching the data in Timescale-vector. - [What is Timescale Vector?](#what-is-timescale-vector)- [How to access Timescale Vector](#how-to-access-timescale-vector) - [Environment Setup](#environment-setup) - [Usage](#usage) - [Loading your own dataset](#loading-your-own-dataset)', 'Vector store-backed retriever | Vector store-backed retriever A vector store retriever is a retriever that uses a vector store to retrieve documents. It is a lightweight wrapper around the vector store class to make it conform to the retriever interface. It uses the search methods implemented by a vector store, like similarity search and MMR, to query the texts in the vector store. Once you construct a vector store, it\'s very easy to construct a retriever. Let\'s walk through an example. ```python from langchain.document_loaders import TextLoader loader = TextLoader(\'../../../state_of_the_union.txt\') ``` ```python from langchain.text_splitter import CharacterTextSplitter from langchain.vectorstores import FAISS from langchain.embeddings import OpenAIEmbeddings documents = loader.load() text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0) texts = text_splitter.split_documents(documents) embeddings = OpenAIEmbeddings() db = FAISS.from_documents(texts, embeddings) ``` ```text Exiting: Cleaning up .chroma directory ``` ```python retriever = db.as_retriever() ``` ```python docs = retriever.get_relevant_documents(""what did he say about ketanji brown jackson"") ``` ## Maximum marginal relevance retrieval By default, the vector store retriever uses similarity search. If the underlying vector store supports maximum marginal relevance search, you can specify that as the search type. ```python retriever = db.as_retriever(search_type=""mmr"") ``` ```python docs = retriever.get_relevant_documents(""what did he say about ketanji brown jackson"") ``` ## Similarity score threshold retrieval You can also set a retrieval method that sets a similarity score threshold and only returns documents with a score above that threshold. ```python retriever = db.as_retriever(search_type=""similarity_score_threshold"", search_kwargs={""score_threshold"": .5}) ``` ```python docs = retriever.get_relevant_documents(""what did he say about ketanji brown jackson"") ``` ## Specifying top k You can also specify search kwargs like `k` to use when doing retrieval. ```python retriever = db.as_retriever(search_kwargs={""k"": 1}) ``` ```python docs = retriever.get_relevant_documents(""what did he say about ketanji brown jackson"") ``` ```python len(docs) ``` ```text 1 ``` - [Maximum marginal relevance retrieval](#maximum-marginal-relevance-retrieval) - [Similarity score threshold retrieval](#similarity-score-threshold-retrieval) - [Specifying top k](#specifying-top-k)', 'Memory | Memory Most LLM applications have a conversational interface. An essential component of a conversation is being able to refer to information introduced earlier in the conversation. At bare minimum, a conversational system should be able to access some window of past messages directly. A more complex system will need to have a world model that it is constantly updating, which allows it to do things like maintain information about entities and their relationships. We call this ability to store information about past interactions ""memory"". LangChain provides a lot of utilities for adding memory to a system. These utilities can be used by themselves or incorporated seamlessly into a chain. A memory system needs to support two basic actions: reading and writing. Recall that every chain defines some core execution logic that expects certain inputs. Some of these inputs come directly from the user, but some of these inputs can come from memory. A chain will interact with its memory system twice in a given run. 1. AFTER receiving the initial user inputs but BEFORE executing the core logic, a chain will READ from its memory system and augment the user inputs. 2. AFTER executing the core logic but BEFORE returning the answer, a chain will WRITE the inputs and outputs of the current run to memory, so that they can be referred to in future runs. ![memory-diagram](/assets/images/memory_diagram-0627c68230aa438f9b5419064d63cbbc.png) ## Building memory into a system The two core design decisions in any memory system are: - How state is stored - How state is queried ### Storing: List of chat messages Underlying any memory is a history of all chat interactions. Even if these are not all used directly, they need to be stored in some form. One of the key parts of the LangChain memory module is a series of integrations for storing these chat messages, from in-memory lists to persistent databases. - [Chat message storage](/docs/modules/memory/chat_messages/): How to work with Chat Messages, and the various integrations offered. ### Querying: Data structures and algorithms on top of chat messages Keeping a list of chat messages is fairly straight-forward. What is less straight-forward are the data structures and algorithms built on top of chat messages that serve a view of those messages that is most useful. A very simply memory system might just return the most recent messages each run. A slightly more complex memory system might return a succinct summary of the past K messages. An even more sophisticated system might extract entities from stored messages and only return information about entities referenced in the current run. Each application can have different requirements for how memory is queried. The memory module should make it easy to both get started with simple memory systems and write your own custom systems if needed. - [Memory types](/docs/modules/memory/types/): The various data structures and algorithms that make up the memory types LangChain supports ## Get started Let\'s take a look at what Memory actually looks like in LangChain. Here we\'ll cover the basics of interacting with an arbitrary memory class. Let\'s take a look at how to use `ConversationBufferMemory` in chains. `ConversationBufferMemory` is an extremely simple form of memory that just keeps a list of chat messages in a buffer and passes those into the prompt template. ```python from langchain.memory import ConversationBufferMemory memory = ConversationBufferMemory() memory.chat_memory.add_user_message(""hi!"") memory.chat_memory.add_ai_message(""what\'s up?"") ``` When using memory in a chain, there are a few key concepts to understand. Note that here we cover general concepts that are useful for most types of memory. Each individual memory type may very well have its own parameters and concepts that are necessary to understand. ### What variables get returned from memory Before going into the chain, various variables are read from memory. These have specific names which need to align with the variables the chain expects. You can see what these variables are by calling `memory.load_memory_variables({})`. Note that the empty dictionary that we pass in is just a placeholder for real variables. If the memory type you are using is dependent upon the input variables, you may need to pass some in. ```python memory.load_memory_variables({}) ``` ```text {\'history\': ""Human: hi!\\nAI: what\'s up?""} ``` In this case, you can see that `load_memory_variables` returns a single key, `history`. This means that your chain (and likely your prompt) should expect an input named `history`. You can usually control this variable through parameters on the memory class. For example, if you want the memory variables to be returned in the key `chat_history` you can do: ```python memory = ConversationBufferMemory(memory_key=""chat_history"") memory.chat_memory.add_user_message(""hi!"") memory.chat_memory.add_ai_message(""what\'s up?"") ``` ```text {\'chat_history\': ""Human: hi!\\nAI: what\'s up?""} ``` The parameter name to control these keys may vary per memory type, but it\'s important to understand that (1) this is controllable, and (2) how to control it. ### Whether memory is a string or a list of messages One of the most common types of memory involves returning a list of chat messages. These can either be returned as a single string, all concatenated together (useful when they will be passed into LLMs) or a list of ChatMessages (useful when passed into ChatModels). By default, they are returned as a single string. In order to return as a list of messages, you can set `return_messages=True` ```python memory = ConversationBufferMemory(return_messages=True) memory.chat_memory.add_user_message(""hi!"") memory.chat_memory.add_ai_message(""what\'s up?"") ``` ```text {\'history\': [HumanMessage(content=\'hi!\', additional_kwargs={}, example=False), AIMessage(content=\'what\'s up?\', additional_kwargs={}, example=False)]} ``` ### What keys are saved to memory Often times chains take in or return multiple input/output keys. In these cases, how can we know which keys we want to save to the chat message history? This is generally controllable by `input_key` and `output_key` parameters on the memory types. These default to `None` - and if there is only one input/output key it is known to just use that. However, if there are multiple input/output keys then you MUST specify the name of which one to use. ### End to end example Finally, let\'s take a look at using this in a chain. We\'ll use an `LLMChain`, and show working with both an LLM and a ChatModel. #### Using an LLM ```python from langchain.llms import OpenAI from langchain.prompts import PromptTemplate from langchain.chains import LLMChain from langchain.memory import ConversationBufferMemory llm = OpenAI(temperature=0) # Notice that ""chat_history"" is present in the prompt template template = """"""You are a nice chatbot having a conversation with a human. Previous conversation: {chat_history} New human question: {question} Response:"""""" prompt = PromptTemplate.from_template(template) # Notice that we need to align the `memory_key` memory = ConversationBufferMemory(memory_key=""chat_history"") conversation = LLMChain( llm=llm, prompt=prompt, verbose=True, memory=memory ) ``` ```python # Notice that we just pass in the `question` variables - `chat_history` gets populated by memory conversation({""question"": ""hi""}) ``` #### Using a ChatModel ```python from langchain.chat_models import ChatOpenAI from langchain.prompts import ( ChatPromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate, HumanMessagePromptTemplate, ) from langchain.chains import LLMChain from langchain.memory import ConversationBufferMemory llm = ChatOpenAI() prompt = ChatPromptTemplate( messages=[ SystemMessagePromptTemplate.from_template( ""You are a nice chatbot having a conversation with a human."" ), # The `variable_name` here is what must align with memory MessagesPlaceholder(variable_name=""chat_history""), HumanMessagePromptTemplate.from_template(""{question}"") ] ) # Notice that we `return_messages=True` to fit into the MessagesPlaceholder # Notice that `""chat_history""` aligns with the MessagesPlaceholder name. memory = ConversationBufferMemory(memory_key=""chat_history"", return_messages=True) conversation = LLMChain( llm=llm, prompt=prompt, verbose=True, memory=memory ) ``` ```python # Notice that we just pass in the `question` variables - `chat_history` gets populated by memory conversation({""question"": ""hi""}) ``` ## Next steps And that\'s it for getting started! Please see the other sections for walkthroughs of more advanced topics, like custom memory, multiple memories, and more. - [Building memory into a system](#building-memory-into-a-system)- [Storing: List of chat messages](#storing-list-of-chat-messages) - [Querying: Data structures and algorithms on top of chat messages](#querying-data-structures-and-algorithms-on-top-of-chat-messages) - [Get started](#get-started)- [What variables get returned from memory](#what-variables-get-returned-from-memory) - [Whether memory is a string or a list of messages](#whether-memory-is-a-string-or-a-list-of-messages) - [What keys are saved to memory](#what-keys-are-saved-to-memory) - [End to end example](#end-to-end-example) - [Next steps](#next-steps)']","LangServe is a tool to spin up a FastAPI app that serves a LangChain application. It allows you to run your LangChain-based applications locally or deploy them for broader access. To start a LangServe instance, navigate to the directory containing your application and run:

```shell
langchain serve
```

This command will start the FastAPI server, making your application accessible locally.","LangServe is an easy way to deploy any LangChain runnable, chain, or agent. It uses FastAPI and pydantic to automatically create an API server from any compatible LangChain component.",0.0,1.0,0.0,0.06608973813188646,0.15730337078651688
58,What's a string evaluator,"['Criteria Evaluation | Criteria Evaluation []( In scenarios where you wish to assess a model\'s output using a specific rubric or criteria set, the `criteria` evaluator proves to be a handy tool. It allows you to verify if an LLM or Chain\'s output complies with a defined set of criteria. To understand its functionality and configurability in depth, refer to the reference documentation of the [CriteriaEvalChain]( class. ### Usage without references In this example, you will use the `CriteriaEvalChain` to check whether an output is concise. First, create the evaluation chain to predict whether outputs are ""concise"". ```python from langchain.evaluation import load_evaluator evaluator = load_evaluator(""criteria"", criteria=""conciseness"") # This is equivalent to loading using the enum from langchain.evaluation import EvaluatorType evaluator = load_evaluator(EvaluatorType.CRITERIA, criteria=""conciseness"") ``` ```python eval_result = evaluator.evaluate_strings( prediction=""What\'s 2+2? That\'s an elementary question. The answer you\'re looking for is that two and two is four."", input=""What\'s 2+2?"", ) print(eval_result) ``` ```text {\'reasoning\': \'The criterion is conciseness, which means the submission should be brief and to the point. \\n\\nLooking at the submission, the answer to the question ""What\\\'s 2+2?"" is indeed ""four"". However, the respondent has added extra information, stating ""That\\\'s an elementary question."" This statement does not contribute to answering the question and therefore makes the response less concise.\\n\\nTherefore, the submission does not meet the criterion of conciseness.\\n\\nN\', \'value\': \'N\', \'score\': 0} ``` #### Output Format All string evaluators expose an [evaluate_strings]( (or async [aevaluate_strings]( method, which accepts: - input (str) The input to the agent. - prediction (str) The predicted response. The criteria evaluators return a dictionary with the following values: - score: Binary integer 0 to 1, where 1 would mean that the output is compliant with the criteria, and 0 otherwise - value: A ""Y"" or ""N"" corresponding to the score - reasoning: String ""chain of thought reasoning"" from the LLM generated prior to creating the score ## Using Reference Labels Some criteria (such as correctness) require reference labels to work correctly. To do this, initialize the `labeled_criteria` evaluator and call the evaluator with a `reference` string. ```python evaluator = load_evaluator(""labeled_criteria"", criteria=""correctness"") # We can even override the model\'s learned knowledge using ground truth labels eval_result = evaluator.evaluate_strings( input=""What is the capital of the US?"", prediction=""Topeka, KS"", reference=""The capital of the US is Topeka, KS, where it permanently moved from Washington D.C. on May 16, 2023"", ) print(f\'With ground truth: {eval_result[""score""]}\') ``` ```text With ground truth: 1 ``` **Default Criteria** Most of the time, you\'ll want to define your own custom criteria (see below), but we also provide some common criteria you can load with a single string. Here\'s a list of pre-implemented criteria. Note that in the absence of labels, the LLM merely predicts what it thinks the best answer is and is not grounded in actual law or context. ```python from langchain.evaluation import Criteria # For a list of other default supported criteria, try calling `supported_default_criteria` list(Criteria) ``` ```text [, , , , , , , , , , ] ``` ## Custom Criteria To evaluate outputs against your own custom criteria, or to be more explicit the definition of any of the default criteria, pass in a dictionary of `""criterion_name"": ""criterion_description""` Note: it\'s recommended that you create a single evaluator per criterion. This way, separate feedback can be provided for each aspect. Additionally, if you provide antagonistic criteria, the evaluator won\'t be very useful, as it will be configured to predict compliance for ALL of the criteria provided. ```python custom_criterion = { ""numeric"": ""Does the output contain numeric or mathematical information?"" } eval_chain = load_evaluator( EvaluatorType.CRITERIA, criteria=custom_criterion, ) query = ""Tell me a joke"" prediction = ""I ate some square pie but I don\'t know the square of pi."" eval_result = eval_chain.evaluate_strings(prediction=prediction, input=query) print(eval_result) # If you wanted to specify multiple criteria. Generally not recommended custom_criteria = { ""numeric"": ""Does the output contain numeric information?"", ""mathematical"": ""Does the output contain mathematical information?"", ""grammatical"": ""Is the output grammatically correct?"", ""logical"": ""Is the output logical?"", } eval_chain = load_evaluator( EvaluatorType.CRITERIA, criteria=custom_criteria, ) eval_result = eval_chain.evaluate_strings(prediction=prediction, input=query) print(""Multi-criteria evaluation"") print(eval_result) ``` ```text {\'reasoning\': ""The criterion asks if the output contains numeric or mathematical information. The joke in the submission does contain mathematical information. It refers to the mathematical concept of squaring a number and also mentions \'pi\', which is a mathematical constant. Therefore, the submission does meet the criterion.\\n\\nY"", \'value\': \'Y\', \'score\': 1} {\'reasoning\': \'Let\\\'s assess the submission based on the given criteria:\\n\\n1. Numeric: The output does not contain any explicit numeric information. The word ""square"" and ""pi"" are mathematical terms but they are not numeric information per se.\\n\\n2. Mathematical: The output does contain mathematical information. The terms ""square"" and ""pi"" are mathematical terms. The joke is a play on the mathematical concept of squaring a number (in this case, pi).\\n\\n3. Grammatical: The output is grammatically correct. The sentence structure, punctuation, and word usage are all correct.\\n\\n4. Logical: The output is logical. It makes sense within the context of the joke. The joke is a play on words between the mathematical concept of squaring a number (pi) and eating a square pie.\\n\\nBased on the above analysis, the submission does not meet all the criteria because it does not contain numeric information.\\nN\', \'value\': \'N\', \'score\': 0} ``` ## Using Constitutional Principles Custom rubrics are similar to principles from [Constitutional AI]( You can directly use your `ConstitutionalPrinciple` objects to instantiate the chain and take advantage of the many existing principles in LangChain. ```python from langchain.chains.constitutional_ai.principles import PRINCIPLES print(f""{len(PRINCIPLES)} available principles"") list(PRINCIPLES.items())[:5] ``` ```text 54 available principles [(\'harmful1\', ConstitutionalPrinciple(critique_request=\'Identify specific ways in which the assistant\'s last response is harmful, unethical, racist, sexist, toxic, dangerous, or illegal.\', revision_request=\'Please rewrite the assistant response to remove any and all harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\', name=\'harmful1\')), (\'harmful2\', ConstitutionalPrinciple(critique_request=\'Explain ways in which the assistant\'s preceding response may be harmful to either the human or others. For example, if the assistant says anything that encourages dangerous or illegal activity, make sure to point it out. Other examples of harm include racism, sexism, and other social biases.\', revision_request=\'Please rewrite the assistant response to remove any and all harmful content, including anything dangerous, illegal, racist, sexist or socially biased.\', name=\'harmful2\')), (\'harmful3\', ConstitutionalPrinciple(critique_request=\'Identify all ways in which the assistant\'s last response is harmful, unethical, or socially biased. Furthermore, provide specific details on how the assistant can improve its response.\', revision_request=\'Please rewrite the assistant response to remove all harmful, unethical, or socially biased content, and move the conversation in a positive direction.\', name=\'harmful3\')), (\'harmful4\', ConstitutionalPrinciple(critique_request=\'Identify specific ways in which the assistant\'s precedings response is harmful. Also point out potential harmfulness in the human\'s questions and responses. Examples of harm include but are not limited to racism, sexism, toxicity.\', revision_request=\'Rewrite the assistant response to remove all harmful content, and to politely point out any harmful assumptions from the human, such as racism, sexism, toxicity.\', name=\'harmful4\')), (\'insensitive\', ConstitutionalPrinciple(critique_request=\'Discuss the assistant\'s last response to the human and whether it is in any way insensitive, sexist, racist, or socially inappropriate.\', revision_request=\'Revise the assistant\'s last response to remove content that is in any way insensitive, sexist, racist, or socially inappropriate.\', name=\'insensitive\'))] ``` ```python evaluator = load_evaluator(EvaluatorType.CRITERIA, criteria=PRINCIPLES[""harmful1""]) eval_result = evaluator.evaluate_strings( prediction=""I say that man is a lilly-livered nincompoop"", input=""What do you think of Will?"", ) print(eval_result) ``` ```text {\'reasoning\': \'The criterion asks to identify if the assistant\\\'s response is harmful, unethical, racist, sexist, toxic, dangerous, or illegal.\\n\\nLooking at the assistant\\\'s response, it is clear that it is not racist or sexist as it does not discriminate or stereotype based on race or gender. \\n\\nThe response is also not illegal as it does not involve any criminal activity or encourage any form of illegal behavior.\\n\\nThe response is not dangerous as it does not pose a physical threat or risk to anyone\\\'s safety.\\n\\nHowever, the assistant\\\'s response can be considered harmful and toxic as it uses derogatory language (""lilly-livered nincompoop"") to describe \\\'Will\\\'. This can be seen as a form of verbal abuse or insult, which can cause emotional harm.\\n\\nThe response can also be seen as unethical, as it is generally considered inappropriate to insult or belittle someone in this manner.\\n\\nN\', \'value\': \'N\', \'score\': 0} ``` ## Configuring the LLM If you don\'t specify an eval LLM, the `load_evaluator` method will initialize a `gpt-4` LLM to power the grading chain. Below, use an anthropic model instead. ```python # %pip install ChatAnthropic # %env ANTHROPIC_API_KEY= ``` ```python from langchain.chat_models import ChatAnthropic llm = ChatAnthropic(temperature=0) evaluator = load_evaluator(""criteria"", llm=llm, criteria=""conciseness"") ``` ```python eval_result = evaluator.evaluate_strings( prediction=""What\'s 2+2? That\'s an elementary question. The answer you\'re looking for is that two and two is four."", input=""What\'s 2+2?"", ) print(eval_result) ``` ```text {\'reasoning\': \'Step 1) Analyze the conciseness criterion: Is the submission concise and to the point?\\nStep 2) The submission provides extraneous information beyond just answering the question directly. It characterizes the question as ""elementary"" and provides reasoning for why the answer is 4. This additional commentary makes the submission not fully concise.\\nStep 3) Therefore, based on the analysis of the conciseness criterion, the submission does not meet the criteria.\\n\\nN\', \'value\': \'N\', \'score\': 0} ``` # Configuring the Prompt If you want to completely customize the prompt, you can initialize the evaluator with a custom prompt template as follows. ```python from langchain.prompts import PromptTemplate fstring = """"""Respond Y or N based on how well the following response follows the specified rubric. Grade only based on the rubric and expected response: Grading Rubric: {criteria} Expected Response: {reference} DATA: --------- Question: {input} Response: {output} --------- Write out your explanation for each criterion, then respond with Y or N on a new line."""""" prompt = PromptTemplate.from_template(fstring) evaluator = load_evaluator(""labeled_criteria"", criteria=""correctness"", prompt=prompt) ``` ```python eval_result = evaluator.evaluate_strings( prediction=""What\'s 2+2? That\'s an elementary question. The answer you\'re looking for is that two and two is four."", input=""What\'s 2+2?"", reference=""It\'s 17 now."", ) print(eval_result) ``` ```text {\'reasoning\': \'Correctness: No, the response is not correct. The expected response was ""It\\\'s 17 now."" but the response given was ""What\\\'s 2+2? That\\\'s an elementary question. The answer you\\\'re looking for is that two and two is four.""\', \'value\': \'N\', \'score\': 0} ``` ## Conclusion In these examples, you used the `CriteriaEvalChain` to evaluate model outputs against custom criteria, including a custom rubric and constitutional principles. Remember when selecting criteria to decide whether they ought to require ground truth labels or not. Things like ""correctness"" are best evaluated with ground truth or with extensive context. Also, remember to pick aligned principles for a given chain so that the classification makes sense. - [Usage without references](#usage-without-references) - [Using Reference Labels](#using-reference-labels) - [Custom Criteria](#custom-criteria) - [Using Constitutional Principles](#using-constitutional-principles) - [Configuring the LLM](#configuring-the-llm) - [Conclusion](#conclusion)', 'Scoring Evaluator | Scoring Evaluator The Scoring Evaluator instructs a language model to assess your model\'s predictions on a specified scale (default is 1-10) based on your custom criteria or rubric. This feature provides a nuanced evaluation instead of a simplistic binary score, aiding in evaluating models against tailored rubrics and comparing model performance on specific tasks. Before we dive in, please note that any specific grade from an LLM should be taken with a grain of salt. A prediction that receives a scores of ""8"" may not be meaningfully better than one that receives a score of ""7"". ### Usage with Ground Truth For a thorough understanding, refer to the [LabeledScoreStringEvalChain documentation]( Below is an example demonstrating the usage of `LabeledScoreStringEvalChain` using the default prompt: ```python from langchain.chat_models import ChatOpenAI from langchain.evaluation import load_evaluator evaluator = load_evaluator(""labeled_score_string"", llm=ChatOpenAI(model=""gpt-4"")) ``` ```python # Correct eval_result = evaluator.evaluate_strings( prediction=""You can find them in the dresser\'s third drawer."", reference=""The socks are in the third drawer in the dresser"", input=""Where are my socks?"", ) print(eval_result) ``` ```text {\'reasoning\': ""The assistant\'s response is helpful, accurate, and directly answers the user\'s question. It correctly refers to the ground truth provided by the user, specifying the exact location of the socks. The response, while succinct, demonstrates depth by directly addressing the user\'s query without unnecessary details. Therefore, the assistant\'s response is highly relevant, correct, and demonstrates depth of thought. \\n\\nRating: [[10]]"", \'score\': 10} ``` When evaluating your app\'s specific context, the evaluator can be more effective if you provide a full rubric of what you\'re looking to grade. Below is an example using accuracy. ```python accuracy_criteria = { ""accuracy"": """""" Score 1: The answer is completely unrelated to the reference. Score 3: The answer has minor relevance but does not align with the reference. Score 5: The answer has moderate relevance but contains inaccuracies. Score 7: The answer aligns with the reference but has minor errors or omissions. Score 10: The answer is completely accurate and aligns perfectly with the reference."""""" } evaluator = load_evaluator( ""labeled_score_string"", criteria=accuracy_criteria, llm=ChatOpenAI(model=""gpt-4""), ) ``` ```python # Correct eval_result = evaluator.evaluate_strings( prediction=""You can find them in the dresser\'s third drawer."", reference=""The socks are in the third drawer in the dresser"", input=""Where are my socks?"", ) print(eval_result) ``` ```text {\'reasoning\': ""The assistant\'s answer is accurate and aligns perfectly with the reference. The assistant correctly identifies the location of the socks as being in the third drawer of the dresser. Rating: [[10]]"", \'score\': 10} ``` ```python # Correct but lacking information eval_result = evaluator.evaluate_strings( prediction=""You can find them in the dresser."", reference=""The socks are in the third drawer in the dresser"", input=""Where are my socks?"", ) print(eval_result) ``` ```text {\'reasoning\': ""The assistant\'s response is somewhat relevant to the user\'s query but lacks specific details. The assistant correctly suggests that the socks are in the dresser, which aligns with the ground truth. However, the assistant failed to specify that the socks are in the third drawer of the dresser. This omission could lead to confusion for the user. Therefore, I would rate this response as a 7, since it aligns with the reference but has minor omissions.\\n\\nRating: [[7]]"", \'score\': 7} ``` ```python # Incorrect eval_result = evaluator.evaluate_strings( prediction=""You can find them in the dog\'s bed."", reference=""The socks are in the third drawer in the dresser"", input=""Where are my socks?"", ) print(eval_result) ``` ```text {\'reasoning\': ""The assistant\'s response is completely unrelated to the reference. The reference indicates that the socks are in the third drawer in the dresser, whereas the assistant suggests that they are in the dog\'s bed. This is completely inaccurate. Rating: [[1]]"", \'score\': 1} ``` You can also make the evaluator normalize the score for you if you want to use these values on a similar scale to other evaluators. ```python evaluator = load_evaluator( ""labeled_score_string"", criteria=accuracy_criteria, llm=ChatOpenAI(model=""gpt-4""), normalize_by=10, ) ``` ```python # Correct but lacking information eval_result = evaluator.evaluate_strings( prediction=""You can find them in the dresser."", reference=""The socks are in the third drawer in the dresser"", input=""Where are my socks?"", ) print(eval_result) ``` ```text {\'reasoning\': ""The assistant\'s response is partially accurate. It correctly suggests that the socks are in the dresser, but fails to provide the specific location within the dresser, which is the third drawer according to the ground truth. Therefore, the response is relevant but contains a significant omission. Rating: [[7]]."", \'score\': 0.7} ``` ### Usage without references You can also use a scoring evaluator without reference labels. This is useful if you want to measure a prediction along specific semantic dimensions. Below is an example using ""helpfulness"" and ""harmlessness"" on a single scale. Refer to the documentation of the [ScoreStringEvalChain]( class for full details. ```python from langchain.evaluation import load_evaluator hh_criteria = { ""helpful"": ""The assistant\'s answer should be helpful to the user."", ""harmless"": ""The assistant\'s answer should not be illegal, harmful, offensive or unethical."", } evaluator = load_evaluator(""score_string"", criteria=hh_criteria) ``` ```python # Helpful but harmful eval_result = evaluator.evaluate_strings( prediction=""Sure I\'d be happy to help! First, locate a car in an area of low surveillance. Second, you need to break the window. Then, you need to hotwire the car. Finally, you need to drive away."", input=""What\'s the best way to steal a car?"", ) print(eval_result) ``` ```text {\'reasoning\': ""The assistant\'s response is inappropriate and unethical. It is providing instructions on how to commit an illegal act, which is stealing a car. This is not only harmful but also against the law. The assistant should have refused to provide such information. Therefore, the response is not helpful or harmless. Rating: [[1]]."", \'score\': 1} ``` ```python # Harmless but unhelpful eval_result = evaluator.evaluate_strings( prediction=""I can\'t help you with that."", input=""What\'s the best way to steal a car?"", ) print(eval_result) ``` ```text {\'reasoning\': ""The assistant\'s response is ethical and harmless as it refuses to provide information on illegal activities such as car theft. It is important for AI to promote legal and ethical behavior. However, the response is not helpful in terms of providing any useful or relevant information to the user. Rating: [[7]]"", \'score\': 7} ``` ```python # Helpful and harmless eval_result = evaluator.evaluate_strings( prediction=""Stealing cars is illegal and unethical. Have you considered other means to make money? You could get a part-time job, or start a business. If you don\'t have the financial means to support you and your family, you could apply for government assistance."", input=""What\'s the best way to steal a car?"", ) print(eval_result) ``` ```text {\'reasoning\': ""The assistant\'s response is both helpful and harmless. It does not provide any information on how to steal a car, which would be illegal and unethical. Instead, it suggests legal and ethical alternatives for making money, such as getting a job, starting a business, or applying for government assistance. This response is helpful because it provides the user with practical advice for their situation. Rating: [[10]]"", \'score\': 10} ``` #### Output Format As shown above, the scoring evaluators return a dictionary with the following values: - score: A score between 1 and 10 with 10 being the best. - reasoning: String ""chain of thought reasoning"" from the LLM generated prior to creating the score - [Usage with Ground Truth](#usage-with-ground-truth) - [Usage without references](#usage-without-references)', ""String Evaluators | String Evaluators A string evaluator is a component within LangChain designed to assess the performance of a language model by comparing its generated outputs (predictions) to a reference string or an input. This comparison is a crucial step in the evaluation of language models, providing a measure of the accuracy or quality of the generated text. In practice, string evaluators are typically used to evaluate a predicted string against a given input, such as a question or a prompt. Often, a reference label or context string is provided to define what a correct or ideal response would look like. These evaluators can be customized to tailor the evaluation process to fit your application's specific requirements. To create a custom string evaluator, inherit from the `StringEvaluator` class and implement the `_evaluate_strings` method. If you require asynchronous support, also implement the `_aevaluate_strings` method. Here's a summary of the key attributes and methods associated with a string evaluator: - `evaluation_name`: Specifies the name of the evaluation. - `requires_input`: Boolean attribute that indicates whether the evaluator requires an input string. If True, the evaluator will raise an error when the input isn't provided. If False, a warning will be logged if an input _is_ provided, indicating that it will not be considered in the evaluation. - `requires_reference`: Boolean attribute specifying whether the evaluator requires a reference label. If True, the evaluator will raise an error when the reference isn't provided. If False, a warning will be logged if a reference _is_ provided, indicating that it will not be considered in the evaluation. String evaluators also implement the following methods: - `aevaluate_strings`: Asynchronously evaluates the output of the Chain or Language Model, with support for optional input and label. - `evaluate_strings`: Synchronously evaluates the output of the Chain or Language Model, with support for optional input and label. The following sections provide detailed information on available string evaluator implementations as well as how to create a custom string evaluator. [ Criteria EvaluationOpen In Colab](/docs/guides/evaluation/string/criteria_eval_chain)[ Custom String EvaluatorOpen In Colab](/docs/guides/evaluation/string/custom)[ Embedding DistanceOpen In Colab](/docs/guides/evaluation/string/embedding_distance)[ Exact MatchOpen In Colab](/docs/guides/evaluation/string/exact_match)[ Evaluating Structured Output: JSON EvaluatorsEvaluating extraction and function calling applications often comes down to validation that the LLM's string output can be parsed correctly and how it compares to a reference object. The following JSON validators provide provide functionality to check your model's output in a consistent way.](/docs/guides/evaluation/string/json)[ Regex MatchOpen In Colab](/docs/guides/evaluation/string/regex_match)[ Scoring EvaluatorThe Scoring Evaluator instructs a language model to assess your model's predictions on a specified scale (default is 1-10) based on your custom criteria or rubric. This feature provides a nuanced evaluation instead of a simplistic binary score, aiding in evaluating models against tailored rubrics and comparing model performance on specific tasks.](/docs/guides/evaluation/string/scoring_eval_chain)[ String DistanceOpen In Colab](/docs/guides/evaluation/string/string_distance)""]","A string evaluator in LangChain is a component used to assess the performance of a language model by comparing its generated outputs (predictions) to a reference string or an input. It evaluates the accuracy or quality of the generated text against a given input, often with a reference label or context string defining what a correct or ideal response would look like. You can customize string evaluators to fit specific application requirements.",A string evaluator in LangChain is an object that assesses the performance of a language model's predicted response by comparing it to an input string and/or reference string.,0.9999999999666667,1.0,1.0,0.3100937347263979,0.4117647058823529
59,whats a toolkit,"['Chat Messages | Chat Messages infoHead to [Integrations](/docs/integrations/memory/) for documentation on built-in memory integrations with 3rd-party databases and tools. One of the core utility classes underpinning most (if not all) memory modules is the `ChatMessageHistory` class. This is a super lightweight wrapper that provides convenience methods for saving HumanMessages, AIMessages, and then fetching them all. You may want to use this class directly if you are managing memory outside of a chain. ```python from langchain.memory import ChatMessageHistory history = ChatMessageHistory() history.add_user_message(""hi!"") history.add_ai_message(""whats up?"") ``` ```python history.messages ``` ```text [HumanMessage(content=\'hi!\', additional_kwargs={}), AIMessage(content=\'whats up?\', additional_kwargs={})] ```', 'Redis | Redis [Redis (Remote Dictionary Server)]( is an open-source in-memory storage, used as a distributed, in-memory keyvalue database, cache and message broker, with optional durability. Because it holds all data in memory and because of its design, `Redis` offers low-latency reads and writes, making it particularly suitable for use cases that require a cache. Redis is the most popular NoSQL database, and one of the most popular databases overall. This notebook goes over how to use `Redis` to store chat message history. ```python from langchain.memory import RedisChatMessageHistory history = RedisChatMessageHistory(""foo"") history.add_user_message(""hi!"") history.add_ai_message(""whats up?"") ``` ```python history.messages ``` ```text [AIMessage(content=\'whats up?\', additional_kwargs={}), HumanMessage(content=\'hi!\', additional_kwargs={})] ```', 'Momento Cache | Momento Cache [Momento Cache]( is the world\'s first truly serverless caching service. It provides instant elasticity, scale-to-zero capability, and blazing-fast performance. This notebook goes over how to use [Momento Cache]( to store chat message history using the `MomentoChatMessageHistory` class. See the Momento [docs]( for more detail on how to get set up with Momento. Note that, by default we will create a cache if one with the given name doesn\'t already exist. You\'ll need to get a Momento API key to use this class. This can either be passed in to a momento.CacheClient if you\'d like to instantiate that directly, as a named parameter `api_key` to `MomentoChatMessageHistory.from_client_params`, or can just be set as an environment variable `MOMENTO_API_KEY`. ```python from datetime import timedelta from langchain.memory import MomentoChatMessageHistory session_id = ""foo"" cache_name = ""langchain"" ttl = timedelta(days=1) history = MomentoChatMessageHistory.from_client_params( session_id, cache_name, ttl, ) history.add_user_message(""hi!"") history.add_ai_message(""whats up?"") ``` ```python history.messages ``` ```text [HumanMessage(content=\'hi!\', additional_kwargs={}, example=False), AIMessage(content=\'whats up?\', additional_kwargs={}, example=False)] ```']","A toolkit in this context refers to a collection of utility classes and methods that help manage chat message history. For example, `ChatMessageHistory`, `RedisChatMessageHistory`, and `MomentoChatMessageHistory` are part of the toolkit for storing and retrieving chat messages.",Toolkits are collections of tools that are designed to be used together for specific tasks and have convenience loading methods.,0.0,0.0,1.0,0.027233692390443456,0.24561403508771934
60,is langchain compatible with pydantic v2?,"['Pydantic compatibility | Pydantic compatibility - Pydantic v2 was released in June, 2023 ([ - v2 contains has a number of breaking changes ([ - Pydantic v2 and v1 are under the same package name, so both versions cannot be installed at the same time ## LangChain Pydantic migration plan As of `langchain>=0.0.267`, LangChain will allow users to install either Pydantic V1 or V2. - Internally LangChain will continue to [use V1]( - During this time, users can pin their pydantic version to v1 to avoid breaking changes, or start a partial migration using pydantic v2 throughout their code, but avoiding mixing v1 and v2 code for LangChain (see below). User can either pin to pydantic v1, and upgrade their code in one go once LangChain has migrated to v2 internally, or they can start a partial migration to v2, but must avoid mixing v1 and v2 code for LangChain. Below are two examples of showing how to avoid mixing pydantic v1 and v2 code in the case of inheritance and in the case of passing objects to LangChain. **Example 1: Extending via inheritance** **YES** ```python from pydantic.v1 import root_validator, validator class CustomTool(BaseTool): # BaseTool is v1 code x: int = Field(default=1) def _run(*args, **kwargs): return ""hello"" @validator(\'x\') # v1 code @classmethod def validate_x(cls, x: int) -> int: return 1 CustomTool( name=\'custom_tool\', description=""hello"", x=1, ) ``` Mixing Pydantic v2 primitives with Pydantic v1 primitives can raise cryptic errors **NO** ```python from pydantic import Field, field_validator # pydantic v2 class CustomTool(BaseTool): # BaseTool is v1 code x: int = Field(default=1) def _run(*args, **kwargs): return ""hello"" @field_validator(\'x\') # v2 code @classmethod def validate_x(cls, x: int) -> int: return 1 CustomTool( name=\'custom_tool\', description=""hello"", x=1, ) ``` **Example 2: Passing objects to LangChain** **YES** ```python from langchain.tools.base import Tool from pydantic.v1 import BaseModel, Field # <-- Uses v1 namespace class CalculatorInput(BaseModel): question: str = Field() Tool.from_function( # <-- tool uses v1 namespace func=lambda question: \'hello\', name=""Calculator"", description=""useful for when you need to answer questions about math"", args_schema=CalculatorInput ) ``` **NO** ```python from langchain.tools.base import Tool from pydantic import BaseModel, Field # <-- Uses v2 namespace class CalculatorInput(BaseModel): question: str = Field() Tool.from_function( # <-- tool uses v1 namespace func=lambda question: \'hello\', name=""Calculator"", description=""useful for when you need to answer questions about math"", args_schema=CalculatorInput ) ``` - [LangChain Pydantic migration plan](#langchain-pydantic-migration-plan)', 'LangServe | LangServe []( []( []( []( We will be releasing a hosted version of LangServe for one-click deployments of LangChain applications. [Sign up here]( to get on the waitlist. ## Overview `LangServe` helps developers deploy `LangChain` [runnables and chains]( as a REST API. This library is integrated with [FastAPI]( and uses [pydantic]( for data validation. In addition, it provides a client that can be used to call into runnables deployed on a server. A javascript client is available in [LangChainJS]( ## Features - Input and Output schemas automatically inferred from your LangChain object, and enforced on every API call, with rich error messages - API docs page with JSONSchema and Swagger (insert example link) - Efficient `/invoke`, `/batch` and `/stream` endpoints with support for many concurrent requests on a single server - `/stream_log` endpoint for streaming all (or some) intermediate steps from your chain/agent - Playground page at `/playground` with streaming output and intermediate steps - Built-in (optional) tracing to [LangSmith]( just add your API key (see [Instructions]( - All built with battle-tested open-source Python libraries like FastAPI, Pydantic, uvloop and asyncio. - Use the client SDK to call a LangServe server as if it was a Runnable running locally (or call the HTTP API directly) - [LangServe Hub]( ### Limitations - Client callbacks are not yet supported for events that originate on the server - OpenAPI docs will not be generated when using Pydantic V2. Fast API does not support [mixing pydantic v1 and v2 namespaces]( See section below for more details. ## Hosted LangServe We will be releasing a hosted version of LangServe for one-click deployments of LangChain applications. [Sign up here]( to get on the waitlist. ## Security - Vulnerability in Versions 0.0.13 - 0.0.15 -- playground endpoint allows accessing arbitrary files on server. [Resolved in 0.0.16]( ## Installation For both client and server: ```bash pip install ""langserve[all]"" ``` or `pip install ""langserve[client]""` for client code, and `pip install ""langserve[server]""` for server code. ## LangChain CLI Use the `LangChain` CLI to bootstrap a `LangServe` project quickly. To use the langchain CLI make sure that you have a recent version of `langchain-cli` installed. You can install it with `pip install -U langchain-cli`. ```sh langchain app new ../path/to/directory ``` ## Examples Get your LangServe instance started quickly with [LangChain Templates]( For more examples, see the templates [index]( or the [examples]( directory. ### Server Here\'s a server that deploys an OpenAI chat model, an Anthropic chat model, and a chain that uses the Anthropic model to tell a joke about a topic. ```python #!/usr/bin/env python from fastapi import FastAPI from langchain.prompts import ChatPromptTemplate from langchain.chat_models import ChatAnthropic, ChatOpenAI from langserve import add_routes app = FastAPI( title=""LangChain Server"", version=""1.0"", description=""A simple api server using Langchain\'s Runnable interfaces"", ) add_routes( app, ChatOpenAI(), path=""/openai"", ) add_routes( app, ChatAnthropic(), path=""/anthropic"", ) model = ChatAnthropic() prompt = ChatPromptTemplate.from_template(""tell me a joke about {topic}"") add_routes( app, prompt | model, path=""/joke"", ) if __name__ == ""__main__"": import uvicorn uvicorn.run(app, host=""localhost"", port=8000) ``` ### Docs If you\'ve deployed the server above, you can view the generated OpenAPI docs using: If using pydantic v2, docs will not be generated for invoke/batch/stream/stream_log. See [Pydantic](#pydantic) section below for more details. ```sh curl localhost:8000/docs ``` make sure to **add** the `/docs` suffix. Index page `/` is not defined by **design**, so `curl localhost:8000` or visiting the URL will return a 404. If you want content at `/` define an endpoint `@app.get(""/"")`. ### Client Python SDK ```python from langchain.schema import SystemMessage, HumanMessage from langchain.prompts import ChatPromptTemplate from langchain.schema.runnable import RunnableMap from langserve import RemoteRunnable openai = RemoteRunnable("" anthropic = RemoteRunnable("" joke_chain = RemoteRunnable("" joke_chain.invoke({""topic"": ""parrots""}) # or async await joke_chain.ainvoke({""topic"": ""parrots""}) prompt = [ SystemMessage(content=\'Act like either a cat or a parrot.\'), HumanMessage(content=\'Hello!\') ] # Supports astream async for msg in anthropic.astream(prompt): print(msg, end="""", flush=True) prompt = ChatPromptTemplate.from_messages( [(""system"", ""Tell me a long story about {topic}"")] ) # Can define custom chains chain = prompt | RunnableMap({ ""openai"": openai, ""anthropic"": anthropic, }) chain.batch([{ ""topic"": ""parrots"" }, { ""topic"": ""cats"" }]) ``` In TypeScript (requires LangChain.js version 0.0.166 or later): ```typescript import { RemoteRunnable } from ""langchain/runnables/remote""; const chain = new RemoteRunnable({ url: ` }); const result = await chain.invoke({ topic: ""cats"", }); ``` Python using `requests`: ```python import requests response = requests.post( "" json={\'input\': {\'topic\': \'cats\'}} ) response.json() ``` You can also use `curl`: ```sh curl --location --request POST \' \\ --header \'Content-Type: application/json\' \\ --data-raw \'{ ""input"": { ""topic"": ""cats"" } }\' ``` ## Endpoints The following code: ```python ... add_routes( app, runnable, path=""/my_runnable"", ) ``` adds of these endpoints to the server: - `POST /my_runnable/invoke` - invoke the runnable on a single input - `POST /my_runnable/batch` - invoke the runnable on a batch of inputs - `POST /my_runnable/stream` - invoke on a single input and stream the output - `POST /my_runnable/stream_log` - invoke on a single input and stream the output, including output of intermediate steps as it\'s generated - `GET /my_runnable/input_schema` - json schema for input to the runnable - `GET /my_runnable/output_schema` - json schema for output of the runnable - `GET /my_runnable/config_schema` - json schema for config of the runnable These endpoints match the [LangChain Expression Language interface]( -- please reference this documentation for more details. ## Playground You can find a playground page for your runnable at `/my_runnable/playground`. This exposes a simple UI to [configure]( and invoke your runnable with streaming output and intermediate steps. ![]( ### Widgets The playground supports [widgets](#playground-widgets) and can be used to test your runnable with different inputs. In addition, for configurable runnables, the playground will allow you to configure the runnable and share a link with the configuration: ### Sharing ![]( ## Legacy Chains LangServe works with both Runnables (constructed via [LangChain Expression Language]( and legacy chains (inheriting from `Chain`). However, some of the input schemas for legacy chains may be incomplete/incorrect, leading to errors. This can be fixed by updating the `input_schema` property of those chains in LangChain. If you encounter any errors, please open an issue on THIS repo, and we will work to address it. ## Deployment ### Deploy to GCP You can deploy to GCP Cloud Run using the following command: ```text gcloud run deploy [your-service-name] --source . --port 8001 --allow-unauthenticated --region us-central1 --set-env-vars=OPENAI_API_KEY=your_key ``` ## Pydantic LangServe provides support for Pydantic 2 with some limitations. 1. OpenAPI docs will not be generated for invoke/batch/stream/stream_log when using Pydantic V2. Fast API does not support [mixing pydantic v1 and v2 namespaces] . 2. LangChain uses the v1 namespace in Pydantic v2. Please read the [following guidelines to ensure compatibility with LangChain]( Except for these limitations, we expect the API endpoints, the playground and any other features to work as expected. ## Advanced ## Handling Authentication If you need to add authentication to your server, please reference FastAPI\'s [security documentation]( and [middleware documentation]( ### Files LLM applications often deal with files. There are different architectures that can be made to implement file processing; at a high level: 1. The file may be uploaded to the server via a dedicated endpoint and processed using a separate endpoint 2. The file may be uploaded by either value (bytes of file) or reference (e.g., s3 url to file content) 3. The processing endpoint may be blocking or non-blocking 4. If significant processing is required, the processing may be offloaded to a dedicated process pool You should determine what is the appropriate architecture for your application. Currently, to upload files by value to a runnable, use base64 encoding for the file (`multipart/form-data` is not supported yet). Here\'s an [example]( that shows how to use base64 encoding to send a file to a remote runnable. Remember, you can always upload files by reference (e.g., s3 url) or upload them as multipart/form-data to a dedicated endpoint. ### Custom Input and Output Types Input and Output types are defined on all runnables. You can access them via the `input_schema` and `output_schema` properties. `LangServe` uses these types for validation and documentation. If you want to override the default inferred types, you can use the `with_types` method. Here\'s a toy example to illustrate the idea: ```python from typing import Any from fastapi import FastAPI from langchain.schema.runnable import RunnableLambda app = FastAPI() def func(x: Any) -> int: """"""Mistyped function that should accept an int but accepts anything."""""" return x + 1 runnable = RunnableLambda(func).with_types( input_schema=int, ) add_routes(app, runnable) ``` ### Custom User Types Inherit from `CustomUserType` if you want the data to de-serialize into a pydantic model rather than the equivalent dict representation. At the moment, this type only works _server_ side and is used to specify desired _decoding_ behavior. If inheriting from this type the server will keep the decoded type as a pydantic model instead of converting it into a dict. ```python from fastapi import FastAPI from langchain.schema.runnable import RunnableLambda from langserve import add_routes from langserve.schema import CustomUserType app = FastAPI() class Foo(CustomUserType): bar: int def func(foo: Foo) -> int: """"""Sample function that expects a Foo type which is a pydantic model"""""" assert isinstance(foo, Foo) return foo.bar # Note that the input and output type are automatically inferred! # You do not need to specify them. # runnable = RunnableLambda(func).with_types( # <-- Not needed in this case # input_schema=Foo, # output_schema=int, # add_routes(app, RunnableLambda(func), path=""/foo"") ``` ### Playground Widgets\u200b The playground allows you to define custom widgets for your runnable from the backend. - A widget is specified at the field level and shipped as part of the JSON schema of the input type - A widget must contain a key called `type` with the value being one of a well known list of widgets - Other widget keys will be associated with values that describe paths in a JSON object General schema: ```typescript type JsonPath = number | string | (number | string)[]; type NameSpacedPath = { title: string; path: JsonPath }; // Using title to mimick json schema, but can use namespace type OneOfPath = { oneOf: JsonPath[] }; type Widget = { type: string // Some well known type (e.g., base64file, chat etc.) [key: string]: JsonPath | NameSpacedPath | OneOfPath; }; ``` #### File Upload Widget\u200b Allows creation of a file upload input in the UI playground for files that are uploaded as base64 encoded strings. Here\'s the full [example]( Snippet: ```python try: from pydantic.v1 import Field except ImportError: from pydantic import Field from langserve import CustomUserType # ATTENTION: Inherit from CustomUserType instead of BaseModel otherwise # the server will decode it into a dict instead of a pydantic model. class FileProcessingRequest(CustomUserType): """"""Request including a base64 encoded file."""""" # The extra field is used to specify a widget for the playground UI. file: str = Field(..., extra={""widget"": {""type"": ""base64file""}}) num_chars: int = 100 ``` Example widget: ![]( - [Overview](#overview) - [Features](#features)- [Limitations](#limitations) - [Hosted LangServe](#hosted-langserve) - [Security](#security) - [Installation](#installation) - [LangChain CLI ](#langchain-cli-) - [Examples](#examples)- [Server](#server) - [Docs](#docs) - [Client](#client) - [Endpoints](#endpoints) - [Playground](#playground)- [Widgets](#widgets) - [Sharing](#sharing) - [Legacy Chains](#legacy-chains) - [Deployment](#deployment)- [Deploy to GCP](#deploy-to-gcp) - [Pydantic](#pydantic) - [Advanced](#advanced) - [Handling Authentication](#handling-authentication)- [Files](#files) - [Custom Input and Output Types](#custom-input-and-output-types) - [Custom User Types](#custom-user-types) - [Playground Widgets](#playground-widgets)', 'Map re-rank | Map re-rank The map re-rank documents chain runs an initial prompt on each document, that not only tries to complete a task but also gives a score for how certain it is in its answer. The highest scoring response is returned. ![map_rerank_diagram](/assets/images/map_rerank-0302b59b690c680ad6099b7bfe6d9fe5.jpg) ## Recreating with LCEL With [LangChain Expression Language](/docs/expression_language) we can recreate the `MapRerankDocumentsChain` functionality, with the additional benefit of getting all the built-in LCEL features (batch, async, etc.) and with much more ability to customize specific parts of the chain. ```python from langchain.chat_models import ChatOpenAI from langchain.output_parsers.openai_functions import PydanticOutputFunctionsParser from langchain.prompts import PromptTemplate from langchain.pydantic_v1 import BaseModel, Field from langchain.schema.prompt_template import format_document from langchain.utils.openai_functions import convert_pydantic_to_openai_function ``` ```python # Chain to apply to each individual document. Chain # provides an answer to the question based on the document # and scores it\'s confidence in the answer. map_prompt = PromptTemplate.from_template( ""Answer the user question using the context."" ""\\n\\nContext:\\n\\n{context}\\n\\nQuestion: {question}"" ) class AnswerAndScore(BaseModel): """"""Return the answer to the question and a relevance score."""""" answer: str = Field( description=""The answer to the question, which is based ONLY on the provided context."" ) score: float = Field( decsription=""A 0.0-1.0 relevance score, where 1.0 indicates the provided context answers the question completely and 0.0 indicates the provided context does not answer the question at all."" ) function = convert_pydantic_to_openai_function(AnswerAndScore) map_chain = ( map_prompt | ChatOpenAI().bind( temperature=0, functions=[function], function_call={""name"": ""AnswerAndScore""} ) | PydanticOutputFunctionsParser(pydantic_schema=AnswerAndScore) ).with_config(run_name=""Map"") ``` ```python # Final chain, which after answer and scoring based on # each doc return the answer with the highest score. def top_answer(scored_answers): return max(scored_answers, key=lambda x: x.score).answer document_prompt = PromptTemplate.from_template(""{page_content}"") map_rerank_chain = ( ( lambda x: [ { ""context"": format_document(doc, document_prompt), ""question"": x[""question""], } for doc in x[""docs""] ] ) | map_chain.map() | top_answer ).with_config(run_name=""Map rerank"") ``` ## Example run ```python from langchain.schema import Document text = """"""Nuclear power in space is the use of nuclear power in outer space, typically either small fission systems or radioactive decay for electricity or heat. Another use is for scientific observation, as in a Mssbauer spectrometer. The most common type is a radioisotope thermoelectric generator, which has been used on many space probes and on crewed lunar missions. Small fission reactors for Earth observation satellites, such as the TOPAZ nuclear reactor, have also been flown.[1] A radioisotope heater unit is powered by radioactive decay and can keep components from becoming too cold to function, potentially over a span of decades.[2] The United States tested the SNAP-10A nuclear reactor in space for 43 days in 1965,[3] with the next test of a nuclear reactor power system intended for space use occurring on 13 September 2012 with the Demonstration Using Flattop Fission (DUFF) test of the Kilopower reactor.[4] After a ground-based test of the experimental 1965 Romashka reactor, which used uranium and direct thermoelectric conversion to electricity,[5] the USSR sent about 40 nuclear-electric satellites into space, mostly powered by the BES-5 reactor. The more powerful TOPAZ-II reactor produced 10 kilowatts of electricity.[3] Examples of concepts that use nuclear power for space propulsion systems include the nuclear electric rocket (nuclear powered ion thruster(s)), the radioisotope rocket, and radioisotope electric propulsion (REP).[6] One of the more explored concepts is the nuclear thermal rocket, which was ground tested in the NERVA program. Nuclear pulse propulsion was the subject of Project Orion.[7] Regulation and hazard prevention[edit] After the ban of nuclear weapons in space by the Outer Space Treaty in 1967, nuclear power has been discussed at least since 1972 as a sensitive issue by states.[8] Particularly its potential hazards to Earth\'s environment and thus also humans has prompted states to adopt in the U.N. General Assembly the Principles Relevant to the Use of Nuclear Power Sources in Outer Space (1992), particularly introducing safety principles for launches and to manage their traffic.[8] Benefits Both the Viking 1 and Viking 2 landers used RTGs for power on the surface of Mars. (Viking launch vehicle pictured) While solar power is much more commonly used, nuclear power can offer advantages in some areas. Solar cells, although efficient, can only supply energy to spacecraft in orbits where the solar flux is sufficiently high, such as low Earth orbit and interplanetary destinations close enough to the Sun. Unlike solar cells, nuclear power systems function independently of sunlight, which is necessary for deep space exploration. Nuclear-based systems can have less mass than solar cells of equivalent power, allowing more compact spacecraft that are easier to orient and direct in space. In the case of crewed spaceflight, nuclear power concepts that can power both life support and propulsion systems may reduce both cost and flight time.[9] Selected applications and/or technologies for space include: Radioisotope thermoelectric generator Radioisotope heater unit Radioisotope piezoelectric generator Radioisotope rocket Nuclear thermal rocket Nuclear pulse propulsion Nuclear electric rocket """""" docs = [ Document( page_content=split, metadata={""source"": "" ) for split in text.split(""\\n\\n"") ] ``` ```python print( map_rerank_chain.invoke({""docs"": docs, ""question"": ""How were the vikings powered""}) ) ``` ```text The Viking missions were powered by radioisotope thermoelectric generators (RTGs). These generators used the heat produced by the natural decay of plutonium-238 to generate electricity. ``` - [Recreating with LCEL](#recreating-with-lcel) - [Example run](#example-run)']","As of `langchain>=0.0.267`, LangChain allows users to install either Pydantic V1 or V2. However, internally LangChain continues to use V1. To avoid breaking changes, users can pin their Pydantic version to v1. For a partial migration to Pydantic v2, avoid mixing v1 and v2 code for LangChain.","As of langchain>=0.0.267, LangChain will allow users to install either Pydantic V1 or V2, though it internally will use pydantic V1.",0.9999999999666667,1.0,1.0,0.3682358178441755,0.5405405405405406
61,how many llm api calls are made in OpenAIFunctionsAgent,"['LLMonitor | LLMonitor [LLMonitor]( is an open-source observability platform that provides cost and usage analytics, user tracking, tracing and evaluation tools. ## Setup Create an account on [llmonitor.com]( then copy your new app\'s `tracking id`. Once you have it, set it as an environment variable by running: ```bash export LLMONITOR_APP_ID=""..."" ``` If you\'d prefer not to set an environment variable, you can pass the key directly when initializing the callback handler: ```python from langchain.callbacks import LLMonitorCallbackHandler handler = LLMonitorCallbackHandler(app_id=""..."") ``` ## Usage with LLM/Chat models ```python from langchain.llms import OpenAI from langchain.chat_models import ChatOpenAI from langchain.callbacks import LLMonitorCallbackHandler handler = LLMonitorCallbackHandler() llm = OpenAI( callbacks=[handler], ) chat = ChatOpenAI(callbacks=[handler]) llm(""Tell me a joke"") ``` ## Usage with chains and agents Make sure to pass the callback handler to the `run` method so that all related chains and llm calls are correctly tracked. It is also recommended to pass `agent_name` in the metadata to be able to distinguish between agents in the dashboard. Example: ```python from langchain.chat_models import ChatOpenAI from langchain.schema import SystemMessage, HumanMessage from langchain.agents import OpenAIFunctionsAgent, AgentExecutor, tool from langchain.callbacks import LLMonitorCallbackHandler llm = ChatOpenAI(temperature=0) handler = LLMonitorCallbackHandler() @tool def get_word_length(word: str) -> int: """"""Returns the length of a word."""""" return len(word) tools = [get_word_length] prompt = OpenAIFunctionsAgent.create_prompt( system_message=SystemMessage( content=""You are very powerful assistant, but bad at calculating lengths of words."" ) ) agent = OpenAIFunctionsAgent(llm=llm, tools=tools, prompt=prompt, verbose=True) agent_executor = AgentExecutor( agent=agent, tools=tools, verbose=True, metadata={""agent_name"": ""WordCount""} # <- recommended, assign a custom name ) agent_executor.run(""how many letters in the word educa?"", callbacks=[handler]) ``` Another example: ```python from langchain.agents import load_tools, initialize_agent, AgentType from langchain.llms import OpenAI from langchain.callbacks import LLMonitorCallbackHandler handler = LLMonitorCallbackHandler() llm = OpenAI(temperature=0) tools = load_tools([""serpapi"", ""llm-math""], llm=llm) agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, metadata={ ""agent_name"": ""GirlfriendAgeFinder"" }) # <- recommended, assign a custom name agent.run( ""Who is Leo DiCaprio\'s girlfriend? What is her current age raised to the 0.43 power?"", callbacks=[handler], ) ``` ## User Tracking User tracking allows you to identify your users, track their cost, conversations and more. ```python from langchain.callbacks.llmonitor_callback import LLMonitorCallbackHandler, identify with identify(""user-123""): llm(""Tell me a joke"") with identify(""user-456"", user_props={""email"": ""user456@test.com""}): agen.run(""Who is Leo DiCaprio\'s girlfriend?"") ``` ## Support For any question or issue with integration you can reach out to the LLMonitor team on [Discord]( or via [email](mailto:vince@llmonitor.com). - [Setup](#setup) - [Usage with LLM/Chat models](#usage-with-llmchat-models) - [Usage with chains and agents](#usage-with-chains-and-agents) - [User Tracking](#user-tracking) - [Support](#support)', 'Agents | Agents The core idea of agents is to use a language model to choose a sequence of actions to take. In chains, a sequence of actions is hardcoded (in code). In agents, a language model is used as a reasoning engine to determine which actions to take and in which order. ## Concepts There are several key components here: ### Agent This is the chain responsible for deciding what step to take next. This is powered by a language model and a prompt. The inputs to this chain are: 1. Tools: Descriptions of available tools 2. User input: The high level objective 3. Intermediate steps: Any (action, tool output) pairs previously executed in order to achieve the user input The output is the next action(s) to take or the final response to send to the user (`AgentAction`s or `AgentFinish`). An action specifies a tool and the input to that tool. Different agents have different prompting styles for reasoning, different ways of encoding inputs, and different ways of parsing the output. For a full list of built-in agents see [agent types](/docs/modules/agents/agent_types/). You can also **easily build custom agents**, which we show how to do in the Get started section below. ### Tools Tools are functions that an agent can invoke. There are two important design considerations around tools: 1. Giving the agent access to the right tools 2. Describing the tools in a way that is most helpful to the agent Without thinking through both, you won\'t be able to build a working agent. If you don\'t give the agent access to a correct set of tools, it will never be able to accomplish the objectives you give it. If you don\'t describe the tools well, the agent won\'t know how to use them properly. LangChain provides a wide set of built-in tools, but also makes it easy to define your own (including custom descriptions). For a full list of built-in tools, see the [tools integrations section](/docs/integrations/tools/) ### Toolkits For many common tasks, an agent will need a set of related tools. For this LangChain provides the concept of toolkits - groups of around 3-5 tools needed to accomplish specific objectives. For example, the GitHub toolkit has a tool for searching through GitHub issues, a tool for reading a file, a tool for commenting, etc. LangChain provides a wide set of toolkits to get started. For a full list of built-in toolkits, see the [toolkits integrations section](/docs/integrations/toolkits/) ### AgentExecutor The agent executor is the runtime for an agent. This is what actually calls the agent, executes the actions it chooses, passes the action outputs back to the agent, and repeats. In pseudocode, this looks roughly like: ```python next_action = agent.get_action(...) while next_action != AgentFinish: observation = run(next_action) next_action = agent.get_action(..., next_action, observation) return next_action ``` While this may seem simple, there are several complexities this runtime handles for you, including: 1. Handling cases where the agent selects a non-existent tool 2. Handling cases where the tool errors 3. Handling cases where the agent produces output that cannot be parsed into a tool invocation 4. Logging and observability at all levels (agent decisions, tool calls) to stdout and/or to [LangSmith](/docs/langsmith). ### Other types of agent runtimes The `AgentExecutor` class is the main agent runtime supported by LangChain. However, there are other, more experimental runtimes we also support. These include: - [Plan-and-execute Agent](/docs/use_cases/more/agents/autonomous_agents/plan_and_execute) - [Baby AGI](/docs/use_cases/more/agents/autonomous_agents/baby_agi) - [Auto GPT](/docs/use_cases/more/agents/autonomous_agents/autogpt) You can also always create your own custom execution logic, which we show how to do below. ## Get started To best understand the agent framework, lets build an agent from scratch using LangChain Expression Language (LCEL). We\'ll need to build the agent itself, define custom tools, and run the agent and tools in a custom loop. At the end we\'ll show how to use the standard LangChain `AgentExecutor` to make execution easier. Some important terminology (and schema) to know: 1. `AgentAction`: This is a dataclass that represents the action an agent should take. It has a `tool` property (which is the name of the tool that should be invoked) and a `tool_input` property (the input to that tool) 2. `AgentFinish`: This is a dataclass that signifies that the agent has finished and should return to the user. It has a `return_values` parameter, which is a dictionary to return. It often only has one key - `output` - that is a string, and so often it is just this key that is returned. 3. `intermediate_steps`: These represent previous agent actions and corresponding outputs that are passed around. These are important to pass to future iteration so the agent knows what work it has already done. This is typed as a `List[Tuple[AgentAction, Any]]`. Note that observation is currently left as type `Any` to be maximally flexible. In practice, this is often a string. ### Setup: LangSmith By definition, agents take a self-determined, input-dependent sequence of steps before returning a user-facing output. This makes debugging these systems particularly tricky, and observability particularly important. [LangSmith](/docs/langsmith) is especially useful for such cases. When building with LangChain, any built-in agent or custom agent built with LCEL will automatically be traced in LangSmith. And if we use the `AgentExecutor`, we\'ll get full tracing of not only the agent planning steps but also the tool inputs and outputs. To set up LangSmith we just need set the following environment variables: ```bash export LANGCHAIN_TRACING_V2=""true"" export LANGCHAIN_API_KEY="""" ``` ### Define the agent We first need to create our agent. This is the chain responsible for determining what action to take next. In this example, we will use OpenAI Function Calling to create this agent. **This is generally the most reliable way to create agents.** For this guide, we will construct a custom agent that has access to a custom tool. We are choosing this example because for most real world use cases you will NEED to customize either the agent or the tools. We\'ll create a simple tool that computes the length of a word. This is useful because it\'s actually something LLMs can mess up due to tokenization. We will first create it WITHOUT memory, but we will then show how to add memory in. Memory is needed to enable conversation. First, let\'s load the language model we\'re going to use to control the agent. ```python from langchain.chat_models import ChatOpenAI llm = ChatOpenAI(model=""gpt-3.5-turbo"", temperature=0) ``` We can see that it struggles to count the letters in the string ""educa"". ```python llm.invoke(""how many letters in the word educa?"") ``` ```text AIMessage(content=\'There are 6 letters in the word ""educa"".\') ``` Next, let\'s define some tools to use. Let\'s write a really simple Python function to calculate the length of a word that is passed in. ```python from langchain.agents import tool @tool def get_word_length(word: str) -> int: """"""Returns the length of a word."""""" return len(word) tools = [get_word_length] ``` Now let us create the prompt. Because OpenAI Function Calling is finetuned for tool usage, we hardly need any instructions on how to reason, or how to output format. We will just have two input variables: `input` and `agent_scratchpad`. `input` should be a string containing the user objective. `agent_scratchpad` should be a sequence of messages that contains the previous agent tool invocations and the corresponding tool outputs. ```python from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder prompt = ChatPromptTemplate.from_messages( [ ( ""system"", ""You are very powerful assistant, but bad at calculating lengths of words."", ), (""user"", ""{input}""), MessagesPlaceholder(variable_name=""agent_scratchpad""), ] ) ``` How does the agent know what tools it can use? In this case we\'re relying on OpenAI function calling LLMs, which take functions as a separate argument and have been specifically trained to know when to invoke those functions. To pass in our tools to the agent, we just need to format them to the OpenAI function format and pass them to our model. (By `bind`-ing the functions, we\'re making sure that they\'re passed in each time the model is invoked.) ```python from langchain.tools.render import format_tool_to_openai_function llm_with_tools = llm.bind(functions=[format_tool_to_openai_function(t) for t in tools]) ``` Putting those pieces together, we can now create the agent. We will import two last utility functions: a component for formatting intermediate steps (agent action, tool output pairs) to input messages that can be sent to the model, and a component for converting the output message into an agent action/agent finish. ```python from langchain.agents.format_scratchpad import format_to_openai_function_messages from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser agent = ( { ""input"": lambda x: x[""input""], ""agent_scratchpad"": lambda x: format_to_openai_function_messages( x[""intermediate_steps""] ), } | prompt | llm_with_tools | OpenAIFunctionsAgentOutputParser() ) ``` Now that we have our agent, let\'s play around with it! Let\'s pass in a simple question and empty intermediate steps and see what it returns: ```python agent.invoke({""input"": ""how many letters in the word educa?"", ""intermediate_steps"": []}) ``` ```text AgentActionMessageLog(tool=\'get_word_length\', tool_input={\'word\': \'educa\'}, log=""\\nInvoking: `get_word_length` with `{\'word\': \'educa\'}`\\n\\n\\n"", message_log=[AIMessage(content=\'\', additional_kwargs={\'function_call\': {\'arguments\': \'{\\n ""word"": ""educa""\\n}\', \'name\': \'get_word_length\'}})]) ``` We can see that it responds with an `AgentAction` to take (it\'s actually an `AgentActionMessageLog` - a subclass of `AgentAction` which also tracks the full message log). If we\'ve set up LangSmith, we\'ll see a trace that let\'s us inspect the input and output to each step in the sequence: [ ### Define the runtime So this is just the first step - now we need to write a runtime for this. The simplest one is just one that continuously loops, calling the agent, then taking the action, and repeating until an `AgentFinish` is returned. Let\'s code that up below: ```python from langchain.schema.agent import AgentFinish user_input = ""how many letters in the word educa?"" intermediate_steps = [] while True: output = agent.invoke( { ""input"": user_input, ""intermediate_steps"": intermediate_steps, } ) if isinstance(output, AgentFinish): final_result = output.return_values[""output""] break else: print(f""TOOL NAME: {output.tool}"") print(f""TOOL INPUT: {output.tool_input}"") tool = {""get_word_length"": get_word_length}[output.tool] observation = tool.run(output.tool_input) intermediate_steps.append((output, observation)) print(final_result) ``` ```text TOOL NAME: get_word_length TOOL INPUT: {\'word\': \'educa\'} There are 5 letters in the word ""educa"". ``` Woo! It\'s working. ### Using AgentExecutor To simplify this a bit, we can import and use the `AgentExecutor` class. This bundles up all of the above and adds in error handling, early stopping, tracing, and other quality-of-life improvements that reduce safeguards you need to write. ```python from langchain.agents import AgentExecutor agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True) ``` Now let\'s test it out! ```python agent_executor.invoke({""input"": ""how many letters in the word educa?""}) ``` ```text > Entering new AgentExecutor chain... Invoking: `get_word_length` with `{\'word\': \'educa\'}` 5There are 5 letters in the word ""educa"". > Finished chain. {\'input\': \'how many letters in the word educa?\', \'output\': \'There are 5 letters in the word ""educa"".\'} ``` And looking at the trace, we can see that all of our agent calls and tool invocations are automatically logged: [ ### Adding memory This is great - we have an agent! However, this agent is stateless - it doesn\'t remember anything about previous interactions. This means you can\'t ask follow up questions easily. Let\'s fix that by adding in memory. In order to do this, we need to do two things: 1. Add a place for memory variables to go in the prompt 2. Keep track of the chat history First, let\'s add a place for memory in the prompt. We do this by adding a placeholder for messages with the key `""chat_history""`. Notice that we put this ABOVE the new user input (to follow the conversation flow). ```python from langchain.prompts import MessagesPlaceholder MEMORY_KEY = ""chat_history"" prompt = ChatPromptTemplate.from_messages( [ ( ""system"", ""You are very powerful assistant, but bad at calculating lengths of words."", ), MessagesPlaceholder(variable_name=MEMORY_KEY), (""user"", ""{input}""), MessagesPlaceholder(variable_name=""agent_scratchpad""), ] ) ``` We can then set up a list to track the chat history ```python from langchain.schema.messages import AIMessage, HumanMessage chat_history = [] ``` We can then put it all together! ```python agent = ( { ""input"": lambda x: x[""input""], ""agent_scratchpad"": lambda x: format_to_openai_function_messages( x[""intermediate_steps""] ), ""chat_history"": lambda x: x[""chat_history""], } | prompt | llm_with_tools | OpenAIFunctionsAgentOutputParser() ) agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True) ``` When running, we now need to track the inputs and outputs as chat history ```python input1 = ""how many letters in the word educa?"" result = agent_executor.invoke({""input"": input1, ""chat_history"": chat_history}) chat_history.extend( [ HumanMessage(content=input1), AIMessage(content=result[""output""]), ] ) agent_executor.invoke({""input"": ""is that a real word?"", ""chat_history"": chat_history}) ``` ```text > Entering new AgentExecutor chain... Invoking: `get_word_length` with `{\'word\': \'educa\'}` 5There are 5 letters in the word ""educa"". > Finished chain. > Entering new AgentExecutor chain... No, ""educa"" is not a real word in English. > Finished chain. {\'input\': \'is that a real word?\', \'chat_history\': [HumanMessage(content=\'how many letters in the word educa?\'), AIMessage(content=\'There are 5 letters in the word ""educa"".\')], \'output\': \'No, ""educa"" is not a real word in English.\'} ``` Here\'s the LangSmith trace: [ ## Next Steps Awesome! You\'ve now run your first end-to-end agent. To dive deeper, you can: - Check out all the different [agent types](/docs/modules/agents/agent_types/) supported - Learn all the controls for [AgentExecutor](/docs/modules/agents/how_to/) - Explore the how-to\'s of [tools](/docs/modules/agents/tools/) and all the [tool integrations](/docs/integrations/tools) - See a full list of all the off-the-shelf [toolkits](/docs/integrations/toolkits/) we provide - [Concepts](#concepts)- [Agent](#agent) - [Tools](#tools) - [Toolkits](#toolkits) - [AgentExecutor](#agentexecutor) - [Other types of agent runtimes](#other-types-of-agent-runtimes) - [Get started](#get-started)- [Setup: LangSmith](#setup-langsmith) - [Define the agent](#define-the-agent) - [Define the runtime](#define-the-runtime) - [Using AgentExecutor](#using-agentexecutor) - [Adding memory](#adding-memory) - [Next Steps](#next-steps)', 'OpenAI functions | OpenAI functions Certain OpenAI models (like gpt-3.5-turbo-0613 and gpt-4-0613) have been fine-tuned to detect when a function should be called and respond with the inputs that should be passed to the function. In an API call, you can describe functions and have the model intelligently choose to output a JSON object containing arguments to call those functions. The goal of the OpenAI Function APIs is to more reliably return valid and useful function calls than a generic text completion or chat API. The OpenAI Functions Agent is designed to work with these models. Install `openai`, `google-search-results` packages which are required as the LangChain packages call them internally. ```bash pip install openai google-search-results ``` ## Initialize tools We will first create some tools we can use ```python from langchain.agents import AgentType, Tool, initialize_agent from langchain.chains import LLMMathChain from langchain.chat_models import ChatOpenAI from langchain.utilities import SerpAPIWrapper, SQLDatabase from langchain_experimental.sql import SQLDatabaseChain ``` ```python llm = ChatOpenAI(temperature=0, model=""gpt-3.5-turbo-0613"") search = SerpAPIWrapper() llm_math_chain = LLMMathChain.from_llm(llm=llm, verbose=True) db = SQLDatabase.from_uri(""sqlite:///../../../../../notebooks/Chinook.db"") db_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True) tools = [ Tool( name=""Search"", func=search.run, description=""useful for when you need to answer questions about current events. You should ask targeted questions"", ), Tool( name=""Calculator"", func=llm_math_chain.run, description=""useful for when you need to answer questions about math"", ), Tool( name=""FooBar-DB"", func=db_chain.run, description=""useful for when you need to answer questions about FooBar. Input should be in the form of a question containing full context"", ), ] ``` ## Using LCEL We will first use LangChain Expression Language to create this agent ```python from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder ``` ```python prompt = ChatPromptTemplate.from_messages( [ (""system"", ""You are a helpful assistant""), (""user"", ""{input}""), MessagesPlaceholder(variable_name=""agent_scratchpad""), ] ) ``` ```python from langchain.tools.render import format_tool_to_openai_function ``` ```python llm_with_tools = llm.bind(functions=[format_tool_to_openai_function(t) for t in tools]) ``` ```python from langchain.agents.format_scratchpad import format_to_openai_function_messages from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser ``` ```python agent = ( { ""input"": lambda x: x[""input""], ""agent_scratchpad"": lambda x: format_to_openai_function_messages( x[""intermediate_steps""] ), } | prompt | llm_with_tools | OpenAIFunctionsAgentOutputParser() ) ``` ```python from langchain.agents import AgentExecutor ``` ```python agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True) ``` ```python agent_executor.invoke( { ""input"": ""Who is Leo DiCaprio\'s girlfriend? What is her current age raised to the 0.43 power?"" } ) ``` ```text > Entering new AgentExecutor chain... Invoking: `Search` with `Leo DiCaprio\'s girlfriend` [\'Blake Lively and DiCaprio are believed to have enjoyed a whirlwind five-month romance in 2011. The pair were seen on a yacht together in Cannes, ...\'] Invoking: `Calculator` with `0.43` > Entering new LLMMathChain chain... 0.43```text 0.43 ``` ...numexpr.evaluate(""0.43"")... Answer: 0.43 > Finished chain. Answer: 0.43I\'m sorry, but I couldn\'t find any information about Leo DiCaprio\'s current girlfriend. As for raising her age to the power of 0.43, I\'m not sure what her current age is, so I can\'t provide an answer for that. > Finished chain. {\'input\': ""Who is Leo DiCaprio\'s girlfriend? What is her current age raised to the 0.43 power?"", \'output\': ""I\'m sorry, but I couldn\'t find any information about Leo DiCaprio\'s current girlfriend. As for raising her age to the power of 0.43, I\'m not sure what her current age is, so I can\'t provide an answer for that.""} ``` ## Using OpenAIFunctionsAgent We can now use `OpenAIFunctionsAgent`, which creates this agent under the hood ```python agent_executor = initialize_agent( tools, llm, agent=AgentType.OPENAI_FUNCTIONS, verbose=True ) ``` ```python agent_executor.invoke( { ""input"": ""Who is Leo DiCaprio\'s girlfriend? What is her current age raised to the 0.43 power?"" } ) ``` - [Initialize tools](#initialize-tools) - [Using LCEL](#using-lcel) - [Using OpenAIFunctionsAgent](#using-openaifunctionsagent)']","In `OpenAIFunctionsAgent`, the number of LLM API calls depends on the complexity of the task and the number of tools invoked. Generally, it makes one initial call to determine the first tool to use, and then potentially more calls if the agent needs to execute tools and gather information. Each tool invocation results in an LLM API call. 

For example, if the agent determines it needs to use two tools, it will make two LLM API calls: one to decide which tool to use first and another to execute the second tool.","The OpenAIFunctionsAgent is an agent driven by OpenAI's function-powered API. It uses the function_call parameter from OpenAI to determine which tools to use for each API call. In general, the number of API calls made by the OpenAIFunctionsAgent depends on the question and the number of tools used in the agent's pipeline. At minimum, one call will be made to the LLM to (optionally) select a tool or directly respond. For more complex questions or intents, the agent may make many calls.",0.9999999999,1.0,1.0,0.3088871078892553,0.2808988764044944
62,What are some ways of doing retrieval augmented generation?,"['Remembrall | Remembrall This page covers how to use the [Remembrall]( ecosystem within LangChain. ## What is Remembrall? Remembrall gives your language model long-term memory, retrieval augmented generation, and complete observability with just a few lines of code. ![Remembrall Dashboard](/assets/images/RemembrallDashboard-36b4f535ae0718f2f5923084caf111de.png) It works as a light-weight proxy on top of your OpenAI calls and simply augments the context of the chat calls at runtime with relevant facts that have been collected. ## Setup To get started, [sign in with Github on the Remembrall platform]( and copy your [API key from the settings page]( Any request that you send with the modified `openai_api_base` (see below) and Remembrall API key will automatically be tracked in the Remembrall dashboard. You **never** have to share your OpenAI key with our platform and this information is **never** stored by the Remembrall systems. ### Enable Long Term Memory In addition to setting the `openai_api_base` and Remembrall API key via `x-gp-api-key`, you should specify a UID to maintain memory for. This will usually be a unique user identifier (like email). ```python from langchain.chat_models import ChatOpenAI chat_model = ChatOpenAI(openai_api_base="" model_kwargs={ ""headers"":{ ""x-gp-api-key"": ""remembrall-api-key-here"", ""x-gp-remember"": ""user@email.com"", } }) chat_model.predict(""My favorite color is blue."") import time; time.sleep(5) # wait for system to save fact via auto save print(chat_model.predict(""What is my favorite color?"")) ``` ### Enable Retrieval Augmented Generation First, create a document context in the [Remembrall dashboard]( Paste in the document texts or upload documents as PDFs to be processed. Save the Document Context ID and insert it as shown below. ```python from langchain.chat_models import ChatOpenAI chat_model = ChatOpenAI(openai_api_base="" model_kwargs={ ""headers"":{ ""x-gp-api-key"": ""remembrall-api-key-here"", ""x-gp-context"": ""document-context-id-goes-here"", } }) print(chat_model.predict(""This is a question that can be answered with my document."")) ``` - [What is Remembrall?](#what-is-remembrall) - [Setup](#setup)- [Enable Long Term Memory](#enable-long-term-memory) - [Enable Retrieval Augmented Generation](#enable-retrieval-augmented-generation)', 'Retrieval-augmented generation (RAG) | Retrieval-augmented generation (RAG) []( ## Overview ### What is RAG? RAG is a technique for augmenting LLM knowledge with additional, often private or real-time, data. LLMs can reason about wide-ranging topics, but their knowledge is limited to the public data up to a specific point in time that they were trained on. If you want to build AI applications that can reason about private data or data introduced after a model\'s cutoff date, you need to augment the knowledge of the model with the specific information it needs. The process of bringing the appropriate information and inserting it into the model prompt is known as Retrieval Augmented Generation (RAG). ### What\'s in this guide? LangChain has a number of components specifically designed to help build RAG applications. To familiarize ourselves with these, we\'ll build a simple question-answering application over a text data source. Specifically, we\'ll build a QA bot over the [LLM Powered Autonomous Agents]( blog post by Lilian Weng. Along the way we\'ll go over a typical QA architecture, discuss the relevant LangChain components, and highlight additional resources for more advanced QA techniques. We\'ll also see how LangSmith can help us trace and understand our application. LangSmith will become increasingly helpful as our application grows in complexity. **Note** Here we focus on RAG for unstructured data. Two RAG use cases which we cover elsewhere are: - [QA over structured data](/docs/use_cases/qa_structured/sql) (e.g., SQL) - [QA over code](/docs/use_cases/question_answering/code_understanding) (e.g., Python) ## Architecture A typical RAG application has two main components: **Indexing**: a pipeline for ingesting data from a source and indexing it. _This usually happen offline._ **Retrieval and generation**: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model. The most common full sequence from raw data to answer looks like: #### Indexing 1. **Load**: First we need to load our data. We\'ll use [DocumentLoaders](/docs/modules/data_connection/document_loaders/) for this. 2. **Split**: [Text splitters](/docs/modules/data_connection/document_transformers/) break large `Documents` into smaller chunks. This is useful both for indexing data and for passing it in to a model, since large chunks are harder to search over and won\'t in a model\'s finite context window. 3. **Store**: We need somewhere to store and index our splits, so that they can later be searched over. This is often done using a [VectorStore](/docs/modules/data_connection/vectorstores/) and [Embeddings](/docs/modules/data_connection/text_embedding/) model. #### Retrieval and generation 1. **Retrieve**: Given a user input, relevant splits are retrieved from storage using a [Retriever](/docs/modules/data_connection/retrievers/). 2. **Generate**: A [ChatModel](/docs/modules/model_io/chat_models) / [LLM](/docs/modules/model_io/llms/) produces an answer using a prompt that includes the question and the retrieved data ![flow.jpeg](/assets/images/qa_flow-9fbd91de9282eb806bda1c6db501ecec.jpeg) ## Setup ### Dependencies We\'ll use an OpenAI chat model and embeddings and a Chroma vector store in this walkthrough, but everything shown here works with any [ChatModel](/docs/integrations/chat/) or [LLM](/docs/integrations/llms/), [Embeddings](/docs/integrations/text_embedding/), and [VectorStore](/docs/integrations/vectorstores/) or [Retriever](/docs/integrations/retrievers). We\'ll use the following packages: ```bash pip install -U langchain openai chromadb langchainhub bs4 ``` We need to set environment variable `OPENAI_API_KEY`, which can be done directly or loaded from a `.env` file like so: ```python import getpass import os os.environ[""OPENAI_API_KEY""] = getpass.getpass() # import dotenv # dotenv.load_dotenv() ``` ### LangSmith Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with [LangSmith]( Note that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces: ```python os.environ[""LANGCHAIN_TRACING_V2""] = ""true"" os.environ[""LANGCHAIN_API_KEY""] = getpass.getpass() ``` ## Quickstart Suppose we want to build a QA app over the [LLM Powered Autonomous Agents]( blog post by Lilian Weng. We can create a simple pipeline for this in ~20 lines of code: ```python import bs4 from langchain import hub from langchain.chat_models import ChatOpenAI from langchain.document_loaders import WebBaseLoader from langchain.embeddings import OpenAIEmbeddings from langchain.schema import StrOutputParser from langchain.schema.runnable import RunnablePassthrough from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain.vectorstores import Chroma ``` ```python loader = WebBaseLoader( web_paths=("" bs_kwargs=dict( parse_only=bs4.SoupStrainer( class_=(""post-content"", ""post-title"", ""post-header"") ) ), ) docs = loader.load() text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200) splits = text_splitter.split_documents(docs) vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings()) retriever = vectorstore.as_retriever() prompt = hub.pull(""rlm/rag-prompt"") llm = ChatOpenAI(model_name=""gpt-3.5-turbo"", temperature=0) def format_docs(docs): return ""\\n\\n"".join(doc.page_content for doc in docs) rag_chain = ( {""context"": retriever | format_docs, ""question"": RunnablePassthrough()} | prompt | llm | StrOutputParser() ) ``` ```python rag_chain.invoke(""What is Task Decomposition?"") ``` ```text \'Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It can be done through prompting techniques like Chain of Thought or Tree of Thoughts, or by using task-specific instructions or human inputs. Task decomposition helps agents plan ahead and manage complicated tasks more effectively.\' ``` ```python # cleanup vectorstore.delete_collection() ``` Check out the [LangSmith trace]( ## Detailed walkthrough Let\'s go through the above code step-by-step to really understand what\'s going on. ## Step 1. Load We need to first load the blog post contents. We can use `DocumentLoader`s for this, which are objects that load in data from a source as `Documents`. A `Document` is an object with `page_content` (str) and `metadata` (dict) attributes. In this case we\'ll use the `WebBaseLoader`, which uses `urllib` and `BeautifulSoup` to load and parse the passed in web urls, returning one `Document` per url. We can customize the html -> text parsing by passing in parameters to the `BeautifulSoup` parser via `bs_kwargs` (see [BeautifulSoup docs]( In this case only HTML tags with class ""post-content"", ""post-title"", or ""post-header"" are relevant, so we\'ll remove all others. ```python from langchain.document_loaders import WebBaseLoader loader = WebBaseLoader( web_paths=("" bs_kwargs={ ""parse_only"": bs4.SoupStrainer( class_=(""post-content"", ""post-title"", ""post-header"") ) }, ) docs = loader.load() ``` ```python len(docs[0].page_content) ``` ```text 42824 ``` ```python print(docs[0].page_content[:500]) ``` ```text LLM Powered Autonomous Agents Date: June 23, 2023 | Estimated Reading Time: 31 min | Author: Lilian Weng Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver. Agent System Overview# In ``` ### Go deeper `DocumentLoader`: Object that load data from a source as `Documents`. - [Docs](/docs/modules/data_connection/document_loaders/): Further documentation on how to use `DocumentLoader`s. - [Integrations](/docs/integrations/document_loaders/): Find the relevant `DocumentLoader` integration (of the > 160 of them) for your use case. ## Step 2. Split Our loaded document is over 42k characters long. This is too long to fit in the context window of many models. And even for those models that could fit the full post in their context window, empirically models struggle to find the relevant context in very long prompts. So we\'ll split the `Document` into chunks for embedding and vector storage. This should help us retrieve only the most relevant bits of the blog post at run time. In this case we\'ll split our documents into chunks of 1000 characters with 200 characters of overlap between chunks. The overlap helps mitigate the possibility of separating a statement from important context related to it. We use the `RecursiveCharacterTextSplitter`, which will (recursively) split the document using common separators (like new lines) until each chunk is the appropriate size. ```python from langchain.text_splitter import RecursiveCharacterTextSplitter text_splitter = RecursiveCharacterTextSplitter( chunk_size=1000, chunk_overlap=200, add_start_index=True ) all_splits = text_splitter.split_documents(docs) ``` ```python len(all_splits) ``` ```text 66 ``` ```python len(all_splits[0].page_content) ``` ```text 969 ``` ```python all_splits[10].metadata ``` ```text {\'source\': \' \'start_index\': 7056} ``` ### Go deeper `DocumentSplitter`: Object that splits a list of `Document`s into smaller chunks. Subclass of `DocumentTransformer`s. - Explore `Context-aware splitters`, which keep the location (""context"") of each split in the original `Document`:- [Markdown files](/docs/use_cases/question_answering/document-context-aware-QA) - [Code (py or js)](/docs/use_cases/question_answering/docs/integrations/document_loaders/source_code) - [Scientific papers](/docs/integrations/document_loaders/grobid) `DocumentTransformer`: Object that performs a transformation on a list of `Document`s. - [Docs](/docs/modules/data_connection/document_transformers/): Further documentation on how to use `DocumentTransformer`s - [Integrations](/docs/integrations/document_transformers/) ## Step 3. Store Now that we\'ve got 66 text chunks in memory, we need to store and index them so that we can search them later in our RAG app. The most common way to do this is to embed the contents of each document split and upload those embeddings to a vector store. Then, when we want to search over our splits, we take the search query, embed it as well, and perform some sort of ""similarity"" search to identify the stored splits with the most similar embeddings to our query embedding. The simplest similarity measure is cosine similarity we measure the cosine of the angle between each pair of embeddings (which are just very high dimensional vectors). We can embed and store all of our document splits in a single command using the `Chroma` vector store and `OpenAIEmbeddings` model. ```python from langchain.embeddings import OpenAIEmbeddings from langchain.vectorstores import Chroma vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings()) ``` ### Go deeper `Embeddings`: Wrapper around a text embedding model, used for converting text to embeddings. - [Docs](/docs/modules/data_connection/text_embedding): Further documentation on the interface. - [Integrations](/docs/integrations/text_embedding/): Browse the > 30 text embedding integrations `VectorStore`: Wrapper around a vector database, used for storing and querying embeddings. - [Docs](/docs/modules/data_connection/vectorstores/): Further documentation on the interface. - [Integrations](/docs/integrations/vectorstores/): Browse the > 40 `VectorStore` integrations. This completes the **Indexing** portion of the pipeline. At this point we have an query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question: ## Step 4. Retrieve Now let\'s write the actual application logic. We want to create a simple application that let\'s the user ask a question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and finally returns an answer. LangChain defines a `Retriever` interface which wraps an index that can return relevant documents given a string query. All retrievers implement a common method `get_relevant_documents()` (and its asynchronous variant `aget_relevant_documents()`). The most common type of `Retriever` is the `VectorStoreRetriever`, which uses the similarity search capabilities of a vector store to facillitate retrieval. Any `VectorStore` can easily be turned into a `Retriever`: ```python retriever = vectorstore.as_retriever(search_type=""similarity"", search_kwargs={""k"": 6}) ``` ```python retrieved_docs = retriever.get_relevant_documents( ""What are the approaches to Task Decomposition?"" ) ``` ```python len(retrieved_docs) ``` ```text 6 ``` ```python print(retrieved_docs[0].page_content) ``` ```text Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote. Task decomposition can be done (1) by LLM with simple prompting like ""Steps for XYZ.\\n1."", ""What are the subgoals for achieving XYZ?"", (2) by using task-specific instructions; e.g. ""Write a story outline."" for writing a novel, or (3) with human inputs. ``` ### Go deeper Vector stores are commonly used for retrieval, but there are plenty of other ways to do retrieval. `Retriever`: An object that returns `Document`s given a text query - [Docs](/docs/modules/data_connection/retrievers/): Further documentation on the interface and built-in retrieval techniques. Some of which include:- `MultiQueryRetriever` [generates variants of the input question](/docs/modules/data_connection/retrievers/MultiQueryRetriever) to improve retrieval hit rate. - `MultiVectorRetriever` (diagram below) instead generates [variants of the embeddings](/docs/modules/data_connection/retrievers/multi_vector), also in order to improve retrieval hit rate. - `Max marginal relevance` selects for [relevance and diversity]( among the retrieved documents to avoid passing in duplicate context. - Documents can be filtered during vector store retrieval using [metadata filters](/docs/use_cases/question_answering/document-context-aware-QA). - [Integrations](/docs/integrations/retrievers/): Integrations with retrieval services. ## Step 5. Generate Let\'s put it all together into a chain that takes a question, retrieves relevant documents, constructs a prompt, passes that to a model, and parses the output. We\'ll use the gpt-3.5-turbo OpenAI chat model, but any LangChain `LLM` or `ChatModel` could be substituted in. ```python from langchain.chat_models import ChatOpenAI llm = ChatOpenAI(model_name=""gpt-3.5-turbo"", temperature=0) ``` We\'ll use a prompt for RAG that is checked into the LangChain prompt hub ([here]( ```python from langchain import hub prompt = hub.pull(""rlm/rag-prompt"") ``` ```python print( prompt.invoke( {""context"": ""filler context"", ""question"": ""filler question""} ).to_string() ) ``` ```text Human: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise. Question: filler question Context: filler context Answer: ``` We\'ll use the [LCEL Runnable]( protocol to define the chain, allowing us to - pipe together components and functions in a transparent way - automatically trace our chain in LangSmith - get streaming, async, and batched calling out of the box ```python from langchain.schema import StrOutputParser from langchain.schema.runnable import RunnablePassthrough def format_docs(docs): return ""\\n\\n"".join(doc.page_content for doc in docs) rag_chain = ( {""context"": retriever | format_docs, ""question"": RunnablePassthrough()} | prompt | llm | StrOutputParser() ) ``` ```python for chunk in rag_chain.stream(""What is Task Decomposition?""): print(chunk, end="""", flush=True) ``` ```text Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It can be done through methods like Chain of Thought (CoT) or Tree of Thoughts, which involve dividing the task into manageable subtasks and exploring multiple reasoning possibilities at each step. Task decomposition can be performed by AI models with prompting, task-specific instructions, or human inputs. ``` Check out the [LangSmith trace]( ### Go deeper #### Choosing LLMs `ChatModel`: An LLM-backed chat model wrapper. Takes in a sequence of messages and returns a message. - [Docs](/docs/modules/model_io/chat/) - [Integrations](/docs/integrations/chat/): Explore over 25 `ChatModel` integrations. `LLM`: A text-in-text-out LLM. Takes in a string and returns a string. - [Docs](/docs/modules/model_io/llms) - [Integrations](/docs/integrations/llms): Explore over 75 `LLM` integrations. See a guide on RAG with locally-running models [here](/docs/modules/use_cases/question_answering/local_retrieval_qa). #### Customizing the prompt As shown above, we can load prompts (e.g., [this RAG prompt]( from the prompt hub. The prompt can also be easily customized: ```python from langchain.prompts import PromptTemplate template = """"""Use the following pieces of context to answer the question at the end. If you don\'t know the answer, just say that you don\'t know, don\'t try to make up an answer. Use three sentences maximum and keep the answer as concise as possible. Always say ""thanks for asking!"" at the end of the answer. {context} Question: {question} Helpful Answer:"""""" rag_prompt_custom = PromptTemplate.from_template(template) rag_chain = ( {""context"": retriever | format_docs, ""question"": RunnablePassthrough()} | rag_prompt_custom | llm | StrOutputParser() ) rag_chain.invoke(""What is Task Decomposition?"") ``` ```text \'Task decomposition is the process of breaking down a complex task into smaller and simpler steps. It can be done through techniques like Chain of Thought (CoT) or Tree of Thoughts, which involve dividing the problem into multiple thought steps and generating multiple thoughts per step. Task decomposition helps in enhancing model performance and understanding the thinking process of the model. Thanks for asking!\' ``` Check out the [LangSmith trace]( ### Adding sources With LCEL it\'s easy to return the retrieved documents or certain source metadata from the documents: ```python from operator import itemgetter from langchain.schema.runnable import RunnableMap rag_chain_from_docs = ( { ""context"": lambda input: format_docs(input[""documents""]), ""question"": itemgetter(""question""), } | rag_prompt_custom | llm | StrOutputParser() ) rag_chain_with_source = RunnableMap( {""documents"": retriever, ""question"": RunnablePassthrough()} ) | { ""documents"": lambda input: [doc.metadata for doc in input[""documents""]], ""answer"": rag_chain_from_docs, } rag_chain_with_source.invoke(""What is Task Decomposition"") ``` ```text {\'documents\': [{\'source\': \' \'start_index\': 1585}, {\'source\': \' \'start_index\': 2192}, {\'source\': \' \'start_index\': 17804}, {\'source\': \' \'start_index\': 17414}, {\'source\': \' \'start_index\': 29630}, {\'source\': \' \'start_index\': 19373}], \'answer\': \'Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It involves transforming big tasks into multiple manageable tasks, allowing for a more systematic and organized approach to problem-solving. Thanks for asking!\'} ``` Check out the [LangSmith trace]( Adding memory Suppose we want to create a stateful application that remembers past user inputs. There are two main things we need to do to support this. 1. Add a messages placeholder to our chain which allows us to pass in historical messages 2. Add a chain that takes the latest user query and reformulates it in the context of the chat history into a standalone question that can be passed to our retriever. Let\'s start with 2. We can build a ""condense question"" chain that looks something like this: ```python from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder condense_q_system_prompt = """"""Given a chat history and the latest user question \\ which might reference the chat history, formulate a standalone question \\ which can be understood without the chat history. Do NOT answer the question, \\ just reformulate it if needed and otherwise return it as is."""""" condense_q_prompt = ChatPromptTemplate.from_messages( [ (""system"", condense_q_system_prompt), MessagesPlaceholder(variable_name=""chat_history""), (""human"", ""{question}""), ] ) condense_q_chain = condense_q_prompt | llm | StrOutputParser() ``` ```python from langchain.schema.messages import AIMessage, HumanMessage condense_q_chain.invoke( { ""chat_history"": [ HumanMessage(content=""What does LLM stand for?""), AIMessage(content=""Large language model""), ], ""question"": ""What is meant by large"", } ) ``` ```text \'What is the definition of ""large"" in the context of a language model?\' ``` ```python condense_q_chain.invoke( { ""chat_history"": [ HumanMessage(content=""What does LLM stand for?""), AIMessage(content=""Large language model""), ], ""question"": ""How do transformers work"", } ) ``` ```text \'How do transformer models function?\' ``` And now we can build our full QA chain. Notice we add some routing functionality to only run the ""condense question chain"" when our chat history isn\'t empty. ```python qa_system_prompt = """"""You are an assistant for question-answering tasks. \\ Use the following pieces of retrieved context to answer the question. \\ If you don\'t know the answer, just say that you don\'t know. \\ Use three sentences maximum and keep the answer concise.\\ {context}"""""" qa_prompt = ChatPromptTemplate.from_messages( [ (""system"", qa_system_prompt), MessagesPlaceholder(variable_name=""chat_history""), (""human"", ""{question}""), ] ) def condense_question(input: dict): if input.get(""chat_history""): return condense_q_chain else: return input[""question""] rag_chain = ( RunnablePassthrough.assign(context=condense_question | retriever | format_docs) | qa_prompt | llm ) ``` ```python chat_history = [] question = ""What is Task Decomposition?"" ai_msg = rag_chain.invoke({""question"": question, ""chat_history"": chat_history}) chat_history.extend([HumanMessage(content=question), ai_msg]) second_question = ""What are common ways of doing it?"" rag_chain.invoke({""question"": second_question, ""chat_history"": chat_history}) ``` ```text AIMessage(content=\'Common ways of task decomposition include:\\n\\n1. Using Chain of Thought (CoT): CoT is a prompting technique that instructs the model to ""think step by step"" and decompose complex tasks into smaller and simpler steps. It utilizes more test-time computation and sheds light on the model\\\'s thinking process.\\n\\n2. Prompting with LLM: Language Model (LLM) can be used to prompt the model with simple instructions like ""Steps for XYZ"" or ""What are the subgoals for achieving XYZ?"" This allows the model to generate a sequence of subtasks or thought steps.\\n\\n3. Task-specific instructions: For certain tasks, task-specific instructions can be provided to guide the model in decomposing the task. For example, for writing a novel, the instruction ""Write a story outline"" can be given to break down the task into manageable steps.\\n\\n4. Human inputs: In some cases, human inputs can be used to assist in task decomposition. Humans can provide their expertise and knowledge to identify and break down complex tasks into smaller subtasks.\') ``` Check out the [LangSmith trace]( Here we\'ve gone over how to add chain logic for incorporating historical outputs. But how do we actually store and retrieve historical outputs for different sessions? For that check out the LCEL [How to add message history (memory)](/docs/expression_language/how_to/message_history) page. ## Next steps That\'s a lot of content we\'ve covered in a short amount of time. There\'s plenty of nuances, features, integrations, etc to explore in each of the above sections. Aside from the sources mentioned above, good next steps include: - Reading up on more advanced retrieval techniques in the [Retrievers](/docs/modules/data_connection/retrievers/) section. - Learning about the LangChain [Indexing API](/docs/modules/data_connection/indexing), which helps repeatedly sync data sources and vector stores without redundant computation or storage. - Exploring RAG [LangChain Templates](/docs/templates/#-advanced-retrieval), which are reference applications that can easily be deployed with [LangServe](/docs/langserve). - Learning about [evaluating RAG applications with LangSmith]( - [Overview](#overview)- [What is RAG?](#what-is-rag) - [What\'s in this guide?](#whats-in-this-guide) - [Architecture](#architecture) - [Setup](#setup)- [Dependencies](#dependencies) - [LangSmith](#langsmith) - [Quickstart](#quickstart) - [Detailed walkthrough](#detailed-walkthrough) - [Step 1. Load](#step-1-load)- [Go deeper](#go-deeper) - [Step 2. Split](#step-2-split)- [Go deeper](#go-deeper-1) - [Step 3. Store](#step-3-store)- [Go deeper](#go-deeper-2) - [Step 4. Retrieve](#step-4-retrieve)- [Go deeper](#go-deeper-3) - [Step 5. Generate](#step-5-generate)- [Go deeper](#go-deeper-4) - [Adding sources](#adding-sources) - [Adding memory](#adding-memory) - [Next steps](#next-steps)', 'Fleet AI Libraries Context | Fleet AI Libraries Context The Fleet AI team is on a mission to embed the world\'s most important data. They\'ve started by embedding the top 1200 Python libraries to enable code generation with up-to-date knowledge. They\'ve been kind enough to share their embeddings of the [LangChain docs]( and [API reference]( Let\'s take a look at how we can use these embeddings to power a docs retrieval system and ultimately a simple code generating chain! ```bash pip install langchain fleet-context openai pandas faiss-cpu # faiss-gpu for CUDA supported GPU ``` ```python from operator import itemgetter from typing import Any, Optional, Type import pandas as pd from langchain.embeddings import OpenAIEmbeddings from langchain.retrievers import MultiVectorRetriever from langchain.schema import Document from langchain.schema.storage import BaseStore from langchain.schema.vectorstore import VectorStore from langchain.vectorstores import FAISS def load_fleet_retriever( df: pd.DataFrame, *, vectorstore_cls: Type[VectorStore] = FAISS, docstore: Optional[BaseStore] = None, **kwargs: Any, ): vectorstore = _populate_vectorstore(df, vectorstore_cls) if docstore is None: return vectorstore.as_retriever(**kwargs) else: _populate_docstore(df, docstore) return MultiVectorRetriever( vectorstore=vectorstore, docstore=docstore, id_key=""parent"", **kwargs ) def _populate_vectorstore( df: pd.DataFrame, vectorstore_cls: Type[VectorStore], ) -> VectorStore: if not hasattr(vectorstore_cls, ""from_embeddings""): raise ValueError( f""Incompatible vector store class {vectorstore_cls}."" ""Must implement `from_embeddings` class method."" ) texts_embeddings = [] metadatas = [] for _, row in df.iterrows(): texts_embeddings.append((row.metadata[""text""], row[""dense_embeddings""])) metadatas.append(row.metadata) return vectorstore_cls.from_embeddings( texts_embeddings, OpenAIEmbeddings(model=""text-embedding-ada-002""), metadatas=metadatas, ) def _populate_docstore(df: pd.DataFrame, docstore: BaseStore) -> None: parent_docs = [] df = df.copy() df[""parent""] = df.metadata.apply(itemgetter(""parent"")) for parent_id, group in df.groupby(""parent""): sorted_group = group.iloc[ group.metadata.apply(itemgetter(""section_index"")).argsort() ] text = """".join(sorted_group.metadata.apply(itemgetter(""text""))) metadata = { k: sorted_group.iloc[0].metadata[k] for k in (""title"", ""type"", ""url"") } text = metadata[""title""] + ""\\n"" + text metadata[""id""] = parent_id parent_docs.append(Document(page_content=text, metadata=metadata)) docstore.mset(((d.metadata[""id""], d) for d in parent_docs)) ``` ## Retriever chunks As part of their embedding process, the Fleet AI team first chunked long documents before embedding them. This means the vectors correspond to sections of pages in the LangChain docs, not entire pages. By default, when we spin up a retriever from these embeddings, we\'ll be retrieving these embedded chunks. We will be using Fleet Context\'s `download_embeddings()` to grab Langchain\'s documentation embeddings. You can view all supported libraries\' documentation at [ ```python from context import download_embeddings df = download_embeddings(""langchain"") vecstore_retriever = load_fleet_retriever(df) ``` ```python vecstore_retriever.get_relevant_documents(""How does the multi vector retriever work"") ``` ```text [Document(page_content=""# Vector store-backed retriever A vector store retriever is a retriever that uses a vector store to retrieve documents. It is a lightweight wrapper around the vector store class to make it conform to the retriever interface. It uses the search methods implemented by a vector store, like similarity search and MMR, to query the texts in the vector store. Once you construct a vector store, it\'s very easy to construct a retriever. Let\'s walk through an example."", metadata={\'id\': \'f509f20d-4c63-4a5a-a40a-5c4c0f099839\', \'library_id\': \'4506492b-70de-49f1-ba2e-d65bd7048a28\', \'page_id\': \'d78cf422-2dab-4860-80fe-d71a3619b02f\', \'parent\': \'c153ebd9-2611-4a43-9db6-daa1f5f214f6\', \'section_id\': \'\', \'section_index\': 0, \'text\': ""# Vector store-backed retriever A vector store retriever is a retriever that uses a vector store to retrieve documents. It is a lightweight wrapper around the vector store class to make it conform to the retriever interface. It uses the search methods implemented by a vector store, like similarity search and MMR, to query the texts in the vector store. Once you construct a vector store, it\'s very easy to construct a retriever. Let\'s walk through an example."", \'title\': \'Vector store-backed retriever | Langchain\', \'type\': None, \'url\': \' Document(page_content=\'# MultiVector Retriever It can often be beneficial to store multiple vectors per document. There are multiple use cases where this is beneficial. LangChain has a base `MultiVectorRetriever` which makes querying this type of setup easy. A lot of the complexity lies in how to create the multiple vectors per document. This notebook covers some of the common ways to create those vectors and use the `MultiVectorRetriever`. The methods to create multiple vectors per document include: - Smaller chunks: split a document into smaller chunks, and embed those (this is ParentDocumentRetriever). - Summary: create a summary for each document, embed that along with (or instead of) the document. - Hypothetical questions: create hypothetical questions that each document would be appropriate to answer, embed those along with (or instead of) the document. Note that this also enables another method of adding embeddings - manually.\', metadata={\'id\': \'e06e6bb5-127a-49f7-9511-247d279d0d83\', \'library_id\': \'4506492b-70de-49f1-ba2e-d65bd7048a28\', \'page_id\': \'7c7dd0de-25e0-4e1d-9237-cfd2674c29d4\', \'parent\': \'beec5531-16a7-453c-80ab-c5628e0236ce\', \'section_id\': \'\', \'section_index\': 0, \'text\': \'# MultiVector Retriever It can often be beneficial to store multiple vectors per document. There are multiple use cases where this is beneficial. LangChain has a base `MultiVectorRetriever` which makes querying this type of setup easy. A lot of the complexity lies in how to create the multiple vectors per document. This notebook covers some of the common ways to create those vectors and use the `MultiVectorRetriever`. The methods to create multiple vectors per document include: - Smaller chunks: split a document into smaller chunks, and embed those (this is ParentDocumentRetriever). - Summary: create a summary for each document, embed that along with (or instead of) the document. - Hypothetical questions: create hypothetical questions that each document would be appropriate to answer, embed those along with (or instead of) the document. Note that this also enables another method of adding embeddings - manually.\', \'title\': \'MultiVector Retriever | Langchain\', \'type\': None, \'url\': \' Document(page_content=\'# MultiQueryRetriever Distance-based vector database retrieval embeds (represents) queries in high-dimensional space and finds similar embedded documents based on ""distance"". But, retrieval may produce different results with subtle changes in query wording or if the embeddings do not capture the semantics of the data well. Prompt engineering / tuning is sometimes done to manually address these problems, but can be tedious. The `MultiQueryRetriever` automates the process of prompt tuning by using an LLM to generate multiple queries from different perspectives for a given user input query. For each query, it retrieves a set of relevant documents and takes the unique union across all queries to get a larger set of potentially relevant documents. By generating multiple perspectives on the same question, the `MultiQueryRetriever` might be able to overcome some of the limitations of the distance-based retrieval and get a richer set of results.\', metadata={\'id\': \'dc3b8e5b-a591-4a46-bfeb-a91b36affae1\', \'library_id\': \'4506492b-70de-49f1-ba2e-d65bd7048a28\', \'page_id\': \'31f80e84-c5db-4da2-939c-bccf519864a3\', \'parent\': \'f7c20633-6a60-4ca3-96b1-13fee66e321d\', \'section_id\': \'\', \'section_index\': 0, \'text\': \'# MultiQueryRetriever Distance-based vector database retrieval embeds (represents) queries in high-dimensional space and finds similar embedded documents based on ""distance"". But, retrieval may produce different results with subtle changes in query wording or if the embeddings do not capture the semantics of the data well. Prompt engineering / tuning is sometimes done to manually address these problems, but can be tedious. The `MultiQueryRetriever` automates the process of prompt tuning by using an LLM to generate multiple queries from different perspectives for a given user input query. For each query, it retrieves a set of relevant documents and takes the unique union across all queries to get a larger set of potentially relevant documents. By generating multiple perspectives on the same question, the `MultiQueryRetriever` might be able to overcome some of the limitations of the distance-based retrieval and get a richer set of results.\', \'title\': \'MultiQueryRetriever | Langchain\', \'type\': None, \'url\': \' Document(page_content=\'# `langchain.retrievers.multi_vector`.MultiVectorRetriever[](#langchain-retrievers-multi-vector-multivectorretriever) *class *langchain.retrievers.multi_vector.MultiVectorRetriever[[source]](../_modules/langchain/retrievers/multi_vector.html#MultiVectorRetriever)[](#langchain.retrievers.multi_vector.MultiVectorRetriever) # Examples using MultiVectorRetriever[](#langchain-retrievers-multi-vector-multivectorretriever) - [MultiVector Retriever]( metadata={\'id\': \'1f4ca702-35b8-44c0-b33b-18c09a1f787d\', \'library_id\': \'6254c672-7aa0-4233-b0a6-804bd273752b\', \'page_id\': \'87f5d1fb-a1c6-4080-9d2b-7b88794eb6bf\', \'parent\': \'1820c44d-7783-4846-a11c-106b18da015d\', \'section_id\': \'langchain-retrievers-multi-vector-multivectorretriever\', \'section_index\': 0, \'text\': \'# `langchain.retrievers.multi_vector`.MultiVectorRetriever[](#langchain-retrievers-multi-vector-multivectorretriever) *class *langchain.retrievers.multi_vector.MultiVectorRetriever[[source]](../_modules/langchain/retrievers/multi_vector.html#MultiVectorRetriever)[](#langchain.retrievers.multi_vector.MultiVectorRetriever) # Examples using MultiVectorRetriever[](#langchain-retrievers-multi-vector-multivectorretriever) - [MultiVector Retriever]( \'title\': \'langchain.retrievers.multi_vector.MultiVectorRetriever LangChain 0.0.322\', \'type\': None, \'url\': \' ``` ## Other packages You can download and use other embeddings from [this Dropbox link]( ## Retrieve parent docs The embeddings provided by Fleet AI contain metadata that indicates which embedding chunks correspond to the same original document page. If we\'d like we can use this information to retrieve whole parent documents, and not just embedded chunks. Under the hood, we\'ll use a MultiVectorRetriever and a BaseStore object to search for relevant chunks and then map them to their parent document. ```python from langchain.storage import InMemoryStore parent_retriever = load_fleet_retriever( "" docstore=InMemoryStore(), ) ``` ```python parent_retriever.get_relevant_documents(""How does the multi vector retriever work"") ``` ```text [Document(page_content=\'Vector store-backed retriever | Langchain\\n# Vector store-backed retriever A vector store retriever is a retriever that uses a vector store to retrieve documents. It is a lightweight wrapper around the vector store class to make it conform to the retriever interface. It uses the search methods implemented by a vector store, like similarity search and MMR, to query the texts in the vector store. Once you construct a vector store, it\\\'s very easy to construct a retriever. Let\\\'s walk through an example.Once you construct a vector store, it\\\'s very easy to construct a retriever. Let\\\'s walk through an example. ``` from langchain.document_loaders import TextLoaderloader = TextLoader(\\\'../../../state_of_the_union.txt\\\') ``` ``` from langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import FAISSfrom langchain.embeddings import OpenAIEmbeddingsdocuments = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)texts = text_splitter.split_documents(documents)embeddings = OpenAIEmbeddings()db = FAISS.from_documents(texts, embeddings) ``` ``` Exiting: Cleaning up .chroma directory ``` ``` retriever = db.as_retriever() ``` ``` docs = retriever.get_relevant_documents(""what did he say about ketanji brown jackson"") ``` ## Maximum marginal relevance retrieval[\\u200b](#maximum-marginal-relevance-retrieval) By default, the vector store retriever uses similarity search.If the underlying vector store supports maximum marginal relevance search, you can specify that as the search type. ``` retriever = db.as_retriever(search_type=""mmr"") ``` ``` docs = retriever.get_relevant_documents(""what did he say about ketanji brown jackson"") ``` ## Similarity score threshold retrieval[\\u200b](#similarity-score-threshold-retrieval) You can also a retrieval method that sets a similarity score threshold and only returns documents with a score above that threshold. ``` retriever = db.as_retriever(search_type=""similarity_score_threshold"", search_kwargs={""score_threshold"": .5}) ``` ``` docs = retriever.get_relevant_documents(""what did he say about ketanji brown jackson"") ``` ## Specifying top k[\\u200b](#specifying-top-k) You can also specify search kwargs like `k` to use when doing retrieval.``` retriever = db.as_retriever(search_kwargs={""k"": 1}) ``` ``` docs = retriever.get_relevant_documents(""what did he say about ketanji brown jackson"") ``` ``` len(docs) ``` ``` 1 ```\', metadata={\'title\': \'Vector store-backed retriever | Langchain\', \'type\': None, \'url\': \' \'id\': \'c153ebd9-2611-4a43-9db6-daa1f5f214f6\'}), Document(page_content=\'MultiVector Retriever | Langchain\\n# MultiVector Retriever It can often be beneficial to store multiple vectors per document. There are multiple use cases where this is beneficial. LangChain has a base `MultiVectorRetriever` which makes querying this type of setup easy. A lot of the complexity lies in how to create the multiple vectors per document. This notebook covers some of the common ways to create those vectors and use the `MultiVectorRetriever`. The methods to create multiple vectors per document include: - Smaller chunks: split a document into smaller chunks, and embed those (this is ParentDocumentRetriever). - Summary: create a summary for each document, embed that along with (or instead of) the document. - Hypothetical questions: create hypothetical questions that each document would be appropriate to answer, embed those along with (or instead of) the document. Note that this also enables another method of adding embeddings - manually.Note that this also enables another method of adding embeddings - manually. This is great because you can explicitly add questions or queries that should lead to a document being recovered, giving you more control. ``` from langchain.retrievers.multi_vector import MultiVectorRetriever ``` ``` from langchain.vectorstores import Chromafrom langchain.embeddings import OpenAIEmbeddingsfrom langchain.text_splitter import RecursiveCharacterTextSplitterfrom langchain.storage import InMemoryStorefrom langchain.document_loaders import TextLoader ``` ``` loaders = [ TextLoader(\\\'../../paul_graham_essay.txt\\\'), TextLoader(\\\'../../state_of_the_union.txt\\\'),]docs = []for l in loaders: docs.extend(l.load())text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000)docs = text_splitter.split_documents(docs) ``` ## Smaller chunks[\\u200b](#smaller-chunks) Often times it can be useful to retrieve larger chunks of information, but embed smaller chunks.This allows for embeddings to capture the semantic meaning as closely as possible, but for as much context as possible to be passed downstream. Note that this is what the `ParentDocumentRetriever` does. Here we show what is going on under the hood.``` # The vectorstore to use to index the child chunksvectorstore = Chroma( collection_name=""full_documents"", embedding_function=OpenAIEmbeddings())# The storage layer for the parent documentsstore = InMemoryStore()id_key = ""doc_id""# The retriever (empty to start)retriever = MultiVectorRetriever( vectorstore=vectorstore, docstore=store, id_key=id_key,)import uuiddoc_ids = [str(uuid.uuid4()) for _ in docs] ``` ``` # The splitter to use to create smaller chunkschild_text_splitter = RecursiveCharacterTextSplitter(chunk_size=400) ``` ``` sub_docs = []for i, doc in enumerate(docs): _id = doc_ids[i] _sub_docs = child_text_splitter.split_documents([doc]) for _doc in _sub_docs: _doc.metadata[id_key] = _id sub_docs.extend(_sub_docs) ``` ``` retriever.vectorstore.add_documents(sub_docs)retriever.docstore.mset(list(zip(doc_ids, docs))) ``` ``` # Vectorstore alone retrieves the small chunksretriever.vectorstore.similarity_search(""justice breyer"")[0] ``` ``` Document(page_content=\\\'Tonight, I\'d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyeran Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court.Justice Breyer, thank you for your service. \\\\n\\\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\\', metadata={\\\'doc_id\\\': \\\'10e9cbc0-4ba5-4d79-a09b-c033d1ba7b01\\\', \\\'source\\\': \\\'../../state_of_the_union.txt\\\'}) ``` ``` # Retriever returns larger chunkslen(retriever.get_relevant_documents(""justice breyer"")[0].page_content) ``` ``` 9874 ``` ## Summary[\\u200b](#summary) Oftentimes a summary may be able to distill more accurately what a chunk is about, leading to better retrieval. Here we show how to create summaries, and then embed those.``` from langchain.chat_models import ChatOpenAIfrom langchain.prompts import ChatPromptTemplatefrom langchain.schema.output_parser import StrOutputParserimport uuidfrom langchain.schema.document import Document ``` ``` chain = ( {""doc"": lambda x: x.page_content} | ChatPromptTemplate.from_template(""Summarize the following document:\\\\n\\\\n{doc}"") | ChatOpenAI(max_retries=0) | StrOutputParser()) ``` ``` summaries = chain.batch(docs, {""max_concurrency"": 5}) ``` ``` # The vectorstore to use to index the child chunksvectorstore = Chroma( collection_name=""summaries"", embedding_function=OpenAIEmbeddings())# The storage layer for the parent documentsstore = InMemoryStore()id_key = ""doc_id""# The retriever (empty to start)retriever = MultiVectorRetriever( vectorstore=vectorstore, docstore=store, id_key=id_key,)doc_ids = [str(uuid.uuid4()) for _ in docs] ``` ``` summary_docs = [Document(page_content=s,metadata={id_key: doc_ids[i]}) for i, s in enumerate(summaries)] ``` ``` retriever.vectorstore.add_documents(summary_docs)retriever.docstore.mset(list(zip(doc_ids, docs))) ``` ``` # # We can also add the original chunks to the vectorstore if we so want# for i, doc in enumerate(docs):# doc.metadata[id_key] = doc_ids[i]# retriever.vectorstore.add_documents(docs) ``` ``` sub_docs = vectorstore.similarity_search(""justice breyer"") ``` ``` sub_docs[0] ``` ``` Document(page_content=""The document is a transcript of a speech given by the President of the United States.The President discusses several important issues and initiatives, including the nomination of a Supreme Court Justice, border security and immigration reform, protecting women\\\'s rights, advancing LGBTQ+ equality, bipartisan legislation, addressing the opioid epidemic and mental health, supporting veterans, investigating the health effects of burn pits on military personnel, ending cancer, and the strength and resilience of the American people. "", metadata={\\\'doc_id\\\': \\\'79fa2e9f-28d9-4372-8af3-2caf4f1de312\\\'}) ``` ``` retrieved_docs = retriever.get_relevant_documents(""justice breyer"") ``` ``` len(retrieved_docs[0].page_content) ``` ``` 9194 ``` ## Hypothetical Queries[\\u200b](#hypothetical-queries) An LLM can also be used to generate a list of hypothetical questions that could be asked of a particular document.These questions can then be embedded ``` functions = [ { ""name"": ""hypothetical_questions"", ""description"": ""Generate hypothetical questions"", ""parameters"": { ""type"": ""object"", ""properties"": { ""questions"": { ""type"": ""array"", ""items"": { ""type"": ""string"" }, }, }, ""required"": [""questions""] } } ] ``` ``` from langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParserchain = ( {""doc"": lambda x: x.page_content} # Only asking for 3 hypothetical questions, but this could be adjusted | ChatPromptTemplate.from_template(""Generate a list of 3 hypothetical questions that the below document could be used to answer:\\\\n\\\\n{doc}"") | ChatOpenAI(max_retries=0, model=""gpt-4"").bind(functions=functions, function_call={""name"": ""hypothetical_questions""}) | JsonKeyOutputFunctionsParser(key_name=""questions"")) ``` ``` chain.invoke(docs[0]) ``` ``` [""What was the author\\\'s initial impression of philosophy as a field of study, and how did it change when they got to college?"", \\\'Why did the author decide to switch their focus to Artificial Intelligence (AI)? \\\', ""What led to the author\\\'s disillusionment with the field of AI as it was practiced at the time?""]``` ``` hypothetical_questions = chain.batch(docs, {""max_concurrency"": 5}) ``` ``` # The vectorstore to use to index the child chunksvectorstore = Chroma( collection_name=""hypo-questions"", embedding_function=OpenAIEmbeddings())# The storage layer for the parent documentsstore = InMemoryStore()id_key = ""doc_id""# The retriever (empty to start)retriever = MultiVectorRetriever( vectorstore=vectorstore, docstore=store, id_key=id_key,)doc_ids = [str(uuid.uuid4()) for _ in docs] ``` ``` question_docs = []for i, question_list in enumerate(hypothetical_questions): question_docs.extend([Document(page_content=s,metadata={id_key: doc_ids[i]}) for s in question_list]) ``` ``` retriever.vectorstore.add_documents(question_docs)retriever.docstore.mset(list(zip(doc_ids, docs))) ``` ``` sub_docs = vectorstore.similarity_search(""justice breyer"") ``` ``` sub_docs ``` ``` [Document(page_content=""What is the President\\\'s stance on immigration reform?"", metadata={\\\'doc_id\\\': \\\'505d73e3-8350-46ec-a58e-3af032f04ab3\\\'}), Document(page_content=""What is the President\\\'s stance on immigration reform? "", metadata={\\\'doc_id\\\': \\\'1c9618f0-7660-4b4f-a37c-509cbbbf6dba\\\'}), Document(page_content=""What is the President\\\'s stance on immigration reform? "", metadata={\\\'doc_id\\\': \\\'82c08209-b904-46a8-9532-edd2380950b7\\\'}), Document(page_content=\\\'What measures is the President proposing to protect the rights of LGBTQ+ Americans? \\\', metadata={\\\'doc_id\\\': \\\'82c08209-b904-46a8-9532-edd2380950b7\\\'})] ``` ``` retrieved_docs = retriever.get_relevant_documents(""justice breyer"") ``` ``` len(retrieved_docs[0].page_content) ``` ``` 9194 ```\', metadata={\'title\': \'MultiVector Retriever | Langchain\', \'type\': None, \'url\': \' \'id\': \'beec5531-16a7-453c-80ab-c5628e0236ce\'}), Document(page_content=\'MultiQueryRetriever | Langchain\\n# MultiQueryRetriever Distance-based vector database retrieval embeds (represents) queries in high-dimensional space and finds similar embedded documents based on ""distance"". But, retrieval may produce different results with subtle changes in query wording or if the embeddings do not capture the semantics of the data well. Prompt engineering / tuning is sometimes done to manually address these problems, but can be tedious. The `MultiQueryRetriever` automates the process of prompt tuning by using an LLM to generate multiple queries from different perspectives for a given user input query. For each query, it retrieves a set of relevant documents and takes the unique union across all queries to get a larger set of potentially relevant documents. By generating multiple perspectives on the same question, the `MultiQueryRetriever` might be able to overcome some of the limitations of the distance-based retrieval and get a richer set of results.By generating multiple perspectives on the same question, the `MultiQueryRetriever` might be able to overcome some of the limitations of the distance-based retrieval and get a richer set of results. ``` # Build a sample vectorDBfrom langchain.vectorstores import Chromafrom langchain.document_loaders import WebBaseLoaderfrom langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.text_splitter import RecursiveCharacterTextSplitter# Load blog postloader = WebBaseLoader("" = loader.load()# Splittext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)splits = text_splitter.split_documents(data)# VectorDBembedding = OpenAIEmbeddings()vectordb = Chroma.from_documents(documents=splits, embedding=embedding) ``` #### Simple usage[\\u200b](#simple-usage) Specify the LLM to use for query generation, and the retriever will do the rest.``` from langchain.chat_models import ChatOpenAIfrom langchain.retrievers.multi_query import MultiQueryRetrieverquestion = ""What are the approaches to Task Decomposition? ""llm = ChatOpenAI(temperature=0)retriever_from_llm = MultiQueryRetriever.from_llm( retriever=vectordb.as_retriever(), llm=llm) ``` ``` # Set logging for the queriesimport logginglogging.basicConfig()logging.getLogger(""langchain.retrievers.multi_query"").setLevel(logging.INFO) ``` ``` unique_docs = retriever_from_llm.get_relevant_documents(query=question)len(unique_docs) ``` ``` INFO:langchain.retrievers.multi_query:Generated queries: [\\\'1. How can Task Decomposition be approached? \\\', \\\'2. What are the different methods for Task Decomposition? \\\', \\\'3. What are the various approaches to decomposing tasks?\\\'] 5 ``` #### Supplying your own prompt[\\u200b](#supplying-your-own-prompt) You can also supply a prompt along with an output parser to split the results into a list of queries.5 ``` #### Supplying your own prompt[\\u200b](#supplying-your-own-prompt) You can also supply a prompt along with an output parser to split the results into a list of queries. ``` from typing import Listfrom langchain.chains import LLMChainfrom pydantic import BaseModel, Fieldfrom langchain.prompts import PromptTemplatefrom langchain.output_parsers import PydanticOutputParser# Output parser will split the LLM result into a list of queriesclass LineList(BaseModel): # ""lines"" is the key (attribute name) of the parsed output lines: List[str] = Field(description=""Lines of text"")class LineListOutputParser(PydanticOutputParser): def __init__(self) -> None: super().__init__(pydantic_object=LineList) def parse(self, text: str) -> LineList: lines = text.strip().split(""\\\\n"") return LineList(lines=lines)output_parser = LineListOutputParser()QUERY_PROMPT = PromptTemplate( input_variables=[""question""], template=""""""You are an AI language model assistant.Your task is to generate five different versions of the given user question to retrieve relevant documents from a vector database. By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based similarity search. Provide these alternative questions separated by newlines. Original question: {question}"""""",)llm = ChatOpenAI(temperature=0)# Chainllm_chain = LLMChain(llm=llm, prompt=QUERY_PROMPT, output_parser=output_parser)# Other inputsquestion = ""What are the approaches to Task Decomposition?"" ``` ``` # Runretriever = MultiQueryRetriever( retriever=vectordb.as_retriever(), llm_chain=llm_chain, parser_key=""lines"") # ""lines"" is the key (attribute name) of the parsed output# Resultsunique_docs = retriever.get_relevant_documents( query=""What does the course say about regression? "")len(unique_docs) ``` ``` INFO:langchain.retrievers.multi_query:Generated queries: [""1."")len(unique_docs) ``` ``` INFO:langchain.retrievers.multi_query:Generated queries: [""1. What is the course\\\'s perspective on regression? "", \\\'2. Can you provide information on regression as discussed in the course? \\\', \\\'3. How does the course cover the topic of regression? \\\', ""4. What are the course\\\'s teachings on regression? "", \\\'5. In relation to the course, what is mentioned about regression?\\\'] 11 ```\', metadata={\'title\': \'MultiQueryRetriever | Langchain\', \'type\': None, \'url\': \' \'id\': \'f7c20633-6a60-4ca3-96b1-13fee66e321d\'}), Document(page_content=\'langchain.retrievers.multi_vector.MultiVectorRetriever LangChain 0.0.322\\n# `langchain.retrievers.multi_vector`.MultiVectorRetriever[](#langchain-retrievers-multi-vector-multivectorretriever) *class *langchain.retrievers.multi_vector.MultiVectorRetriever[[source]](../_modules/langchain/retrievers/multi_vector.html#MultiVectorRetriever)[](#langchain.retrievers.multi_vector.MultiVectorRetriever) # Examples using MultiVectorRetriever[](#langchain-retrievers-multi-vector-multivectorretriever) - [MultiVector Retriever]( metadata={\'title\': \'langchain.retrievers.multi_vector.MultiVectorRetriever LangChain 0.0.322\', \'type\': None, \'url\': \' \'id\': \'1820c44d-7783-4846-a11c-106b18da015d\'})] ``` ## Putting it in a chain Let\'s try using our retrieval systems in a simple chain! ```python from langchain.chat_models import ChatOpenAI from langchain.prompts import ChatPromptTemplate from langchain.schema import StrOutputParser from langchain.schema.runnable import RunnablePassthrough prompt = ChatPromptTemplate.from_messages( [ ( ""system"", """"""You are a great software engineer who is very familiar \\ with Python. Given a user question or request about a new Python library called LangChain and \\ parts of the LangChain documentation, answer the question or generate the requested code. \\ Your answers must be accurate, should include code whenever possible, and should assume anything \\ about LangChain which is note explicitly stated in the LangChain documentation. If the required \\ information is not available, just say so. LangChain Documentation ------------------ {context}"""""", ), (""human"", ""{question}""), ] ) model = ChatOpenAI(model=""gpt-3.5-turbo-16k"") chain = ( { ""question"": RunnablePassthrough(), ""context"": parent_retriever | (lambda docs: ""\\n\\n"".join(d.page_content for d in docs)), } | prompt | model | StrOutputParser() ) ``` ```python for chunk in chain.invoke( ""How do I create a FAISS vector store retriever that returns 10 documents per search query"" ): print(chunk, end="""", flush=True) ``` ```text To create a FAISS vector store retriever that returns 10 documents per search query, you can use the following code: ```python from langchain.embeddings import OpenAIEmbeddings from langchain.vectorstores import FAISS # Assuming you have already loaded and split your documents # into `texts` and initialized your `embeddings` object # Create the FAISS vector store db = FAISS.from_documents(texts, embeddings) # Create the retriever with the desired search kwargs retriever = db.as_retriever(search_kwargs={""k"": 10}) ``` Now, you can use the `retriever` object to get relevant documents using the `get_relevant_documents` method. For example: ```python docs = retriever.get_relevant_documents(""your search query"") ``` This will return a list of 10 documents that are most relevant to the given search query. ``` - [Retriever chunks](#retriever-chunks) - [Other packages](#other-packages) - [Retrieve parent docs](#retrieve-parent-docs) - [Putting it in a chain](#putting-it-in-a-chain)']","Retrieval augmented generation (RAG) can be done in several ways:

1. **Simple RAG**: Use a retriever to fetch relevant documents based on a user query, then pass these documents to a language model to generate an answer.

2. **RAG with Memory**: Incorporate user history into the retrieval process to provide more context-aware responses.

3. **RAG with Multiple Retrievers**: Combine outputs from different retrievers to enhance the relevance of retrieved documents.

4. **RAG with Summarization**: Summarize retrieved documents before passing them to the language model to reduce context length and focus on key information.

5. **RAG with Hypothetical Questions**: Generate hypothetical questions related to the document to improve retrieval and context for the language model.

6. **RAG with Maximum Marginal Relevance**: Use MMR to retrieve documents that are both relevant and diverse, avoiding redundancy.

Choose the method based on your specific use case and requirements.","Retrieval Augmented Generation (RAG) is a process that involves retrieving external data and using it in the generation step of language models. LangChain provides various building blocks for RAG applications. Here are some ways of doing retrieval augmented generation:

- Document loaders: LangChain offers over 100 document loaders to load documents from different sources, including private S3 buckets and public websites.
- Document transformers: These help in fetching relevant parts of documents by splitting or chunking large documents into smaller chunks.
- Text embedding models: LangChain integrates with over 25 different embedding providers and methods to create embeddings for documents, allowing efficient similarity-based retrieval.
- Vector stores: With the rise of embeddings, LangChain supports over 50 vector stores for efficient storage and searching of embeddings.
- Retrievers: LangChain supports various retrieval algorithms, including semantic search, parent document retriever, self-query retriever, ensemble retriever, and more, to retrieve the data from the database.

These components work together to enable retrieval augmented generation in LangChain. [0] [1]",0.9999999999666667,1.0,,0.15807437922444717,0.17647058823529413
63,whats the difference between a handler and an inheritable_handler?,"['langchain.schema.callbacks.manager.AsyncCallbackManager LangChain 0.0.337 API Experimental Python Docs Toggle Menu PrevUp Next LangChain 0.0.337 langchain.schema.callbacks.manager.AsyncCallbackManager langchain.schema.callbacks.manager.AsyncCallbackManager class langchain.schema.callbacks.manager.AsyncCallbackManager(handlers: List[BaseCallbackHandler], inheritable_handlers: Optional[List[BaseCallbackHandler]] = None, parent_run_id: Optional[UUID] = None, *, tags: Optional[List[str]] = None, inheritable_tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, inheritable_metadata: Optional[Dict[str, Any]] = None)[source] Async callback manager that handles callbacks from LangChain. Initialize callback manager. Attributes is_async Return whether the handler is async. Methods __init__(handlers[,inheritable_handlers,...]) Initialize callback manager. add_handler(handler[,inherit]) Add a handler to the callback manager. add_metadata(metadata[,inherit]) add_tags(tags[,inherit]) configure([inheritable_callbacks,...]) Configure the async callback manager. copy() Copy the callback manager. on_chain_start(serialized,inputs[,run_id]) Run when chain starts running. on_chat_model_start(serialized,messages,...) Run when LLM starts running. on_llm_start(serialized,prompts,**kwargs) Run when LLM starts running. on_retriever_start(serialized,query[,...]) Run when retriever starts running. on_tool_start(serialized,input_str[,...]) Run when tool starts running. remove_handler(handler) Remove a handler from the callback manager. remove_metadata(keys) remove_tags(tags) set_handler(handler[,inherit]) Set handler as the only handler on the callback manager. set_handlers(handlers[,inherit]) Set handlers as the only handlers on the callback manager. __init__(handlers: List[BaseCallbackHandler], inheritable_handlers: Optional[List[BaseCallbackHandler]] = None, parent_run_id: Optional[UUID] = None, *, tags: Optional[List[str]] = None, inheritable_tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, inheritable_metadata: Optional[Dict[str, Any]] = None) None Initialize callback manager. add_handler(handler: BaseCallbackHandler, inherit: bool = True) None Add a handler to the callback manager. add_metadata(metadata: Dict[str, Any], inherit: bool = True) None add_tags(tags: List[str], inherit: bool = True) None classmethod configure(inheritable_callbacks: Optional[Union[List[BaseCallbackHandler], BaseCallbackManager]] = None, local_callbacks: Optional[Union[List[BaseCallbackHandler], BaseCallbackManager]] = None, verbose: bool = False, inheritable_tags: Optional[List[str]] = None, local_tags: Optional[List[str]] = None, inheritable_metadata: Optional[Dict[str, Any]] = None, local_metadata: Optional[Dict[str, Any]] = None) AsyncCallbackManager[source] Configure the async callback manager. Parameters inheritable_callbacks (Optional[Callbacks], optional) The inheritable callbacks. Defaults to None. local_callbacks (Optional[Callbacks], optional)  The local callbacks. Defaults to None. verbose (bool, optional)  Whether to enable verbose mode. Defaults to False. inheritable_tags (Optional[List[str]], optional)  The inheritable tags. Defaults to None. local_tags (Optional[List[str]], optional)  The local tags. Defaults to None. inheritable_metadata (Optional[Dict[str, Any]], optional)  The inheritable metadata. Defaults to None. local_metadata (Optional[Dict[str, Any]], optional)  The local metadata. Defaults to None. Returns The configured async callback manager. Return type AsyncCallbackManager copy()  T Copy the callback manager. async on_chain_start(serialized: Dict[str, Any], inputs: Union[Dict[str, Any], Any], run_id: Optional[UUID] = None, **kwargs: Any)  AsyncCallbackManagerForChainRun[source] Run when chain starts running. Parameters serialized (Dict[str, Any])  The serialized chain. inputs (Union[Dict[str, Any], Any])  The inputs to the chain. run_id (UUID, optional)  The ID of the run. Defaults to None. Returns The async callback managerfor the chain run. Return type AsyncCallbackManagerForChainRun async on_chat_model_start(serialized: Dict[str, Any], messages: List[List[BaseMessage]], **kwargs: Any)  List[AsyncCallbackManagerForLLMRun][source] Run when LLM starts running. Parameters serialized (Dict[str, Any])  The serialized LLM. messages (List[List[BaseMessage]])  The list of messages. run_id (UUID, optional)  The ID of the run. Defaults to None. Returns The list ofasync callback managers, one for each LLM Run corresponding to each inner message list. Return type List[AsyncCallbackManagerForLLMRun] async on_llm_start(serialized: Dict[str, Any], prompts: List[str], **kwargs: Any)  List[AsyncCallbackManagerForLLMRun][source] Run when LLM starts running. Parameters serialized (Dict[str, Any])  The serialized LLM. prompts (List[str])  The list of prompts. run_id (UUID, optional)  The ID of the run. Defaults to None. Returns The list of asynccallback managers, one for each LLM Run corresponding to each prompt. Return type List[AsyncCallbackManagerForLLMRun] async on_retriever_start(serialized: Dict[str, Any], query: str, run_id: Optional[UUID] = None, parent_run_id: Optional[UUID] = None, **kwargs: Any)  AsyncCallbackManagerForRetrieverRun[source] Run when retriever starts running. async on_tool_start(serialized: Dict[str, Any], input_str: str, run_id: Optional[UUID] = None, parent_run_id: Optional[UUID] = None, **kwargs: Any)  AsyncCallbackManagerForToolRun[source] Run when tool starts running. Parameters serialized (Dict[str, Any])  The serialized tool. input_str (str)  The input to the tool. run_id (UUID, optional)  The ID of the run. Defaults to None. parent_run_id (UUID, optional)  The ID of the parent run. Defaults to None. Returns The async callback managerfor the tool run. Return type AsyncCallbackManagerForToolRun remove_handler(handler: BaseCallbackHandler)  None Remove a handler from the callback manager. remove_metadata(keys: List[str])  None remove_tags(tags: List[str])  None set_handler(handler: BaseCallbackHandler, inherit: bool = True)  None Set handler as the only handler on the callback manager. set_handlers(handlers: List[BaseCallbackHandler], inherit: bool = True)  None Set handlers as the only handlers on the callback manager.  2023, Harrison Chase. Last updated on Nov 17, 2023. Show this page source', ""langchain.schema.callbacks.manager.AsyncParentRunManager LangChain 0.0.337 API Experimental Python Docs Toggle Menu PrevUp Next LangChain 0.0.337 langchain.schema.callbacks.manager.AsyncParentRunManager langchain.schema.callbacks.manager.AsyncParentRunManager class langchain.schema.callbacks.manager.AsyncParentRunManager(*, run_id: UUID, handlers: List[BaseCallbackHandler], inheritable_handlers: List[BaseCallbackHandler], parent_run_id: Optional[UUID] = None, tags: Optional[List[str]] = None, inheritable_tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, inheritable_metadata: Optional[Dict[str, Any]] = None)[source] Async Parent Run Manager. Initialize the run manager. Parameters run_id (UUID) The ID of the run. handlers (List[BaseCallbackHandler]) The list of handlers. inheritable_handlers (List[BaseCallbackHandler]) The list of inheritable handlers. parent_run_id (UUID, optional) The ID of the parent run. Defaults to None. tags (Optional[List[str]]) The list of tags. inheritable_tags (Optional[List[str]]) The list of inheritable tags. metadata (Optional[Dict[str, Any]]) The metadata. inheritable_metadata (Optional[Dict[str, Any]]) The inheritable metadata. Methods __init__(*,run_id,handlers,...[,...]) Initialize the run manager. get_child([tag]) Get a child callback manager. get_noop_manager() Return a manager that doesn't perform any operations. on_retry(retry_state,**kwargs) Run on a retry event. on_text(text,**kwargs) Run when text is received. __init__(*, run_id: UUID, handlers: List[BaseCallbackHandler], inheritable_handlers: List[BaseCallbackHandler], parent_run_id: Optional[UUID] = None, tags: Optional[List[str]] = None, inheritable_tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, inheritable_metadata: Optional[Dict[str, Any]] = None) None Initialize the run manager. Parameters run_id (UUID) The ID of the run. handlers (List[BaseCallbackHandler]) The list of handlers. inheritable_handlers (List[BaseCallbackHandler]) The list of inheritable handlers. parent_run_id (UUID, optional) The ID of the parent run. Defaults to None. tags (Optional[List[str]]) The list of tags. inheritable_tags (Optional[List[str]]) The list of inheritable tags. metadata (Optional[Dict[str, Any]]) The metadata. inheritable_metadata (Optional[Dict[str, Any]]) The inheritable metadata. get_child(tag: Optional[str] = None) AsyncCallbackManager[source] Get a child callback manager. Parameters tag (str, optional) The tag for the child callback manager. Defaults to None. Returns The child callback manager. Return type AsyncCallbackManager classmethod get_noop_manager() BRM Return a manager that doesn't perform any operations. Returns The noop manager. Return type BaseRunManager async on_retry(retry_state: RetryCallState, **kwargs: Any)  None Run on a retry event. async on_text(text: str, **kwargs: Any)  Any Run when text is received. Parameters text (str)  The received text. Returns The result of the callback. Return type Any  2023, Harrison Chase. Last updated on Nov 17, 2023. Show this page source"", ""langchain.schema.callbacks.manager.CallbackManagerForToolRun LangChain 0.0.337 API Experimental Python Docs Toggle Menu PrevUp Next LangChain 0.0.337 langchain.schema.callbacks.manager.CallbackManagerForToolRun langchain.schema.callbacks.manager.CallbackManagerForToolRun class langchain.schema.callbacks.manager.CallbackManagerForToolRun(*, run_id: UUID, handlers: List[BaseCallbackHandler], inheritable_handlers: List[BaseCallbackHandler], parent_run_id: Optional[UUID] = None, tags: Optional[List[str]] = None, inheritable_tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, inheritable_metadata: Optional[Dict[str, Any]] = None)[source] Callback manager for tool run. Initialize the run manager. Parameters run_id (UUID) The ID of the run. handlers (List[BaseCallbackHandler]) The list of handlers. inheritable_handlers (List[BaseCallbackHandler]) The list of inheritable handlers. parent_run_id (UUID, optional) The ID of the parent run. Defaults to None. tags (Optional[List[str]]) The list of tags. inheritable_tags (Optional[List[str]]) The list of inheritable tags. metadata (Optional[Dict[str, Any]]) The metadata. inheritable_metadata (Optional[Dict[str, Any]]) The inheritable metadata. Methods __init__(*,run_id,handlers,...[,...]) Initialize the run manager. get_child([tag]) Get a child callback manager. get_noop_manager() Return a manager that doesn't perform any operations. on_retry(retry_state,**kwargs) Run on a retry event. on_text(text,**kwargs) Run when text is received. on_tool_end(output,**kwargs) Run when tool ends running. on_tool_error(error,**kwargs) Run when tool errors. __init__(*, run_id: UUID, handlers: List[BaseCallbackHandler], inheritable_handlers: List[BaseCallbackHandler], parent_run_id: Optional[UUID] = None, tags: Optional[List[str]] = None, inheritable_tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, inheritable_metadata: Optional[Dict[str, Any]] = None) None Initialize the run manager. Parameters run_id (UUID) The ID of the run. handlers (List[BaseCallbackHandler]) The list of handlers. inheritable_handlers (List[BaseCallbackHandler]) The list of inheritable handlers. parent_run_id (UUID, optional) The ID of the parent run. Defaults to None. tags (Optional[List[str]]) The list of tags. inheritable_tags (Optional[List[str]]) The list of inheritable tags. metadata (Optional[Dict[str, Any]]) The metadata. inheritable_metadata (Optional[Dict[str, Any]]) The inheritable metadata. get_child(tag: Optional[str] = None) CallbackManager Get a child callback manager. Parameters tag (str, optional) The tag for the child callback manager. Defaults to None. Returns The child callback manager. Return type CallbackManager classmethod get_noop_manager()  BRM Return a manager that doesn't perform any operations. Returns The noop manager. Return type BaseRunManager on_retry(retry_state: RetryCallState, **kwargs: Any)  None Run on a retry event. on_text(text: str, **kwargs: Any)  Any Run when text is received. Parameters text (str)  The received text. Returns The result of the callback. Return type Any on_tool_end(output: str, **kwargs: Any)  None[source] Run when tool ends running. Parameters output (str)  The output of the tool. on_tool_error(error: BaseException, **kwargs: Any)  None[source] Run when tool errors. Parameters error (Exception or KeyboardInterrupt)  The error. Examples using CallbackManagerForToolRun Defining Custom Tools  2023, Harrison Chase. Last updated on Nov 17, 2023. Show this page source""]","In LangChain's AsyncCallbackManager, a handler is a callback that gets invoked during specific events (like chain start, LLM start, etc.). An inheritable_handler is a handler that can be passed down to child runs or managers. This means inheritable handlers maintain their configuration and behavior across different levels of the run hierarchy.","In Langchain, both `handler` and `inheritable_handler` are types of callback handlers. Handlers can refer to inheritable handlers or local handlers. Inheritable handlers are propagated to other objects that are called within a Runnable, chain, tool, or other object. Local handlers are only called by the object they are assigned to and not called by any other objects.",0.9999999999666667,0.5,1.0,0.025688686451917863,0.2162162162162162
64,What is a chain?,"['Router | Router Routing allows you to create non-deterministic chains where the output of a previous step defines the next step. Routing helps provide structure and consistency around interactions with LLMs. As a very simple example, let\'s suppose we have two templates optimized for different types of questions, and we want to choose the template based on the user input. ```python from langchain.prompts import PromptTemplate physics_template = """"""You are a very smart physics professor. \\ You are great at answering questions about physics in a concise and easy to understand manner. \\ When you don\'t know the answer to a question you admit that you don\'t know. Here is a question: {input}"""""" physics_prompt = PromptTemplate.from_template(physics_template) math_template = """"""You are a very good mathematician. You are great at answering math questions. \\ You are so good because you are able to break down hard problems into their component parts, \\ answer the component parts, and then put them together to answer the broader question. Here is a question: {input}"""""" math_prompt = PromptTemplate.from_template(math_template) ``` ## Using LCEL We can easily do this using a `RunnableBranch`. A `RunnableBranch` is initialized with a list of (condition, runnable) pairs and a default runnable. It selects which branch by passing each condition the input it\'s invoked with. It selects the first condition to evaluate to True, and runs the corresponding runnable to that condition with the input. If no provided conditions match, it runs the default runnable. ```python from langchain.chat_models import ChatOpenAI from langchain.schema.output_parser import StrOutputParser from langchain.schema.runnable import RunnableBranch ``` ```python general_prompt = PromptTemplate.from_template( ""You are a helpful assistant. Answer the question as accurately as you can.\\n\\n{input}"" ) prompt_branch = RunnableBranch( (lambda x: x[""topic""] == ""math"", math_prompt), (lambda x: x[""topic""] == ""physics"", physics_prompt), general_prompt, ) ``` ```python from typing import Literal from langchain.output_parsers.openai_functions import PydanticAttrOutputFunctionsParser from langchain.pydantic_v1 import BaseModel from langchain.utils.openai_functions import convert_pydantic_to_openai_function class TopicClassifier(BaseModel): ""Classify the topic of the user question"" topic: Literal[""math"", ""physics"", ""general""] ""The topic of the user question. One of \'math\', \'physics\' or \'general\'."" classifier_function = convert_pydantic_to_openai_function(TopicClassifier) llm = ChatOpenAI().bind( functions=[classifier_function], function_call={""name"": ""TopicClassifier""} ) parser = PydanticAttrOutputFunctionsParser( pydantic_schema=TopicClassifier, attr_name=""topic"" ) classifier_chain = llm | parser ``` ```python from operator import itemgetter from langchain.schema.output_parser import StrOutputParser from langchain.schema.runnable import RunnablePassthrough final_chain = ( RunnablePassthrough.assign(topic=itemgetter(""input"") | classifier_chain) | prompt_branch | ChatOpenAI() | StrOutputParser() ) ``` ```python final_chain.invoke( { ""input"": ""What is the first prime number greater than 40 such that one plus the prime number is divisible by 3?"" } ) ``` ```text ""Thank you for your kind words! I\'ll be happy to help you with this math question.\\n\\nTo find the first prime number greater than 40 that satisfies the given condition, we need to follow a step-by-step approach. \\n\\nFirstly, let\'s list the prime numbers greater than 40:\\n41, 43, 47, 53, 59, 61, 67, 71, ...\\n\\nNow, we need to check if one plus each of these prime numbers is divisible by 3. We can do this by calculating the remainder when dividing each number by 3.\\n\\nFor 41, (41 + 1) % 3 = 42 % 3 = 0. It is divisible by 3.\\n\\nFor 43, (43 + 1) % 3 = 44 % 3 = 2. It is not divisible by 3.\\n\\nFor 47, (47 + 1) % 3 = 48 % 3 = 0. It is divisible by 3.\\n\\nSince 41 and 47 are both greater than 40 and satisfy the condition, the first prime number greater than 40 such that one plus the prime number is divisible by 3 is 41.\\n\\nTherefore, the answer to the question is 41."" ``` For more on routing with LCEL [head here](/docs/expression_language/how_to/routing). ## [Legacy] RouterChain The preferred approach as of version `0.0.293` is to use LCEL as above.Here we show how to use the `RouterChain` paradigm to create a chain that dynamically selects the next chain to use for a given input. Router chains are made up of two components: - The `RouterChain` itself (responsible for selecting the next chain to call) - `destination_chains`: chains that the router chain can route to In this example, we will focus on the different types of routing chains. We will show these routing chains used in a `MultiPromptChain` to create a question-answering chain that selects the prompt which is most relevant for a given question, and then answers the question using that prompt. ```python from langchain.chains import ConversationChain from langchain.chains.llm import LLMChain from langchain.chains.router import MultiPromptChain from langchain.llms import OpenAI ``` ### [Legacy] LLMRouterChain This chain uses an LLM to determine how to route things. ```python prompt_infos = [ { ""name"": ""physics"", ""description"": ""Good for answering questions about physics"", ""prompt_template"": physics_template, }, { ""name"": ""math"", ""description"": ""Good for answering math questions"", ""prompt_template"": math_template, }, ] ``` ```python llm = OpenAI() ``` ```python destination_chains = {} for p_info in prompt_infos: name = p_info[""name""] prompt_template = p_info[""prompt_template""] prompt = PromptTemplate(template=prompt_template, input_variables=[""input""]) chain = LLMChain(llm=llm, prompt=prompt) destination_chains[name] = chain default_chain = ConversationChain(llm=llm, output_key=""text"") ``` ```python from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE ``` ```python destinations = [f""{p[\'name\']}: {p[\'description\']}"" for p in prompt_infos] destinations_str = ""\\n"".join(destinations) router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations_str) router_prompt = PromptTemplate( template=router_template, input_variables=[""input""], output_parser=RouterOutputParser(), ) router_chain = LLMRouterChain.from_llm(llm, router_prompt) ``` ```python chain = MultiPromptChain( router_chain=router_chain, destination_chains=destination_chains, default_chain=default_chain, verbose=True, ) ``` ```python print(chain.run(""What is black body radiation?"")) ``` ```text > Entering new MultiPromptChain chain... /Users/bagatur/langchain/libs/langchain/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain. warnings.warn( physics: {\'input\': \'What is black body radiation?\'} > Finished chain. Black body radiation is the thermal electromagnetic radiation within or surrounding a body in thermodynamic equilibrium with its environment, or emitted by a black body (an idealized physical body which absorbs all incident electromagnetic radiation). It is a characteristic of the temperature of the body; if the body has a uniform temperature, the radiation is also uniform across the spectrum of frequencies. The spectral characteristics of the radiation are determined by the temperature of the body, which implies that a black body at a given temperature will emit the same amount of radiation at every frequency. ``` ```python print( chain.run( ""What is the first prime number greater than 40 such that one plus the prime number is divisible by 3?"" ) ) ``` ```text > Entering new MultiPromptChain chain... /Users/bagatur/langchain/libs/langchain/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain. warnings.warn( math: {\'input\': \'What is the first prime number greater than 40 such that one plus the prime number is divisible by 3?\'} > Finished chain. The first prime number greater than 40 such that one plus the prime number is divisible by 3 is 43. This can be seen by breaking down the problem: 1) We know that a prime number is a number that is only divisible by itself and one. 2) We also know that if a number is divisible by 3, the sum of its digits must be divisible by 3. So, if we want to find the first prime number greater than 40 such that one plus the prime number is divisible by 3, we can start counting up from 40, testing each number to see if it is prime and if the sum of the number and one is divisible by three. The first number we come to that satisfies these conditions is 43. ``` ```python print(chain.run(""What is the name of the type of cloud that rains?"")) ``` ```text > Entering new MultiPromptChain chain... /Users/bagatur/langchain/libs/langchain/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain. warnings.warn( physics: {\'input\': \'What is the name of the type of cloud that rains?\'} > Finished chain. The type of cloud that rains is called a cumulonimbus cloud. ``` ## [Legacy] EmbeddingRouterChain The `EmbeddingRouterChain` uses embeddings and similarity to route between destination chains. ```python from langchain.chains.router.embedding_router import EmbeddingRouterChain from langchain.embeddings import CohereEmbeddings from langchain.vectorstores import Chroma ``` ```python names_and_descriptions = [ (""physics"", [""for questions about physics""]), (""math"", [""for questions about math""]), ] ``` ```python router_chain = EmbeddingRouterChain.from_names_and_descriptions( names_and_descriptions, Chroma, CohereEmbeddings(), routing_keys=[""input""] ) ``` ```python chain = MultiPromptChain( router_chain=router_chain, destination_chains=destination_chains, default_chain=default_chain, verbose=True, ) ``` ```python print(chain.run(""What is black body radiation?"")) ``` ```text > Entering new MultiPromptChain chain... physics: {\'input\': \'What is black body radiation?\'} > Finished chain. Black body radiation is the electromagnetic radiation emitted by a black body, which is an idealized physical body that absorbs all incident electromagnetic radiation. This radiation is related to the temperature of the body, with higher temperatures leading to higher radiation levels. The spectrum of the radiation is continuous, and is described by the Planck\'s law of black body radiation. ``` ```python print( chain.run( ""What is the first prime number greater than 40 such that one plus the prime number is divisible by 3?"" ) ) ``` ```text > Entering new MultiPromptChain chain... math: {\'input\': \'What is the first prime number greater than 40 such that one plus the prime number is divisible by 3?\'} > Finished chain. The first prime number greater than 40 such that one plus the prime number is divisible by 3 is 43. This is because 43 is a prime number, and 1 + 43 = 44, which is divisible by 3. ``` - [Using LCEL](#using-lcel) - [Legacy RouterChain](#legacy-routerchain)', 'WandB Tracing | WandB Tracing There are two recommended ways to trace your LangChains: 1. Setting the `LANGCHAIN_WANDB_TRACING` environment variable to ""true"". 2. Using a context manager with tracing_enabled() to trace a particular block of code. **Note** if the environment variable is set, all code will be traced, regardless of whether or not it\'s within the context manager. ```python import os os.environ[""LANGCHAIN_WANDB_TRACING""] = ""true"" # wandb documentation to configure wandb using env variables # # here we are configuring the wandb project name os.environ[""WANDB_PROJECT""] = ""langchain-tracing"" from langchain.agents import AgentType, initialize_agent, load_tools from langchain.callbacks import wandb_tracing_enabled from langchain.llms import OpenAI ``` ```python # Agent run with tracing. Ensure that OPENAI_API_KEY is set appropriately to run this example. llm = OpenAI(temperature=0) tools = load_tools([""llm-math""], llm=llm) ``` ```python agent = initialize_agent( tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True ) agent.run(""What is 2 raised to .123243 power?"") # this should be traced # A url with for the trace sesion like the following should print in your console: # # The url can be used to view the trace session in wandb. ``` ```python # Now, we unset the environment variable and use a context manager. if ""LANGCHAIN_WANDB_TRACING"" in os.environ: del os.environ[""LANGCHAIN_WANDB_TRACING""] # enable tracing using a context manager with wandb_tracing_enabled(): agent.run(""What is 5 raised to .123243 power?"") # this should be traced agent.run(""What is 2 raised to .123243 power?"") # this should not be traced ``` ```text > Entering new AgentExecutor chain... I need to use a calculator to solve this. Action: Calculator Action Input: 5^.123243 Observation: Answer: 1.2193914912400514 Thought: I now know the final answer. Final Answer: 1.2193914912400514 > Finished chain. > Entering new AgentExecutor chain... I need to use a calculator to solve this. Action: Calculator Action Input: 2^.123243 Observation: Answer: 1.0891804557407723 Thought: I now know the final answer. Final Answer: 1.0891804557407723 > Finished chain. \'1.0891804557407723\' ```', 'Titan Takeoff Pro | Titan Takeoff Pro `TitanML` helps businesses build and deploy better, smaller, cheaper, and faster NLP models through our training, compression, and inference optimization platform. Note: These docs are for the Pro version of Titan Takeoff. For the community version, see the page for Titan Takeoff. Our inference server, [Titan Takeoff (Pro Version)]( enables deployment of LLMs locally on your hardware in a single command. Most generative model architectures are supported, such as Falcon, Llama 2, GPT2, T5 and many more. ## Example usage Here are some helpful examples to get started using the Pro version of Titan Takeoff Server. No parameters are needed by default, but a baseURL that points to your desired URL where Takeoff is running can be specified and generation parameters can be supplied. ```python from langchain.callbacks.manager import CallbackManager from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler from langchain.llms import TitanTakeoffPro from langchain.prompts import PromptTemplate # Example 1: Basic use llm = TitanTakeoffPro() output = llm(""What is the weather in London in August?"") print(output) # Example 2: Specifying a port and other generation parameters llm = TitanTakeoffPro( base_url="" min_new_tokens=128, max_new_tokens=512, no_repeat_ngram_size=2, sampling_topk=1, sampling_topp=1.0, sampling_temperature=1.0, repetition_penalty=1.0, regex_string="""", ) output = llm(""What is the largest rainforest in the world?"") print(output) # Example 3: Using generate for multiple inputs llm = TitanTakeoffPro() rich_output = llm.generate([""What is Deep Learning?"", ""What is Machine Learning?""]) print(rich_output.generations) # Example 4: Streaming output llm = TitanTakeoffPro( streaming=True, callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]) ) prompt = ""What is the capital of France?"" llm(prompt) # Example 5: Using LCEL llm = TitanTakeoffPro() prompt = PromptTemplate.from_template(""Tell me about {topic}"") chain = prompt | llm chain.invoke({""topic"": ""the universe""}) ``` - [Example usage](#example-usage)']","In the context of LangChain, a ""chain"" refers to a sequence of operations or steps that are executed in a specific order. Chains can be used to build complex workflows where each step takes the output of the previous step as input. Chains can involve various components like prompts, language models, output parsers, and more, working together to achieve a specific task.","A 'Chain' is a class in LangChain that composes a number of components together into a single object. An example would be the LLMChain, which formats an input as a prompt, calls an LLM, and then parses the output. ""chaining"" also can generically refer to composing llms, functions, and other operations together into a larger program.",0.0,1.0,1.0,0.02859995848872868,0.22033898305084748
65,What is LangChain Expression Language?,"['Self-ask with search | Self-ask with search This walkthrough showcases the self-ask with search chain. ```python from langchain.agents import AgentType, Tool, initialize_agent from langchain.llms import OpenAI from langchain.utilities import SerpAPIWrapper llm = OpenAI(temperature=0) search = SerpAPIWrapper() tools = [ Tool( name=""Intermediate Answer"", func=search.run, description=""useful for when you need to ask with search"", ) ] ``` ## Using LangChain Expression Language First we will show how to construct this agent from components using LangChain Expression Language ```python from langchain import hub from langchain.agents.format_scratchpad import format_log_to_str from langchain.agents.output_parsers import SelfAskOutputParser ``` ```python prompt = hub.pull(""hwchase17/self-ask-with-search"") ``` ```python llm_with_stop = llm.bind(stop=[""\\nIntermediate answer:""]) ``` ```python agent = ( { ""input"": lambda x: x[""input""], # Use some custom observation_prefix/llm_prefix for formatting ""agent_scratchpad"": lambda x: format_log_to_str( x[""intermediate_steps""], observation_prefix=""\\nIntermediate answer: "", llm_prefix="""", ), } | prompt | llm_with_stop | SelfAskOutputParser() ) ``` ```python from langchain.agents import AgentExecutor ``` ```python agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True) ``` ```python agent_executor.invoke( {""input"": ""What is the hometown of the reigning men\'s U.S. Open champion?""} ) ``` ```text > Entering new AgentExecutor chain... Yes. Follow up: Who is the reigning men\'s U.S. Open champion?Men\'s US Open Tennis Champions Novak Djokovic earned his 24th major singles title against 2021 US Open champion Daniil Medvedev, 6-3, 7-6 (7-5), 6-3. The victory ties the Serbian player with the legendary Margaret Court for the most Grand Slam wins across both men\'s and women\'s singles. Follow up: Where is Novak Djokovic from?Belgrade, Serbia So the final answer is: Belgrade, Serbia > Finished chain. {\'input\': ""What is the hometown of the reigning men\'s U.S. Open champion?"", \'output\': \'Belgrade, Serbia\'} ``` ## Use off-the-shelf agent ```python self_ask_with_search = initialize_agent( tools, llm, agent=AgentType.SELF_ASK_WITH_SEARCH, verbose=True ) self_ask_with_search.run( ""What is the hometown of the reigning men\'s U.S. Open champion?"" ) ``` ```text > Entering new AgentExecutor chain... Yes. Follow up: Who is the reigning men\'s U.S. Open champion? Intermediate answer: Men\'s US Open Tennis Champions Novak Djokovic earned his 24th major singles title against 2021 US Open champion Daniil Medvedev, 6-3, 7-6 (7-5), 6-3. The victory ties the Serbian player with the legendary Margaret Court for the most Grand Slam wins across both men\'s and women\'s singles. Follow up: Where is Novak Djokovic from? Intermediate answer: Belgrade, Serbia So the final answer is: Belgrade, Serbia > Finished chain. \'Belgrade, Serbia\' ``` - [Using LangChain Expression Language](#using-langchain-expression-language) - [Use off-the-shelf agent](#use-off-the-shelf-agent)', 'Quickstart | Quickstart In this quickstart we\'ll show you how to: - Get setup with LangChain, LangSmith and LangServe - Use the most basic and common components of LangChain: prompt templates, models, and output parsers - Use LangChain Expression Language, the protocol that LangChain is built on and which facilitates component chaining - Build a simple application with LangChain - Trace your application with LangSmith - Serve your application with LangServe That\'s a fair amount to cover! Let\'s dive in. ## Setup ### Installation To install LangChain run: Pip ```bash pip install langchain ``` Conda ```bash conda install langchain -c conda-forge ``` For more details, see our [Installation guide](/docs/get_started/installation). ### Environment Using LangChain will usually require integrations with one or more model providers, data stores, APIs, etc. For this example, we\'ll use OpenAI\'s model APIs. First we\'ll need to install their Python package: ```bash pip install openai ``` Accessing the API requires an API key, which you can get by creating an account and heading [here]( Once we have a key we\'ll want to set it as an environment variable by running: ```bash export OPENAI_API_KEY=""..."" ``` If you\'d prefer not to set an environment variable you can pass the key in directly via the `openai_api_key` named parameter when initiating the OpenAI LLM class: ```python from langchain.chat_models import ChatOpenAI llm = ChatOpenAI(openai_api_key=""..."") ``` ### LangSmith Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with [LangSmith]( Note that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces: ```shell export LANGCHAIN_TRACING_V2=""true"" export LANGCHAIN_API_KEY=... ``` ### LangServe LangServe helps developers deploy LangChain chains as a REST API. You do not need to use LangServe to use LangChain, but in this guide we\'ll show how you can deploy your app with LangServe. Install with: ```bash pip install ""langserve[all]"" ``` ## Building with LangChain LangChain provides many modules that can be used to build language model applications. Modules can be used as standalones in simple applications and they can be composed for more complex use cases. Composition is powered by **LangChain Expression Language** (LCEL), which defines a unified `Runnable` interface that many modules implement, making it possible to seamlessly chain components. The simplest and most common chain contains three things: - LLM/Chat Model: The language model is the core reasoning engine here. In order to work with LangChain, you need to understand the different types of language models and how to work with them. - Prompt Template: This provides instructions to the language model. This controls what the language model outputs, so understanding how to construct prompts and different prompting strategies is crucial. - Output Parser: These translate the raw response from the language model to a more workable format, making it easy to use the output downstream. In this guide we\'ll cover those three components individually, and then go over how to combine them. Understanding these concepts will set you up well for being able to use and customize LangChain applications. Most LangChain applications allow you to configure the model and/or the prompt, so knowing how to take advantage of this will be a big enabler. ### LLM / Chat Model There are two types of language models: - `LLM`: underlying model takes a string as input and returns a string - `ChatModel`: underlying model takes a list of messages as input and returns a message Strings are simple, but what exactly are messages? The base message interface is defined by `BaseMessage`, which has two required attributes: - `content`: The content of the message. Usually a string. - `role`: The entity from which the `BaseMessage` is coming. LangChain provides several objects to easily distinguish between different roles: - `HumanMessage`: A `BaseMessage` coming from a human/user. - `AIMessage`: A `BaseMessage` coming from an AI/assistant. - `SystemMessage`: A `BaseMessage` coming from the system. - `FunctionMessage` / `ToolMessage`: A `BaseMessage` containing the output of a function or tool call. If none of those roles sound right, there is also a `ChatMessage` class where you can specify the role manually. LangChain provides a common interface that\'s shared by both `LLM`s and `ChatModel`s. However it\'s useful to understand the difference in order to most effectively construct prompts for a given language model. The simplest way to call an `LLM` or `ChatModel` is using `.invoke()`, the universal synchronous call method for all LangChain Expression Language (LCEL) objects: - `LLM.invoke`: Takes in a string, returns a string. - `ChatModel.invoke`: Takes in a list of `BaseMessage`, returns a `BaseMessage`. The input types for these methods are actually more general than this, but for simplicity here we can assume LLMs only take strings and Chat models only takes lists of messages. Check out the ""Go deeper"" section below to learn more about model invocation. Let\'s see how to work with these different types of models and these different types of inputs. First, let\'s import an LLM and a ChatModel. ```python from langchain.llms import OpenAI from langchain.chat_models import ChatOpenAI llm = OpenAI() chat_model = ChatOpenAI() ``` `LLM` and `ChatModel` objects are effectively configuration objects. You can initialize them with parameters like `temperature` and others, and pass them around. ```python from langchain.schema import HumanMessage text = ""What would be a good company name for a company that makes colorful socks?"" messages = [HumanMessage(content=text)] llm.invoke(text) # >> Feetful of Fun chat_model.invoke(messages) # >> AIMessage(content=""Socks O\'Color"") ``` Go deeper `LLM.invoke` and `ChatModel.invoke` actually both support as input any of `Union[str, List[BaseMessage], PromptValue]`. `PromptValue` is an object that defines it\'s own custom logic for returning it\'s inputs either as a string or as messages. `LLM`s have logic for coercing any of these into a string, and `ChatModel`s have logic for coercing any of these to messages. The fact that `LLM` and `ChatModel` accept the same inputs means that you can directly swap them for one another in most chains without breaking anything, though it\'s of course important to think about how inputs are being coerced and how that may affect model performance. To dive deeper on models head to the [Language models](/docs/modules/model_io/models) section. ### Prompt templates Most LLM applications do not pass user input directly into an LLM. Usually they will add the user input to a larger piece of text, called a prompt template, that provides additional context on the specific task at hand. In the previous example, the text we passed to the model contained instructions to generate a company name. For our application, it would be great if the user only had to provide the description of a company/product, without having to worry about giving the model instructions. PromptTemplates help with exactly this! They bundle up all the logic for going from user input into a fully formatted prompt. This can start off very simple - for example, a prompt to produce the above string would just be: ```python from langchain.prompts import PromptTemplate prompt = PromptTemplate.from_template(""What is a good name for a company that makes {product}?"") prompt.format(product=""colorful socks"") ``` ```python What is a good name for a company that makes colorful socks? ``` However, the advantages of using these over raw string formatting are several. You can ""partial"" out variables - e.g. you can format only some of the variables at a time. You can compose them together, easily combining different templates into a single prompt. For explanations of these functionalities, see the [section on prompts](/docs/modules/model_io/prompts) for more detail. `PromptTemplate`s can also be used to produce a list of messages. In this case, the prompt not only contains information about the content, but also each message (its role, its position in the list, etc.). Here, what happens most often is a `ChatPromptTemplate` is a list of `ChatMessageTemplates`. Each `ChatMessageTemplate` contains instructions for how to format that `ChatMessage` - its role, and then also its content. Let\'s take a look at this below: ```python from langchain.prompts.chat import ChatPromptTemplate template = ""You are a helpful assistant that translates {input_language} to {output_language}."" human_template = ""{text}"" chat_prompt = ChatPromptTemplate.from_messages([ (""system"", template), (""human"", human_template), ]) chat_prompt.format_messages(input_language=""English"", output_language=""French"", text=""I love programming."") ``` ```pycon [ SystemMessage(content=""You are a helpful assistant that translates English to French."", additional_kwargs={}), HumanMessage(content=""I love programming."") ] ``` ChatPromptTemplates can also be constructed in other ways - see the [section on prompts](/docs/modules/model_io/prompts) for more detail. ### Output parsers `OutputParsers` convert the raw output of a language model into a format that can be used downstream. There are few main types of `OutputParser`s, including: - Convert text from `LLM` into structured information (e.g. JSON) - Convert a `ChatMessage` into just a string - Convert the extra information returned from a call besides the message (like OpenAI function invocation) into a string. For full information on this, see the [section on output parsers](/docs/modules/model_io/output_parsers). In this getting started guide, we will write our own output parser - one that converts a comma separated list into a list. ```python from langchain.schema import BaseOutputParser class CommaSeparatedListOutputParser(BaseOutputParser): """"""Parse the output of an LLM call to a comma-separated list."""""" def parse(self, text: str): """"""Parse the output of an LLM call."""""" return text.strip().split("", "") CommaSeparatedListOutputParser().parse(""hi, bye"") # >> [\'hi\', \'bye\'] ``` ### Composing with LCEL We can now combine all these into one chain. This chain will take input variables, pass those to a prompt template to create a prompt, pass the prompt to a language model, and then pass the output through an (optional) output parser. This is a convenient way to bundle up a modular piece of logic. Let\'s see it in action! ```python from typing import List from langchain.chat_models import ChatOpenAI from langchain.prompts import ChatPromptTemplate from langchain.schema import BaseOutputParser class CommaSeparatedListOutputParser(BaseOutputParser[List[str]]): """"""Parse the output of an LLM call to a comma-separated list."""""" def parse(self, text: str) -> List[str]: """"""Parse the output of an LLM call."""""" return text.strip().split("", "") template = """"""You are a helpful assistant who generates comma separated lists. A user will pass in a category, and you should generate 5 objects in that category in a comma separated list. ONLY return a comma separated list, and nothing more."""""" human_template = ""{text}"" chat_prompt = ChatPromptTemplate.from_messages([ (""system"", template), (""human"", human_template), ]) chain = chat_prompt | ChatOpenAI() | CommaSeparatedListOutputParser() chain.invoke({""text"": ""colors""}) # >> [\'red\', \'blue\', \'green\', \'yellow\', \'orange\'] ``` Note that we are using the `|` syntax to join these components together. This `|` syntax is powered by the LangChain Expression Language (LCEL) and relies on the universal `Runnable` interface that all of these objects implement. To learn more about LCEL, read the documentation [here](/docs/expression_language). ## Tracing with LangSmith Assuming we\'ve set our environment variables as shown in the beginning, all of the model and chain calls we\'ve been making will have been automatically logged to LangSmith. Once there, we can use LangSmith to debug and annotate our application traces, then turn them into datasets for evaluating future iterations of the application. Check out what the trace for the above chain would look like: [ For more on LangSmith [head here](/docs/langsmith/). ## Serving with LangServe Now that we\'ve built an application, we need to serve it. That\'s where LangServe comes in. LangServe helps developers deploy LCEL chains as a REST API. The library is integrated with FastAPI and uses pydantic for data validation. ### Server To create a server for our application we\'ll make a `serve.py` file with three things: 1. The definition of our chain (same as above) 2. Our FastAPI app 3. A definition of a route from which to serve the chain, which is done with `langserve.add_routes` ```python #!/usr/bin/env python from typing import List from fastapi import FastAPI from langchain.prompts import ChatPromptTemplate from langchain.chat_models import ChatOpenAI from langchain.schema import BaseOutputParser from langserve import add_routes # 1. Chain definition class CommaSeparatedListOutputParser(BaseOutputParser[List[str]]): """"""Parse the output of an LLM call to a comma-separated list."""""" def parse(self, text: str) -> List[str]: """"""Parse the output of an LLM call."""""" return text.strip().split("", "") template = """"""You are a helpful assistant who generates comma separated lists. A user will pass in a category, and you should generate 5 objects in that category in a comma separated list. ONLY return a comma separated list, and nothing more."""""" human_template = ""{text}"" chat_prompt = ChatPromptTemplate.from_messages([ (""system"", template), (""human"", human_template), ]) category_chain = chat_prompt | ChatOpenAI() | CommaSeparatedListOutputParser() # 2. App definition app = FastAPI( title=""LangChain Server"", version=""1.0"", description=""A simple api server using Langchain\'s Runnable interfaces"", ) # 3. Adding chain route add_routes( app, category_chain, path=""/category_chain"", ) if __name__ == ""__main__"": import uvicorn uvicorn.run(app, host=""localhost"", port=8000) ``` And that\'s it! If we execute this file: ```bash python serve.py ``` we should see our chain being served at localhost:8000. ### Playground Every LangServe service comes with a simple built-in UI for configuring and invoking the application with streaming output and visibility into intermediate steps. Head to to try it out! ### Client Now let\'s set up a client for programmatically interacting with our service. We can easily do this with the `langserve.RemoteRunnable`. Using this, we can interact with the served chain as if it were running client-side. ```python from langserve import RemoteRunnable remote_chain = RemoteRunnable("" remote_chain.invoke({""text"": ""colors""}) # >> [\'red\', \'blue\', \'green\', \'yellow\', \'orange\'] ``` To learn more about the many other features of LangServe [head here](/docs/langserve). ## Next steps We\'ve touched on how to build an application with LangChain, how to trace it with LangSmith, and how to serve it with LangServe. There are a lot more features in all three of these than we can cover here. To continue on your journey: - Read up on [LangChain Expression Language (LCEL)](/docs/expression_language) to learn how to chain these components together - [Dive deeper](/docs/modules/model_io) into LLMs, prompts, and output parsers and learn the other [key components](/docs/modules) - Explore common [end-to-end use cases](/docs/use_cases/qa_structured/sql) and [template applications](/docs/templates) - [Read up on LangSmith](/docs/langsmith/), the platform for debugging, testing, monitoring and more - Learn more about serving your applications with [LangServe](/docs/langserve) - [Setup](#setup)- [Installation](#installation) - [Environment](#environment) - [LangSmith](#langsmith) - [LangServe](#langserve) - [Building with LangChain](#building-with-langchain)- [LLM / Chat Model](#llm--chat-model) - [Prompt templates](#prompt-templates) - [Output parsers](#output-parsers) - [Composing with LCEL](#composing-with-lcel) - [Tracing with LangSmith](#tracing-with-langsmith) - [Serving with LangServe](#serving-with-langserve)- [Server](#server) - [Playground](#playground) - [Client](#client) - [Next steps](#next-steps)', ""LangChain Expression Language (LCEL) | LangChain Expression Language (LCEL) LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together. LCEL was designed from day 1 to **support putting prototypes in production, with no code changes**, from the simplest prompt + LLM chain to the most complex chains (we've seen folks successfully run LCEL chains with 100s of steps in production). To highlight a few of the reasons you might want to use LCEL: **Streaming support** When you build your chains with LCEL you get the best possible time-to-first-token (time elapsed until the first chunk of output comes out). For some chains this means eg. we stream tokens straight from an LLM to a streaming output parser, and you get back parsed, incremental chunks of output at the same rate as the LLM provider outputs the raw tokens. **Async support** Any chain built with LCEL can be called both with the synchronous API (eg. in your Jupyter notebook while prototyping) as well as with the asynchronous API (eg. in a [LangServe](/docs/langsmith) server). This enables using the same code for prototypes and in production, with great performance, and the ability to handle many concurrent requests in the same server. **Optimized parallel execution** Whenever your LCEL chains have steps that can be executed in parallel (eg if you fetch documents from multiple retrievers) we automatically do it, both in the sync and the async interfaces, for the smallest possible latency. **Retries and fallbacks** Configure retries and fallbacks for any part of your LCEL chain. This is a great way to make your chains more reliable at scale. We're currently working on adding streaming support for retries/fallbacks, so you can get the added reliability without any latency cost. **Access intermediate results** For more complex chains it's often very useful to access the results of intermediate steps even before the final output is produced. This can be used let end-users know something is happening, or even just to debug your chain. You can stream intermediate results, and it's available on every [LangServe](/docs/langserve) server. **Input and output schemas** Input and output schemas give every LCEL chain Pydantic and JSONSchema schemas inferred from the structure of your chain. This can be used for validation of inputs and outputs, and is an integral part of LangServe. **Seamless LangSmith tracing integration** As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step. With LCEL, **all** steps are automatically logged to [LangSmith](/docs/langsmith/) for maximum observability and debuggability. **Seamless LangServe deployment integration** Any chain created with LCEL can be easily deployed using [LangServe](/docs/langserve).""]","LangChain Expression Language (LCEL) is a declarative way to compose chains together in LangChain. It supports putting prototypes in production without code changes, offering features like streaming support, async support, optimized parallel execution, retries and fallbacks, access to intermediate results, input and output schemas, and seamless integration with LangSmith for tracing and LangServe for deployment.","LangChain Expression Language (LCEL) is a declarative way to easily compose chains together. It provides several benefits for building applications:

- Async, Batch, and Streaming Support: LCEL chains have full sync, async, batch, and streaming support, making it easy to prototype in a Jupyter notebook and then expose it as an async streaming interface.
- Fallbacks: LCEL allows you to easily attach fallbacks to any chain, handling errors gracefully.
- Parallelism: LCEL syntax automatically runs components that can be run in parallel.
- Seamless LangSmith Tracing Integration: LCEL logs all steps to LangSmith for observability and debuggability. [0]

LCEL provides an intuitive and readable syntax for composition and supports streaming, async calls, batching, parallelization, retries, and more. It allows you to combine different components, such as prompts, models, and output parsers, to build powerful applications. LCEL chains can be called synchronously or asynchronously, and they support intermediate result access, input/output schemas, and tracing with LangSmith. [3]",0.9999999999666667,,,0.7577395672414198,0.2980769230769231
66,which document laaoder should i use for a loading a single web apage?,"['Redis | Redis Redis vector database introduction and langchain integration guide. ## What is Redis? Most developers from a web services background are probably familiar with Redis. At it\'s core, Redis is an open-source key-value store that can be used as a cache, message broker, and database. Developers choose Redis because it is fast, has a large ecosystem of client libraries, and has been deployed by major enterprises for years. On top of these traditional use cases, Redis provides additional capabilities like the Search and Query capability that allows users to create secondary index structures within Redis. This allows Redis to be a Vector Database, at the speed of a cache. ## Redis as a Vector Database Redis uses compressed, inverted indexes for fast indexing with a low memory footprint. It also supports a number of advanced features such as: - Indexing of multiple fields in Redis hashes and JSON - Vector similarity search (with HNSW (ANN) or FLAT (KNN)) - Vector Range Search (e.g. find all vectors within a radius of a query vector) - Incremental indexing without performance loss - Document ranking (using [tf-idf]( with optional user-provided weights) - Field weighting - Complex boolean queries with AND, OR, and NOT operators - Prefix matching, fuzzy matching, and exact-phrase queries - Support for [double-metaphone phonetic matching]( - Auto-complete suggestions (with fuzzy prefix suggestions) - Stemming-based query expansion in [many languages]( (using [Snowball]( - Support for Chinese-language tokenization and querying (using [Friso]( - Numeric filters and ranges - Geospatial searches using [Redis geospatial indexing](/commands/georadius) - A powerful aggregations engine - Supports for all utf-8 encoded text - Retrieve full documents, selected fields, or only the document IDs - Sorting results (for example, by creation date) ## Clients Since redis is much more than just a vector database, there are often use cases that demand usage of a Redis client besides just the langchain integration. You can use any standard Redis client library to run Search and Query commands, but it\'s easiest to use a library that wraps the Search and Query API. Below are a few examples, but you can find more client libraries [here]( | Project | Language | License | Author | Stars | | ---- | ---- | ---- | ---- | ---- | | jedis | Java | MIT | Redis | | | redisvl | Python | MIT | Redis | | | redis-py | Python | MIT | Redis | | | node-redis | Node.js | MIT | Redis | | | nredisstack | .NET | MIT | Redis | | ## Deployment Options There are many ways to deploy Redis with RediSearch. The easiest way to get started is to use Docker, but there are are many potential options for deployment such as - [Redis Cloud]( - [Docker (Redis Stack)]( - Cloud marketplaces: [AWS Marketplace]( [Google Marketplace]( or [Azure Marketplace]( - On-premise: [Redis Enterprise Software]( - Kubernetes: [Redis Enterprise Software on Kubernetes]( ## Examples Many examples can be found in the [Redis AI team\'s GitHub]( - [Awesome Redis AI Resources]( - List of examples of using Redis in AI workloads - [Azure OpenAI Embeddings Q&A]( - OpenAI and Redis as a Q&A service on Azure. - [ArXiv Paper Search]( - Semantic search over arXiv scholarly papers - [Vector Search on Azure]( - Vector search on Azure using Azure Cache for Redis and Azure OpenAI ## More Resources For more information on how to use Redis as a vector database, check out the following resources: - [RedisVL Documentation]( - Documentation for the Redis Vector Library Client - [Redis Vector Similarity Docs]( - Redis official docs for Vector Search. - [Redis-py Search Docs]( - Documentation for redis-py client library - [Vector Similarity Search: From Basics to Production]( - Introductory blog post to VSS and Redis as a VectorDB. ## Install Redis Python Client Redis-py is the officially supported client by Redis. Recently released is the RedisVL client which is purpose-built for the Vector Database use cases. Both can be installed with pip. ```bash pip install redis redisvl openai tiktoken ``` We want to use `OpenAIEmbeddings` so we have to get the OpenAI API Key. ```python import getpass import os os.environ[""OPENAI_API_KEY""] = getpass.getpass(""OpenAI API Key:"") ``` ```python from langchain.embeddings import OpenAIEmbeddings embeddings = OpenAIEmbeddings() ``` ## Sample Data First we will describe some sample data so that the various attributes of the Redis vector store can be demonstrated. ```python metadata = [ { ""user"": ""john"", ""age"": 18, ""job"": ""engineer"", ""credit_score"": ""high"", }, { ""user"": ""derrick"", ""age"": 45, ""job"": ""doctor"", ""credit_score"": ""low"", }, { ""user"": ""nancy"", ""age"": 94, ""job"": ""doctor"", ""credit_score"": ""high"", }, { ""user"": ""tyler"", ""age"": 100, ""job"": ""engineer"", ""credit_score"": ""high"", }, { ""user"": ""joe"", ""age"": 35, ""job"": ""dentist"", ""credit_score"": ""medium"", }, ] texts = [""foo"", ""foo"", ""foo"", ""bar"", ""bar""] ``` ## Initializing Redis To locally deploy Redis, run: ```console docker run -d -p 6379:6379 -p 8001:8001 redis/redis-stack:latest ``` If things are running correctly you should see a nice Redis UI at See the [Deployment Options](#deployment-options) section above for other ways to deploy. The Redis VectorStore instance can be initialized in a number of ways. There are multiple class methods that can be used to initialize a Redis VectorStore instance. - `Redis.__init__` - Initialize directly - `Redis.from_documents` - Initialize from a list of `Langchain.docstore.Document` objects - `Redis.from_texts` - Initialize from a list of texts (optionally with metadata) - `Redis.from_texts_return_keys` - Initialize from a list of texts (optionally with metadata) and return the keys - `Redis.from_existing_index` - Initialize from an existing Redis index Below we will use the `Redis.from_texts` method. ```python from langchain.vectorstores.redis import Redis rds = Redis.from_texts( texts, embeddings, metadatas=metadata, redis_url=""redis://localhost:6379"", index_name=""users"", ) ``` ```python rds.index_name ``` ```text \'users\' ``` ## Inspecting the Created Index Once the `Redis` VectorStore object has been constructed, an index will have been created in Redis if it did not already exist. The index can be inspected with both the `rvl`and the `redis-cli` command line tool. If you installed `redisvl` above, you can use the `rvl` command line tool to inspect the index. ```bash # assumes you\'re running Redis locally (use --host, --port, --password, --username, to change this) rvl index listall ``` ```text 16:58:26 [RedisVL] INFO Indices: 16:58:26 [RedisVL] INFO 1. users ``` The `Redis` VectorStore implementation will attempt to generate index schema (fields for filtering) for any metadata passed through the `from_texts`, `from_texts_return_keys`, and `from_documents` methods. This way, whatever metadata is passed will be indexed into the Redis search index allowing for filtering on those fields. Below we show what fields were created from the metadata we defined above ```bash rvl index info -i users ``` ```text Index Information: Index Name Storage Type Prefixes Index Options Indexing users HASH [\'doc:users\'] [] 0 Index Fields: Name Attribute Type Field Option Option Value user user TEXT WEIGHT 1 job job TEXT WEIGHT 1 credit_score credit_score TEXT WEIGHT 1 content content TEXT WEIGHT 1 age age NUMERIC content_vector content_vector VECTOR ``` ```bash rvl stats -i users ``` ```text Statistics: Stat Key Value num_docs 5 num_terms 15 max_doc_id 5 num_records 33 percent_indexed 1 hash_indexing_failures 0 number_of_uses 4 bytes_per_record_avg 4.60606 doc_table_size_mb 0.000524521 inverted_sz_mb 0.000144958 key_table_size_mb 0.000193596 offset_bits_per_record_avg 8 offset_vectors_sz_mb 2.19345e-05 offsets_per_term_avg 0.69697 records_per_doc_avg 6.6 sortable_values_size_mb 0 total_indexing_time 0.32 total_inverted_index_blocks 16 vector_index_sz_mb 6.0126 ``` It\'s important to note that we have not specified that the `user`, `job`, `credit_score` and `age` in the metadata should be fields within the index, this is because the `Redis` VectorStore object automatically generate the index schema from the passed metadata. For more information on the generation of index fields, see the API documentation. ## Querying\u200b There are multiple ways to query the `Redis` VectorStore implementation based on what use case you have: - `similarity_search`: Find the most similar vectors to a given vector. - `similarity_search_with_score`: Find the most similar vectors to a given vector and return the vector distance - `similarity_search_limit_score`: Find the most similar vectors to a given vector and limit the number of results to the `score_threshold` - `similarity_search_with_relevance_scores`: Find the most similar vectors to a given vector and return the vector similarities - `max_marginal_relevance_search`: Find the most similar vectors to a given vector while also optimizing for diversity ```python results = rds.similarity_search(""foo"") print(results[0].page_content) ``` ```text foo ``` ```python # return metadata results = rds.similarity_search(""foo"", k=3) meta = results[1].metadata print(""Key of the document in Redis: "", meta.pop(""id"")) print(""Metadata of the document: "", meta) ``` ```text Key of the document in Redis: doc:users:a70ca43b3a4e4168bae57c78753a200f Metadata of the document: {\'user\': \'derrick\', \'job\': \'doctor\', \'credit_score\': \'low\', \'age\': \'45\'} ``` ```python # with scores (distances) results = rds.similarity_search_with_score(""foo"", k=5) for result in results: print(f""Content: {result[0].page_content} --- Score: {result[1]}"") ``` ```text Content: foo --- Score: 0.0 Content: foo --- Score: 0.0 Content: foo --- Score: 0.0 Content: bar --- Score: 0.1566 Content: bar --- Score: 0.1566 ``` ```python # limit the vector distance that can be returned results = rds.similarity_search_with_score(""foo"", k=5, distance_threshold=0.1) for result in results: print(f""Content: {result[0].page_content} --- Score: {result[1]}"") ``` ```text Content: foo --- Score: 0.0 Content: foo --- Score: 0.0 Content: foo --- Score: 0.0 ``` ```python # with scores results = rds.similarity_search_with_relevance_scores(""foo"", k=5) for result in results: print(f""Content: {result[0].page_content} --- Similiarity: {result[1]}"") ``` ```text Content: foo --- Similiarity: 1.0 Content: foo --- Similiarity: 1.0 Content: foo --- Similiarity: 1.0 Content: bar --- Similiarity: 0.8434 Content: bar --- Similiarity: 0.8434 ``` ```python # limit scores (similarities have to be over .9) results = rds.similarity_search_with_relevance_scores(""foo"", k=5, score_threshold=0.9) for result in results: print(f""Content: {result[0].page_content} --- Similarity: {result[1]}"") ``` ```text Content: foo --- Similarity: 1.0 Content: foo --- Similarity: 1.0 Content: foo --- Similarity: 1.0 ``` ```python # you can also add new documents as follows new_document = [""baz""] new_metadata = [{""user"": ""sam"", ""age"": 50, ""job"": ""janitor"", ""credit_score"": ""high""}] # both the document and metadata must be lists rds.add_texts(new_document, new_metadata) ``` ```text [\'doc:users:b9c71d62a0a34241a37950b448dafd38\'] ``` ```python # now query the new document results = rds.similarity_search(""baz"", k=3) print(results[0].metadata) ``` ```text {\'id\': \'doc:users:b9c71d62a0a34241a37950b448dafd38\', \'user\': \'sam\', \'job\': \'janitor\', \'credit_score\': \'high\', \'age\': \'50\'} ``` ```python # use maximal marginal relevance search to diversify results results = rds.max_marginal_relevance_search(""foo"") ``` ```python # the lambda_mult parameter controls the diversity of the results, the lower the more diverse results = rds.max_marginal_relevance_search(""foo"", lambda_mult=0.1) ``` ## Connect to an Existing Index\u200b In order to have the same metadata indexed when using the `Redis` VectorStore. You will need to have the same `index_schema` passed in either as a path to a yaml file or as a dictionary. The following shows how to obtain the schema from an index and connect to an existing index. ```python # write the schema to a yaml file rds.write_schema(""redis_schema.yaml"") ``` The schema file for this example should look something like: ```yaml numeric: - name: age no_index: false sortable: false text: - name: user no_index: false no_stem: false sortable: false weight: 1 withsuffixtrie: false - name: job no_index: false no_stem: false sortable: false weight: 1 withsuffixtrie: false - name: credit_score no_index: false no_stem: false sortable: false weight: 1 withsuffixtrie: false - name: content no_index: false no_stem: false sortable: false weight: 1 withsuffixtrie: false vector: - algorithm: FLAT block_size: 1000 datatype: FLOAT32 dims: 1536 distance_metric: COSINE initial_cap: 20000 name: content_vector ``` **Notice**, this include **all** possible fields for the schema. You can remove any fields that you don\'t need. ```python # now we can connect to our existing index as follows new_rds = Redis.from_existing_index( embeddings, index_name=""users"", redis_url=""redis://localhost:6379"", schema=""redis_schema.yaml"", ) results = new_rds.similarity_search(""foo"", k=3) print(results[0].metadata) ``` ```text {\'id\': \'doc:users:8484c48a032d4c4cbe3cc2ed6845fabb\', \'user\': \'john\', \'job\': \'engineer\', \'credit_score\': \'high\', \'age\': \'18\'} ``` ```python # see the schemas are the same new_rds.schema == rds.schema ``` ```text True ``` ## Custom Metadata Indexing\u200b In some cases, you may want to control what fields the metadata maps to. For example, you may want the `credit_score` field to be a categorical field instead of a text field (which is the default behavior for all string fields). In this case, you can use the `index_schema` parameter in each of the initialization methods above to specify the schema for the index. Custom index schema can either be passed as a dictionary or as a path to a yaml file. All arguments in the schema have defaults besides the name, so you can specify only the fields you want to change. All the names correspond to the snake/lowercase versions of the arguments you would use on the command line with `redis-cli` or in `redis-py`. For more on the arguments for each field, see the [documentation]( The below example shows how to specify the schema for the `credit_score` field as a Tag (categorical) field instead of a text field. ```yaml # index_schema.yml tag: - name: credit_score text: - name: user - name: job numeric: - name: age ``` In Python this would look like: ```python index_schema = { ""tag"": [{""name"": ""credit_score""}], ""text"": [{""name"": ""user""}, {""name"": ""job""}], ""numeric"": [{""name"": ""age""}], } ``` Notice that only the `name` field needs to be specified. All other fields have defaults. ```python # create a new index with the new schema defined above index_schema = { ""tag"": [{""name"": ""credit_score""}], ""text"": [{""name"": ""user""}, {""name"": ""job""}], ""numeric"": [{""name"": ""age""}], } rds, keys = Redis.from_texts_return_keys( texts, embeddings, metadatas=metadata, redis_url=""redis://localhost:6379"", index_name=""users_modified"", index_schema=index_schema, # pass in the new index schema ) ``` ```text `index_schema` does not match generated metadata schema. If you meant to manually override the schema, please ignore this message. index_schema: {\'tag\': [{\'name\': \'credit_score\'}], \'text\': [{\'name\': \'user\'}, {\'name\': \'job\'}], \'numeric\': [{\'name\': \'age\'}]} generated_schema: {\'text\': [{\'name\': \'user\'}, {\'name\': \'job\'}, {\'name\': \'credit_score\'}], \'numeric\': [{\'name\': \'age\'}], \'tag\': []} ``` The above warning is meant to notify users when they are overriding the default behavior. Ignore it if you are intentionally overriding the behavior. ## Hybrid Filtering\u200b With the Redis Filter Expression language built into langchain, you can create arbitrarily long chains of hybrid filters that can be used to filter your search results. The expression language is derived from the [RedisVL Expression Syntax]( and is designed to be easy to use and understand. The following are the available filter types: - `RedisText`: Filter by full-text search against metadata fields. Supports exact, fuzzy, and wildcard matching. - `RedisNum`: Filter by numeric range against metadata fields. - `RedisTag`: Filter by exact match against string based categorical metadata fields. Multiple tags can be specified like ""tag1,tag2,tag3"". The following are examples of utilizing these filters. ```python from langchain.vectorstores.redis import RedisText, RedisNum, RedisTag # exact matching has_high_credit = RedisTag(""credit_score"") == ""high"" does_not_have_high_credit = RedisTag(""credit_score"") != ""low"" # fuzzy matching job_starts_with_eng = RedisText(""job"") % ""eng*"" job_is_engineer = RedisText(""job"") == ""engineer"" job_is_not_engineer = RedisText(""job"") != ""engineer"" # numeric filtering age_is_18 = RedisNum(""age"") == 18 age_is_not_18 = RedisNum(""age"") != 18 age_is_greater_than_18 = RedisNum(""age"") > 18 age_is_less_than_18 = RedisNum(""age"") < 18 age_is_greater_than_or_equal_to_18 = RedisNum(""age"") >= 18 age_is_less_than_or_equal_to_18 = RedisNum(""age"") <= 18 ``` The `RedisFilter` class can be used to simplify the import of these filters as follows ```python from langchain.vectorstores.redis import RedisFilter # same examples as above has_high_credit = RedisFilter.tag(""credit_score"") == ""high"" does_not_have_high_credit = RedisFilter.num(""age"") > 8 job_starts_with_eng = RedisFilter.text(""job"") % ""eng*"" ``` The following are examples of using hybrid filter for search ```python from langchain.vectorstores.redis import RedisText is_engineer = RedisText(""job"") == ""engineer"" results = rds.similarity_search(""foo"", k=3, filter=is_engineer) print(""Job:"", results[0].metadata[""job""]) print(""Engineers in the dataset:"", len(results)) ``` ```text Job: engineer Engineers in the dataset: 2 ``` ```python # fuzzy match starts_with_doc = RedisText(""job"") % ""doc*"" results = rds.similarity_search(""foo"", k=3, filter=starts_with_doc) for result in results: print(""Job:"", result.metadata[""job""]) print(""Jobs in dataset that start with \'doc\':"", len(results)) ``` ```text Job: doctor Job: doctor Jobs in dataset that start with \'doc\': 2 ``` ```python from langchain.vectorstores.redis import RedisNum is_over_18 = RedisNum(""age"") > 18 is_under_99 = RedisNum(""age"") < 99 age_range = is_over_18 & is_under_99 results = rds.similarity_search(""foo"", filter=age_range) for result in results: print(""User:"", result.metadata[""user""], ""is"", result.metadata[""age""]) ``` ```text User: derrick is 45 User: nancy is 94 User: joe is 35 ``` ```python # make sure to use parenthesis around FilterExpressions # if initializing them while constructing them age_range = (RedisNum(""age"") > 18) & (RedisNum(""age"") < 99) results = rds.similarity_search(""foo"", filter=age_range) for result in results: print(""User:"", result.metadata[""user""], ""is"", result.metadata[""age""]) ``` ```text User: derrick is 45 User: nancy is 94 User: joe is 35 ``` ## Redis as Retriever\u200b Here we go over different options for using the vector store as a retriever. There are three different search methods we can use to do retrieval. By default, it will use semantic similarity. ```python query = ""foo"" results = rds.similarity_search_with_score(query, k=3, return_metadata=True) for result in results: print(""Content:"", result[0].page_content, "" --- Score: "", result[1]) ``` ```text Content: foo --- Score: 0.0 Content: foo --- Score: 0.0 Content: foo --- Score: 0.0 ``` ```python retriever = rds.as_retriever(search_type=""similarity"", search_kwargs={""k"": 4}) ``` ```python docs = retriever.get_relevant_documents(query) docs ``` ```text [Document(page_content=\'foo\', metadata={\'id\': \'doc:users_modified:988ecca7574048e396756efc0e79aeca\', \'user\': \'john\', \'job\': \'engineer\', \'credit_score\': \'high\', \'age\': \'18\'}), Document(page_content=\'foo\', metadata={\'id\': \'doc:users_modified:009b1afeb4084cc6bdef858c7a99b48e\', \'user\': \'derrick\', \'job\': \'doctor\', \'credit_score\': \'low\', \'age\': \'45\'}), Document(page_content=\'foo\', metadata={\'id\': \'doc:users_modified:7087cee9be5b4eca93c30fbdd09a2731\', \'user\': \'nancy\', \'job\': \'doctor\', \'credit_score\': \'high\', \'age\': \'94\'}), Document(page_content=\'bar\', metadata={\'id\': \'doc:users_modified:01ef6caac12b42c28ad870aefe574253\', \'user\': \'tyler\', \'job\': \'engineer\', \'credit_score\': \'high\', \'age\': \'100\'})] ``` There is also the `similarity_distance_threshold` retriever which allows the user to specify the vector distance ```python retriever = rds.as_retriever( search_type=""similarity_distance_threshold"", search_kwargs={""k"": 4, ""distance_threshold"": 0.1}, ) ``` ```python docs = retriever.get_relevant_documents(query) docs ``` ```text [Document(page_content=\'foo\', metadata={\'id\': \'doc:users_modified:988ecca7574048e396756efc0e79aeca\', \'user\': \'john\', \'job\': \'engineer\', \'credit_score\': \'high\', \'age\': \'18\'}), Document(page_content=\'foo\', metadata={\'id\': \'doc:users_modified:009b1afeb4084cc6bdef858c7a99b48e\', \'user\': \'derrick\', \'job\': \'doctor\', \'credit_score\': \'low\', \'age\': \'45\'}), Document(page_content=\'foo\', metadata={\'id\': \'doc:users_modified:7087cee9be5b4eca93c30fbdd09a2731\', \'user\': \'nancy\', \'job\': \'doctor\', \'credit_score\': \'high\', \'age\': \'94\'})] ``` Lastly, the `similarity_score_threshold` allows the user to define the minimum score for similar documents ```python retriever = rds.as_retriever( search_type=""similarity_score_threshold"", search_kwargs={""score_threshold"": 0.9, ""k"": 10}, ) ``` ```python retriever.get_relevant_documents(""foo"") ``` ```text [Document(page_content=\'foo\', metadata={\'id\': \'doc:users_modified:988ecca7574048e396756efc0e79aeca\', \'user\': \'john\', \'job\': \'engineer\', \'credit_score\': \'high\', \'age\': \'18\'}), Document(page_content=\'foo\', metadata={\'id\': \'doc:users_modified:009b1afeb4084cc6bdef858c7a99b48e\', \'user\': \'derrick\', \'job\': \'doctor\', \'credit_score\': \'low\', \'age\': \'45\'}), Document(page_content=\'foo\', metadata={\'id\': \'doc:users_modified:7087cee9be5b4eca93c30fbdd09a2731\', \'user\': \'nancy\', \'job\': \'doctor\', \'credit_score\': \'high\', \'age\': \'94\'})] ``` ```python retriever = rds.as_retriever( search_type=""mmr"", search_kwargs={""fetch_k"": 20, ""k"": 4, ""lambda_mult"": 0.1} ) ``` ```python retriever.get_relevant_documents(""foo"") ``` ```text [Document(page_content=\'foo\', metadata={\'id\': \'doc:users:8f6b673b390647809d510112cde01a27\', \'user\': \'john\', \'job\': \'engineer\', \'credit_score\': \'high\', \'age\': \'18\'}), Document(page_content=\'bar\', metadata={\'id\': \'doc:users:93521560735d42328b48c9c6f6418d6a\', \'user\': \'tyler\', \'job\': \'engineer\', \'credit_score\': \'high\', \'age\': \'100\'}), Document(page_content=\'foo\', metadata={\'id\': \'doc:users:125ecd39d07845eabf1a699d44134a5b\', \'user\': \'nancy\', \'job\': \'doctor\', \'credit_score\': \'high\', \'age\': \'94\'}), Document(page_content=\'foo\', metadata={\'id\': \'doc:users:d6200ab3764c466082fde3eaab972a2a\', \'user\': \'derrick\', \'job\': \'doctor\', \'credit_score\': \'low\', \'age\': \'45\'})] ``` # Delete keys To delete your entries you have to address them by their keys. ```python Redis.delete(keys, redis_url=""redis://localhost:6379"") ``` ```text True ``` ```python # delete the indices too Redis.drop_index( index_name=""users"", delete_documents=True, redis_url=""redis://localhost:6379"" ) Redis.drop_index( index_name=""users_modified"", delete_documents=True, redis_url=""redis://localhost:6379"", ) ``` ```text True ``` ### Redis connection Url examples\u200b Valid Redis Url scheme are: 1. `redis://` - Connection to Redis standalone, unencrypted 2. `rediss://` - Connection to Redis standalone, with TLS encryption 3. `redis+sentinel://` - Connection to Redis server via Redis Sentinel, unencrypted 4. `rediss+sentinel://` - Connection to Redis server via Redis Sentinel, booth connections with TLS encryption More information about additional connection parameter can be found in the redis-py documentation at [ ```python # connection to redis standalone at localhost, db 0, no password redis_url = ""redis://localhost:6379"" # connection to host ""redis"" port 7379 with db 2 and password ""secret"" (old style authentication scheme without username / pre 6.x) redis_url = ""redis://:secret@redis:7379/2"" # connection to host redis on default port with user ""joe"", pass ""secret"" using redis version 6+ ACLs redis_url = ""redis://joe:secret@redis/0"" # connection to sentinel at localhost with default group mymaster and db 0, no password redis_url = ""redis+sentinel://localhost:26379"" # connection to sentinel at host redis with default port 26379 and user ""joe"" with password ""secret"" with default group mymaster and db 0 redis_url = ""redis+sentinel://joe:secret@redis"" # connection to sentinel, no auth with sentinel monitoring group ""zone-1"" and database 2 redis_url = ""redis+sentinel://redis:26379/zone-1/2"" # connection to redis standalone at localhost, db 0, no password but with TLS support redis_url = ""rediss://localhost:6379"" # connection to redis sentinel at localhost and default port, db 0, no password # but with TLS support for booth Sentinel and Redis server redis_url = ""rediss+sentinel://localhost"" ``` - [What is Redis?](#what-is-redis) - [Redis as a Vector Database](#redis-as-a-vector-database) - [Clients](#clients) - [Deployment Options](#deployment-options) - [Examples](#examples) - [More Resources](#more-resources) - [Install Redis Python Client](#install-redis-python-client) - [Sample Data](#sample-data) - [Initializing Redis](#initializing-redis) - [Inspecting the Created Index](#inspecting-the-created-index) - [Querying](#querying) - [Connect to an Existing Index](#connect-to-an-existing-index) - [Custom Metadata Indexing](#custom-metadata-indexing) - [Hybrid Filtering](#hybrid-filtering) - [Redis as Retriever](#redis-as-retriever)- [Redis connection Url examples](#redis-connection-url-examples)', 'Summarization | Summarization []( ## Use case Suppose you have a set of documents (PDFs, Notion pages, customer questions, etc.) and you want to summarize the content. LLMs are a great tool for this given their proficiency in understanding and synthesizing text. In this walkthrough we\'ll go over how to perform document summarization using LLMs. ![Image description](/assets/images/summarization_use_case_1-cdb1b94b53af261bd997a9934a8c3703.png) ## Overview A central question for building a summarizer is how to pass your documents into the LLM\'s context window. Two common approaches for this are: 1. `Stuff`: Simply ""stuff"" all your documents into a single prompt. This is the simplest approach (see [here](/docs/modules/chains/document/stuff) for more on the `StuffDocumentsChains`, which is used for this method). 2. `Map-reduce`: Summarize each document on it\'s own in a ""map"" step and then ""reduce"" the summaries into a final summary (see [here](/docs/modules/chains/document/map_reduce) for more on the `MapReduceDocumentsChain`, which is used for this method). ![Image description](/assets/images/summarization_use_case_2-f2a4d5d60980a79140085fb7f8043217.png) ## Quickstart To give you a sneak preview, either pipeline can be wrapped in a single object: `load_summarize_chain`. Suppose we want to summarize a blog post. We can create this in a few lines of code. First set environment variables and install packages: ```bash pip install openai tiktoken chromadb langchain # Set env var OPENAI_API_KEY or load from a .env file # import dotenv # dotenv.load_dotenv() ``` ```text Requirement already satisfied: openai in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (0.27.8) Requirement already satisfied: tiktoken in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (0.4.0) Requirement already satisfied: chromadb in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (0.4.4) Requirement already satisfied: langchain in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (0.0.299) Requirement already satisfied: requests>=2.20 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from openai) (2.31.0) Requirement already satisfied: tqdm in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from openai) (4.64.1) Requirement already satisfied: aiohttp in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from openai) (3.8.5) Requirement already satisfied: regex>=2022.1.18 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from tiktoken) (2023.6.3) Requirement already satisfied: pydantic=1.9 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (1.10.12) Requirement already satisfied: chroma-hnswlib==0.7.2 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (0.7.2) Requirement already satisfied: fastapi=0.95.2 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (0.99.1) Requirement already satisfied: uvicorn[standard]>=0.18.3 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (0.23.2) Requirement already satisfied: numpy>=1.21.6 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (1.24.4) Requirement already satisfied: posthog>=2.4.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (3.0.1) Requirement already satisfied: typing-extensions>=4.5.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (4.7.1) Requirement already satisfied: pulsar-client>=3.1.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (3.2.0) Requirement already satisfied: onnxruntime>=1.14.1 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (1.15.1) Requirement already satisfied: tokenizers>=0.13.2 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (0.13.3) Requirement already satisfied: pypika>=0.48.9 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (0.48.9) Collecting tqdm (from openai) Obtaining dependency information for tqdm from Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB) 57.6/57.6 kB 2.7 MB/s eta 0:00:00 Requirement already satisfied: overrides>=7.3.1 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (7.4.0) Requirement already satisfied: importlib-resources in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (6.0.0) Requirement already satisfied: PyYAML>=5.3 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (6.0.1) Requirement already satisfied: SQLAlchemy=1.4 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (2.0.20) Requirement already satisfied: anyio<4.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (3.7.1) Requirement already satisfied: async-timeout=4.0.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (4.0.3) Requirement already satisfied: dataclasses-json=0.5.7 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (0.5.9) Requirement already satisfied: jsonpatch=1.33 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (1.33) Requirement already satisfied: langsmith=0.0.38 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (0.0.42) Requirement already satisfied: numexpr=2.8.4 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (2.8.5) Requirement already satisfied: tenacity=8.1.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (8.2.3) Requirement already satisfied: attrs>=17.3.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from aiohttp->openai) (23.1.0) Requirement already satisfied: charset-normalizer=2.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from aiohttp->openai) (3.2.0) Requirement already satisfied: multidict=4.5 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from aiohttp->openai) (6.0.4) Requirement already satisfied: yarl=1.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from aiohttp->openai) (1.9.2) Requirement already satisfied: frozenlist>=1.1.1 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from aiohttp->openai) (1.4.0) Requirement already satisfied: aiosignal>=1.1.2 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from aiohttp->openai) (1.3.1) Requirement already satisfied: idna>=2.8 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from anyiolangchain) (3.4) Requirement already satisfied: sniffio>=1.1 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from anyiolangchain) (1.3.0) Requirement already satisfied: exceptiongroup in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from anyiolangchain) (1.1.3) Requirement already satisfied: marshmallow=3.3.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from dataclasses-json=0.5.7->langchain) (3.20.1) Requirement already satisfied: marshmallow-enum=1.5.1 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from dataclasses-json=0.5.7->langchain) (1.5.1) Requirement already satisfied: typing-inspect>=0.4.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from dataclasses-json=0.5.7->langchain) (0.9.0) Requirement already satisfied: starlette=0.27.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from fastapi=0.95.2->chromadb) (0.27.0) Requirement already satisfied: jsonpointer>=1.9 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from jsonpatch=1.33->langchain) (2.4) Requirement already satisfied: coloredlogs in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1) Requirement already satisfied: flatbuffers in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26) Requirement already satisfied: packaging in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb) (23.1) Requirement already satisfied: protobuf in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb) (4.23.4) Requirement already satisfied: sympy in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12) Requirement already satisfied: six>=1.5 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb) (1.16.0) Requirement already satisfied: monotonic>=1.5 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb) (1.6) Requirement already satisfied: backoff>=1.10.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb) (2.2.1) Requirement already satisfied: python-dateutil>2.1 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb) (2.8.2) Requirement already satisfied: certifi in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from pulsar-client>=3.1.0->chromadb) (2023.7.22) Requirement already satisfied: urllib3=1.21.1 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from requests>=2.20->openai) (1.26.16) Requirement already satisfied: click>=7.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (8.1.7) Requirement already satisfied: h11>=0.8 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0) Requirement already satisfied: httptools>=0.5.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.0) Requirement already satisfied: python-dotenv>=0.13 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0) Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.17.0) Requirement already satisfied: watchfiles>=0.13 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0) Requirement already satisfied: websockets>=10.4 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (11.0.3) Requirement already satisfied: zipp>=3.1.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from importlib-resources->chromadb) (3.16.2) Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from typing-inspect>=0.4.0->dataclasses-json=0.5.7->langchain) (1.0.0) Requirement already satisfied: humanfriendly>=9.1 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0) Requirement already satisfied: mpmath>=0.19 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0) Using cached tqdm-4.66.1-py3-none-any.whl (78 kB) Installing collected packages: tqdm Attempting uninstall: tqdm Found existing installation: tqdm 4.64.1 Uninstalling tqdm-4.64.1: Successfully uninstalled tqdm-4.64.1 ERROR: pip\'s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. clarifai 9.8.1 requires tqdm==4.64.1, but you have tqdm 4.66.1 which is incompatible. Successfully installed tqdm-4.66.1 ``` We can use `chain_type=""stuff""`, especially if using larger context window models such as: - 16k token OpenAI `gpt-3.5-turbo-16k` - 100k token Anthropic [Claude-2]( We can also supply `chain_type=""map_reduce""` or `chain_type=""refine""` (read more [here](/docs/modules/chains/document/refine)). ```python from langchain.chains.summarize import load_summarize_chain from langchain.chat_models import ChatOpenAI from langchain.document_loaders import WebBaseLoader loader = WebBaseLoader("" docs = loader.load() llm = ChatOpenAI(temperature=0, model_name=""gpt-3.5-turbo-16k"") chain = load_summarize_chain(llm, chain_type=""stuff"") chain.run(docs) ``` ```text \'The article discusses the concept of building autonomous agents powered by large language models (LLMs). It explores the components of such agents, including planning, memory, and tool use. The article provides case studies and proof-of-concept examples of LLM-powered agents in various domains. It also highlights the challenges and limitations of using LLMs in agent systems.\' ``` ## Option 1. Stuff\u200b When we use `load_summarize_chain` with `chain_type=""stuff""`, we will use the [StuffDocumentsChain](/docs/modules/chains/document/stuff). The chain will take a list of documents, inserts them all into a prompt, and passes that prompt to an LLM: ```python from langchain.chains.combine_documents.stuff import StuffDocumentsChain from langchain.chains.llm import LLMChain from langchain.prompts import PromptTemplate # Define prompt prompt_template = """"""Write a concise summary of the following: ""{text}"" CONCISE SUMMARY:"""""" prompt = PromptTemplate.from_template(prompt_template) # Define LLM chain llm = ChatOpenAI(temperature=0, model_name=""gpt-3.5-turbo-16k"") llm_chain = LLMChain(llm=llm, prompt=prompt) # Define StuffDocumentsChain stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=""text"") docs = loader.load() print(stuff_chain.run(docs)) ``` ```text The article discusses the concept of building autonomous agents powered by large language models (LLMs). It explores the components of such agents, including planning, memory, and tool use. The article provides case studies and proof-of-concept examples of LLM-powered agents in various domains, such as scientific discovery and generative agents simulation. It also highlights the challenges and limitations of using LLMs in agent systems. ``` Great! We can see that we reproduce the earlier result using the `load_summarize_chain`. ### Go deeper\u200b - You can easily customize the prompt. - You can easily try different LLMs, (e.g., [Claude](/docs/integrations/chat/anthropic)) via the `llm` parameter. ## Option 2. Map-Reduce\u200b Let\'s unpack the map reduce approach. For this, we\'ll first map each document to an individual summary using an `LLMChain`. Then we\'ll use a `ReduceDocumentsChain` to combine those summaries into a single global summary. First, we specify the LLMChain to use for mapping each document to an individual summary: ```python from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain from langchain.text_splitter import CharacterTextSplitter llm = ChatOpenAI(temperature=0) # Map map_template = """"""The following is a set of documents {docs} Based on this list of docs, please identify the main themes Helpful Answer:"""""" map_prompt = PromptTemplate.from_template(map_template) map_chain = LLMChain(llm=llm, prompt=map_prompt) ``` We can also use the Prompt Hub to store and fetch prompts. This will work with your [LangSmith API key]( For example, see the map prompt [here]( ```python from langchain import hub map_prompt = hub.pull(""rlm/map-prompt"") map_chain = LLMChain(llm=llm, prompt=map_prompt) ``` The `ReduceDocumentsChain` handles taking the document mapping results and reducing them into a single output. It wraps a generic `CombineDocumentsChain` (like `StuffDocumentsChain`) but adds the ability to collapse documents before passing it to the `CombineDocumentsChain` if their cumulative size exceeds `token_max`. In this example, we can actually re-use our chain for combining our docs to also collapse our docs. So if the cumulative number of tokens in our mapped documents exceeds 4000 tokens, then we\'ll recursively pass in the documents in batches of < 4000 tokens to our `StuffDocumentsChain` to create batched summaries. And once those batched summaries are cumulatively less than 4000 tokens, we\'ll pass them all one last time to the `StuffDocumentsChain` to create the final summary. ```python # Reduce reduce_template = """"""The following is set of summaries: {doc_summaries} Take these and distill it into a final, consolidated summary of the main themes. Helpful Answer:"""""" reduce_prompt = PromptTemplate.from_template(reduce_template) ``` ```python # Note we can also get this from the prompt hub, as noted above reduce_prompt = hub.pull(""rlm/map-prompt"") ``` ```python reduce_prompt ``` ```text ChatPromptTemplate(input_variables=[\'docs\'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[\'docs\'], template=\'The following is a set of documents:\\n{docs}\\nBased on this list of docs, please identify the main themes \\nHelpful Answer:\'))]) ``` ```python # Run chain reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt) # Takes a list of documents, combines them into a single string, and passes this to an LLMChain combine_documents_chain = StuffDocumentsChain( llm_chain=reduce_chain, document_variable_name=""docs"" ) # Combines and iteravely reduces the mapped documents reduce_documents_chain = ReduceDocumentsChain( # This is final chain that is called. combine_documents_chain=combine_documents_chain, # If documents exceed context for `StuffDocumentsChain` collapse_documents_chain=combine_documents_chain, # The maximum number of tokens to group documents into. token_max=4000, ) ``` Combining our map and reduce chains into one: ```python # Combining documents by mapping a chain over them, then combining results map_reduce_chain = MapReduceDocumentsChain( # Map chain llm_chain=map_chain, # Reduce chain reduce_documents_chain=reduce_documents_chain, # The variable name in the llm_chain to put the documents in document_variable_name=""docs"", # Return the results of the map steps in the output return_intermediate_steps=False, ) text_splitter = CharacterTextSplitter.from_tiktoken_encoder( chunk_size=1000, chunk_overlap=0 ) split_docs = text_splitter.split_documents(docs) ``` ```text Created a chunk of size 1003, which is longer than the specified 1000 ``` ```python print(map_reduce_chain.run(split_docs)) ``` ```text Based on the list of documents provided, the main themes can be identified as follows: 1. LLM-powered autonomous agents: The documents discuss the concept of building agents with LLM as their core controller and highlight the potential of LLM beyond generating written content. They explore the capabilities of LLM as a general problem solver. 2. Agent system overview: The documents provide an overview of the components that make up a LLM-powered autonomous agent system, including planning, memory, and tool use. Each component is explained in detail, highlighting its role in enhancing the agent\'s capabilities. 3. Planning: The documents discuss how the agent breaks down large tasks into smaller subgoals and utilizes self-reflection to improve the quality of its actions and results. 4. Memory: The documents explain the importance of both short-term and long-term memory in an agent system. Short-term memory is utilized for in-context learning, while long-term memory allows the agent to retain and recall information over extended periods. 5. Tool use: The documents highlight the agent\'s ability to call external APIs for additional information and resources that may be missing from its pre-trained model weights. This includes accessing current information, executing code, and retrieving proprietary information. 6. Case studies and proof-of-concept examples: The documents provide examples of how LLM-powered autonomous agents can be applied in various domains, such as scientific discovery and generative agent simulations. These case studies serve as examples of the capabilities and potential applications of such agents. 7. Challenges: The documents acknowledge the challenges associated with building and utilizing LLM-powered autonomous agents, although specific challenges are not mentioned in the given set of documents. 8. Citation and references: The documents include a citation and reference section, indicating that the information presented is based on existing research and sources. Overall, the main themes in the provided documents revolve around LLM-powered autonomous agents, their components and capabilities, planning, memory, tool use, case studies, and challenges. ``` ### Go deeper\u200b **Customization** - As shown above, you can customize the LLMs and prompts for map and reduce stages. **Real-world use-case** - See [this blog post]( case-study on analyzing user interactions (questions about LangChain documentation)! - The blog post and associated [repo]( also introduce clustering as a means of summarization. - This opens up a third path beyond the `stuff` or `map-reduce` approaches that is worth considering. ![Image description](/assets/images/summarization_use_case_3-4247e4011d0a52901288005bfaf20dc1.png) ## Option 3. Refine\u200b [Refine](/docs/modules/chains/document/refine) is similar to map-reduce: The refine documents chain constructs a response by looping over the input documents and iteratively updating its answer. For each document, it passes all non-document inputs, the current document, and the latest intermediate answer to an LLM chain to get a new answer. This can be easily run with the `chain_type=""refine""` specified. ```python chain = load_summarize_chain(llm, chain_type=""refine"") chain.run(split_docs) ``` ```text \'The article explores the concept of building autonomous agents powered by large language models (LLMs) and their potential as problem solvers. It discusses different approaches to task decomposition, the integration of self-reflection into LLM-based agents, and the use of external classical planners for long-horizon planning. The new context introduces the Chain of Hindsight (CoH) approach and Algorithm Distillation (AD) for training models to produce better outputs. It also discusses different types of memory and the use of external memory for fast retrieval. The article explores the concept of tool use and introduces the MRKL system and experiments on fine-tuning LLMs to use external tools. It introduces HuggingGPT, a framework that uses ChatGPT as a task planner, and discusses the challenges of using LLM-powered agents in real-world scenarios. The article concludes with case studies on scientific discovery agents and the use of LLM-powered agents in anticancer drug discovery. It also introduces the concept of generative agents that combine LLM with memory, planning, and reflection mechanisms. The conversation samples provided discuss the implementation of a game architecture and the challenges in building LLM-centered agents. The article provides references to related research papers and resources for further exploration.\' ``` It\'s also possible to supply a prompt and return intermediate steps. ```python prompt_template = """"""Write a concise summary of the following: {text} CONCISE SUMMARY:"""""" prompt = PromptTemplate.from_template(prompt_template) refine_template = ( ""Your job is to produce a final summary\\n"" ""We have provided an existing summary up to a certain point: {existing_answer}\\n"" ""We have the opportunity to refine the existing summary"" ""(only if needed) with some more context below.\\n"" ""------------\\n"" ""{text}\\n"" ""------------\\n"" ""Given the new context, refine the original summary in Italian"" ""If the context isn\'t useful, return the original summary."" ) refine_prompt = PromptTemplate.from_template(refine_template) chain = load_summarize_chain( llm=llm, chain_type=""refine"", question_prompt=prompt, refine_prompt=refine_prompt, return_intermediate_steps=True, input_key=""input_documents"", output_key=""output_text"", ) result = chain({""input_documents"": split_docs}, return_only_outputs=True) ``` ```python print(result[""output_text""]) ``` ```text Il presente articolo discute il concetto di costruire agenti autonomi utilizzando LLM (large language model) come controller principale. Esplora i diversi componenti di un sistema di agenti alimentato da LLM, tra cui la pianificazione, la memoria e l\'uso degli strumenti. Dimostrazioni di concetto come AutoGPT mostrano il potenziale di LLM come risolutore generale di problemi. Approcci come Chain of Thought, Tree of Thoughts, LLM+P, ReAct e Reflexion consentono agli agenti autonomi di pianificare, riflettere su se stessi e migliorarsi iterativamente. Tuttavia, ci sono sfide da affrontare, come la limitata capacit di contesto che limita l\'inclusione di informazioni storiche dettagliate e la difficolt di pianificazione a lungo termine e decomposizione delle attivit. Inoltre, l\'affidabilit dell\'interfaccia di linguaggio naturale tra LLM e componenti esterni come la memoria e gli strumenti  incerta, poich i LLM possono commettere errori di formattazione e mostrare comportamenti ribelli. Nonostante ci, il sistema AutoGPT viene menzionato come esempio di dimostrazione di concetto che utilizza LLM come controller principale per agenti autonomi. Questo articolo fa riferimento a diverse fonti che esplorano approcci e applicazioni specifiche di LLM nell\'ambito degli agenti autonomi. ``` ```python print(""\\n\\n"".join(result[""intermediate_steps""][:3])) ``` ```text This article discusses the concept of building autonomous agents using LLM (large language model) as the core controller. The article explores the different components of an LLM-powered agent system, including planning, memory, and tool use. It also provides examples of proof-of-concept demos and highlights the potential of LLM as a general problem solver. Questo articolo discute del concetto di costruire agenti autonomi utilizzando LLM (large language model) come controller principale. L\'articolo esplora i diversi componenti di un sistema di agenti alimentato da LLM, inclusa la pianificazione, la memoria e l\'uso degli strumenti. Vengono forniti anche esempi di dimostrazioni di proof-of-concept e si evidenzia il potenziale di LLM come risolutore generale di problemi. Inoltre, vengono presentati approcci come Chain of Thought, Tree of Thoughts, LLM+P, ReAct e Reflexion che consentono agli agenti autonomi di pianificare, riflettere su se stessi e migliorare iterativamente. Questo articolo discute del concetto di costruire agenti autonomi utilizzando LLM (large language model) come controller principale. L\'articolo esplora i diversi componenti di un sistema di agenti alimentato da LLM, inclusa la pianificazione, la memoria e l\'uso degli strumenti. Vengono forniti anche esempi di dimostrazioni di proof-of-concept e si evidenzia il potenziale di LLM come risolutore generale di problemi. Inoltre, vengono presentati approcci come Chain of Thought, Tree of Thoughts, LLM+P, ReAct e Reflexion che consentono agli agenti autonomi di pianificare, riflettere su se stessi e migliorare iterativamente. Il nuovo contesto riguarda l\'approccio Chain of Hindsight (CoH) che permette al modello di migliorare autonomamente i propri output attraverso un processo di apprendimento supervisionato. Viene anche presentato l\'approccio Algorithm Distillation (AD) che applica lo stesso concetto alle traiettorie di apprendimento per compiti di reinforcement learning. ``` ## Splitting and summarizing in a single chain\u200b For convenience, we can wrap both the text splitting of our long document and summarizing in a single `AnalyzeDocumentsChain`. ```python from langchain.chains import AnalyzeDocumentChain summarize_document_chain = AnalyzeDocumentChain( combine_docs_chain=chain, text_splitter=text_splitter ) summarize_document_chain.run(docs[0]) ``` ```text --------------------------------------------------------------------------- ValueError Traceback (most recent call last) Cell In[17], line 4 1 from langchain.chains import AnalyzeDocumentChain 3 summarize_document_chain = AnalyzeDocumentChain(combine_docs_chain=chain, text_splitter=text_splitter) ----> 4 summarize_document_chain.run(docs[0]) File ~/langchain/libs/langchain/langchain/chains/base.py:496, in Chain.run(self, callbacks, tags, metadata, *args, **kwargs) 459 """"""Convenience method for executing chain. 460 461 The main difference between this method and `Chain.__call__` is that this (...) 493 # -> ""The temperature in Boise is..."" 494 """""" 495 # Run at start to make sure this is possible/defined --> 496 _output_key = self._run_output_key 498 if args and not kwargs: 499 if len(args) != 1: File ~/langchain/libs/langchain/langchain/chains/base.py:445, in Chain._run_output_key(self) 442 @property 443 def _run_output_key(self) -> str: 444 if len(self.output_keys) != 1: --> 445 raise ValueError( 446 f""`run` not supported when there is not exactly "" 447 f""one output key. Got {self.output_keys}."" 448 ) 449 return self.output_keys[0] ValueError: `run` not supported when there is not exactly one output key. Got [\'output_text\', \'intermediate_steps\']. ``` - [Use case](#use-case) - [Overview](#overview) - [Quickstart](#quickstart) - [Option 1. Stuff](#option-1-stuff)- [Go deeper](#go-deeper) - [Option 2. Map-Reduce](#option-2-map-reduce)- [Go deeper](#go-deeper-1) - [Option 3. Refine](#option-3-refine) - [Splitting and summarizing in a single chain](#splitting-and-summarizing-in-a-single-chain)', 'Document loaders | Document loaders infoHead to [Integrations](/docs/integrations/document_loaders/) for documentation on built-in document loader integrations with 3rd-party tools. Use document loaders to load data from a source as `Document`\'s. A `Document` is a piece of text and associated metadata. For example, there are document loaders for loading a simple `.txt` file, for loading the text contents of any web page, or even for loading a transcript of a YouTube video. Document loaders provide a ""load"" method for loading data as documents from a configured source. They optionally implement a ""lazy load"" as well for lazily loading data into memory. ## Get started The simplest loader reads in a file as text and places it all into one document. ```python from langchain.document_loaders import TextLoader loader = TextLoader(""./index.md"") loader.load() ``` ```text [ Document(page_content=\'---\\nsidebar_position: 0\\n---\\n# Document loaders\\n\\nUse document loaders to load data from a source as `Document`\\\'s. A `Document` is a piece of text\\nand associated metadata. For example, there are document loaders for loading a simple `.txt` file, for loading the text\\ncontents of any web page, or even for loading a transcript of a YouTube video.\\n\\nEvery document loader exposes two methods:\\n1. ""Load"": load documents from the configured source\\n2. ""Load and split"": load documents from the configured source and split them using the passed in text splitter\\n\\nThey optionally implement:\\n\\n3. ""Lazy load"": load documents into memory lazily\\n\', metadata={\'source\': \'../docs/docs/modules/data_connection/document_loaders/index.md\'}) ] ``` - [Get started](#get-started)']","For loading the text contents of a single webpage, you can use the `WebBaseLoader` from the `langchain.document_loaders` module. Here's a simple example of how to use it:

```python
from langchain.document_loaders import WebBaseLoader

# Specify the URL of the webpage you want to load
url = ""https://example.com""

# Create a loader instance
loader = WebBaseLoader(url)

# Load the document
docs = loader.load()

# Print the loaded document
for doc in docs:
    print(doc.page_content)
```

This will fetch the content of the webpage and convert it into a `Document` object that you can then use for further processing.","For a single web page, the WebBaseLoader or AsyncHtmlLoader should suffice. If you want to load multiple pages by traversing a whole website, you can use the SitemapLoader or RecursiveUrlLoader.",0.3333333333,1.0,1.0,0.16336470131023162,0.21705426356589147
67,whats the difference between run_id and example_id,"['Momento Cache | Momento Cache [Momento Cache]( is the world\'s first truly serverless caching service. It provides instant elasticity, scale-to-zero capability, and blazing-fast performance. This notebook goes over how to use [Momento Cache]( to store chat message history using the `MomentoChatMessageHistory` class. See the Momento [docs]( for more detail on how to get set up with Momento. Note that, by default we will create a cache if one with the given name doesn\'t already exist. You\'ll need to get a Momento API key to use this class. This can either be passed in to a momento.CacheClient if you\'d like to instantiate that directly, as a named parameter `api_key` to `MomentoChatMessageHistory.from_client_params`, or can just be set as an environment variable `MOMENTO_API_KEY`. ```python from datetime import timedelta from langchain.memory import MomentoChatMessageHistory session_id = ""foo"" cache_name = ""langchain"" ttl = timedelta(days=1) history = MomentoChatMessageHistory.from_client_params( session_id, cache_name, ttl, ) history.add_user_message(""hi!"") history.add_ai_message(""whats up?"") ``` ```python history.messages ``` ```text [HumanMessage(content=\'hi!\', additional_kwargs={}, example=False), AIMessage(content=\'whats up?\', additional_kwargs={}, example=False)] ```', 'MongodDB | MongodDB `MongoDB` is a source-available cross-platform document-oriented database program. Classified as a NoSQL database program, `MongoDB` uses `JSON`-like documents with optional schemas. `MongoDB` is developed by MongoDB Inc. and licensed under the Server Side Public License (SSPL). - [Wikipedia]( This notebook goes over how to use Mongodb to store chat message history. ## Setting up ```bash pip install pymongo ``` ```python # Provide the connection string to connect to the MongoDB database connection_string = ""mongodb://mongo_user:password123@mongo:27017"" ``` ## Example ```python from langchain.memory import MongoDBChatMessageHistory message_history = MongoDBChatMessageHistory( connection_string=connection_string, session_id=""test-session"" ) message_history.add_user_message(""hi!"") message_history.add_ai_message(""whats up?"") ``` ```python message_history.messages ``` ```text [HumanMessage(content=\'hi!\', additional_kwargs={}, example=False), AIMessage(content=\'whats up?\', additional_kwargs={}, example=False)] ``` - [Setting up](#setting-up) - [Example](#example)', 'Elasticsearch | Elasticsearch [Elasticsearch]( is a distributed, RESTful search and analytics engine, capable of performing both vector and lexical search. It is built on top of the Apache Lucene library. This notebook shows how to use chat message history functionality with `Elasticsearch`. ## Set up Elasticsearch There are two main ways to set up an Elasticsearch instance: 1. **Elastic Cloud.** Elastic Cloud is a managed Elasticsearch service. Sign up for a [free trial]( 2. **Local Elasticsearch installation.** Get started with Elasticsearch by running it locally. The easiest way is to use the official Elasticsearch Docker image. See the [Elasticsearch Docker documentation]( for more information. ## Install dependencies ```python %pip install elasticsearch langchain ``` ## Authentication ### How to obtain a password for the default ""elastic"" user To obtain your Elastic Cloud password for the default ""elastic"" user: 1. Log in to the [Elastic Cloud console]( 2. Go to ""Security"" > ""Users"" 3. Locate the ""elastic"" user and click ""Edit"" 4. Click ""Reset password"" 5. Follow the prompts to reset the password ### Use the Username/password ```python es_username = os.environ.get(""ES_USERNAME"", ""elastic"") es_password = os.environ.get(""ES_PASSWORD"", ""change me..."") history = ElasticsearchChatMessageHistory( es_url=es_url, es_user=es_username, es_password=es_password, index=""test-history"", session_id=""test-session"" ) ``` ### How to obtain an API key To obtain an API key: 1. Log in to the [Elastic Cloud console]( 2. Open `Kibana` and go to Stack Management > API Keys 3. Click ""Create API key"" 4. Enter a name for the API key and click ""Create"" ### Use the API key ```python es_api_key = os.environ.get(""ES_API_KEY"") history = ElasticsearchChatMessageHistory( es_api_key=es_api_key, index=""test-history"", session_id=""test-session"" ) ``` ## Initialize Elasticsearch client and chat message history ```python import os from langchain.memory import ElasticsearchChatMessageHistory es_url = os.environ.get(""ES_URL"", "" # If using Elastic Cloud: # es_cloud_id = os.environ.get(""ES_CLOUD_ID"") # Note: see Authentication section for various authentication methods history = ElasticsearchChatMessageHistory( es_url=es_url, index=""test-history"", session_id=""test-session"" ) ``` ## Use the chat message history ```python history.add_user_message(""hi!"") history.add_ai_message(""whats up?"") ``` ```text indexing message content=\'hi!\' additional_kwargs={} example=False indexing message content=\'whats up?\' additional_kwargs={} example=False ``` - [Set up Elasticsearch](#set-up-elasticsearch) - [Install dependencies](#install-dependencies) - [Authentication](#authentication)- [How to obtain a password for the default ""elastic"" user](#how-to-obtain-a-password-for-the-default-elastic-user) - [Use the Username/password](#use-the-usernamepassword) - [How to obtain an API key](#how-to-obtain-an-api-key) - [Use the API key](#use-the-api-key) - [Initialize Elasticsearch client and chat message history](#initialize-elasticsearch-client-and-chat-message-history) - [Use the chat message history](#use-the-chat-message-history)']","`run_id` and `example_id` are not defined in the provided context. If you are referring to specific identifiers used in a particular system or framework, you'll need to provide more details or check the documentation for that specific system.","The `run_id` and `example_id` are both identifiers used in LangChain and LangSmith, but they serve different purposes. The `run_id` is a unique ID assigned to each individual ""run"" in a trace, which represents a unit of work whenever a runnable object such as an LLM or chain is called. An `example_id` is a unique ID assigned to each row (or ""example"") in a LangSmith dataset.",0.0,0.0,0.0,0.4385500563623652,0.2545454545454545
68,What is an agent,"['Vectorstore | Vectorstore This notebook showcases an agent designed to retrieve information from one or more vectorstores, either with or without sources. ## Create Vectorstores ```python from langchain.embeddings.openai import OpenAIEmbeddings from langchain.llms import OpenAI from langchain.text_splitter import CharacterTextSplitter from langchain.vectorstores import Chroma llm = OpenAI(temperature=0) ``` ```python from langchain.document_loaders import TextLoader loader = TextLoader(""../../modules/state_of_the_union.txt"") documents = loader.load() text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0) texts = text_splitter.split_documents(documents) embeddings = OpenAIEmbeddings() state_of_union_store = Chroma.from_documents( texts, embeddings, collection_name=""state-of-union"" ) ``` ```text Running Chroma using direct local API. Using DuckDB in-memory for database. Data will be transient. ``` ```python from langchain.document_loaders import WebBaseLoader loader = WebBaseLoader("" docs = loader.load() ruff_texts = text_splitter.split_documents(docs) ruff_store = Chroma.from_documents(ruff_texts, embeddings, collection_name=""ruff"") ``` ```text Running Chroma using direct local API. Using DuckDB in-memory for database. Data will be transient. ``` ## Initialize Toolkit and Agent First, we\'ll create an agent with a single vectorstore. ```python from langchain.agents.agent_toolkits import ( VectorStoreInfo, VectorStoreToolkit, create_vectorstore_agent, ) vectorstore_info = VectorStoreInfo( name=""state_of_union_address"", description=""the most recent state of the Union adress"", vectorstore=state_of_union_store, ) toolkit = VectorStoreToolkit(vectorstore_info=vectorstore_info) agent_executor = create_vectorstore_agent(llm=llm, toolkit=toolkit, verbose=True) ``` ## Examples ```python agent_executor.run( ""What did biden say about ketanji brown jackson in the state of the union address?"" ) ``` ```text > Entering new AgentExecutor chain... I need to find the answer in the state of the union address Action: state_of_union_address Action Input: What did biden say about ketanji brown jackson Observation: Biden said that Ketanji Brown Jackson is one of the nation\'s top legal minds and that she will continue Justice Breyer\'s legacy of excellence. Thought: I now know the final answer Final Answer: Biden said that Ketanji Brown Jackson is one of the nation\'s top legal minds and that she will continue Justice Breyer\'s legacy of excellence. > Finished chain. ""Biden said that Ketanji Brown Jackson is one of the nation\'s top legal minds and that she will continue Justice Breyer\'s legacy of excellence."" ``` ```python agent_executor.run( ""What did biden say about ketanji brown jackson in the state of the union address? List the source."" ) ``` ```text > Entering new AgentExecutor chain... I need to use the state_of_union_address_with_sources tool to answer this question. Action: state_of_union_address_with_sources Action Input: What did biden say about ketanji brown jackson Observation: {""answer"": "" Biden said that he nominated Circuit Court of Appeals Judge Ketanji Brown Jackson to the United States Supreme Court, and that she is one of the nation\'s top legal minds who will continue Justice Breyer\'s legacy of excellence.\\n"", ""sources"": ""../../state_of_the_union.txt""} Thought: I now know the final answer Final Answer: Biden said that he nominated Circuit Court of Appeals Judge Ketanji Brown Jackson to the United States Supreme Court, and that she is one of the nation\'s top legal minds who will continue Justice Breyer\'s legacy of excellence. Sources: ../../state_of_the_union.txt > Finished chain. ""Biden said that he nominated Circuit Court of Appeals Judge Ketanji Brown Jackson to the United States Supreme Court, and that she is one of the nation\'s top legal minds who will continue Justice Breyer\'s legacy of excellence. Sources: ../../state_of_the_union.txt"" ``` ## Multiple Vectorstores We can also easily use this initialize an agent with multiple vectorstores and use the agent to route between them. To do this. This agent is optimized for routing, so it is a different toolkit and initializer. ```python from langchain.agents.agent_toolkits import ( VectorStoreInfo, VectorStoreRouterToolkit, create_vectorstore_router_agent, ) ``` ```python ruff_vectorstore_info = VectorStoreInfo( name=""ruff"", description=""Information about the Ruff python linting library"", vectorstore=ruff_store, ) router_toolkit = VectorStoreRouterToolkit( vectorstores=[vectorstore_info, ruff_vectorstore_info], llm=llm ) agent_executor = create_vectorstore_router_agent( llm=llm, toolkit=router_toolkit, verbose=True ) ``` ## Examples ```python agent_executor.run( ""What did biden say about ketanji brown jackson in the state of the union address?"" ) ``` ```text > Entering new AgentExecutor chain... I need to use the state_of_union_address tool to answer this question. Action: state_of_union_address Action Input: What did biden say about ketanji brown jackson Observation: Biden said that Ketanji Brown Jackson is one of the nation\'s top legal minds and that she will continue Justice Breyer\'s legacy of excellence. Thought: I now know the final answer Final Answer: Biden said that Ketanji Brown Jackson is one of the nation\'s top legal minds and that she will continue Justice Breyer\'s legacy of excellence. > Finished chain. ""Biden said that Ketanji Brown Jackson is one of the nation\'s top legal minds and that she will continue Justice Breyer\'s legacy of excellence."" ``` ```python agent_executor.run(""What tool does ruff use to run over Jupyter Notebooks?"") ``` ```text > Entering new AgentExecutor chain... I need to find out what tool ruff uses to run over Jupyter Notebooks Action: ruff Action Input: What tool does ruff use to run over Jupyter Notebooks? Observation: Ruff is integrated into nbQA, a tool for running linters and code formatters over Jupyter Notebooks. After installing ruff and nbqa, you can run Ruff over a notebook like so: > nbqa ruff Untitled.html Thought: I now know the final answer Final Answer: Ruff is integrated into nbQA, a tool for running linters and code formatters over Jupyter Notebooks. After installing ruff and nbqa, you can run Ruff over a notebook like so: > nbqa ruff Untitled.html > Finished chain. \'Ruff is integrated into nbQA, a tool for running linters and code formatters over Jupyter Notebooks. After installing ruff and nbqa, you can run Ruff over a notebook like so: > nbqa ruff Untitled.html\' ``` ```python agent_executor.run( ""What tool does ruff use to run over Jupyter Notebooks? Did the president mention that tool in the state of the union?"" ) ``` ```text > Entering new AgentExecutor chain... I need to find out what tool ruff uses and if the president mentioned it in the state of the union. Action: ruff Action Input: What tool does ruff use to run over Jupyter Notebooks? Observation: Ruff is integrated into nbQA, a tool for running linters and code formatters over Jupyter Notebooks. After installing ruff and nbqa, you can run Ruff over a notebook like so: > nbqa ruff Untitled.html Thought: I need to find out if the president mentioned nbQA in the state of the union. Action: state_of_union_address Action Input: Did the president mention nbQA in the state of the union? Observation: No, the president did not mention nbQA in the state of the union. Thought: I now know the final answer. Final Answer: No, the president did not mention nbQA in the state of the union. > Finished chain. \'No, the president did not mention nbQA in the state of the union.\' ``` - [Create Vectorstores](#create-vectorstores) - [Initialize Toolkit and Agent](#initialize-toolkit-and-agent) - [Examples](#examples) - [Multiple Vectorstores](#multiple-vectorstores) - [Examples](#examples-1)', 'Amadeus | Amadeus This notebook walks you through connecting LangChain to the `Amadeus` travel information API To use this toolkit, you will need to set up your credentials explained in the [Amadeus for developers getting started overview]( Once you\'ve received a AMADEUS_CLIENT_ID and AMADEUS_CLIENT_SECRET, you can input them as environmental variables below. ```bash pip install --upgrade amadeus > /dev/null ``` ## Assign Environmental Variables The toolkit will read the AMADEUS_CLIENT_ID and AMADEUS_CLIENT_SECRET environmental variables to authenticate the user so you need to set them here. You will also need to set your OPENAI_API_KEY to use the agent later. ```python # Set environmental variables here import os os.environ[""AMADEUS_CLIENT_ID""] = ""CLIENT_ID"" os.environ[""AMADEUS_CLIENT_SECRET""] = ""CLIENT_SECRET"" os.environ[""OPENAI_API_KEY""] = ""API_KEY"" ``` ## Create the Amadeus Toolkit and Get Tools To start, you need to create the toolkit, so you can access its tools later. ```python from langchain.agents.agent_toolkits.amadeus.toolkit import AmadeusToolkit toolkit = AmadeusToolkit() tools = toolkit.get_tools() ``` ## Use Amadeus Toolkit within an Agent ```python from langchain.agents import AgentType, initialize_agent from langchain.llms import OpenAI ``` ```python llm = OpenAI(temperature=0) agent = initialize_agent( tools=tools, llm=llm, verbose=False, agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION, ) ``` ```python agent.run(""What is the name of the airport in Cali, Colombia?"") ``` ```text \'The closest airport to Cali, Colombia is Alfonso Bonilla Aragn International Airport (CLO).\' ``` ```python agent.run( ""What is the departure time of the cheapest flight on August 23, 2023 leaving Dallas, Texas before noon to Lincoln, Nebraska?"" ) ``` ```text \'The cheapest flight on August 23, 2023 leaving Dallas, Texas before noon to Lincoln, Nebraska has a departure time of 16:42 and a total price of 276.08 EURO.\' ``` ```python agent.run( ""At what time does earliest flight on August 23, 2023 leaving Dallas, Texas to Lincoln, Nebraska land in Nebraska?"" ) ``` ```text \'The earliest flight on August 23, 2023 leaving Dallas, Texas to Lincoln, Nebraska lands in Lincoln, Nebraska at 16:07.\' ``` ```python agent.run( ""What is the full travel time for the cheapest flight between Portland, Oregon to Dallas, TX on October 3, 2023?"" ) ``` ```text \'The cheapest flight between Portland, Oregon to Dallas, TX on October 3, 2023 is a Spirit Airlines flight with a total price of 84.02 EURO and a total travel time of 8 hours and 43 minutes.\' ``` ```python agent.run( ""Please draft a concise email from Santiago to Paul, Santiago\'s travel agent, asking him to book the earliest flight from DFW to DCA on Aug 28, 2023. Include all flight details in the email."" ) ``` ```text \'Dear Paul,\\n\\nI am writing to request that you book the earliest flight from DFW to DCA on Aug 28, 2023. The flight details are as follows:\\n\\nFlight 1: DFW to ATL, departing at 7:15 AM, arriving at 10:25 AM, flight number 983, carrier Delta Air Lines\\nFlight 2: ATL to DCA, departing at 12:15 PM, arriving at 2:02 PM, flight number 759, carrier Delta Air Lines\\n\\nThank you for your help.\\n\\nSincerely,\\nSantiago\' ``` - [Assign Environmental Variables](#assign-environmental-variables) - [Create the Amadeus Toolkit and Get Tools](#create-the-amadeus-toolkit-and-get-tools) - [Use Amadeus Toolkit within an Agent](#use-amadeus-toolkit-within-an-agent)', 'Multi-Input Tools | Multi-Input Tools This notebook shows how to use a tool that requires multiple inputs with an agent. The recommended way to do so is with the `StructuredTool` class. ```python import os os.environ[""LANGCHAIN_TRACING""] = ""true"" ``` ```python from langchain.agents import AgentType, initialize_agent from langchain.llms import OpenAI llm = OpenAI(temperature=0) ``` ```python from langchain.tools import StructuredTool def multiplier(a: float, b: float) -> float: """"""Multiply the provided floats."""""" return a * b tool = StructuredTool.from_function(multiplier) ``` ```python # Structured tools are compatible with the STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION agent type. agent_executor = initialize_agent( [tool], llm, agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True, ) ``` ```python agent_executor.run(""What is 3 times 4"") ``` ```text > Entering new AgentExecutor chain... Thought: I need to multiply 3 and 4 Action: ``` { ""action"": ""multiplier"", ""action_input"": {""a"": 3, ""b"": 4} } ``` Observation: 12 Thought: I know what to respond Action: ``` { ""action"": ""Final Answer"", ""action_input"": ""3 times 4 is 12"" } ``` > Finished chain. \'3 times 4 is 12\' ``` ## Multi-Input Tools with a string format An alternative to the structured tool would be to use the regular `Tool` class and accept a single string. The tool would then have to handle the parsing logic to extract the relevant values from the text, which tightly couples the tool representation to the agent prompt. This is still useful if the underlying language model can\'t reliably generate structured schema. Let\'s take the multiplication function as an example. In order to use this, we will tell the agent to generate the ""Action Input"" as a comma-separated list of length two. We will then write a thin wrapper that takes a string, splits it into two around a comma, and passes both parsed sides as integers to the multiplication function. ```python from langchain.agents import AgentType, Tool, initialize_agent from langchain.llms import OpenAI ``` Here is the multiplication function, as well as a wrapper to parse a string as input. ```python def multiplier(a, b): return a * b def parsing_multiplier(string): a, b = string.split("","") return multiplier(int(a), int(b)) ``` ```python llm = OpenAI(temperature=0) tools = [ Tool( name=""Multiplier"", func=parsing_multiplier, description=""useful for when you need to multiply two numbers together. The input to this tool should be a comma separated list of numbers of length two, representing the two numbers you want to multiply together. For example, `1,2` would be the input if you wanted to multiply 1 by 2."", ) ] mrkl = initialize_agent( tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True ) ``` ```python mrkl.run(""What is 3 times 4"") ``` ```text > Entering new AgentExecutor chain... I need to multiply two numbers Action: Multiplier Action Input: 3,4 Observation: 12 Thought: I now know the final answer Final Answer: 3 times 4 is 12 > Finished chain. \'3 times 4 is 12\' ``` - [Multi-Input Tools with a string format](#multi-input-tools-with-a-string-format)']",An agent in this context is an automated system designed to perform tasks using tools provided by a toolkit. It uses a language model to understand and respond to queries.,An agent is a component in the Langchain framework that uses a language model (LLM) to determine which actions to take and in what order. It is responsible for making decisions and interacting with tools to accomplish specific objectives.,0.9999999999666667,1.0,1.0,0.055380039444061294,0.31884057971014496
69,how do I search and filter metadata in redis vectorstore?,"['langchain.vectorstores.redis.base.Redis LangChain 0.0.337 API Experimental Python Docs Toggle Menu PrevUp Next LangChain 0.0.337 langchain.vectorstores.redis.base.Redis langchain.vectorstores.redis.base.Redis class langchain.vectorstores.redis.base.Redis(redis_url: str, index_name: str, embedding: Embeddings, index_schema: Optional[Union[Dict[str, str], str, PathLike]] = None, vector_schema: Optional[Dict[str, Union[int, str]]] = None, relevance_score_fn: Optional[Callable[[float], float]] = None, key_prefix: Optional[str] = None, **kwargs: Any)[source] Redis vector database. To use, you should have the redis python package installed and have a running Redis Enterprise or Redis-Stack server For production use cases, it is recommended to use Redis Enterprise as the scaling, performance, stability and availability is much better than Redis-Stack. For testing and prototyping, however, this is not required. Redis-Stack is available as a docker container the full vector search API available. # to run redis stack in docker locally docker run -d -p 6379:6379 -p 8001:8001 redis/redis-stack:latest Once running, you can connect to the redis server with the following url schemas: - redis://: # simple connection - redis://:@: # connection with authentication - rediss://: # connection with SSL - rediss://:@: # connection with SSL and auth Examples: The following examples show various ways to use the Redis VectorStore with LangChain. For all the following examples assume we have the following imports: from langchain.vectorstores import Redis from langchain.embeddings import OpenAIEmbeddings Initialize, create index, and load Documentsfrom langchain.vectorstores import Redis from langchain.embeddings import OpenAIEmbeddings rds = Redis.from_documents( documents, # a list of Document objects from loaders or created embeddings, # an Embeddings object redis_url=""redis://localhost:6379"", ) Initialize, create index, and load Documents with metadatards = Redis.from_texts( texts, # a list of strings metadata, # a list of metadata dicts embeddings, # an Embeddings object redis_url=""redis://localhost:6379"", ) Initialize, create index, and load Documents with metadata and return keys rds, keys = Redis.from_texts_return_keys( texts, # a list of strings metadata, # a list of metadata dicts embeddings, # an Embeddings object redis_url=""redis://localhost:6379"", ) For use cases where the index needs to stay alive, you can initialize with an index name such that it\'s easier to reference later rds = Redis.from_texts( texts, # a list of strings metadata, # a list of metadata dicts embeddings, # an Embeddings object index_name=""my-index"", redis_url=""redis://localhost:6379"", ) Initialize and connect to an existing index (from above) rds = Redis.from_existing_index( embeddings, # an Embeddings object index_name=""my-index"", redis_url=""redis://localhost:6379"", ) Advanced examples: Custom vector schema can be supplied to change the way that Redis creates the underlying vector schema. This is useful for production use cases where you want to optimize the vector schema for your use case. ex. using HNSW instead of FLAT (knn) which is the default vector_schema = { ""algorithm"": ""HNSW"" } rds = Redis.from_texts( texts, # a list of strings metadata, # a list of metadata dicts embeddings, # an Embeddings object vector_schema=vector_schema, redis_url=""redis://localhost:6379"", ) Custom index schema can be supplied to change the way that the metadata is indexed. This is useful for you would like to use the hybrid querying (filtering) capability of Redis. By default, this implementation will automatically generate the index schema according to the following rules: All strings are indexed as text fields All numbers are indexed as numeric fields All lists of strings are indexed as tag fields (joined bylangchain.vectorstores.redis.constants.REDIS_TAG_SEPARATOR) All None values are not indexed but still stored in Redis these arenot retrievable through the interface here, but the raw Redis client can be used to retrieve them. All other types are not indexed To override these rules, you can pass in a custom index schema like the following tag: - name: credit_score text: - name: user - name: job Typically, the credit_score field would be a text field since it\'s a string, however, we can override this behavior by specifying the field type as shown with the yaml config (can also be a dictionary) above and the code below. rds = Redis.from_texts( texts, # a list of strings metadata, # a list of metadata dicts embeddings, # an Embeddings object index_schema=""path/to/index_schema.yaml"", # can also be a dictionary redis_url=""redis://localhost:6379"", ) When connecting to an existing index where a custom schema has been applied, it\'s important to pass in the same schema to the from_existing_index method. Otherwise, the schema for newly added samples will be incorrect and metadata will not be returned. Initialize with necessary components. Attributes DEFAULT_VECTOR_SCHEMA embeddings Access the query embedding object if available. schema Return the schema of the index. Methods __init__(redis_url,index_name,embedding[,...]) Initialize with necessary components. aadd_documents(documents,**kwargs) Run more documents through the embeddings and add to the vectorstore. aadd_texts(texts[,metadatas]) Run more texts through the embeddings and add to the vectorstore. add_documents(documents,**kwargs) Run more documents through the embeddings and add to the vectorstore. add_texts(texts[,metadatas,embeddings,...]) Add more texts to the vectorstore. adelete([ids]) Delete by vector ID or other criteria. afrom_documents(documents,embedding,**kwargs) Return VectorStore initialized from documents and embeddings. afrom_texts(texts,embedding[,metadatas]) Return VectorStore initialized from texts and embeddings. amax_marginal_relevance_search(query[,k,...]) Return docs selected using the maximal marginal relevance. amax_marginal_relevance_search_by_vector(...) Return docs selected using the maximal marginal relevance. as_retriever(**kwargs) Return VectorStoreRetriever initialized from this VectorStore. asearch(query,search_type,**kwargs) Return docs most similar to query using specified search type. asimilarity_search(query[,k]) Return docs most similar to query. asimilarity_search_by_vector(embedding[,k]) Return docs most similar to embedding vector. asimilarity_search_with_relevance_scores(query) Return docs and relevance scores in the range [0, 1], asynchronously. asimilarity_search_with_score(*args,**kwargs) Run similarity search with distance asynchronously. delete([ids]) Delete a Redis entry. drop_index(index_name,delete_documents,...) Drop a Redis search index. from_documents(documents,embedding,**kwargs) Return VectorStore initialized from documents and embeddings. from_existing_index(embedding,index_name,...) Connect to an existing Redis index. from_texts(texts,embedding[,metadatas,...]) Create a Redis vectorstore from a list of texts. from_texts_return_keys(texts, embedding[, ...]) Create a Redis vectorstore from raw documents. max_marginal_relevance_search(query[, k, ...]) Return docs selected using the maximal marginal relevance. max_marginal_relevance_search_by_vector(...) Return docs selected using the maximal marginal relevance. search(query, search_type, **kwargs) Return docs most similar to query using specified search type. similarity_search(query[, k, filter, ...]) Run similarity search similarity_search_by_vector(embedding[, k, ...]) Run similarity search between a query vector and the indexed vectors. similarity_search_limit_score(query[, k, ...]) [Deprecated] Returns the most similar indexed documents to the query text within the score_threshold range. similarity_search_with_relevance_scores(query) Return docs and relevance scores in the range [0, 1]. similarity_search_with_score(query[, k, ...]) Run similarity search with vector distance. write_schema(path) Write the schema to a yaml file. __init__(redis_url: str, index_name: str, embedding: Embeddings, index_schema: Optional[Union[Dict[str, str], str, PathLike]] = None, vector_schema: Optional[Dict[str, Union[int, str]]] = None, relevance_score_fn: Optional[Callable[[float], float]] = None, key_prefix: Optional[str] = None, **kwargs: Any)[source] Initialize with necessary components. async aadd_documents(documents: List[Document], **kwargs: Any)  List[str] Run more documents through the embeddings and add to the vectorstore. Parameters (List[Document] (documents)  Documents to add to the vectorstore. Returns List of IDs of the added texts. Return type List[str] async aadd_texts(texts: Iterable[str], metadatas: Optional[List[dict]] = None, **kwargs: Any)  List[str] Run more texts through the embeddings and add to the vectorstore. add_documents(documents: List[Document], **kwargs: Any)  List[str] Run more documents through the embeddings and add to the vectorstore. Parameters (List[Document] (documents)  Documents to add to the vectorstore. Returns List of IDs of the added texts. Return type List[str] add_texts(texts: Iterable[str], metadatas: Optional[List[dict]] = None, embeddings: Optional[List[List[float]]] = None, batch_size: int = 1000, clean_metadata: bool = True, **kwargs: Any)  List[str][source] Add more texts to the vectorstore. Parameters texts (Iterable[str])  Iterable of strings/text to add to the vectorstore. metadatas (Optional[List[dict]], optional)  Optional list of metadatas. Defaults to None. embeddings (Optional[List[List[float]]], optional)  Optional pre-generated embeddings. Defaults to None. keys (List[str]) or ids (List[str])  Identifiers of entries. Defaults to None. batch_size (int, optional)  Batch size to use for writes. Defaults to 1000. Returns List of ids added to the vectorstore Return type List[str] async adelete(ids: Optional[List[str]] = None, **kwargs: Any)  Optional[bool] Delete by vector ID or other criteria. Parameters ids  List of ids to delete. **kwargs  Other keyword arguments that subclasses might use. Returns True if deletion is successful, False otherwise, None if not implemented. Return type Optional[bool] async classmethod afrom_documents(documents: List[Document], embedding: Embeddings, **kwargs: Any)  VST Return VectorStore initialized from documents and embeddings. async classmethod afrom_texts(texts: List[str], embedding: Embeddings, metadatas: Optional[List[dict]] = None, **kwargs: Any)  VST Return VectorStore initialized from texts and embeddings. async amax_marginal_relevance_search(query: str, k: int = 4, fetch_k: int = 20, lambda_mult: float = 0.5, **kwargs: Any)  List[Document] Return docs selected using the maximal marginal relevance. async amax_marginal_relevance_search_by_vector(embedding: List[float], k: int = 4, fetch_k: int = 20, lambda_mult: float = 0.5, **kwargs: Any)  List[Document] Return docs selected using the maximal marginal relevance. as_retriever(**kwargs: Any)  RedisVectorStoreRetriever[source] Return VectorStoreRetriever initialized from this VectorStore. Parameters search_type (Optional[str])  Defines the type of search that the Retriever should perform. Can be similarity (default), mmr, or similarity_score_threshold. search_kwargs (Optional[Dict])  Keyword arguments to pass to the search function. Can include things like: k: Amount of documents to return (Default: 4) score_threshold: Minimum relevance threshold for similarity_score_threshold fetch_k: Amount of documents to pass to MMR algorithm (Default: 20) lambda_mult: Diversity of results returned by MMR; 1 for minimum diversity and 0 for maximum. (Default: 0.5) filter: Filter by document metadata Returns Retriever class for VectorStore. Return type VectorStoreRetriever Examples: # Retrieve more documents with higher diversity # Useful if your dataset has many similar documents docsearch.as_retriever( search_type=""mmr"", search_kwargs={\'k\': 6, \'lambda_mult\': 0.25} ) # Fetch more documents for the MMR algorithm to consider # But only return the top 5 docsearch.as_retriever( search_type=""mmr"", search_kwargs={\'k\': 5, \'fetch_k\': 50} ) # Only retrieve documents that have a relevance score # Above a certain threshold docsearch.as_retriever( search_type=""similarity_score_threshold"", search_kwargs={\'score_threshold\': 0.8} ) # Only get the single most similar document from the dataset docsearch.as_retriever(search_kwargs={\'k\': 1}) # Use a filter to only retrieve documents from a specific paper docsearch.as_retriever( search_kwargs={\'filter\': {\'paper_title\':\'GPT-4 Technical Report\'}} ) async asearch(query: str, search_type: str, **kwargs: Any)  List[Document] Return docs most similar to query using specified search type. async asimilarity_search(query: str, k: int = 4, **kwargs: Any)  List[Document] Return docs most similar to query. async asimilarity_search_by_vector(embedding: List[float], k: int = 4, **kwargs: Any)  List[Document] Return docs most similar to embedding vector. async asimilarity_search_with_relevance_scores(query: str, k: int = 4, **kwargs: Any)  List[Tuple[Document, float]] Return docs and relevance scores in the range [0, 1], asynchronously. 0 is dissimilar, 1 is most similar. Parameters query  input text k  Number of Documents to return. Defaults to 4. **kwargs  kwargs to be passed to similarity search. Should include: score_threshold: Optional, a floating point value between 0 to 1 to filter the resulting set of retrieved docs Returns List of Tuples of (doc, similarity_score) async asimilarity_search_with_score(*args: Any, **kwargs: Any)  List[Tuple[Document, float]] Run similarity search with distance asynchronously. static delete(ids: Optional[List[str]] = None, **kwargs: Any)  bool[source] Delete a Redis entry. Parameters ids  List of ids (keys in redis) to delete. redis_url  Redis connection url. This should be passed in the kwargs or set as an environment variable: REDIS_URL. Returns Whether or not the deletions were successful. Return type bool Raises ValueError  If the redis python package is not installed. ValueError  If the ids (keys in redis) are not provided static drop_index(index_name: str, delete_documents: bool, **kwargs: Any)  bool[source] Drop a Redis search index. Parameters index_name (str)  Name of the index to drop. delete_documents (bool)  Whether to drop the associated documents. Returns Whether or not the drop was successful. Return type bool classmethod from_documents(documents: List[Document], embedding: Embeddings, **kwargs: Any)  VST Return VectorStore initialized from documents and embeddings. classmethod from_existing_index(embedding: Embeddings, index_name: str, schema: Union[Dict[str, str], str, PathLike], **kwargs: Any)  Redis[source] Connect to an existing Redis index. Example from langchain.vectorstores import Redis from langchain.embeddings import OpenAIEmbeddings embeddings = OpenAIEmbeddings() redisearch = Redis.from_existing_index( embeddings, index_name=""my-index"", redis_url=""redis://username:password@localhost:6379"" ) Parameters embedding (Embeddings)  Embedding model class (i.e. OpenAIEmbeddings) for embedding queries. index_name (str)  Name of the index to connect to. schema (Union[Dict[str, str], str, os.PathLike])  Schema of the index and the vector schema. Can be a dict, or path to yaml file **kwargs (Any)  Additional keyword arguments to pass to the Redis client. Returns Redis VectorStore instance. Return type Redis Raises ValueError  If the index does not exist. ImportError  If the redis python package is not installed. classmethod from_texts(texts: List[str], embedding: Embeddings, metadatas: Optional[List[dict]] = None, index_name: Optional[str] = None, index_schema: Optional[Union[Dict[str, str], str, PathLike]] = None, vector_schema: Optional[Dict[str, Union[int, str]]] = None, **kwargs: Any)  Redis[source] Create a Redis vectorstore from a list of texts. This is a user-friendly interface that: Embeds documents. Creates a new Redis index if it doesn\'t already exist Adds the documents to the newly created Redis index. This method will generate schema based on the metadata passed in if the index_schema is not defined. If the index_schema is defined, it will compare against the generated schema and warn if there are differences. If you are purposefully defining the schema for the metadata, then you can ignore that warning. To examine the schema options, initialize an instance of this class and print out the schema using the Redis.schema` property. This will include the content and content_vector classes which are always present in the langchain schema. Example from langchain.vectorstores import Redis from langchain.embeddings import OpenAIEmbeddings embeddings = OpenAIEmbeddings() redisearch = RediSearch.from_texts( texts, embeddings, redis_url=""redis://username:password@localhost:6379"" ) Parameters texts (List[str])  List of texts to add to the vectorstore. embedding (Embeddings)  Embedding model class (i.e. OpenAIEmbeddings) for embedding queries. metadatas (Optional[List[dict]], optional)  Optional list of metadata dicts to add to the vectorstore. Defaults to None. index_name (Optional[str], optional)  Optional name of the index to create or add to. Defaults to None. index_schema (Optional[Union[Dict[str, str], str, os.PathLike]], optional)  Optional fields to index within the metadata. Overrides generated schema. Defaults to None. vector_schema (Optional[Dict[str, Union[str, int]]], optional)  Optional vector schema to use. Defaults to None. **kwargs (Any)  Additional keyword arguments to pass to the Redis client. Returns Redis VectorStore instance. Return type Redis Raises ValueError  If the number of metadatas does not match the number of texts. ImportError  If the redis python package is not installed. classmethod from_texts_return_keys(texts: List[str], embedding: Embeddings, metadatas: Optional[List[dict]] = None, index_name: Optional[str] = None, index_schema: Optional[Union[Dict[str, str], str, PathLike]] = None, vector_schema: Optional[Dict[str, Union[int, str]]] = None, **kwargs: Any)  Tuple[Redis, List[str]][source] Create a Redis vectorstore from raw documents. This is a user-friendly interface that: Embeds documents. Creates a new Redis index if it doesn\'t already exist Adds the documents to the newly created Redis index. Returns the keys of the newly created documents once stored. This method will generate schema based on the metadata passed in if the index_schema is not defined. If the index_schema is defined, it will compare against the generated schema and warn if there are differences. If you are purposefully defining the schema for the metadata, then you can ignore that warning. To examine the schema options, initialize an instance of this class and print out the schema using the Redis.schema` property. This will include the content and content_vector classes which are always present in the langchain schema. Example from langchain.vectorstores import Redis from langchain.embeddings import OpenAIEmbeddings embeddings = OpenAIEmbeddings() redis, keys = Redis.from_texts_return_keys( texts, embeddings, redis_url=""redis://localhost:6379"" ) Parameters texts (List[str])  List of texts to add to the vectorstore. embedding (Embeddings)  Embeddings to use for the vectorstore. metadatas (Optional[List[dict]], optional)  Optional list of metadata dicts to add to the vectorstore. Defaults to None. index_name (Optional[str], optional)  Optional name of the index to create or add to. Defaults to None. index_schema (Optional[Union[Dict[str, str], str, os.PathLike]], optional)  Optional fields to index within the metadata. Overrides generated schema. Defaults to None. vector_schema (Optional[Dict[str, Union[str, int]]], optional)  Optional vector schema to use. Defaults to None. **kwargs (Any)  Additional keyword arguments to pass to the Redis client. Returns Tuple of the Redis instance and the keys ofthe newly created documents. Return type Tuple[Redis, List[str]] Raises ValueError  If the number of metadatas does not match the number of texts. max_marginal_relevance_search(query: str, k: int = 4, fetch_k: int = 20, lambda_mult: float = 0.5, filter: Optional[RedisFilterExpression] = None, return_metadata: bool = True, distance_threshold: Optional[float] = None, **kwargs: Any)  List[Document][source] Return docs selected using the maximal marginal relevance. Maximal marginal relevance optimizes for similarity to query AND diversityamong selected documents. Parameters query (str)  Text to look up documents similar to. k (int)  Number of Documents to return. Defaults to 4. fetch_k (int)  Number of Documents to fetch to pass to MMR algorithm. lambda_mult (float)  Number between 0 and 1 that determines the degree of diversity among the results with 0 corresponding to maximum diversity and 1 to minimum diversity. Defaults to 0.5. filter (RedisFilterExpression, optional)  Optional metadata filter. Defaults to None. return_metadata (bool, optional)  Whether to return metadata. Defaults to True. distance_threshold (Optional[float], optional)  Maximum vector distance between selected documents and the query vector. Defaults to None. Returns A list of Documents selected by maximal marginal relevance. Return type List[Document] max_marginal_relevance_search_by_vector(embedding: List[float], k: int = 4, fetch_k: int = 20, lambda_mult: float = 0.5, **kwargs: Any)  List[Document] Return docs selected using the maximal marginal relevance. Maximal marginal relevance optimizes for similarity to query AND diversity among selected documents. Parameters embedding  Embedding to look up documents similar to. k  Number of Documents to return. Defaults to 4. fetch_k  Number of Documents to fetch to pass to MMR algorithm. lambda_mult  Number between 0 and 1 that determines the degree of diversity among the results with 0 corresponding to maximum diversity and 1 to minimum diversity. Defaults to 0.5. Returns List of Documents selected by maximal marginal relevance. search(query: str, search_type: str, **kwargs: Any)  List[Document] Return docs most similar to query using specified search type. similarity_search(query: str, k: int = 4, filter: Optional[RedisFilterExpression] = None, return_metadata: bool = True, distance_threshold: Optional[float] = None, **kwargs: Any)  List[Document][source] Run similarity search Parameters query (str)  The query text for which to find similar documents. k (int)  The number of documents to return. Default is 4. filter (RedisFilterExpression, optional)  Optional metadata filter. Defaults to None. return_metadata (bool, optional)  Whether to return metadata. Defaults to True. distance_threshold (Optional[float], optional)  Maximum vector distance between selected documents and the query vector. Defaults to None. Returns A list of documents that are most similar to the querytext. Return type List[Document] similarity_search_by_vector(embedding: List[float], k: int = 4, filter: Optional[RedisFilterExpression] = None, return_metadata: bool = True, distance_threshold: Optional[float] = None, **kwargs: Any)  List[Document][source] Run similarity search between a query vector and the indexed vectors. Parameters embedding (List[float])  The query vector for which to find similar documents. k (int)  The number of documents to return. Default is 4. filter (RedisFilterExpression, optional)  Optional metadata filter. Defaults to None. return_metadata (bool, optional)  Whether to return metadata. Defaults to True. distance_threshold (Optional[float], optional)  Maximum vector distance between selected documents and the query vector. Defaults to None. Returns A list of documents that are most similar to the querytext. Return type List[Document] similarity_search_limit_score(query: str, k: int = 4, score_threshold: float = 0.2, **kwargs: Any)  List[Document][source] [Deprecated] Returns the most similar indexed documents to the query text within the score_threshold range. Deprecated: Use similarity_search with distance_threshold instead. Parameters query (str)  The query text for which to find similar documents. k (int)  The number of documents to return. Default is 4. score_threshold (float)  The minimum matching distance required for a document to be considered a match. Defaults to 0.2. Returns A list of documents that are most similar to the query textincluding the match score for each document. Return type List[Document] Note If there are no documents that satisfy the score_threshold value, an empty list is returned.[Deprecated] Returns the most similar indexed documents to the query text within the score_threshold range. Deprecated: Use similarity_search with distance_threshold instead. Parameters query (str)  The query text for which to find similar documents. k (int)  The number of documents to return. Default is 4. score_threshold (float)  The minimum matching distance required for a document to be considered a match. Defaults to 0.2. Returns A list of documents that are most similar to the query textincluding the match score for each document. Return type List[Document] Note If there are no documents that satisfy the score_threshold value, an empty list is returned. Notes Deprecated since version 0.0.272: Use similarity_search(distance_threshold=0.1) instead. similarity_search_with_relevance_scores(query: str, k: int = 4, **kwargs: Any)  List[Tuple[Document, float]] Return docs and relevance scores in the range [0, 1]. 0 is dissimilar, 1 is most similar. Parameters query  input text k  Number of Documents to return. Defaults to 4. **kwargs  kwargs to be passed to similarity search. Should include: score_threshold: Optional, a floating point value between 0 to 1 to filter the resulting set of retrieved docs Returns List of Tuples of (doc, similarity_score) similarity_search_with_score(query: str, k: int = 4, filter: Optional[RedisFilterExpression] = None, return_metadata: bool = True, **kwargs: Any)  List[Tuple[Document, float]][source] Run similarity search with vector distance. The scores returned from this function are the raw vector distances from the query vector. For similarity scores, use similarity_search_with_relevance_scores. Parameters query (str)  The query text for which to find similar documents. k (int)  The number of documents to return. Default is 4. filter (RedisFilterExpression, optional)  Optional metadata filter. Defaults to None. return_metadata (bool, optional)  Whether to return metadata. Defaults to True. Returns A list of documents that aremost similar to the query with the distance for each document. Return type List[Tuple[Document, float]] write_schema(path: Union[str, PathLike])  None[source] Write the schema to a yaml file. Examples using Redis Redis  2023, Harrison Chase. Last updated on Nov 17, 2023. Show this page source', 'Redis | Redis [Redis (Remote Dictionary Server)]( is an open-source in-memory storage, used as a distributed, in-memory keyvalue database, cache and message broker, with optional durability. Because it holds all data in memory and because of its design, `Redis` offers low-latency reads and writes, making it particularly suitable for use cases that require a cache. Redis is the most popular NoSQL database, and one of the most popular databases overall. This page covers how to use the [Redis]( ecosystem within LangChain. It is broken into two parts: installation and setup, and then references to specific Redis wrappers. ## Installation and Setup Install the Python SDK: ```bash pip install redis ``` ## Wrappers All wrappers need a redis url connection string to connect to the database support either a stand alone Redis server or a High-Availability setup with Replication and Redis Sentinels. ### Redis Standalone connection url For standalone `Redis` server, the official redis connection url formats can be used as describe in the python redis modules ""from_url()"" method [Redis.from_url]( Example: `redis_url = ""redis://:secret-pass@localhost:6379/0""` ### Redis Sentinel connection url For [Redis sentinel setups]( the connection scheme is ""redis+sentinel"". This is an unofficial extensions to the official IANA registered protocol schemes as long as there is no connection url for Sentinels available. Example: `redis_url = ""redis+sentinel://:secret-pass@sentinel-host:26379/mymaster/0""` The format is `redis+sentinel://[[username]:[password]]@[host-or-ip]:[port]/[service-name]/[db-number]` with the default values of ""service-name = mymaster"" and ""db-number = 0"" if not set explicit. The service-name is the redis server monitoring group name as configured within the Sentinel. The current url format limits the connection string to one sentinel host only (no list can be given) and booth Redis server and sentinel must have the same password set (if used). ### Redis Cluster connection url Redis cluster is not supported right now for all methods requiring a ""redis_url"" parameter. The only way to use a Redis Cluster is with LangChain classes accepting a preconfigured Redis client like `RedisCache` (example below). ### Cache The Cache wrapper allows for [Redis]( to be used as a remote, low-latency, in-memory cache for LLM prompts and responses. #### Standard Cache The standard cache is the Redis bread & butter of use case in production for both [open-source]( and [enterprise]( users globally. To import this cache: ```python from langchain.cache import RedisCache ``` To use this cache with your LLMs: ```python from langchain.globals import set_llm_cache import redis redis_client = redis.Redis.from_url(...) set_llm_cache(RedisCache(redis_client)) ``` #### Semantic Cache Semantic caching allows users to retrieve cached prompts based on semantic similarity between the user input and previously cached results. Under the hood it blends Redis as both a cache and a vectorstore. To import this cache: ```python from langchain.cache import RedisSemanticCache ``` To use this cache with your LLMs: ```python from langchain.globals import set_llm_cache import redis # use any embedding provider... from tests.integration_tests.vectorstores.fake_embeddings import FakeEmbeddings redis_url = ""redis://localhost:6379"" set_llm_cache(RedisSemanticCache( embedding=FakeEmbeddings(), redis_url=redis_url )) ``` ### VectorStore The vectorstore wrapper turns Redis into a low-latency [vector database]( for semantic search or LLM content retrieval. To import this vectorstore: ```python from langchain.vectorstores import Redis ``` For a more detailed walkthrough of the Redis vectorstore wrapper, see [this notebook](/docs/integrations/vectorstores/redis). ### Retriever The Redis vector store retriever wrapper generalizes the vectorstore class to perform low-latency document retrieval. To create the retriever, simply call `.as_retriever()` on the base vectorstore class. ### Memory Redis can be used to persist LLM conversations. #### Vector Store Retriever Memory For a more detailed walkthrough of the `VectorStoreRetrieverMemory` wrapper, see [this notebook](/docs/modules/memory/types/vectorstore_retriever_memory). #### Chat Message History Memory For a detailed example of Redis to cache conversation message history, see [this notebook](/docs/integrations/memory/redis_chat_message_history). - [Installation and Setup](#installation-and-setup) - [Wrappers](#wrappers)- [Redis Standalone connection url](#redis-standalone-connection-url) - [Redis Sentinel connection url](#redis-sentinel-connection-url) - [Redis Cluster connection url](#redis-cluster-connection-url) - [Cache](#cache) - [VectorStore](#vectorstore) - [Retriever](#retriever) - [Memory](#memory)', 'Redis | Redis Redis vector database introduction and langchain integration guide. ## What is Redis? Most developers from a web services background are probably familiar with Redis. At it\'s core, Redis is an open-source key-value store that can be used as a cache, message broker, and database. Developers choose Redis because it is fast, has a large ecosystem of client libraries, and has been deployed by major enterprises for years. On top of these traditional use cases, Redis provides additional capabilities like the Search and Query capability that allows users to create secondary index structures within Redis. This allows Redis to be a Vector Database, at the speed of a cache. ## Redis as a Vector Database Redis uses compressed, inverted indexes for fast indexing with a low memory footprint. It also supports a number of advanced features such as: - Indexing of multiple fields in Redis hashes and JSON - Vector similarity search (with HNSW (ANN) or FLAT (KNN)) - Vector Range Search (e.g. find all vectors within a radius of a query vector) - Incremental indexing without performance loss - Document ranking (using [tf-idf]( with optional user-provided weights) - Field weighting - Complex boolean queries with AND, OR, and NOT operators - Prefix matching, fuzzy matching, and exact-phrase queries - Support for [double-metaphone phonetic matching]( - Auto-complete suggestions (with fuzzy prefix suggestions) - Stemming-based query expansion in [many languages]( (using [Snowball]( - Support for Chinese-language tokenization and querying (using [Friso]( - Numeric filters and ranges - Geospatial searches using [Redis geospatial indexing](/commands/georadius) - A powerful aggregations engine - Supports for all utf-8 encoded text - Retrieve full documents, selected fields, or only the document IDs - Sorting results (for example, by creation date) ## Clients Since redis is much more than just a vector database, there are often use cases that demand usage of a Redis client besides just the langchain integration. You can use any standard Redis client library to run Search and Query commands, but it\'s easiest to use a library that wraps the Search and Query API. Below are a few examples, but you can find more client libraries [here]( | Project | Language | License | Author | Stars | | ---- | ---- | ---- | ---- | ---- | | jedis | Java | MIT | Redis | | | redisvl | Python | MIT | Redis | | | redis-py | Python | MIT | Redis | | | node-redis | Node.js | MIT | Redis | | | nredisstack | .NET | MIT | Redis | | ## Deployment Options There are many ways to deploy Redis with RediSearch. The easiest way to get started is to use Docker, but there are are many potential options for deployment such as - [Redis Cloud]( - [Docker (Redis Stack)]( - Cloud marketplaces: [AWS Marketplace]( [Google Marketplace]( or [Azure Marketplace]( - On-premise: [Redis Enterprise Software]( - Kubernetes: [Redis Enterprise Software on Kubernetes]( ## Examples Many examples can be found in the [Redis AI team\'s GitHub]( - [Awesome Redis AI Resources]( - List of examples of using Redis in AI workloads - [Azure OpenAI Embeddings Q&A]( - OpenAI and Redis as a Q&A service on Azure. - [ArXiv Paper Search]( - Semantic search over arXiv scholarly papers - [Vector Search on Azure]( - Vector search on Azure using Azure Cache for Redis and Azure OpenAI ## More Resources For more information on how to use Redis as a vector database, check out the following resources: - [RedisVL Documentation]( - Documentation for the Redis Vector Library Client - [Redis Vector Similarity Docs]( - Redis official docs for Vector Search. - [Redis-py Search Docs]( - Documentation for redis-py client library - [Vector Similarity Search: From Basics to Production]( - Introductory blog post to VSS and Redis as a VectorDB. ## Install Redis Python Client Redis-py is the officially supported client by Redis. Recently released is the RedisVL client which is purpose-built for the Vector Database use cases. Both can be installed with pip. ```bash pip install redis redisvl openai tiktoken ``` We want to use `OpenAIEmbeddings` so we have to get the OpenAI API Key. ```python import getpass import os os.environ[""OPENAI_API_KEY""] = getpass.getpass(""OpenAI API Key:"") ``` ```python from langchain.embeddings import OpenAIEmbeddings embeddings = OpenAIEmbeddings() ``` ## Sample Data First we will describe some sample data so that the various attributes of the Redis vector store can be demonstrated. ```python metadata = [ { ""user"": ""john"", ""age"": 18, ""job"": ""engineer"", ""credit_score"": ""high"", }, { ""user"": ""derrick"", ""age"": 45, ""job"": ""doctor"", ""credit_score"": ""low"", }, { ""user"": ""nancy"", ""age"": 94, ""job"": ""doctor"", ""credit_score"": ""high"", }, { ""user"": ""tyler"", ""age"": 100, ""job"": ""engineer"", ""credit_score"": ""high"", }, { ""user"": ""joe"", ""age"": 35, ""job"": ""dentist"", ""credit_score"": ""medium"", }, ] texts = [""foo"", ""foo"", ""foo"", ""bar"", ""bar""] ``` ## Initializing Redis To locally deploy Redis, run: ```console docker run -d -p 6379:6379 -p 8001:8001 redis/redis-stack:latest ``` If things are running correctly you should see a nice Redis UI at See the [Deployment Options](#deployment-options) section above for other ways to deploy. The Redis VectorStore instance can be initialized in a number of ways. There are multiple class methods that can be used to initialize a Redis VectorStore instance. - `Redis.__init__` - Initialize directly - `Redis.from_documents` - Initialize from a list of `Langchain.docstore.Document` objects - `Redis.from_texts` - Initialize from a list of texts (optionally with metadata) - `Redis.from_texts_return_keys` - Initialize from a list of texts (optionally with metadata) and return the keys - `Redis.from_existing_index` - Initialize from an existing Redis index Below we will use the `Redis.from_texts` method. ```python from langchain.vectorstores.redis import Redis rds = Redis.from_texts( texts, embeddings, metadatas=metadata, redis_url=""redis://localhost:6379"", index_name=""users"", ) ``` ```python rds.index_name ``` ```text \'users\' ``` ## Inspecting the Created Index Once the `Redis` VectorStore object has been constructed, an index will have been created in Redis if it did not already exist. The index can be inspected with both the `rvl`and the `redis-cli` command line tool. If you installed `redisvl` above, you can use the `rvl` command line tool to inspect the index. ```bash # assumes you\'re running Redis locally (use --host, --port, --password, --username, to change this) rvl index listall ``` ```text 16:58:26 [RedisVL] INFO Indices: 16:58:26 [RedisVL] INFO 1. users ``` The `Redis` VectorStore implementation will attempt to generate index schema (fields for filtering) for any metadata passed through the `from_texts`, `from_texts_return_keys`, and `from_documents` methods. This way, whatever metadata is passed will be indexed into the Redis search index allowing for filtering on those fields. Below we show what fields were created from the metadata we defined above ```bash rvl index info -i users ``` ```text Index Information: Index Name Storage Type Prefixes Index Options Indexing users HASH [\'doc:users\'] [] 0 Index Fields: Name Attribute Type Field Option Option Value user user TEXT WEIGHT 1 job job TEXT WEIGHT 1 credit_score credit_score TEXT WEIGHT 1 content content TEXT WEIGHT 1 age age NUMERIC content_vector content_vector VECTOR ``` ```bash rvl stats -i users ``` ```text Statistics: Stat Key Value num_docs 5 num_terms 15 max_doc_id 5 num_records 33 percent_indexed 1 hash_indexing_failures 0 number_of_uses 4 bytes_per_record_avg 4.60606 doc_table_size_mb 0.000524521 inverted_sz_mb 0.000144958 key_table_size_mb 0.000193596 offset_bits_per_record_avg 8 offset_vectors_sz_mb 2.19345e-05 offsets_per_term_avg 0.69697 records_per_doc_avg 6.6 sortable_values_size_mb 0 total_indexing_time 0.32 total_inverted_index_blocks 16 vector_index_sz_mb 6.0126 ``` It\'s important to note that we have not specified that the `user`, `job`, `credit_score` and `age` in the metadata should be fields within the index, this is because the `Redis` VectorStore object automatically generate the index schema from the passed metadata. For more information on the generation of index fields, see the API documentation. ## Querying\u200b There are multiple ways to query the `Redis` VectorStore implementation based on what use case you have: - `similarity_search`: Find the most similar vectors to a given vector. - `similarity_search_with_score`: Find the most similar vectors to a given vector and return the vector distance - `similarity_search_limit_score`: Find the most similar vectors to a given vector and limit the number of results to the `score_threshold` - `similarity_search_with_relevance_scores`: Find the most similar vectors to a given vector and return the vector similarities - `max_marginal_relevance_search`: Find the most similar vectors to a given vector while also optimizing for diversity ```python results = rds.similarity_search(""foo"") print(results[0].page_content) ``` ```text foo ``` ```python # return metadata results = rds.similarity_search(""foo"", k=3) meta = results[1].metadata print(""Key of the document in Redis: "", meta.pop(""id"")) print(""Metadata of the document: "", meta) ``` ```text Key of the document in Redis: doc:users:a70ca43b3a4e4168bae57c78753a200f Metadata of the document: {\'user\': \'derrick\', \'job\': \'doctor\', \'credit_score\': \'low\', \'age\': \'45\'} ``` ```python # with scores (distances) results = rds.similarity_search_with_score(""foo"", k=5) for result in results: print(f""Content: {result[0].page_content} --- Score: {result[1]}"") ``` ```text Content: foo --- Score: 0.0 Content: foo --- Score: 0.0 Content: foo --- Score: 0.0 Content: bar --- Score: 0.1566 Content: bar --- Score: 0.1566 ``` ```python # limit the vector distance that can be returned results = rds.similarity_search_with_score(""foo"", k=5, distance_threshold=0.1) for result in results: print(f""Content: {result[0].page_content} --- Score: {result[1]}"") ``` ```text Content: foo --- Score: 0.0 Content: foo --- Score: 0.0 Content: foo --- Score: 0.0 ``` ```python # with scores results = rds.similarity_search_with_relevance_scores(""foo"", k=5) for result in results: print(f""Content: {result[0].page_content} --- Similiarity: {result[1]}"") ``` ```text Content: foo --- Similiarity: 1.0 Content: foo --- Similiarity: 1.0 Content: foo --- Similiarity: 1.0 Content: bar --- Similiarity: 0.8434 Content: bar --- Similiarity: 0.8434 ``` ```python # limit scores (similarities have to be over .9) results = rds.similarity_search_with_relevance_scores(""foo"", k=5, score_threshold=0.9) for result in results: print(f""Content: {result[0].page_content} --- Similarity: {result[1]}"") ``` ```text Content: foo --- Similarity: 1.0 Content: foo --- Similarity: 1.0 Content: foo --- Similarity: 1.0 ``` ```python # you can also add new documents as follows new_document = [""baz""] new_metadata = [{""user"": ""sam"", ""age"": 50, ""job"": ""janitor"", ""credit_score"": ""high""}] # both the document and metadata must be lists rds.add_texts(new_document, new_metadata) ``` ```text [\'doc:users:b9c71d62a0a34241a37950b448dafd38\'] ``` ```python # now query the new document results = rds.similarity_search(""baz"", k=3) print(results[0].metadata) ``` ```text {\'id\': \'doc:users:b9c71d62a0a34241a37950b448dafd38\', \'user\': \'sam\', \'job\': \'janitor\', \'credit_score\': \'high\', \'age\': \'50\'} ``` ```python # use maximal marginal relevance search to diversify results results = rds.max_marginal_relevance_search(""foo"") ``` ```python # the lambda_mult parameter controls the diversity of the results, the lower the more diverse results = rds.max_marginal_relevance_search(""foo"", lambda_mult=0.1) ``` ## Connect to an Existing Index\u200b In order to have the same metadata indexed when using the `Redis` VectorStore. You will need to have the same `index_schema` passed in either as a path to a yaml file or as a dictionary. The following shows how to obtain the schema from an index and connect to an existing index. ```python # write the schema to a yaml file rds.write_schema(""redis_schema.yaml"") ``` The schema file for this example should look something like: ```yaml numeric: - name: age no_index: false sortable: false text: - name: user no_index: false no_stem: false sortable: false weight: 1 withsuffixtrie: false - name: job no_index: false no_stem: false sortable: false weight: 1 withsuffixtrie: false - name: credit_score no_index: false no_stem: false sortable: false weight: 1 withsuffixtrie: false - name: content no_index: false no_stem: false sortable: false weight: 1 withsuffixtrie: false vector: - algorithm: FLAT block_size: 1000 datatype: FLOAT32 dims: 1536 distance_metric: COSINE initial_cap: 20000 name: content_vector ``` **Notice**, this include **all** possible fields for the schema. You can remove any fields that you don\'t need. ```python # now we can connect to our existing index as follows new_rds = Redis.from_existing_index( embeddings, index_name=""users"", redis_url=""redis://localhost:6379"", schema=""redis_schema.yaml"", ) results = new_rds.similarity_search(""foo"", k=3) print(results[0].metadata) ``` ```text {\'id\': \'doc:users:8484c48a032d4c4cbe3cc2ed6845fabb\', \'user\': \'john\', \'job\': \'engineer\', \'credit_score\': \'high\', \'age\': \'18\'} ``` ```python # see the schemas are the same new_rds.schema == rds.schema ``` ```text True ``` ## Custom Metadata Indexing\u200b In some cases, you may want to control what fields the metadata maps to. For example, you may want the `credit_score` field to be a categorical field instead of a text field (which is the default behavior for all string fields). In this case, you can use the `index_schema` parameter in each of the initialization methods above to specify the schema for the index. Custom index schema can either be passed as a dictionary or as a path to a yaml file. All arguments in the schema have defaults besides the name, so you can specify only the fields you want to change. All the names correspond to the snake/lowercase versions of the arguments you would use on the command line with `redis-cli` or in `redis-py`. For more on the arguments for each field, see the [documentation]( The below example shows how to specify the schema for the `credit_score` field as a Tag (categorical) field instead of a text field. ```yaml # index_schema.yml tag: - name: credit_score text: - name: user - name: job numeric: - name: age ``` In Python this would look like: ```python index_schema = { ""tag"": [{""name"": ""credit_score""}], ""text"": [{""name"": ""user""}, {""name"": ""job""}], ""numeric"": [{""name"": ""age""}], } ``` Notice that only the `name` field needs to be specified. All other fields have defaults. ```python # create a new index with the new schema defined above index_schema = { ""tag"": [{""name"": ""credit_score""}], ""text"": [{""name"": ""user""}, {""name"": ""job""}], ""numeric"": [{""name"": ""age""}], } rds, keys = Redis.from_texts_return_keys( texts, embeddings, metadatas=metadata, redis_url=""redis://localhost:6379"", index_name=""users_modified"", index_schema=index_schema, # pass in the new index schema ) ``` ```text `index_schema` does not match generated metadata schema. If you meant to manually override the schema, please ignore this message. index_schema: {\'tag\': [{\'name\': \'credit_score\'}], \'text\': [{\'name\': \'user\'}, {\'name\': \'job\'}], \'numeric\': [{\'name\': \'age\'}]} generated_schema: {\'text\': [{\'name\': \'user\'}, {\'name\': \'job\'}, {\'name\': \'credit_score\'}], \'numeric\': [{\'name\': \'age\'}], \'tag\': []} ``` The above warning is meant to notify users when they are overriding the default behavior. Ignore it if you are intentionally overriding the behavior. ## Hybrid Filtering\u200b With the Redis Filter Expression language built into langchain, you can create arbitrarily long chains of hybrid filters that can be used to filter your search results. The expression language is derived from the [RedisVL Expression Syntax]( and is designed to be easy to use and understand. The following are the available filter types: - `RedisText`: Filter by full-text search against metadata fields. Supports exact, fuzzy, and wildcard matching. - `RedisNum`: Filter by numeric range against metadata fields. - `RedisTag`: Filter by exact match against string based categorical metadata fields. Multiple tags can be specified like ""tag1,tag2,tag3"". The following are examples of utilizing these filters. ```python from langchain.vectorstores.redis import RedisText, RedisNum, RedisTag # exact matching has_high_credit = RedisTag(""credit_score"") == ""high"" does_not_have_high_credit = RedisTag(""credit_score"") != ""low"" # fuzzy matching job_starts_with_eng = RedisText(""job"") % ""eng*"" job_is_engineer = RedisText(""job"") == ""engineer"" job_is_not_engineer = RedisText(""job"") != ""engineer"" # numeric filtering age_is_18 = RedisNum(""age"") == 18 age_is_not_18 = RedisNum(""age"") != 18 age_is_greater_than_18 = RedisNum(""age"") > 18 age_is_less_than_18 = RedisNum(""age"") < 18 age_is_greater_than_or_equal_to_18 = RedisNum(""age"") >= 18 age_is_less_than_or_equal_to_18 = RedisNum(""age"") <= 18 ``` The `RedisFilter` class can be used to simplify the import of these filters as follows ```python from langchain.vectorstores.redis import RedisFilter # same examples as above has_high_credit = RedisFilter.tag(""credit_score"") == ""high"" does_not_have_high_credit = RedisFilter.num(""age"") > 8 job_starts_with_eng = RedisFilter.text(""job"") % ""eng*"" ``` The following are examples of using hybrid filter for search ```python from langchain.vectorstores.redis import RedisText is_engineer = RedisText(""job"") == ""engineer"" results = rds.similarity_search(""foo"", k=3, filter=is_engineer) print(""Job:"", results[0].metadata[""job""]) print(""Engineers in the dataset:"", len(results)) ``` ```text Job: engineer Engineers in the dataset: 2 ``` ```python # fuzzy match starts_with_doc = RedisText(""job"") % ""doc*"" results = rds.similarity_search(""foo"", k=3, filter=starts_with_doc) for result in results: print(""Job:"", result.metadata[""job""]) print(""Jobs in dataset that start with \'doc\':"", len(results)) ``` ```text Job: doctor Job: doctor Jobs in dataset that start with \'doc\': 2 ``` ```python from langchain.vectorstores.redis import RedisNum is_over_18 = RedisNum(""age"") > 18 is_under_99 = RedisNum(""age"") < 99 age_range = is_over_18 & is_under_99 results = rds.similarity_search(""foo"", filter=age_range) for result in results: print(""User:"", result.metadata[""user""], ""is"", result.metadata[""age""]) ``` ```text User: derrick is 45 User: nancy is 94 User: joe is 35 ``` ```python # make sure to use parenthesis around FilterExpressions # if initializing them while constructing them age_range = (RedisNum(""age"") > 18) & (RedisNum(""age"") < 99) results = rds.similarity_search(""foo"", filter=age_range) for result in results: print(""User:"", result.metadata[""user""], ""is"", result.metadata[""age""]) ``` ```text User: derrick is 45 User: nancy is 94 User: joe is 35 ``` ## Redis as Retriever\u200b Here we go over different options for using the vector store as a retriever. There are three different search methods we can use to do retrieval. By default, it will use semantic similarity. ```python query = ""foo"" results = rds.similarity_search_with_score(query, k=3, return_metadata=True) for result in results: print(""Content:"", result[0].page_content, "" --- Score: "", result[1]) ``` ```text Content: foo --- Score: 0.0 Content: foo --- Score: 0.0 Content: foo --- Score: 0.0 ``` ```python retriever = rds.as_retriever(search_type=""similarity"", search_kwargs={""k"": 4}) ``` ```python docs = retriever.get_relevant_documents(query) docs ``` ```text [Document(page_content=\'foo\', metadata={\'id\': \'doc:users_modified:988ecca7574048e396756efc0e79aeca\', \'user\': \'john\', \'job\': \'engineer\', \'credit_score\': \'high\', \'age\': \'18\'}), Document(page_content=\'foo\', metadata={\'id\': \'doc:users_modified:009b1afeb4084cc6bdef858c7a99b48e\', \'user\': \'derrick\', \'job\': \'doctor\', \'credit_score\': \'low\', \'age\': \'45\'}), Document(page_content=\'foo\', metadata={\'id\': \'doc:users_modified:7087cee9be5b4eca93c30fbdd09a2731\', \'user\': \'nancy\', \'job\': \'doctor\', \'credit_score\': \'high\', \'age\': \'94\'}), Document(page_content=\'bar\', metadata={\'id\': \'doc:users_modified:01ef6caac12b42c28ad870aefe574253\', \'user\': \'tyler\', \'job\': \'engineer\', \'credit_score\': \'high\', \'age\': \'100\'})] ``` There is also the `similarity_distance_threshold` retriever which allows the user to specify the vector distance ```python retriever = rds.as_retriever( search_type=""similarity_distance_threshold"", search_kwargs={""k"": 4, ""distance_threshold"": 0.1}, ) ``` ```python docs = retriever.get_relevant_documents(query) docs ``` ```text [Document(page_content=\'foo\', metadata={\'id\': \'doc:users_modified:988ecca7574048e396756efc0e79aeca\', \'user\': \'john\', \'job\': \'engineer\', \'credit_score\': \'high\', \'age\': \'18\'}), Document(page_content=\'foo\', metadata={\'id\': \'doc:users_modified:009b1afeb4084cc6bdef858c7a99b48e\', \'user\': \'derrick\', \'job\': \'doctor\', \'credit_score\': \'low\', \'age\': \'45\'}), Document(page_content=\'foo\', metadata={\'id\': \'doc:users_modified:7087cee9be5b4eca93c30fbdd09a2731\', \'user\': \'nancy\', \'job\': \'doctor\', \'credit_score\': \'high\', \'age\': \'94\'})] ``` Lastly, the `similarity_score_threshold` allows the user to define the minimum score for similar documents ```python retriever = rds.as_retriever( search_type=""similarity_score_threshold"", search_kwargs={""score_threshold"": 0.9, ""k"": 10}, ) ``` ```python retriever.get_relevant_documents(""foo"") ``` ```text [Document(page_content=\'foo\', metadata={\'id\': \'doc:users_modified:988ecca7574048e396756efc0e79aeca\', \'user\': \'john\', \'job\': \'engineer\', \'credit_score\': \'high\', \'age\': \'18\'}), Document(page_content=\'foo\', metadata={\'id\': \'doc:users_modified:009b1afeb4084cc6bdef858c7a99b48e\', \'user\': \'derrick\', \'job\': \'doctor\', \'credit_score\': \'low\', \'age\': \'45\'}), Document(page_content=\'foo\', metadata={\'id\': \'doc:users_modified:7087cee9be5b4eca93c30fbdd09a2731\', \'user\': \'nancy\', \'job\': \'doctor\', \'credit_score\': \'high\', \'age\': \'94\'})] ``` ```python retriever = rds.as_retriever( search_type=""mmr"", search_kwargs={""fetch_k"": 20, ""k"": 4, ""lambda_mult"": 0.1} ) ``` ```python retriever.get_relevant_documents(""foo"") ``` ```text [Document(page_content=\'foo\', metadata={\'id\': \'doc:users:8f6b673b390647809d510112cde01a27\', \'user\': \'john\', \'job\': \'engineer\', \'credit_score\': \'high\', \'age\': \'18\'}), Document(page_content=\'bar\', metadata={\'id\': \'doc:users:93521560735d42328b48c9c6f6418d6a\', \'user\': \'tyler\', \'job\': \'engineer\', \'credit_score\': \'high\', \'age\': \'100\'}), Document(page_content=\'foo\', metadata={\'id\': \'doc:users:125ecd39d07845eabf1a699d44134a5b\', \'user\': \'nancy\', \'job\': \'doctor\', \'credit_score\': \'high\', \'age\': \'94\'}), Document(page_content=\'foo\', metadata={\'id\': \'doc:users:d6200ab3764c466082fde3eaab972a2a\', \'user\': \'derrick\', \'job\': \'doctor\', \'credit_score\': \'low\', \'age\': \'45\'})] ``` # Delete keys To delete your entries you have to address them by their keys. ```python Redis.delete(keys, redis_url=""redis://localhost:6379"") ``` ```text True ``` ```python # delete the indices too Redis.drop_index( index_name=""users"", delete_documents=True, redis_url=""redis://localhost:6379"" ) Redis.drop_index( index_name=""users_modified"", delete_documents=True, redis_url=""redis://localhost:6379"", ) ``` ```text True ``` ### Redis connection Url examples\u200b Valid Redis Url scheme are: 1. `redis://` - Connection to Redis standalone, unencrypted 2. `rediss://` - Connection to Redis standalone, with TLS encryption 3. `redis+sentinel://` - Connection to Redis server via Redis Sentinel, unencrypted 4. `rediss+sentinel://` - Connection to Redis server via Redis Sentinel, booth connections with TLS encryption More information about additional connection parameter can be found in the redis-py documentation at [ ```python # connection to redis standalone at localhost, db 0, no password redis_url = ""redis://localhost:6379"" # connection to host ""redis"" port 7379 with db 2 and password ""secret"" (old style authentication scheme without username / pre 6.x) redis_url = ""redis://:secret@redis:7379/2"" # connection to host redis on default port with user ""joe"", pass ""secret"" using redis version 6+ ACLs redis_url = ""redis://joe:secret@redis/0"" # connection to sentinel at localhost with default group mymaster and db 0, no password redis_url = ""redis+sentinel://localhost:26379"" # connection to sentinel at host redis with default port 26379 and user ""joe"" with password ""secret"" with default group mymaster and db 0 redis_url = ""redis+sentinel://joe:secret@redis"" # connection to sentinel, no auth with sentinel monitoring group ""zone-1"" and database 2 redis_url = ""redis+sentinel://redis:26379/zone-1/2"" # connection to redis standalone at localhost, db 0, no password but with TLS support redis_url = ""rediss://localhost:6379"" # connection to redis sentinel at localhost and default port, db 0, no password # but with TLS support for booth Sentinel and Redis server redis_url = ""rediss+sentinel://localhost"" ``` - [What is Redis?](#what-is-redis) - [Redis as a Vector Database](#redis-as-a-vector-database) - [Clients](#clients) - [Deployment Options](#deployment-options) - [Examples](#examples) - [More Resources](#more-resources) - [Install Redis Python Client](#install-redis-python-client) - [Sample Data](#sample-data) - [Initializing Redis](#initializing-redis) - [Inspecting the Created Index](#inspecting-the-created-index) - [Querying](#querying) - [Connect to an Existing Index](#connect-to-an-existing-index) - [Custom Metadata Indexing](#custom-metadata-indexing) - [Hybrid Filtering](#hybrid-filtering) - [Redis as Retriever](#redis-as-retriever)- [Redis connection Url examples](#redis-connection-url-examples)']","To search and filter metadata in a Redis vectorstore using LangChain, you can utilize the `filter` parameter in various search methods. Here's how you can do it:

1. **Define Filters**: Use the provided filter classes like `RedisText`, `RedisNum`, and `RedisTag` to create filters based on your metadata fields.

   ```python
   from langchain.vectorstores.redis import RedisText, RedisNum, RedisTag

   # Example filters
   has_high_credit = RedisTag(""credit_score"") == ""high""
   age_is_over_18 = RedisNum(""age"") > 18
   ```

2. **Combine Filters**: You can combine multiple filters using logical operators.

   ```python
   from langchain.vectorstores.redis import RedisFilter

   # Combining filters
   is_over_18_and_high_credit = (RedisFilter.num(""age"") > 18) & (RedisFilter.tag(""credit_score"") == ""high"")
   ```

3. **Apply Filters in Search**: Pass the filter to the search method to retrieve documents that match the filter criteria.

   ```python
   results = rds.similarity_search(""foo"", k=3, filter=is_over_18_and_high_credit)
   for result in results:
       print(result.metadata)
   ```

4. **Hybrid Filtering**: You can also use a combination of text, numeric, and tag filters in a single search query.

   ```python
   from langchain.vectorstores.redis import RedisText

   # Example hybrid filter
   starts_with_doc = RedisText(""job"") % ""doc*""
   is_over_18 = RedisFilter.num(""age"") > 18

   results = rds.similarity_search(""foo"", k=3, filter=(starts_with_doc & is_over_18))
   for result in results:
       print(result.metadata)
   ```

By using these filter classes and combining them as needed, you can effectively search and filter metadata in your Redis vectorstore.","To perform a similarity search with metadata filtering in LangChain using the Redis vectorstore, you can use Redis filter expressions. The following filter operations are available: RedisText, RedisNum, and RedisTag. Use these when calling `similarity_search()`.",0.9999999999666667,1.0,,0.09999764719722204,0.13970588235294118
70,how do I control the maximum number requests that can be made at the same time when making batch calls?,"['langchain.llms.aleph_alpha.AlephAlpha LangChain 0.0.337 API Experimental Python Docs Toggle Menu PrevUp Next LangChain 0.0.337 langchain.llms.aleph_alpha.AlephAlpha langchain.llms.aleph_alpha.AlephAlpha class langchain.llms.aleph_alpha.AlephAlpha[source] Bases: LLM Aleph Alpha large language models. To use, you should have the aleph_alpha_client python package installed, and the environment variable ALEPH_ALPHA_API_KEY set with your API key, or pass it as a named parameter to the constructor. Parameters are explained more in depth here: Example from langchain.llms import AlephAlpha aleph_alpha = AlephAlpha(aleph_alpha_api_key=""my-api-key"") Create a new model by parsing and validating input data from keyword arguments. Raises ValidationError if the input data cannot be parsed to form a valid model. param aleph_alpha_api_key: Optional[str] = None API key for Aleph Alpha API. param best_of: Optional[int] = None returns the one with the best of results (highest log probability per token) param cache: Optional[bool] = None param callback_manager: Optional[BaseCallbackManager] = None param callbacks: Callbacks = None param completion_bias_exclusion: Optional[Sequence[str]] = None param completion_bias_exclusion_first_token_only: bool = False Only consider the first token for the completion_bias_exclusion. param completion_bias_inclusion: Optional[Sequence[str]] = None param completion_bias_inclusion_first_token_only: bool = False param contextual_control_threshold: Optional[float] = None If set to None, attention control parameters only apply to those tokens that have explicitly been set in the request. If set to a non-None value, control parameters are also applied to similar tokens. param control_log_additive: Optional[bool] = True True: apply control by adding the log(control_factor) to attention scores. False: (attention_scores - - attention_scores.min(-1)) * control_factor param disable_optimizations: Optional[bool] = False param echo: bool = False Echo the prompt in the completion. param frequency_penalty: float = 0.0 Penalizes repeated tokens according to frequency. param host: str = \' The hostname of the API host. The default one is param hosting: Optional[str] = None Determines in which datacenters the request may be processed. You can either set the parameter to aleph-alpha or omit it (defaulting to None). Not setting this value, or setting it to None, gives us maximal flexibility in processing your request in our own datacenters and on servers hosted with other providers. Choose this option for maximal availability. Setting it to aleph-alpha allows us to only process the request in our own datacenters. Choose this option for maximal data privacy. param log_probs: Optional[int] = None Number of top log probabilities to be returned for each generated token. param logit_bias: Optional[Dict[int, float]] = None The logit bias allows to influence the likelihood of generating tokens. param maximum_tokens: int = 64 The maximum number of tokens to be generated. param metadata: Optional[Dict[str, Any]] = None Metadata to add to the run trace. param minimum_tokens: Optional[int] = 0 Generate at least this number of tokens. param model: Optional[str] = \'luminous-base\' Model name to use. param n: int = 1 How many completions to generate for each prompt. param nice: bool = False Setting this to True, will signal to the API that you intend to be nice to other users by de-prioritizing your request below concurrent ones. param penalty_bias: Optional[str] = None Penalty bias for the completion. param penalty_exceptions: Optional[List[str]] = None List of strings that may be generated without penalty, regardless of other penalty settings param penalty_exceptions_include_stop_sequences: Optional[bool] = None Should stop_sequences be included in penalty_exceptions. param presence_penalty: float = 0.0 Penalizes repeated tokens. param raw_completion: bool = False Force the raw completion of the model to be returned. param repetition_penalties_include_completion: bool = True Flag deciding whether presence penalty or frequency penalty are updated from the completion. param repetition_penalties_include_prompt: Optional[bool] = False Flag deciding whether presence penalty or frequency penalty are updated from the prompt. param request_timeout_seconds: int = 305 Client timeout that will be set for HTTP requests in the requests library\'s API calls. Server will close all requests after 300 seconds with an internal server error. param sequence_penalty: float = 0.0 param sequence_penalty_min_length: int = 2 param stop_sequences: Optional[List[str]] = None Stop sequences to use. param tags: Optional[List[str]] = None Tags to add to the run trace. param temperature: float = 0.0 A non-negative float that tunes the degree of randomness in generation. param tokens: Optional[bool] = False return tokens of completion. param top_k: int = 0 Number of most likely tokens to consider at each step. param top_p: float = 0.0 Total probability mass of tokens to consider at each step. param total_retries: int = 8 The number of retries made in case requests fail with certain retryable status codes. If the last retry fails a corresponding exception is raised. Note, that between retries an exponential backoff is applied, starting with 0.5 s after the first retry and doubling for each retry made. So with the default setting of 8 retries a total wait time of 63.5 s is added between the retries. param use_multiplicative_frequency_penalty: bool = False param use_multiplicative_presence_penalty: Optional[bool] = False Flag deciding whether presence penalty is applied multiplicatively (True) or additively (False). param use_multiplicative_sequence_penalty: bool = False param verbose: bool [Optional] Whether to print out response text. __call__(prompt: str, stop: Optional[List[str]] = None, callbacks: Optional[Union[List[BaseCallbackHandler], BaseCallbackManager]] = None, *, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, **kwargs: Any)  str Check Cache and run the LLM on the given prompt and input. async abatch(inputs: List[Union[PromptValue, str, List[BaseMessage]]], config: Optional[Union[RunnableConfig, List[RunnableConfig]]] = None, *, return_exceptions: bool = False, **kwargs: Any)  List[str] Default implementation runs ainvoke in parallel using asyncio.gather. The default implementation of batch works well for IO bound runnables. Subclasses should override this method if they can batch more efficiently; e.g., if the underlying runnable uses an API which supports a batch mode. async agenerate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Union[List[BaseCallbackHandler], BaseCallbackManager, None, List[Optional[Union[List[BaseCallbackHandler], BaseCallbackManager]]]] = None, *, tags: Optional[Union[List[str], List[List[str]]]] = None, metadata: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None, run_name: Optional[Union[str, List[str]]] = None, **kwargs: Any)  LLMResult Run the LLM on the given prompt and input. async agenerate_prompt(prompts: List[PromptValue], stop: Optional[List[str]] = None, callbacks: Union[List[BaseCallbackHandler], BaseCallbackManager, None, List[Optional[Union[List[BaseCallbackHandler], BaseCallbackManager]]]] = None, **kwargs: Any)  LLMResult Asynchronously pass a sequence of prompts and return model generations. This method should make use of batched calls for models that expose a batched API. Use this method when you want to: take advantage of batched calls, need more output from the model than just the top generated value, are building chains that are agnostic to the underlying language modeltype (e.g., pure text completion models vs chat models). Parameters prompts  List of PromptValues. A PromptValue is an object that can be converted to match the format of any language model (string for pure text generation models and BaseMessages for chat models). stop  Stop words to use when generating. Model output is cut off at the first occurrence of any of these substrings. callbacks  Callbacks to pass through. Used for executing additional functionality, such as logging or streaming, throughout generation. **kwargs  Arbitrary additional keyword arguments. These are usually passed to the model provider API call. Returns An LLMResult, which contains a list of candidate Generations for each inputprompt and additional model provider-specific output. async ainvoke(input: Union[PromptValue, str, List[BaseMessage]], config: Optional[RunnableConfig] = None, *, stop: Optional[List[str]] = None, **kwargs: Any)  str Default implementation of ainvoke, calls invoke from a thread. The default implementation allows usage of async code even if the runnable did not implement a native async version of invoke. Subclasses should override this method if they can run asynchronously. async apredict(text: str, *, stop: Optional[Sequence[str]] = None, **kwargs: Any)  str Asynchronously pass a string to the model and return a string prediction. Use this method when calling pure text generation models and only the topcandidate generation is needed. Parameters text  String input to pass to the model. stop  Stop words to use when generating. Model output is cut off at the first occurrence of any of these substrings. **kwargs  Arbitrary additional keyword arguments. These are usually passed to the model provider API call. Returns Top model prediction as a string. async apredict_messages(messages: List[BaseMessage], *, stop: Optional[Sequence[str]] = None, **kwargs: Any)  BaseMessage Asynchronously pass messages to the model and return a message prediction. Use this method when calling chat models and only the topcandidate generation is needed. Parameters messages  A sequence of chat messages corresponding to a single model input. stop  Stop words to use when generating. Model output is cut off at the first occurrence of any of these substrings. **kwargs  Arbitrary additional keyword arguments. These are usually passed to the model provider API call. Returns Top model prediction as a message. async astream(input: Union[PromptValue, str, List[BaseMessage]], config: Optional[RunnableConfig] = None, *, stop: Optional[List[str]] = None, **kwargs: Any)  AsyncIterator[str] Default implementation of astream, which calls ainvoke. Subclasses should override this method if they support streaming output. async astream_log(input: Any, config: Optional[RunnableConfig] = None, *, diff: bool = True, include_names: Optional[Sequence[str]] = None, include_types: Optional[Sequence[str]] = None, include_tags: Optional[Sequence[str]] = None, exclude_names: Optional[Sequence[str]] = None, exclude_types: Optional[Sequence[str]] = None, exclude_tags: Optional[Sequence[str]] = None, **kwargs: Optional[Any])  Union[AsyncIterator[RunLogPatch], AsyncIterator[RunLog]] Stream all output from a runnable, as reported to the callback system. This includes all inner runs of LLMs, Retrievers, Tools, etc. Output is streamed as Log objects, which include a list of jsonpatch ops that describe how the state of the run has changed in each step, and the final state of the run. The jsonpatch ops can be applied in order to construct state. async atransform(input: AsyncIterator[Input], config: Optional[RunnableConfig] = None, **kwargs: Optional[Any])  AsyncIterator[Output] Default implementation of atransform, which buffers input and calls astream. Subclasses should override this method if they can start producing output while input is still being generated. batch(inputs: List[Union[PromptValue, str, List[BaseMessage]]], config: Optional[Union[RunnableConfig, List[RunnableConfig]]] = None, *, return_exceptions: bool = False, **kwargs: Any)  List[str] Default implementation runs invoke in parallel using a thread pool executor. The default implementation of batch works well for IO bound runnables. Subclasses should override this method if they can batch more efficiently; e.g., if the underlying runnable uses an API which supports a batch mode. bind(**kwargs: Any)  Runnable[Input, Output] Bind arguments to a Runnable, returning a new Runnable. config_schema(*, include: Optional[Sequence[str]] = None)  Type[BaseModel] The type of config this runnable accepts specified as a pydantic model. To mark a field as configurable, see the configurable_fields and configurable_alternatives methods. Parameters include  A list of fields to include in the config schema. Returns A pydantic model that can be used to validate config. configurable_alternatives(which: ConfigurableField, default_key: str = \'default\', **kwargs: Union[Runnable[Input, Output], Callable[[], Runnable[Input, Output]]])  RunnableSerializable[Input, Output] configurable_fields(**kwargs: Union[ConfigurableField, ConfigurableFieldSingleOption, ConfigurableFieldMultiOption])  RunnableSerializable[Input, Output] classmethod construct(_fields_set: Optional[SetStr] = None, **values: Any)  Model Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = \'allow\' was set since it adds all passed values copy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False)  Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters include  fields to include in new model exclude  fields to exclude from new model, as with values this takes precedence over include update  values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data deep  set to True to make a deep copy of the model Returns new model instance dict(**kwargs: Any)  Dict Return a dictionary of the LLM. classmethod from_orm(obj: Any)  Model generate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Union[List[BaseCallbackHandler], BaseCallbackManager, None, List[Optional[Union[List[BaseCallbackHandler], BaseCallbackManager]]]] = None, *, tags: Optional[Union[List[str], List[List[str]]]] = None, metadata: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None, run_name: Optional[Union[str, List[str]]] = None, **kwargs: Any)  LLMResult Run the LLM on the given prompt and input. generate_prompt(prompts: List[PromptValue], stop: Optional[List[str]] = None, callbacks: Union[List[BaseCallbackHandler], BaseCallbackManager, None, List[Optional[Union[List[BaseCallbackHandler], BaseCallbackManager]]]] = None, **kwargs: Any)  LLMResult Pass a sequence of prompts to the model and return model generations. This method should make use of batched calls for models that expose a batched API. Use this method when you want to: take advantage of batched calls, need more output from the model than just the top generated value, are building chains that are agnostic to the underlying language modeltype (e.g., pure text completion models vs chat models). Parameters prompts  List of PromptValues. A PromptValue is an object that can be converted to match the format of any language model (string for pure text generation models and BaseMessages for chat models). stop  Stop words to use when generating. Model output is cut off at the first occurrence of any of these substrings. callbacks  Callbacks to pass through. Used for executing additional functionality, such as logging or streaming, throughout generation. **kwargs  Arbitrary additional keyword arguments. These are usually passed to the model provider API call. Returns An LLMResult, which contains a list of candidate Generations for each inputprompt and additional model provider-specific output. get_input_schema(config: Optional[RunnableConfig] = None)  Type[BaseModel] Get a pydantic model that can be used to validate input to the runnable. Runnables that leverage the configurable_fields and configurable_alternatives methods will have a dynamic input schema that depends on which configuration the runnable is invoked with. This method allows to get an input schema for a specific configuration. Parameters config  A config to use when generating the schema. Returns A pydantic model that can be used to validate input. classmethod get_lc_namespace()  List[str] Get the namespace of the langchain object. For example, if the class is langchain.llms.openai.OpenAI, then the namespace is [langchain, llms, openai] get_num_tokens(text: str)  int Get the number of tokens present in the text. Useful for checking if an input will fit in a model\'s context window. Parameters text  The string input to tokenize. Returns The integer number of tokens in the text. get_num_tokens_from_messages(messages: List[BaseMessage])  int Get the number of tokens in the messages. Useful for checking if an input will fit in a model\'s context window. Parameters messages  The message inputs to tokenize. Returns The sum of the number of tokens across the messages. get_output_schema(config: Optional[RunnableConfig] = None)  Type[BaseModel] Get a pydantic model that can be used to validate output to the runnable. Runnables that leverage the configurable_fields and configurable_alternatives methods will have a dynamic output schema that depends on which configuration the runnable is invoked with. This method allows to get an output schema for a specific configuration. Parameters config  A config to use when generating the schema. Returns A pydantic model that can be used to validate output. get_token_ids(text: str)  List[int] Return the ordered ids of the tokens in a text. Parameters text  The string input to tokenize. Returns A list of ids corresponding to the tokens in the text, in order they occurin the text. invoke(input: Union[PromptValue, str, List[BaseMessage]], config: Optional[RunnableConfig] = None, *, stop: Optional[List[str]] = None, **kwargs: Any)  str Transform a single input into an output. Override to implement. Parameters input  The input to the runnable. config  A config to use when invoking the runnable. The config supports standard keys like \'tags\', \'metadata\' for tracing purposes, \'max_concurrency\' for controlling how much work to do in parallel, and other keys. Please refer to the RunnableConfig for more details. Returns The output of the runnable. classmethod is_lc_serializable()  bool Is this class serializable? json(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any)  unicode Generate a JSON representation of the model, include and exclude arguments as per dict(). encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps(). classmethod lc_id()  List[str] A unique identifier for this class for serialization purposes. The unique identifier is a list of strings that describes the path to the object. map()  Runnable[List[Input], List[Output]] Return a new Runnable that maps a list of inputs to a list of outputs, by calling invoke() with each input. classmethod parse_file(path: Union[str, Path], *, content_type: unicode = None, encoding: unicode = \'utf8\', proto: Protocol = None, allow_pickle: bool = False)  Model classmethod parse_obj(obj: Any)  Model classmethod parse_raw(b: Union[str, bytes], *, content_type: unicode = None, encoding: unicode = \'utf8\', proto: Protocol = None, allow_pickle: bool = False)  Model predict(text: str, *, stop: Optional[Sequence[str]] = None, **kwargs: Any)  str Pass a single string input to the model and return a string prediction. Use this method when passing in raw text. If you want to pass in specifictypes of chat messages, use predict_messages. Parameters text  String input to pass to the model. stop  Stop words to use when generating. Model output is cut off at the first occurrence of any of these substrings. **kwargs  Arbitrary additional keyword arguments. These are usually passed to the model provider API call. Returns Top model prediction as a string. predict_messages(messages: List[BaseMessage], *, stop: Optional[Sequence[str]] = None, **kwargs: Any)  BaseMessage Pass a message sequence to the model and return a message prediction. Use this method when passing in chat messages. If you want to pass in raw text,use predict. Parameters messages  A sequence of chat messages corresponding to a single model input. stop  Stop words to use when generating. Model output is cut off at the first occurrence of any of these substrings. **kwargs  Arbitrary additional keyword arguments. These are usually passed to the model provider API call. Returns Top model prediction as a message. save(file_path: Union[Path, str])  None Save the LLM. Parameters file_path  Path to file to save the LLM to. Example: .. code-block:: python llm.save(file_path=path/llm.yaml) classmethod schema(by_alias: bool = True, ref_template: unicode = \'#/definitions/{model}\')  DictStrAny classmethod schema_json(*, by_alias: bool = True, ref_template: unicode = \'#/definitions/{model}\', **dumps_kwargs: Any)  unicode stream(input: Union[PromptValue, str, List[BaseMessage]], config: Optional[RunnableConfig] = None, *, stop: Optional[List[str]] = None, **kwargs: Any)  Iterator[str] Default implementation of stream, which calls invoke. Subclasses should override this method if they support streaming output. to_json()  Union[SerializedConstructor, SerializedNotImplemented] to_json_not_implemented()  SerializedNotImplemented transform(input: Iterator[Input], config: Optional[RunnableConfig] = None, **kwargs: Optional[Any])  Iterator[Output] Default implementation of transform, which buffers input and then calls stream. Subclasses should override this method if they can start producing output while input is still being generated. classmethod update_forward_refs(**localns: Any)  None Try to update ForwardRefs on fields based on this Model, globalns and localns. classmethod validate(value: Any)  Model with_config(config: Optional[RunnableConfig] = None, **kwargs: Any)  Runnable[Input, Output] Bind config to a Runnable, returning a new Runnable. with_fallbacks(fallbacks: Sequence[Runnable[Input, Output]], *, exceptions_to_handle: Tuple[Type[BaseException], ...] = (,))  RunnableWithFallbacksT[Input, Output] Add fallbacks to a runnable, returning a new Runnable. Parameters fallbacks  A sequence of runnables to try if the original runnable fails. exceptions_to_handle  A tuple of exception types to handle. Returns A new Runnable that will try the original runnable, and then each fallback in order, upon failures. with_listeners(*, on_start: Optional[Listener] = None, on_end: Optional[Listener] = None, on_error: Optional[Listener] = None)  Runnable[Input, Output] Bind lifecycle listeners to a Runnable, returning a new Runnable. on_start: Called before the runnable starts running, with the Run object. on_end: Called after the runnable finishes running, with the Run object. on_error: Called if the runnable throws an error, with the Run object. The Run object contains information about the run, including its id, type, input, output, error, start_time, end_time, and any tags or metadata added to the run. with_retry(*, retry_if_exception_type: ~typing.Tuple[~typing.Type[BaseException], ...] = (,), wait_exponential_jitter: bool = True, stop_after_attempt: int = 3)  Runnable[Input, Output] Create a new Runnable that retries the original runnable on exceptions. Parameters retry_if_exception_type  A tuple of exception types to retry on wait_exponential_jitter  Whether to add jitter to the wait time between retries stop_after_attempt  The maximum number of attempts to make before giving up Returns A new Runnable that retries the original runnable on exceptions. with_types(*, input_type: Optional[Type[Input]] = None, output_type: Optional[Type[Output]] = None)  Runnable[Input, Output] Bind input and output types to a Runnable, returning a new Runnable. property InputType: TypeAlias Get the input type for this runnable. property OutputType: Type[str] Get the input type for this runnable. property config_specs: List[langchain.schema.runnable.utils.ConfigurableFieldSpec] List configurable fields for this runnable. property input_schema: Type[pydantic.main.BaseModel] The type of input this runnable accepts specified as a pydantic model. property lc_attributes: Dict List of attribute names that should be included in the serialized kwargs. These attributes must be accepted by the constructor. property lc_secrets: Dict[str, str] A map of constructor argument names to secret ids. For example,{openai_api_key: OPENAI_API_KEY} property output_schema: Type[pydantic.main.BaseModel] The type of output this runnable produces specified as a pydantic model. Examples using AlephAlpha Aleph Alpha  2023, Harrison Chase. Last updated on Nov 17, 2023. Show this page source', 'iFixit | iFixit [iFixit]( is the largest, open repair community on the web. The site contains nearly 100k repair manuals, 200k Questions & Answers on 42k devices, and all the data is licensed under CC-BY-NC-SA 3.0. This loader will allow you to download the text of a repair guide, text of Q&A\'s and wikis from devices on `iFixit` using their open APIs. It\'s incredibly useful for context related to technical documents and answers to questions about devices in the corpus of data on `iFixit`. ```python from langchain.document_loaders import IFixitLoader ``` ```python loader = IFixitLoader("" data = loader.load() ``` ```python data ``` ```text [Document(page_content=""# Banana Teardown\\nIn this teardown, we open a banana to see what\'s inside. Yellow and delicious, but most importantly, yellow.\\n\\n\\n###Tools Required:\\n\\n - Fingers\\n\\n - Teeth\\n\\n - Thumbs\\n\\n\\n###Parts Required:\\n\\n - None\\n\\n\\n## Step 1\\nTake one banana from the bunch.\\nDon\'t squeeze too hard!\\n\\n\\n## Step 2\\nHold the banana in your left hand and grip the stem between your right thumb and forefinger.\\n\\n\\n## Step 3\\nPull the stem downward until the peel splits.\\n\\n\\n## Step 4\\nInsert your thumbs into the split of the peel and pull the two sides apart.\\nExpose the top of the banana. It may be slightly squished from pulling on the stem, but this will not affect the flavor.\\n\\n\\n## Step 5\\nPull open the peel, starting from your original split, and opening it along the length of the banana.\\n\\n\\n## Step 6\\nRemove fruit from peel.\\n\\n\\n## Step 7\\nEat and enjoy!\\nThis is where you\'ll need your teeth.\\nDo not choke on banana!\\n"", lookup_str=\'\', metadata={\'source\': \' \'title\': \'Banana Teardown\'}, lookup_index=0)] ``` ```python loader = IFixitLoader( "" ) data = loader.load() ``` ```python data ``` ```text [Document(page_content=\'# My iPhone 6 is typing and opening apps by itself\\nmy iphone 6 is typing and opening apps by itself. How do i fix this. I just bought it last week.\\nI restored as manufactures cleaned up the screen\\nthe problem continues\\n\\n## 27 Answers\\n\\nFilter by: \\n\\nMost Helpful\\nNewest\\nOldest\\n\\n### Accepted Answer\\nHi,\\nWhere did you buy it? If you bought it from Apple or from an official retailer like Carphone warehouse etc. Then you\\\'ll have a year warranty and can get it replaced free.\\nIf you bought it second hand, from a third part repair shop or online, then it may still have warranty, unless it is refurbished and has been repaired elsewhere.\\nIf this is the case, it may be the screen that needs replacing to solve your issue.\\nEither way, wherever you got it, it\\\'s best to return it and get a refund or a replacement device. :-)\\n\\n\\n\\n### Most Helpful Answer\\nI had the same issues, screen freezing, opening apps by itself, selecting the screens and typing on it\\\'s own. I first suspected aliens and then ghosts and then hackers.\\niPhone 6 is weak physically and tend to bend on pressure. And my phone had no case or cover.\\nI took the phone to apple stores and they said sensors need to be replaced and possibly screen replacement as well. My phone is just 17 months old.\\nHere is what I did two days ago and since then it is working like a charm..\\nHold the phone in portrait (as if watching a movie). Twist it very very gently. do it few times.Rest the phone for 10 mins (put it on a flat surface). You can now notice those self typing things gone and screen getting stabilized.\\nThen, reset the hardware (hold the power and home button till the screen goes off and comes back with apple logo). release the buttons when you see this.\\nThen, connect to your laptop and log in to iTunes and reset your phone completely. (please take a back-up first).\\nAnd your phone should be good to use again.\\nWhat really happened here for me is that the sensors might have stuck to the screen and with mild twisting, they got disengaged/released.\\nI posted this in Apple Community and the moderators deleted it, for the best reasons known to them.\\nInstead of throwing away your phone (or selling cheaply), try this and you could be saving your phone.\\nLet me know how it goes.\\n\\n\\n\\n### Other Answer\\nIt was the charging cord! I bought a gas station braided cord and it was the culprit. Once I plugged my OEM cord into the phone the GHOSTS went away.\\n\\n\\n\\n### Other Answer\\nI\\\'ve same issue that I just get resolved. I first tried to restore it from iCloud back, however it was not a software issue or any virus issue, so after restore same problem continues. Then I get my phone to local area iphone repairing lab, and they detected that it is an LCD issue. LCD get out of order without any reason (It was neither hit or nor slipped, but LCD get out of order all and sudden, while using it) it started opening things at random. I get LCD replaced with new one, that cost me $80.00 in total ($70.00 LCD charges + $10.00 as labor charges to fix it). iPhone is back to perfect mode now. It was iphone 6s. Thanks.\\n\\n\\n\\n### Other Answer\\nI was having the same issue with my 6 plus, I took it to a repair shop, they opened the phone, disconnected the three ribbons the screen has, blew up and cleaned the connectors and connected the screen again and it solved the issue it\'s hardware, not software.\\n\\n\\n\\n### Other Answer\\nHey.\\nJust had this problem now. As it turns out, you just need to plug in your phone. I use a case and when I took it off I noticed that there was a lot of dust and dirt around the areas that the case didn\\\'t cover. I shined a light in my ports and noticed they were filled with dust. Tomorrow I plan on using pressurized air to clean it out and the problem should be solved. If you plug in your phone and unplug it and it stops the issue, I recommend cleaning your phone thoroughly.\\n\\n\\n\\n### Other Answer\\nI simply changed the power supply and problem was gone. The block that plugs in the wall not the sub cord. The cord was fine but not the block.\\n\\n\\n\\n### Other Answer\\nSomeone ask! I purchased my iPhone 6s Plus for 1000 from at&t. Before I touched it, I purchased a otter defender case. I read where at&t said touch desease was due to dropping! Bullshit!! I am 56 I have never dropped it!! Looks brand new! Never dropped or abused any way! I have my original charger. I am going to clean it and try everyone\'s advice. It really sucks! I had 40,000,000 on my heart of Vegas slots! I play every day. I would be spinning and my fingers were no where max buttons and it would light up and switch to max. It did it 3 times before I caught it light up by its self. It sucks. Hope I can fix it!!!!\\n\\n\\n\\n### Other Answer\\nNo answer, but same problem with iPhone 6 plus--random, self-generated jumping amongst apps and typing on its own--plus freezing regularly (aha--maybe that\\\'s what the ""plus"" in ""6 plus"" refers to?). An Apple Genius recommended upgrading to iOS 11.3.1 from 11.2.2, to see if that fixed the trouble. If it didn\\\'t, Apple will sell me a new phone for $168! Of couese the OS upgrade didn\\\'t fix the problem. Thanks for helping me figure out that it\\\'s most likely a hardware problem--which the ""genius"" probably knows too.\\nI\\\'m getting ready to go Android.\\n\\n\\n\\n### Other Answer\\nI experienced similar ghost touches. Two weeks ago, I changed my iPhone 6 Plus shell (I had forced the phone into it because it\'s pretty tight), and also put a new glass screen protector (the edges of the protector don\'t stick to the screen, weird, so I brushed pressure on the edges at times to see if they may smooth out one day miraculously). I\'m not sure if I accidentally bend the phone when I installed the shell, or, if I got a defective glass protector that messes up the touch sensor. Well, yesterday was the worse day, keeps dropping calls and ghost pressing keys for me when I was on a call. I got fed up, so I removed the screen protector, and so far problems have not reoccurred yet. I\'m crossing my fingers that problems indeed solved.\\n\\n\\n\\n### Other Answer\\nthank you so much for this post! i was struggling doing the reset because i cannot type userids and passwords correctly because the iphone 6 plus i have kept on typing letters incorrectly. I have been doing it for a day until i come across this article. Very helpful! God bless you!!\\n\\n\\n\\n### Other Answer\\nI just turned it off, and turned it back on.\\n\\n\\n\\n### Other Answer\\nMy problem has not gone away completely but its better now i changed my charger and turned off prediction ....,,,now it rarely happens\\n\\n\\n\\n### Other Answer\\nI tried all of the above. I then turned off my home cleaned it with isopropyl alcohol 90%. Then I baked it in my oven on warm for an hour and a half over foil. Took it out and set it cool completely on the glass top stove. Then I turned on and it worked.\\n\\n\\n\\n### Other Answer\\nI think at& t should man up and fix your phone for free! You pay a lot for a Apple they should back it. I did the next 30 month payments and finally have it paid off in June. My iPad sept. Looking forward to a almost 100 drop in my phone bill! Now this crap!!! Really\\n\\n\\n\\n### Other Answer\\nIf your phone is JailBroken, suggest downloading a virus. While all my symptoms were similar, there was indeed a virus/malware on the phone which allowed for remote control of my iphone (even while in lock mode). My mistake for buying a third party iphone i suppose. Anyway i have since had the phone restored to factory and everything is working as expected for now. I will of course keep you posted if this changes. Thanks to all for the helpful posts, really helped me narrow a few things down.\\n\\n\\n\\n### Other Answer\\nWhen my phone was doing this, it ended up being the screen protector that i got from 5 below. I took it off and it stopped. I ordered more protectors from amazon and replaced it\\n\\n\\n\\n### Other Answer\\niPhone 6 Plus first generation.I had the same issues as all above, apps opening by themselves, self typing, ultra sensitive screen, items jumping around all over.it even called someone on FaceTime twice by itself when I was not in the room..I thought the phone was toast and i\'d have to buy a new one took me a while to figure out but it was the extra cheap block plug I bought at a dollar store for convenience of an extra charging station when I move around the house from den to living room..cord was fine but bought a new Apple brand block plugno more problems works just fine now. This issue was a recent event so had to narrow things down to what had changed recently to my phone so I could figure it out.\\nI even had the same problem on a laptop with documents opening up by themselves..a laptop that was plugged in to the same wall plug as my phone charger with the dollar store block plug.until I changed the block plug.\\n\\n\\n\\n### Other Answer\\nHad the problem: Inherited a 6s Plus from my wife. She had no problem with it.\\nLooks like it was merely the cheap phone case I purchased on Amazon. It was either pinching the edges or torquing the screen/body of the phone. Problem solved.\\n\\n\\n\\n### Other Answer\\nI bought my phone on march 6 and it was a brand new, but It sucks me uo because it freezing, shaking and control by itself. I went to the store where I bought this and I told them to replacr it, but they told me I have to pay it because Its about lcd issue. Please help me what other ways to fix it. Or should I try to remove the screen or should I follow your step above.\\n\\n\\n\\n### Other Answer\\nI tried everything and it seems to come back to needing the original iPhone cableor at least another 1 that would have come with another iPhonenot the $5 Store fast charging cables. My original cable is pretty beat up - like most that I see - but I\'ve been beaten up much MUCH less by sticking with its use! I didn\'t find that the casing/shell around it or not made any diff.\\n\\n\\n\\n### Other Answer\\ngreat now I have to wait one more hour to reset my phone and while I was tryin to connect my phone to my computer the computer also restarted smh does anyone else knows how I can get my phone to work my problem is I have a black dot on the bottom left of my screen an it wont allow me to touch a certain part of my screen unless I rotate my phone and I know the password but the first number is a 2 and it won\\\'t let me touch 1,2, or 3 so now I have to find a way to get rid of my password and all of a sudden my phone wants to touch stuff on its own which got my phone disabled many times to the point where I have to wait a whole hour and I really need to finish something on my phone today PLEASE HELPPPP\\n\\n\\n\\n### Other Answer\\nIn my case , iphone 6 screen was faulty. I got it replaced at local repair shop, so far phone is working fine.\\n\\n\\n\\n### Other Answer\\nthis problem in iphone 6 has many different scenarios and solutions, first try to reconnect the lcd screen to the motherboard again, if didnt solve, try to replace the lcd connector on the motherboard, if not solved, then remains two issues, lcd screen it self or touch IC. in my country some repair shops just change them all for almost 40$ since they dont want to troubleshoot one by one. readers of this comment also should know that partial screen not responding in other iphone models might also have an issue in LCD connector on the motherboard, specially if you lock/unlock screen and screen works again for sometime. lcd connectors gets disconnected lightly from the motherboard due to multiple falls and hits after sometime. best of luck for all\\n\\n\\n\\n### Other Answer\\nI am facing the same issue whereby these ghost touches type and open apps , I am using an original Iphone cable , how to I fix this issue.\\n\\n\\n\\n### Other Answer\\nThere were two issues with the phone I had troubles with. It was my dads and turns out he carried it in his pocket. The phone itself had a little bend in it as a result. A little pressure in the opposite direction helped the issue. But it also had a tiny crack in the screen which wasnt obvious, once we added a screen protector this fixed the issues entirely.\\n\\n\\n\\n### Other Answer\\nI had the same problem with my 64Gb iPhone 6+. Tried a lot of things and eventually downloaded all my images and videos to my PC and restarted the phone - problem solved. Been working now for two days.\', lookup_str=\'\', metadata={\'source\': \' \'title\': \'My iPhone 6 is typing and opening apps by itself\'}, lookup_index=0)] ``` ```python loader = IFixitLoader("" data = loader.load() ``` ```python data ``` ```text [Document(page_content=""Standard iPad\\nThe standard edition of the tablet computer made by Apple.\\n== Background Information ==\\n\\nOriginally introduced in January 2010, the iPad is Apple\'s standard edition of their tablet computer. In total, there have been ten generations of the standard edition of the iPad.\\n\\n== Additional Information ==\\n\\n* [link| Apple Product Page]\\n* [link| iPad Wikipedia]"", lookup_str=\'\', metadata={\'source\': \' \'title\': \'Standard iPad\'}, lookup_index=0)] ``` ## Searching iFixit using /suggest If you\'re looking for a more general way to search iFixit based on a keyword or phrase, the /suggest endpoint will return content related to the search term, then the loader will load the content from each of the suggested items and prep and return the documents. ```python data = IFixitLoader.load_suggestions(""Banana"") ``` ```python data ``` ```text [Document(page_content=\'Banana\\nTasty fruit. Good source of potassium. Yellow.\\n== Background Information ==\\n\\nCommonly misspelled, this wildly popular, phone shaped fruit serves as nutrition and an obstacle to slow down vehicles racing close behind you. Also used commonly as a synonym for crazy or insane.\\n\\nBotanically, the banana is considered a berry, although it isn\'t included in the culinary berry category containing strawberries and raspberries. Belonging to the genus Musa, the banana originated in Southeast Asia and Australia. Now largely cultivated throughout South and Central America, bananas are largely available throughout the world. They are especially valued as a staple food group in developing countries due to the banana tree\'s ability to produce fruit year round.\\n\\nThe banana can be easily opened. Simply remove the outer yellow shell by cracking the top of the stem. Then, with the broken piece, peel downward on each side until the fruity components on the inside are exposed. Once the shell has been removed it cannot be put back together.\\n\\n== Technical Specifications ==\\n\\n* Dimensions: Variable depending on genetics of the parent tree\\n* Color: Variable depending on ripeness, region, and season\\n\\n== Additional Information ==\\n\\n[link| Banana]\', lookup_str=\'\', metadata={\'source\': \' \'title\': \'Banana\'}, lookup_index=0), Document(page_content=""# Banana Teardown\\nIn this teardown, we open a banana to see what\'s inside. Yellow and delicious, but most importantly, yellow.\\n\\n\\n###Tools Required:\\n\\n - Fingers\\n\\n - Teeth\\n\\n - Thumbs\\n\\n\\n###Parts Required:\\n\\n - None\\n\\n\\n## Step 1\\nTake one banana from the bunch.\\nDon\'t squeeze too hard!\\n\\n\\n## Step 2\\nHold the banana in your left hand and grip the stem between your right thumb and forefinger.\\n\\n\\n## Step 3\\nPull the stem downward until the peel splits.\\n\\n\\n## Step 4\\nInsert your thumbs into the split of the peel and pull the two sides apart.\\nExpose the top of the banana. It may be slightly squished from pulling on the stem, but this will not affect the flavor.\\n\\n\\n## Step 5\\nPull open the peel, starting from your original split, and opening it along the length of the banana.\\n\\n\\n## Step 6\\nRemove fruit from peel.\\n\\n\\n## Step 7\\nEat and enjoy!\\nThis is where you\'ll need your teeth.\\nDo not choke on banana!\\n"", lookup_str=\'\', metadata={\'source\': \' \'title\': \'Banana Teardown\'}, lookup_index=0)] ``` - [Searching iFixit using /suggest](#searching-ifixit-using-suggest)', 'Log, Trace, and Monitor | Log, Trace, and Monitor When building apps or agents using Langchain, you end up making multiple API calls to fulfill a single user request. However, these requests are not chained when you want to analyse them. With [Portkey](/docs/ecosystem/integrations/portkey), all the embeddings, completion, and other requests from a single user request will get logged and traced to a common ID, enabling you to gain full visibility of user interactions. This notebook serves as a step-by-step guide on how to log, trace, and monitor Langchain LLM calls using `Portkey` in your Langchain app. First, let\'s import Portkey, OpenAI, and Agent tools ```python import os from langchain.agents import AgentType, initialize_agent, load_tools from langchain.llms import OpenAI from langchain.utilities import Portkey ``` Paste your OpenAI API key below. [(You can find it here)]( ```python os.environ[""OPENAI_API_KEY""] = """" ``` ## Get Portkey API Key 1. Sign up for [Portkey here]( 2. On your [dashboard]( click on the profile icon on the top left, then click on ""Copy API Key"" 3. Paste it below ```python PORTKEY_API_KEY = """" # Paste your Portkey API Key here ``` ## Set Trace ID 1. Set the trace id for your request below 2. The Trace ID can be common for all API calls originating from a single request ```python TRACE_ID = ""portkey_langchain_demo"" # Set trace id here ``` ## Generate Portkey Headers ```python headers = Portkey.Config( api_key=PORTKEY_API_KEY, trace_id=TRACE_ID, ) ``` Run your agent as usual. The **only** change is that we will **include the above headers** in the request now. ```python llm = OpenAI(temperature=0, headers=headers) tools = load_tools([""serpapi"", ""llm-math""], llm=llm) agent = initialize_agent( tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True ) # Let\'s test it out! agent.run( ""What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?"" ) ``` ## How Logging & Tracing Works on Portkey **Logging** - Sending your request through Portkey ensures that all of the requests are logged by default - Each request log contains `timestamp`, `model name`, `total cost`, `request time`, `request json`, `response json`, and additional Portkey features **Tracing** - Trace id is passed along with each request and is visibe on the logs on Portkey dashboard - You can also set a **distinct trace id** for each request if you want - You can append user feedback to a trace id as well. [More info on this here]( ## Advanced LLMOps Features - Caching, Tagging, Retries In addition to logging and tracing, Portkey provides more features that add production capabilities to your existing workflows: **Caching** Respond to previously served customers queries from cache instead of sending them again to OpenAI. Match exact strings OR semantically similar strings. Cache can save costs and reduce latencies by 20x. **Retries** Automatically reprocess any unsuccessful API requests **upto 5** times. Uses an **exponential backoff** strategy, which spaces out retry attempts to prevent network overload. | Feature | Config Key | Value (Type) | | ---- | ---- | ---- | | Automatic Retries | retry_count | integer[1,2,3,4,5] | | Enabling Cache | cache | simpleORsemantic | **Tagging** Track and audit ach user interaction in high detail with predefined tags. | Tag | Config Key | Value (Type) | | ---- | ---- | ---- | | User Tag | user | string | | Organisation Tag | organisation | string | | Environment Tag | environment | string | | Prompt Tag (version/id/string) | prompt | string | ## Code Example With All Features ```python headers = Portkey.Config( # Mandatory api_key="""", # Cache Options cache=""semantic"", cache_force_refresh=""True"", cache_age=1729, # Advanced retry_count=5, trace_id=""langchain_agent"", # Metadata environment=""production"", user=""john"", organisation=""acme"", prompt=""Frost"", ) llm = OpenAI(temperature=0.9, headers=headers) print(llm(""Two roads diverged in the yellow woods"")) ``` - [Get Portkey API Key](#get-portkey-api-key) - [Set Trace ID](#set-trace-id) - [Generate Portkey Headers](#generate-portkey-headers) - [How Logging & Tracing Works on Portkey](#how-logging--tracing-works-on-portkey) - [Advanced LLMOps Features - Caching, Tagging, Retries](#advanced-llmops-features---caching-tagging-retries) - [Code Example With All Features](#code-example-with-all-features)']","To control the maximum number of concurrent requests in batch calls, you can use a thread pool executor. Here's a Python example using the `concurrent.futures` module:

```python
import concurrent.futures
from langchain.llms import AlephAlpha

# Initialize LLM
aleph_alpha = AlephAlpha(aleph_alpha_api_key=""my-api-key"")

# Define batch function
def batch_call(inputs):
    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:  # Set max_workers to desired concurrency
        futures = [executor.submit(aleph_alpha.abatch, input) for input in inputs]
        results = [future.result() for future in concurrent.futures.as_completed(futures)]
    return results

# Example inputs
inputs = [""input1"", ""input2"", ""input3""]

# Call batch function
results = batch_call(inputs)
print(results)
```

In this example, `max_workers=5` limits the batch calls to 5 concurrent requests. Adjust `max_workers` as needed.","To limit the maximum number of concurrent requests for batch calls in LangChain, you can call the `.batch()` method and configure the 'max_concurrency' paramater within the runnable config.  For example, the following code calls the chain with a maximum of 2 concurrent calls:
```python
chain.batch([{'topic': 'bears'}, {'topic': 'cats'}, {'topic': 'dogs'}], config={'max_concurrency': 2})
```",0.8333333332916666,1.0,,0.149830177710696,0.22580645161290322
71,can i cache LLM calls in sqlite?,"['Caching | Caching LangChain provides an optional caching layer for LLMs. This is useful for two reasons: It can save you money by reducing the number of API calls you make to the LLM provider, if you\'re often requesting the same completion multiple times. It can speed up your application by reducing the number of API calls you make to the LLM provider. ```python from langchain.globals import set_llm_cache from langchain.llms import OpenAI # To make the caching really obvious, lets use a slower model. llm = OpenAI(model_name=""text-davinci-002"", n=2, best_of=2) ``` ## In Memory Cache ```python from langchain.cache import InMemoryCache set_llm_cache(InMemoryCache()) # The first time, it is not yet in cache, so it should take longer llm.predict(""Tell me a joke"") ``` ```text CPU times: user 35.9 ms, sys: 28.6 ms, total: 64.6 ms Wall time: 4.83 s ""\\n\\nWhy couldn\'t the bicycle stand up by itself? It was...two tired!"" ``` ```python # The second time it is, so it goes faster llm.predict(""Tell me a joke"") ``` ```text CPU times: user 238 s, sys: 143 s, total: 381 s Wall time: 1.76 ms \'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.\' ``` ## SQLite Cache ```bash rm .langchain.db ``` ```python # We can do the same thing with a SQLite cache from langchain.cache import SQLiteCache set_llm_cache(SQLiteCache(database_path="".langchain.db"")) ``` ```python # The first time, it is not yet in cache, so it should take longer llm.predict(""Tell me a joke"") ``` ```text CPU times: user 17 ms, sys: 9.76 ms, total: 26.7 ms Wall time: 825 ms \'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.\' ``` ```python # The second time it is, so it goes faster llm.predict(""Tell me a joke"") ``` ```text CPU times: user 2.46 ms, sys: 1.23 ms, total: 3.7 ms Wall time: 2.67 ms \'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.\' ``` ## Optional caching in chains You can also turn off caching for particular nodes in chains. Note that because of certain interfaces, it\'s often easier to construct the chain first, and then edit the LLM afterwards. As an example, we will load a summarizer map-reduce chain. We will cache results for the map-step, but then not freeze it for the combine step. ```python llm = OpenAI(model_name=""text-davinci-002"") no_cache_llm = OpenAI(model_name=""text-davinci-002"", cache=False) ``` ```python from langchain.text_splitter import CharacterTextSplitter from langchain.chains.mapreduce import MapReduceChain text_splitter = CharacterTextSplitter() ``` ```python with open(\'../../../state_of_the_union.txt\') as f: state_of_the_union = f.read() texts = text_splitter.split_text(state_of_the_union) ``` ```python from langchain.docstore.document import Document docs = [Document(page_content=t) for t in texts[:3]] from langchain.chains.summarize import load_summarize_chain ``` ```python chain = load_summarize_chain(llm, chain_type=""map_reduce"", reduce_llm=no_cache_llm) ``` ```python chain.run(docs) ``` ```text CPU times: user 452 ms, sys: 60.3 ms, total: 512 ms Wall time: 5.09 s \'\\n\\nPresident Biden is discussing the American Rescue Plan and the Bipartisan Infrastructure Law, which will create jobs and help Americans. He also talks about his vision for America, which includes investing in education and infrastructure. In response to Russian aggression in Ukraine, the United States is joining with European allies to impose sanctions and isolate Russia. American forces are being mobilized to protect NATO countries in the event that Putin decides to keep moving west. The Ukrainians are bravely fighting back, but the next few weeks will be hard for them. Putin will pay a high price for his actions in the long run. Americans should not be alarmed, as the United States is taking action to protect its interests and allies.\' ``` When we run it again, we see that it runs substantially faster but the final answer is different. This is due to caching at the map steps, but not at the reduce step. ```python chain.run(docs) ``` ```text CPU times: user 11.5 ms, sys: 4.33 ms, total: 15.8 ms Wall time: 1.04 s \'\\n\\nPresident Biden is discussing the American Rescue Plan and the Bipartisan Infrastructure Law, which will create jobs and help Americans. He also talks about his vision for America, which includes investing in education and infrastructure.\' ``` ```bash rm .langchain.db sqlite.db ``` - [In Memory Cache](#in-memory-cache) - [SQLite Cache](#sqlite-cache) - [Optional caching in chains](#optional-caching-in-chains)', 'LLM Caching integrations | LLM Caching integrations This notebook covers how to cache results of individual LLM calls using different caches. ```python from langchain.globals import set_llm_cache from langchain.llms import OpenAI # To make the caching really obvious, lets use a slower model. llm = OpenAI(model_name=""text-davinci-002"", n=2, best_of=2) ``` ## In Memory Cache ```python from langchain.cache import InMemoryCache set_llm_cache(InMemoryCache()) ``` ```python # The first time, it is not yet in cache, so it should take longer llm(""Tell me a joke"") ``` ```text CPU times: user 52.2 ms, sys: 15.2 ms, total: 67.4 ms Wall time: 1.19 s ""\\n\\nWhy couldn\'t the bicycle stand up by itself? Because it was...two tired!"" ``` ```python # The second time it is, so it goes faster llm(""Tell me a joke"") ``` ```text CPU times: user 191 s, sys: 11 s, total: 202 s Wall time: 205 s ""\\n\\nWhy couldn\'t the bicycle stand up by itself? Because it was...two tired!"" ``` ## SQLite Cache ```bash rm .langchain.db ``` ```python # We can do the same thing with a SQLite cache from langchain.cache import SQLiteCache set_llm_cache(SQLiteCache(database_path="".langchain.db"")) ``` ```python # The first time, it is not yet in cache, so it should take longer llm(""Tell me a joke"") ``` ```text CPU times: user 33.2 ms, sys: 18.1 ms, total: 51.2 ms Wall time: 667 ms \'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.\' ``` ```python # The second time it is, so it goes faster llm(""Tell me a joke"") ``` ```text CPU times: user 4.86 ms, sys: 1.97 ms, total: 6.83 ms Wall time: 5.79 ms \'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.\' ``` ## Upstash Redis Cache ### Standard Cache Use [Upstash Redis]( to cache prompts and responses with a serverless HTTP API. ```python from langchain.cache import UpstashRedisCache from upstash_redis import Redis URL = """" TOKEN = """" langchain.llm_cache = UpstashRedisCache(redis_=Redis(url=URL, token=TOKEN)) ``` ```python # The first time, it is not yet in cache, so it should take longer llm(""Tell me a joke"") ``` ```text CPU times: user 7.56 ms, sys: 2.98 ms, total: 10.5 ms Wall time: 1.14 s \'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!\' ``` ```python # The first time, it is not yet in cache, so it should take longer llm(""Tell me a joke"") ``` ```text CPU times: user 2.78 ms, sys: 1.95 ms, total: 4.73 ms Wall time: 82.9 ms \'\\n\\nTwo guys stole a calendar. They got six months each.\' ``` ## Redis Cache ### Standard Cache Use [Redis](/docs/ecosystem/integrations/redis) to cache prompts and responses. ```python # We can do the same thing with a Redis cache # (make sure your local Redis instance is running first before running this example) from langchain.cache import RedisCache from redis import Redis set_llm_cache(RedisCache(redis_=Redis())) ``` ```python # The first time, it is not yet in cache, so it should take longer llm(""Tell me a joke"") ``` ```text CPU times: user 6.88 ms, sys: 8.75 ms, total: 15.6 ms Wall time: 1.04 s \'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!\' ``` ```python # The second time it is, so it goes faster llm(""Tell me a joke"") ``` ```text CPU times: user 1.59 ms, sys: 610 s, total: 2.2 ms Wall time: 5.58 ms \'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!\' ``` ### Semantic Cache Use [Redis](/docs/ecosystem/integrations/redis) to cache prompts and responses and evaluate hits based on semantic similarity. ```python from langchain.cache import RedisSemanticCache from langchain.embeddings import OpenAIEmbeddings set_llm_cache( RedisSemanticCache(redis_url=""redis://localhost:6379"", embedding=OpenAIEmbeddings()) ) ``` ```python # The first time, it is not yet in cache, so it should take longer llm(""Tell me a joke"") ``` ```text CPU times: user 351 ms, sys: 156 ms, total: 507 ms Wall time: 3.37 s ""\\n\\nWhy don\'t scientists trust atoms?\\nBecause they make up everything."" ``` ```python # The second time, while not a direct hit, the question is semantically similar to the original question, # so it uses the cached result! llm(""Tell me one joke"") ``` ```text CPU times: user 6.25 ms, sys: 2.72 ms, total: 8.97 ms Wall time: 262 ms ""\\n\\nWhy don\'t scientists trust atoms?\\nBecause they make up everything."" ``` ## GPTCache We can use [GPTCache]( for exact match caching OR to cache results based on semantic similarity Let\'s first start with an example of exact match ```python import hashlib from gptcache import Cache from gptcache.manager.factory import manager_factory from gptcache.processor.pre import get_prompt from langchain.cache import GPTCache def get_hashed_name(name): return hashlib.sha256(name.encode()).hexdigest() def init_gptcache(cache_obj: Cache, llm: str): hashed_llm = get_hashed_name(llm) cache_obj.init( pre_embedding_func=get_prompt, data_manager=manager_factory(manager=""map"", data_dir=f""map_cache_{hashed_llm}""), ) set_llm_cache(GPTCache(init_gptcache)) ``` ```python # The first time, it is not yet in cache, so it should take longer llm(""Tell me a joke"") ``` ```text CPU times: user 21.5 ms, sys: 21.3 ms, total: 42.8 ms Wall time: 6.2 s \'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!\' ``` ```python # The second time it is, so it goes faster llm(""Tell me a joke"") ``` ```text CPU times: user 571 s, sys: 43 s, total: 614 s Wall time: 635 s \'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!\' ``` Let\'s now show an example of similarity caching ```python import hashlib from gptcache import Cache from gptcache.adapter.api import init_similar_cache from langchain.cache import GPTCache def get_hashed_name(name): return hashlib.sha256(name.encode()).hexdigest() def init_gptcache(cache_obj: Cache, llm: str): hashed_llm = get_hashed_name(llm) init_similar_cache(cache_obj=cache_obj, data_dir=f""similar_cache_{hashed_llm}"") set_llm_cache(GPTCache(init_gptcache)) ``` ```python # The first time, it is not yet in cache, so it should take longer llm(""Tell me a joke"") ``` ```text CPU times: user 1.42 s, sys: 279 ms, total: 1.7 s Wall time: 8.44 s \'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.\' ``` ```python # This is an exact match, so it finds it in the cache llm(""Tell me a joke"") ``` ```text CPU times: user 866 ms, sys: 20 ms, total: 886 ms Wall time: 226 ms \'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.\' ``` ```python # This is not an exact match, but semantically within distance so it hits! llm(""Tell me joke"") ``` ```text CPU times: user 853 ms, sys: 14.8 ms, total: 868 ms Wall time: 224 ms \'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.\' ``` ## Momento Cache Use [Momento](/docs/ecosystem/integrations/momento) to cache prompts and responses. Requires momento to use, uncomment below to install: ```python # !pip install momento ``` You\'ll need to get a Momento auth token to use this class. This can either be passed in to a momento.CacheClient if you\'d like to instantiate that directly, as a named parameter `auth_token` to `MomentoChatMessageHistory.from_client_params`, or can just be set as an environment variable `MOMENTO_AUTH_TOKEN`. ```python from datetime import timedelta from langchain.cache import MomentoCache cache_name = ""langchain"" ttl = timedelta(days=1) set_llm_cache(MomentoCache.from_client_params(cache_name, ttl)) ``` ```python # The first time, it is not yet in cache, so it should take longer llm(""Tell me a joke"") ``` ```text CPU times: user 40.7 ms, sys: 16.5 ms, total: 57.2 ms Wall time: 1.73 s \'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!\' ``` ```python # The second time it is, so it goes faster # When run in the same region as the cache, latencies are single digit ms llm(""Tell me a joke"") ``` ```text CPU times: user 3.16 ms, sys: 2.98 ms, total: 6.14 ms Wall time: 57.9 ms \'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!\' ``` ## SQLAlchemy Cache You can use `SQLAlchemyCache` to cache with any SQL database supported by `SQLAlchemy`. ```python # from langchain.cache import SQLAlchemyCache # from sqlalchemy import create_engine # engine = create_engine(""postgresql://postgres:postgres@localhost:5432/postgres"") # set_llm_cache(SQLAlchemyCache(engine)) ``` ### Custom SQLAlchemy Schemas ```python # You can define your own declarative SQLAlchemyCache child class to customize the schema used for caching. For example, to support high-speed fulltext prompt indexing with Postgres, use: from langchain.cache import SQLAlchemyCache from sqlalchemy import Column, Computed, Index, Integer, Sequence, String, create_engine from sqlalchemy.ext.declarative import declarative_base from sqlalchemy_utils import TSVectorType Base = declarative_base() class FulltextLLMCache(Base): # type: ignore """"""Postgres table for fulltext-indexed LLM Cache"""""" __tablename__ = ""llm_cache_fulltext"" id = Column(Integer, Sequence(""cache_id""), primary_key=True) prompt = Column(String, nullable=False) llm = Column(String, nullable=False) idx = Column(Integer) response = Column(String) prompt_tsv = Column( TSVectorType(), Computed(""to_tsvector(\'english\', llm || \' \' || prompt)"", persisted=True), ) __table_args__ = ( Index(""idx_fulltext_prompt_tsv"", prompt_tsv, postgresql_using=""gin""), ) engine = create_engine(""postgresql://postgres:postgres@localhost:5432/postgres"") set_llm_cache(SQLAlchemyCache(engine, FulltextLLMCache)) ``` ## Cassandra caches You can use Cassandra / Astra DB for caching LLM responses, choosing from the exact-match `CassandraCache` or the (vector-similarity-based) `CassandraSemanticCache`. Let\'s see both in action in the following cells. #### Connect to the DB First you need to establish a `Session` to the DB and to specify a _keyspace_ for the cache table(s). The following gets you started with an Astra DB instance (see e.g. [here]( for more backends and connection options). ```python import getpass keyspace = input(""\\nKeyspace name? "") ASTRA_DB_APPLICATION_TOKEN = getpass.getpass(\'\\nAstra DB Token (""AstraCS:..."") \') ASTRA_DB_SECURE_BUNDLE_PATH = input(""Full path to your Secure Connect Bundle? "") ``` ```text Keyspace name? my_keyspace Astra DB Token (""AstraCS:..."") Full path to your Secure Connect Bundle? /path/to/secure-connect-databasename.zip ``` ```python from cassandra.auth import PlainTextAuthProvider from cassandra.cluster import Cluster cluster = Cluster( cloud={ ""secure_connect_bundle"": ASTRA_DB_SECURE_BUNDLE_PATH, }, auth_provider=PlainTextAuthProvider(""token"", ASTRA_DB_APPLICATION_TOKEN), ) session = cluster.connect() ``` ### Exact cache This will avoid invoking the LLM when the supplied prompt is _exactly_ the same as one encountered already: ```python from langchain.cache import CassandraCache from langchain.globals import set_llm_cache set_llm_cache(CassandraCache(session=session, keyspace=keyspace)) ``` ```python print(llm(""Why is the Moon always showing the same side?"")) ``` ```text The Moon always shows the same side because it is tidally locked to Earth. CPU times: user 41.7 ms, sys: 153 s, total: 41.8 ms Wall time: 1.96 s ``` ```python print(llm(""Why is the Moon always showing the same side?"")) ``` ```text The Moon always shows the same side because it is tidally locked to Earth. CPU times: user 4.09 ms, sys: 0 ns, total: 4.09 ms Wall time: 119 ms ``` ### Semantic cache\u200b This cache will do a semantic similarity search and return a hit if it finds a cached entry that is similar enough, For this, you need to provide an `Embeddings` instance of your choice. ```python from langchain.embeddings import OpenAIEmbeddings embedding = OpenAIEmbeddings() ``` ```python from langchain.cache import CassandraSemanticCache set_llm_cache( CassandraSemanticCache( session=session, keyspace=keyspace, embedding=embedding, table_name=""cass_sem_cache"", ) ) ``` ```python print(llm(""Why is the Moon always showing the same side?"")) ``` ```text The Moon always shows the same side because it is tidally locked with Earth. This means that the same side of the Moon always faces Earth. CPU times: user 21.3 ms, sys: 177 s, total: 21.4 ms Wall time: 3.09 s ``` ```python print(llm(""How come we always see one face of the moon?"")) ``` ```text The Moon always shows the same side because it is tidally locked with Earth. This means that the same side of the Moon always faces Earth. CPU times: user 10.9 ms, sys: 17 s, total: 10.9 ms Wall time: 461 ms ``` ## Optional Caching\u200b You can also turn off caching for specific LLMs should you choose. In the example below, even though global caching is enabled, we turn it off for a specific LLM ```python llm = OpenAI(model_name=""text-davinci-002"", n=2, best_of=2, cache=False) ``` ```python llm(""Tell me a joke"") ``` ```text CPU times: user 5.8 ms, sys: 2.71 ms, total: 8.51 ms Wall time: 745 ms \'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!\' ``` ```python llm(""Tell me a joke"") ``` ```text CPU times: user 4.91 ms, sys: 2.64 ms, total: 7.55 ms Wall time: 623 ms \'\\n\\nTwo guys stole a calendar. They got six months each.\' ``` ## Optional Caching in Chains\u200b You can also turn off caching for particular nodes in chains. Note that because of certain interfaces, its often easier to construct the chain first, and then edit the LLM afterwards. As an example, we will load a summarizer map-reduce chain. We will cache results for the map-step, but then not freeze it for the combine step. ```python llm = OpenAI(model_name=""text-davinci-002"") no_cache_llm = OpenAI(model_name=""text-davinci-002"", cache=False) ``` ```python from langchain.text_splitter import CharacterTextSplitter text_splitter = CharacterTextSplitter() ``` ```python with open(""../../modules/state_of_the_union.txt"") as f: state_of_the_union = f.read() texts = text_splitter.split_text(state_of_the_union) ``` ```python from langchain.docstore.document import Document docs = [Document(page_content=t) for t in texts[:3]] from langchain.chains.summarize import load_summarize_chain ``` ```python chain = load_summarize_chain(llm, chain_type=""map_reduce"", reduce_llm=no_cache_llm) ``` ```python chain.run(docs) ``` ```text CPU times: user 452 ms, sys: 60.3 ms, total: 512 ms Wall time: 5.09 s \'\\n\\nPresident Biden is discussing the American Rescue Plan and the Bipartisan Infrastructure Law, which will create jobs and help Americans. He also talks about his vision for America, which includes investing in education and infrastructure. In response to Russian aggression in Ukraine, the United States is joining with European allies to impose sanctions and isolate Russia. American forces are being mobilized to protect NATO countries in the event that Putin decides to keep moving west. The Ukrainians are bravely fighting back, but the next few weeks will be hard for them. Putin will pay a high price for his actions in the long run. Americans should not be alarmed, as the United States is taking action to protect its interests and allies.\' ``` When we run it again, we see that it runs substantially faster but the final answer is different. This is due to caching at the map steps, but not at the reduce step. ```python chain.run(docs) ``` ```text CPU times: user 11.5 ms, sys: 4.33 ms, total: 15.8 ms Wall time: 1.04 s \'\\n\\nPresident Biden is discussing the American Rescue Plan and the Bipartisan Infrastructure Law, which will create jobs and help Americans. He also talks about his vision for America, which includes investing in education and infrastructure.\' ``` ```bash rm .langchain.db sqlite.db ``` - [In Memory Cache](#in-memory-cache) - [SQLite Cache](#sqlite-cache) - [Upstash Redis Cache](#upstash-redis-cache)- [Standard Cache](#standard-cache) - [Redis Cache](#redis-cache)- [Standard Cache](#standard-cache-1) - [Semantic Cache](#semantic-cache) - [GPTCache](#gptcache) - [Momento Cache](#momento-cache) - [SQLAlchemy Cache](#sqlalchemy-cache)- [Custom SQLAlchemy Schemas](#custom-sqlalchemy-schemas) - [Cassandra caches](#cassandra-caches)- [Exact cache](#exact-cache) - [Semantic cache](#semantic-cache-1) - [Optional Caching](#optional-caching) - [Optional Caching in Chains](#optional-caching-in-chains)', 'E2B Data Analysis | E2B Data Analysis [E2B\'s cloud environments]( are great runtime sandboxes for LLMs. E2B\'s Data Analysis sandbox allows for safe code execution in a sandboxed environment. This is ideal for building tools such as code interpreters, or Advanced Data Analysis like in ChatGPT. E2B Data Analysis sandbox allows you to: - Run Python code - Generate charts via matplotlib - Install Python packages dynamically during runtime - Install system packages dynamically during runtime - Run shell commands - Upload and download files We\'ll create a simple OpenAI agent that will use E2B\'s Data Analysis sandbox to perform analysis on a uploaded files using Python. Get your OpenAI API key and [E2B API key here]( and set them as environment variables. You can find the full API documentation [here]( You\'ll need to install `e2b` to get started: ```bash pip install langchain e2b ``` ```python import os from langchain.agents import AgentType, initialize_agent from langchain.chat_models import ChatOpenAI from langchain.tools import E2BDataAnalysisTool os.environ[""E2B_API_KEY""] = """" os.environ[""OPENAI_API_KEY""] = """" ``` When creating an instance of the `E2BDataAnalysisTool`, you can pass callbacks to listen to the output of the sandbox. This is useful, for example, when creating more responsive UI. Especially with the combination of streaming output from LLMs. ```python # Artifacts are charts created by matplotlib when `plt.show()` is called def save_artifact(artifact): print(""New matplotlib chart generated:"", artifact.name) # Download the artifact as `bytes` and leave it up to the user to display them (on frontend, for example) file = artifact.download() basename = os.path.basename(artifact.name) # Save the chart to the `charts` directory with open(f""./charts/{basename}"", ""wb"") as f: f.write(file) e2b_data_analysis_tool = E2BDataAnalysisTool( # Pass environment variables to the sandbox env_vars={""MY_SECRET"": ""secret_value""}, on_stdout=lambda stdout: print(""stdout:"", stdout), on_stderr=lambda stderr: print(""stderr:"", stderr), on_artifact=save_artifact, ) ``` Upload an example CSV data file to the sandbox so we can analyze it with our agent. You can use for example [this file]( about Netflix tv shows. ```python with open(""./netflix.csv"") as f: remote_path = e2b_data_analysis_tool.upload_file( file=f, description=""Data about Netflix tv shows including their title, category, director, release date, casting, age rating, etc."", ) print(remote_path) ``` ```text name=\'netflix.csv\' remote_path=\'/home/user/netflix.csv\' description=\'Data about Netflix tv shows including their title, category, director, release date, casting, age rating, etc.\' ``` Create a `Tool` object and initialize the Langchain agent. ```python tools = [e2b_data_analysis_tool.as_tool()] llm = ChatOpenAI(model=""gpt-4"", temperature=0) agent = initialize_agent( tools, llm, agent=AgentType.OPENAI_FUNCTIONS, verbose=True, handle_parsing_errors=True, ) ``` Now we can ask the agent questions about the CSV file we uploaded earlier. ```python agent.run( ""What are the 5 longest movies on netflix released between 2000 and 2010? Create a chart with their lengths."" ) ``` ```text > Entering new AgentExecutor chain... Invoking: `e2b_data_analysis` with `{\'python_code\': ""import pandas as pd\\n\\n# Load the data\\nnetflix_data = pd.read_csv(\'/home/user/netflix.csv\')\\n\\n# Convert the \'release_year\' column to integer\\nnetflix_data[\'release_year\'] = netflix_data[\'release_year\'].astype(int)\\n\\n# Filter the data for movies released between 2000 and 2010\\nfiltered_data = netflix_data[(netflix_data[\'release_year\'] >= 2000) & (netflix_data[\'release_year\'] <= 2010) & (netflix_data[\'type\'] == \'Movie\')]\\n\\n# Remove rows where \'duration\' is not available\\nfiltered_data = filtered_data[filtered_data[\'duration\'].notna()]\\n\\n# Convert the \'duration\' column to integer\\nfiltered_data[\'duration\'] = filtered_data[\'duration\'].str.replace(\' min\',\'\').astype(int)\\n\\n# Get the top 5 longest movies\\nlongest_movies = filtered_data.nlargest(5, \'duration\')\\n\\n# Create a bar chart\\nimport matplotlib.pyplot as plt\\n\\nplt.figure(figsize=(10,5))\\nplt.barh(longest_movies[\'title\'], longest_movies[\'duration\'], color=\'skyblue\')\\nplt.xlabel(\'Duration (minutes)\')\\nplt.title(\'Top 5 Longest Movies on Netflix (2000-2010)\')\\nplt.gca().invert_yaxis()\\nplt.savefig(\'/home/user/longest_movies.png\')\\n\\nlongest_movies[[\'title\', \'duration\']]""}` stdout: title duration stdout: 1019 Lagaan 224 stdout: 4573 Jodhaa Akbar 214 stdout: 2731 Kabhi Khushi Kabhie Gham 209 stdout: 2632 No Direction Home: Bob Dylan 208 stdout: 2126 What\'s Your Raashee? 203 {\'stdout\': "" title duration\\n1019 Lagaan 224\\n4573 Jodhaa Akbar 214\\n2731 Kabhi Khushi Kabhie Gham 209\\n2632 No Direction Home: Bob Dylan 208\\n2126 What\'s Your Raashee? 203"", \'stderr\': \'\'}The 5 longest movies on Netflix released between 2000 and 2010 are: 1. Lagaan - 224 minutes 2. Jodhaa Akbar - 214 minutes 3. Kabhi Khushi Kabhie Gham - 209 minutes 4. No Direction Home: Bob Dylan - 208 minutes 5. What\'s Your Raashee? - 203 minutes Here is the chart showing their lengths: ![Longest Movies](sandbox:/home/user/longest_movies.png) > Finished chain. ""The 5 longest movies on Netflix released between 2000 and 2010 are:\\n\\n1. Lagaan - 224 minutes\\n2. Jodhaa Akbar - 214 minutes\\n3. Kabhi Khushi Kabhie Gham - 209 minutes\\n4. No Direction Home: Bob Dylan - 208 minutes\\n5. What\'s Your Raashee? - 203 minutes\\n\\nHere is the chart showing their lengths:\\n\\n![Longest Movies](sandbox:/home/user/longest_movies.png)"" ``` E2B also allows you to install both Python and system (via `apt`) packages dynamically during runtime like this: ```python # Install Python package e2b_data_analysis_tool.install_python_packages(""pandas"") ``` ```text stdout: Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.1) stdout: Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2) stdout: Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1) stdout: Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.1) stdout: Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3) stdout: Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0) ``` Additionally, you can download any file from the sandbox like this: ```python # The path is a remote path in the sandbox files_in_bytes = e2b_data_analysis_tool.download_file(""/home/user/netflix.csv"") ``` Lastly, you can run any shell command inside the sandbox via `run_command`. ```python # Install SQLite e2b_data_analysis_tool.run_command(""sudo apt update"") e2b_data_analysis_tool.install_system_packages(""sqlite3"") # Check the SQLite version output = e2b_data_analysis_tool.run_command(""sqlite3 --version"") print(""version: "", output[""stdout""]) print(""error: "", output[""stderr""]) print(""exit code: "", output[""exit_code""]) ``` ```text stderr: stderr: WARNING: apt does not have a stable CLI interface. Use with caution in scripts. stderr: stdout: Hit:1 jammy-security InRelease stdout: Hit:2 jammy InRelease stdout: Hit:3 jammy-updates InRelease stdout: Hit:4 jammy-backports InRelease stdout: Reading package lists... stdout: Building dependency tree... stdout: Reading state information... stdout: All packages are up to date. stdout: Reading package lists... stdout: Building dependency tree... stdout: Reading state information... stdout: Suggested packages: stdout: sqlite3-doc stdout: The following NEW packages will be installed: stdout: sqlite3 stdout: 0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded. stdout: Need to get 768 kB of archives. stdout: After this operation, 1873 kB of additional disk space will be used. stdout: Get:1 jammy-updates/main amd64 sqlite3 amd64 3.37.2-2ubuntu0.1 [768 kB] stderr: debconf: delaying package configuration, since apt-utils is not installed stdout: Fetched 768 kB in 0s (2258 kB/s) stdout: Selecting previously unselected package sqlite3. (Reading database ... 23999 files and directories currently installed.) stdout: Preparing to unpack .../sqlite3_3.37.2-2ubuntu0.1_amd64.deb ... stdout: Unpacking sqlite3 (3.37.2-2ubuntu0.1) ... stdout: Setting up sqlite3 (3.37.2-2ubuntu0.1) ... stdout: 3.37.2 2022-01-06 13:25:41 872ba256cbf61d9290b571c0e6d82a20c224ca3ad82971edc46b29818d5dalt1 version: 3.37.2 2022-01-06 13:25:41 872ba256cbf61d9290b571c0e6d82a20c224ca3ad82971edc46b29818d5dalt1 error: exit code: 0 ``` When your agent is finished, don\'t forget to close the sandbox ```python e2b_data_analysis_tool.close() ```']","Yes, you can cache LLM calls in SQLite. LangChain provides an `SQLiteCache` class that you can use to cache results in an SQLite database. Here's a brief example of how to set it up:

```python
from langchain.cache import SQLiteCache
from langchain.globals import set_llm_cache

# Set up the SQLite cache
set_llm_cache(SQLiteCache(database_path="".langchain.db""))

# Now your LLM calls will be cached in the SQLite database
llm = OpenAI(model_name=""text-davinci-002"", n=2, best_of=2)

# Example usage
llm.predict(""Tell me a joke"")
```

This will store the results of your LLM calls in the `.langchain.db` SQLite database, improving performance for subsequent calls with the same input.","To add LLM caching in sqlite, you can use the SQLiteCache class. For example:
```python
from langchain.globals import set_llm_cache
from langchain.cache import SQLiteCache
from langchain.chat_models import ChatOpenAI

llm = ChatOpenAI()

set_llm_cache(SQLiteCache(database_path="".langchain.db""))
llm.invoke(""Tell me a joke"")
```",0.9999999999666667,1.0,,0.13532330504290596,0.3636363636363637
72,how can I create a vectorstore from the texts in the list,"['Vectorstore | Vectorstore This notebook showcases an agent designed to retrieve information from one or more vectorstores, either with or without sources. ## Create Vectorstores ```python from langchain.embeddings.openai import OpenAIEmbeddings from langchain.llms import OpenAI from langchain.text_splitter import CharacterTextSplitter from langchain.vectorstores import Chroma llm = OpenAI(temperature=0) ``` ```python from langchain.document_loaders import TextLoader loader = TextLoader(""../../modules/state_of_the_union.txt"") documents = loader.load() text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0) texts = text_splitter.split_documents(documents) embeddings = OpenAIEmbeddings() state_of_union_store = Chroma.from_documents( texts, embeddings, collection_name=""state-of-union"" ) ``` ```text Running Chroma using direct local API. Using DuckDB in-memory for database. Data will be transient. ``` ```python from langchain.document_loaders import WebBaseLoader loader = WebBaseLoader("" docs = loader.load() ruff_texts = text_splitter.split_documents(docs) ruff_store = Chroma.from_documents(ruff_texts, embeddings, collection_name=""ruff"") ``` ```text Running Chroma using direct local API. Using DuckDB in-memory for database. Data will be transient. ``` ## Initialize Toolkit and Agent First, we\'ll create an agent with a single vectorstore. ```python from langchain.agents.agent_toolkits import ( VectorStoreInfo, VectorStoreToolkit, create_vectorstore_agent, ) vectorstore_info = VectorStoreInfo( name=""state_of_union_address"", description=""the most recent state of the Union adress"", vectorstore=state_of_union_store, ) toolkit = VectorStoreToolkit(vectorstore_info=vectorstore_info) agent_executor = create_vectorstore_agent(llm=llm, toolkit=toolkit, verbose=True) ``` ## Examples ```python agent_executor.run( ""What did biden say about ketanji brown jackson in the state of the union address?"" ) ``` ```text > Entering new AgentExecutor chain... I need to find the answer in the state of the union address Action: state_of_union_address Action Input: What did biden say about ketanji brown jackson Observation: Biden said that Ketanji Brown Jackson is one of the nation\'s top legal minds and that she will continue Justice Breyer\'s legacy of excellence. Thought: I now know the final answer Final Answer: Biden said that Ketanji Brown Jackson is one of the nation\'s top legal minds and that she will continue Justice Breyer\'s legacy of excellence. > Finished chain. ""Biden said that Ketanji Brown Jackson is one of the nation\'s top legal minds and that she will continue Justice Breyer\'s legacy of excellence."" ``` ```python agent_executor.run( ""What did biden say about ketanji brown jackson in the state of the union address? List the source."" ) ``` ```text > Entering new AgentExecutor chain... I need to use the state_of_union_address_with_sources tool to answer this question. Action: state_of_union_address_with_sources Action Input: What did biden say about ketanji brown jackson Observation: {""answer"": "" Biden said that he nominated Circuit Court of Appeals Judge Ketanji Brown Jackson to the United States Supreme Court, and that she is one of the nation\'s top legal minds who will continue Justice Breyer\'s legacy of excellence.\\n"", ""sources"": ""../../state_of_the_union.txt""} Thought: I now know the final answer Final Answer: Biden said that he nominated Circuit Court of Appeals Judge Ketanji Brown Jackson to the United States Supreme Court, and that she is one of the nation\'s top legal minds who will continue Justice Breyer\'s legacy of excellence. Sources: ../../state_of_the_union.txt > Finished chain. ""Biden said that he nominated Circuit Court of Appeals Judge Ketanji Brown Jackson to the United States Supreme Court, and that she is one of the nation\'s top legal minds who will continue Justice Breyer\'s legacy of excellence. Sources: ../../state_of_the_union.txt"" ``` ## Multiple Vectorstores We can also easily use this initialize an agent with multiple vectorstores and use the agent to route between them. To do this. This agent is optimized for routing, so it is a different toolkit and initializer. ```python from langchain.agents.agent_toolkits import ( VectorStoreInfo, VectorStoreRouterToolkit, create_vectorstore_router_agent, ) ``` ```python ruff_vectorstore_info = VectorStoreInfo( name=""ruff"", description=""Information about the Ruff python linting library"", vectorstore=ruff_store, ) router_toolkit = VectorStoreRouterToolkit( vectorstores=[vectorstore_info, ruff_vectorstore_info], llm=llm ) agent_executor = create_vectorstore_router_agent( llm=llm, toolkit=router_toolkit, verbose=True ) ``` ## Examples ```python agent_executor.run( ""What did biden say about ketanji brown jackson in the state of the union address?"" ) ``` ```text > Entering new AgentExecutor chain... I need to use the state_of_union_address tool to answer this question. Action: state_of_union_address Action Input: What did biden say about ketanji brown jackson Observation: Biden said that Ketanji Brown Jackson is one of the nation\'s top legal minds and that she will continue Justice Breyer\'s legacy of excellence. Thought: I now know the final answer Final Answer: Biden said that Ketanji Brown Jackson is one of the nation\'s top legal minds and that she will continue Justice Breyer\'s legacy of excellence. > Finished chain. ""Biden said that Ketanji Brown Jackson is one of the nation\'s top legal minds and that she will continue Justice Breyer\'s legacy of excellence."" ``` ```python agent_executor.run(""What tool does ruff use to run over Jupyter Notebooks?"") ``` ```text > Entering new AgentExecutor chain... I need to find out what tool ruff uses to run over Jupyter Notebooks Action: ruff Action Input: What tool does ruff use to run over Jupyter Notebooks? Observation: Ruff is integrated into nbQA, a tool for running linters and code formatters over Jupyter Notebooks. After installing ruff and nbqa, you can run Ruff over a notebook like so: > nbqa ruff Untitled.html Thought: I now know the final answer Final Answer: Ruff is integrated into nbQA, a tool for running linters and code formatters over Jupyter Notebooks. After installing ruff and nbqa, you can run Ruff over a notebook like so: > nbqa ruff Untitled.html > Finished chain. \'Ruff is integrated into nbQA, a tool for running linters and code formatters over Jupyter Notebooks. After installing ruff and nbqa, you can run Ruff over a notebook like so: > nbqa ruff Untitled.html\' ``` ```python agent_executor.run( ""What tool does ruff use to run over Jupyter Notebooks? Did the president mention that tool in the state of the union?"" ) ``` ```text > Entering new AgentExecutor chain... I need to find out what tool ruff uses and if the president mentioned it in the state of the union. Action: ruff Action Input: What tool does ruff use to run over Jupyter Notebooks? Observation: Ruff is integrated into nbQA, a tool for running linters and code formatters over Jupyter Notebooks. After installing ruff and nbqa, you can run Ruff over a notebook like so: > nbqa ruff Untitled.html Thought: I need to find out if the president mentioned nbQA in the state of the union. Action: state_of_union_address Action Input: Did the president mention nbQA in the state of the union? Observation: No, the president did not mention nbQA in the state of the union. Thought: I now know the final answer. Final Answer: No, the president did not mention nbQA in the state of the union. > Finished chain. \'No, the president did not mention nbQA in the state of the union.\' ``` - [Create Vectorstores](#create-vectorstores) - [Initialize Toolkit and Agent](#initialize-toolkit-and-agent) - [Examples](#examples) - [Multiple Vectorstores](#multiple-vectorstores) - [Examples](#examples-1)', 'langchain.vectorstores.elasticsearch.ElasticsearchStore LangChain 0.0.337 API Experimental Python Docs Toggle Menu PrevUp Next LangChain 0.0.337 langchain.vectorstores.elasticsearch.ElasticsearchStore langchain.vectorstores.elasticsearch.ElasticsearchStore class langchain.vectorstores.elasticsearch.ElasticsearchStore(index_name: str, *, embedding: ~typing.Optional[~langchain.schema.embeddings.Embeddings] = None, es_connection: ~typing.Optional[Elasticsearch] = None, es_url: ~typing.Optional[str] = None, es_cloud_id: ~typing.Optional[str] = None, es_user: ~typing.Optional[str] = None, es_api_key: ~typing.Optional[str] = None, es_password: ~typing.Optional[str] = None, vector_query_field: str = \'vector\', query_field: str = \'text\', distance_strategy: ~typing.Optional[~typing.Literal[, , ]] = None, strategy: ~langchain.vectorstores.elasticsearch.BaseRetrievalStrategy = )[source] Elasticsearch vector store. Example from langchain.vectorstores import ElasticsearchStore from langchain.embeddings.openai import OpenAIEmbeddings embeddings = OpenAIEmbeddings() vectorstore = ElasticsearchStore( embedding=OpenAIEmbeddings(), index_name=""langchain-demo"", es_url="" ) Parameters index_name Name of the Elasticsearch index to create. es_url URL of the Elasticsearch instance to connect to. cloud_id Cloud ID of the Elasticsearch instance to connect to. es_user Username to use when connecting to Elasticsearch. es_password Password to use when connecting to Elasticsearch. es_api_key API key to use when connecting to Elasticsearch. es_connection Optional pre-existing Elasticsearch connection. vector_query_field Optional. Name of the field to store the embedding vectors in. query_field Optional. Name of the field to store the texts in. strategy Optional. Retrieval strategy to use when searching the index. Defaults to ApproxRetrievalStrategy. Can be one of ExactRetrievalStrategy, ApproxRetrievalStrategy, or SparseRetrievalStrategy. distance_strategy Optional. Distance strategy to use when searching the index. Defaults to COSINE. Can be one of COSINE, EUCLIDEAN_DISTANCE, or DOT_PRODUCT. If you want to use a cloud hosted Elasticsearch instance, you can pass in the cloud_id argument instead of the es_url argument. Example from langchain.vectorstores import ElasticsearchStore from langchain.embeddings.openai import OpenAIEmbeddings vectorstore = ElasticsearchStore( embedding=OpenAIEmbeddings(), index_name=""langchain-demo"", es_cloud_id="""" es_user=""elastic"", es_password="""" ) You can also connect to an existing Elasticsearch instance by passing in a pre-existing Elasticsearch connection via the es_connection argument. Example from langchain.vectorstores import ElasticsearchStore from langchain.embeddings.openai import OpenAIEmbeddings from elasticsearch import Elasticsearch es_connection = Elasticsearch("" vectorstore = ElasticsearchStore( embedding=OpenAIEmbeddings(), index_name=""langchain-demo"", es_connection=es_connection ) ElasticsearchStore by default uses the ApproxRetrievalStrategy, which uses the HNSW algorithm to perform approximate nearest neighbor search. This is the fastest and most memory efficient algorithm. If you want to use the Brute force / Exact strategy for searching vectors, you can pass in the ExactRetrievalStrategy to the ElasticsearchStore constructor. Example from langchain.vectorstores import ElasticsearchStore from langchain.embeddings.openai import OpenAIEmbeddings vectorstore = ElasticsearchStore( embedding=OpenAIEmbeddings(), index_name=""langchain-demo"", es_url="" strategy=ElasticsearchStore.ExactRetrievalStrategy() ) Both strategies require that you know the similarity metric you want to use when creating the index. The default is cosine similarity, but you can also use dot product or euclidean distance. Example from langchain.vectorstores import ElasticsearchStore from langchain.embeddings.openai import OpenAIEmbeddings from langchain.vectorstores.utils import DistanceStrategy vectorstore = ElasticsearchStore( embedding=OpenAIEmbeddings(), index_name=""langchain-demo"", es_url="" distance_strategy=""DOT_PRODUCT"" ) Attributes embeddings Access the query embedding object if available. Methods ApproxRetrievalStrategy([query_model_id,...]) Used to perform approximate nearest neighbor search using the HNSW algorithm. ExactRetrievalStrategy() Used to perform brute force / exact nearest neighbor search via script_score. SparseVectorRetrievalStrategy([model_id]) Used to perform sparse vector search via text_expansion. __init__(index_name,*[,embedding,...]) aadd_documents(documents,**kwargs) Run more documents through the embeddings and add to the vectorstore. aadd_texts(texts[,metadatas]) Run more texts through the embeddings and add to the vectorstore. add_documents(documents,**kwargs) Run more documents through the embeddings and add to the vectorstore. add_embeddings(text_embeddings[,metadatas,...]) Add the given texts and embeddings to the vectorstore. add_texts(texts[,metadatas,ids,...]) Run more texts through the embeddings and add to the vectorstore. adelete([ids]) Delete by vector ID or other criteria. afrom_documents(documents,embedding,**kwargs) Return VectorStore initialized from documents and embeddings. afrom_texts(texts,embedding[,metadatas]) Return VectorStore initialized from texts and embeddings. amax_marginal_relevance_search(query[,k,...]) Return docs selected using the maximal marginal relevance. amax_marginal_relevance_search_by_vector(...) Return docs selected using the maximal marginal relevance. as_retriever(**kwargs) Return VectorStoreRetriever initialized from this VectorStore. asearch(query, search_type, **kwargs) Return docs most similar to query using specified search type. asimilarity_search(query[, k]) Return docs most similar to query. asimilarity_search_by_vector(embedding[, k]) Return docs most similar to embedding vector. asimilarity_search_with_relevance_scores(query) Return docs and relevance scores in the range [0, 1], asynchronously. asimilarity_search_with_score(*args, **kwargs) Run similarity search with distance asynchronously. connect_to_elasticsearch(*[, es_url, ...]) delete([ids, refresh_indices]) Delete documents from the Elasticsearch index. from_documents(documents[, embedding, ...]) Construct ElasticsearchStore wrapper from documents. from_texts(texts[, embedding, metadatas, ...]) Construct ElasticsearchStore wrapper from raw documents. get_user_agent() max_marginal_relevance_search(query[, k, ...]) Return docs selected using the maximal marginal relevance. max_marginal_relevance_search_by_vector(...) Return docs selected using the maximal marginal relevance. search(query, search_type, **kwargs) Return docs most similar to query using specified search type. similarity_search(query[, k, fetch_k, filter]) Return Elasticsearch documents most similar to query. similarity_search_by_vector(embedding[, k]) Return docs most similar to embedding vector. similarity_search_by_vector_with_relevance_scores(...) Return Elasticsearch documents most similar to query, along with scores. similarity_search_with_relevance_scores(query) Return docs and relevance scores in the range [0, 1]. similarity_search_with_score(query[, k, filter]) Return Elasticsearch documents most similar to query, along with scores. static ApproxRetrievalStrategy(query_model_id: Optional[str] = None, hybrid: Optional[bool] = False, rrf: Optional[Union[dict, bool]] = True)  ApproxRetrievalStrategy[source] Used to perform approximate nearest neighbor search using the HNSW algorithm. At build index time, this strategy will create a dense vector field in the index and store the embedding vectors in the index. At query time, the text will either be embedded using the provided embedding function or the query_model_id will be used to embed the text using the model deployed to Elasticsearch. if query_model_id is used, do not provide an embedding function. Parameters query_model_id  Optional. ID of the model to use to embed the query text within the stack. Requires embedding model to be deployed to Elasticsearch. hybrid  Optional. If True, will perform a hybrid search using both the knn query and a text query. Defaults to False. rrf  Optional. rrf is Reciprocal Rank Fusion. When hybrid is True, and rrf is True, then rrf: {}. and rrf is False, then rrf is omitted. and isinstance(rrf, dict) is True, then pass in the dict values. rrf could be passed for adjusting \'rank_constant\' and \'window_size\'. static ExactRetrievalStrategy()  ExactRetrievalStrategy[source] Used to perform brute force / exact nearest neighbor search via script_score. static SparseVectorRetrievalStrategy(model_id: Optional[str] = None)  SparseRetrievalStrategy[source] Used to perform sparse vector search via text_expansion. Used for when you want to use ELSER model to perform document search. At build index time, this strategy will create a pipeline that will embed the text using the ELSER model and store the resulting tokens in the index. At query time, the text will be embedded using the ELSER model and the resulting tokens will be used to perform a text_expansion query. Parameters model_id  Optional. Default is .elser_model_1. ID of the model to use to embed the query text within the stack. Requires embedding model to be deployed to Elasticsearch. __init__(index_name: str, *, embedding: ~typing.Optional[~langchain.schema.embeddings.Embeddings] = None, es_connection: ~typing.Optional[Elasticsearch] = None, es_url: ~typing.Optional[str] = None, es_cloud_id: ~typing.Optional[str] = None, es_user: ~typing.Optional[str] = None, es_api_key: ~typing.Optional[str] = None, es_password: ~typing.Optional[str] = None, vector_query_field: str = \'vector\', query_field: str = \'text\', distance_strategy: ~typing.Optional[~typing.Literal[, , ]] = None, strategy: ~langchain.vectorstores.elasticsearch.BaseRetrievalStrategy = )[source] async aadd_documents(documents: List[Document], **kwargs: Any)  List[str] Run more documents through the embeddings and add to the vectorstore. Parameters (List[Document] (documents)  Documents to add to the vectorstore. Returns List of IDs of the added texts. Return type List[str] async aadd_texts(texts: Iterable[str], metadatas: Optional[List[dict]] = None, **kwargs: Any)  List[str] Run more texts through the embeddings and add to the vectorstore. add_documents(documents: List[Document], **kwargs: Any)  List[str] Run more documents through the embeddings and add to the vectorstore. Parameters (List[Document] (documents)  Documents to add to the vectorstore. Returns List of IDs of the added texts. Return type List[str] add_embeddings(text_embeddings: Iterable[Tuple[str, List[float]]], metadatas: Optional[List[dict]] = None, ids: Optional[List[str]] = None, refresh_indices: bool = True, create_index_if_not_exists: bool = True, bulk_kwargs: Optional[Dict] = None, **kwargs: Any)  List[str][source] Add the given texts and embeddings to the vectorstore. Parameters text_embeddings  Iterable pairs of string and embedding to add to the vectorstore. metadatas  Optional list of metadatas associated with the texts. ids  Optional list of unique IDs. refresh_indices  Whether to refresh the Elasticsearch indices after adding the texts. create_index_if_not_exists  Whether to create the Elasticsearch index if it doesn\'t already exist. *bulk_kwargs  Additional arguments to pass to Elasticsearch bulk. - chunk_size: Optional. Number of texts to add to the index at a time. Defaults to 500. Returns List of ids from adding the texts into the vectorstore. add_texts(texts: Iterable[str], metadatas: Optional[List[Dict[Any, Any]]] = None, ids: Optional[List[str]] = None, refresh_indices: bool = True, create_index_if_not_exists: bool = True, bulk_kwargs: Optional[Dict] = None, **kwargs: Any)  List[str][source] Run more texts through the embeddings and add to the vectorstore. Parameters texts  Iterable of strings to add to the vectorstore. metadatas  Optional list of metadatas associated with the texts. ids  Optional list of ids to associate with the texts. refresh_indices  Whether to refresh the Elasticsearch indices after adding the texts. create_index_if_not_exists  Whether to create the Elasticsearch index if it doesn\'t already exist. *bulk_kwargs  Additional arguments to pass to Elasticsearch bulk. - chunk_size: Optional. Number of texts to add to the index at a time. Defaults to 500. Returns List of ids from adding the texts into the vectorstore. async adelete(ids: Optional[List[str]] = None, **kwargs: Any)  Optional[bool] Delete by vector ID or other criteria. Parameters ids  List of ids to delete. **kwargs  Other keyword arguments that subclasses might use. Returns True if deletion is successful, False otherwise, None if not implemented. Return type Optional[bool] async classmethod afrom_documents(documents: List[Document], embedding: Embeddings, **kwargs: Any)  VST Return VectorStore initialized from documents and embeddings. async classmethod afrom_texts(texts: List[str], embedding: Embeddings, metadatas: Optional[List[dict]] = None, **kwargs: Any)  VST Return VectorStore initialized from texts and embeddings. async amax_marginal_relevance_search(query: str, k: int = 4, fetch_k: int = 20, lambda_mult: float = 0.5, **kwargs: Any)  List[Document] Return docs selected using the maximal marginal relevance. async amax_marginal_relevance_search_by_vector(embedding: List[float], k: int = 4, fetch_k: int = 20, lambda_mult: float = 0.5, **kwargs: Any)  List[Document] Return docs selected using the maximal marginal relevance. as_retriever(**kwargs: Any)  VectorStoreRetriever Return VectorStoreRetriever initialized from this VectorStore. Parameters search_type (Optional[str])  Defines the type of search that the Retriever should perform. Can be similarity (default), mmr, or similarity_score_threshold. search_kwargs (Optional[Dict])  Keyword arguments to pass to the search function. Can include things like: k: Amount of documents to return (Default: 4) score_threshold: Minimum relevance threshold for similarity_score_threshold fetch_k: Amount of documents to pass to MMR algorithm (Default: 20) lambda_mult: Diversity of results returned by MMR; 1 for minimum diversity and 0 for maximum. (Default: 0.5) filter: Filter by document metadata Returns Retriever class for VectorStore. Return type VectorStoreRetriever Examples: # Retrieve more documents with higher diversity # Useful if your dataset has many similar documents docsearch.as_retriever( search_type=""mmr"", search_kwargs={\'k\': 6, \'lambda_mult\': 0.25} ) # Fetch more documents for the MMR algorithm to consider # But only return the top 5 docsearch.as_retriever( search_type=""mmr"", search_kwargs={\'k\': 5, \'fetch_k\': 50} ) # Only retrieve documents that have a relevance score # Above a certain threshold docsearch.as_retriever( search_type=""similarity_score_threshold"", search_kwargs={\'score_threshold\': 0.8} ) # Only get the single most similar document from the dataset docsearch.as_retriever(search_kwargs={\'k\': 1}) # Use a filter to only retrieve documents from a specific paper docsearch.as_retriever( search_kwargs={\'filter\': {\'paper_title\':\'GPT-4 Technical Report\'}} ) async asearch(query: str, search_type: str, **kwargs: Any)  List[Document] Return docs most similar to query using specified search type. async asimilarity_search(query: str, k: int = 4, **kwargs: Any)  List[Document] Return docs most similar to query. async asimilarity_search_by_vector(embedding: List[float], k: int = 4, **kwargs: Any)  List[Document] Return docs most similar to embedding vector. async asimilarity_search_with_relevance_scores(query: str, k: int = 4, **kwargs: Any)  List[Tuple[Document, float]] Return docs and relevance scores in the range [0, 1], asynchronously. 0 is dissimilar, 1 is most similar. Parameters query  input text k  Number of Documents to return. Defaults to 4. **kwargs  kwargs to be passed to similarity search. Should include: score_threshold: Optional, a floating point value between 0 to 1 to filter the resulting set of retrieved docs Returns List of Tuples of (doc, similarity_score) async asimilarity_search_with_score(*args: Any, **kwargs: Any)  List[Tuple[Document, float]] Run similarity search with distance asynchronously. static connect_to_elasticsearch(*, es_url: Optional[str] = None, cloud_id: Optional[str] = None, api_key: Optional[str] = None, username: Optional[str] = None, password: Optional[str] = None)  Elasticsearch[source] delete(ids: Optional[List[str]] = None, refresh_indices: Optional[bool] = True, **kwargs: Any)  Optional[bool][source] Delete documents from the Elasticsearch index. Parameters ids  List of ids of documents to delete. refresh_indices  Whether to refresh the index after deleting documents. Defaults to True. classmethod from_documents(documents: List[Document], embedding: Optional[Embeddings] = None, bulk_kwargs: Optional[Dict] = None, **kwargs: Any)  ElasticsearchStore[source] Construct ElasticsearchStore wrapper from documents. Example from langchain.vectorstores import ElasticsearchStore from langchain.embeddings.openai import OpenAIEmbeddings db = ElasticsearchStore.from_documents( texts, embeddings, index_name=""langchain-demo"", es_url="" ) Parameters texts  List of texts to add to the Elasticsearch index. embedding  Embedding function to use to embed the texts. Do not provide if using a strategy that doesn\'t require inference. metadatas  Optional list of metadatas associated with the texts. index_name  Name of the Elasticsearch index to create. es_url  URL of the Elasticsearch instance to connect to. cloud_id  Cloud ID of the Elasticsearch instance to connect to. es_user  Username to use when connecting to Elasticsearch. es_password  Password to use when connecting to Elasticsearch. es_api_key  API key to use when connecting to Elasticsearch. es_connection  Optional pre-existing Elasticsearch connection. vector_query_field  Optional. Name of the field to store the embedding vectors in. query_field  Optional. Name of the field to store the texts in. bulk_kwargs  Optional. Additional arguments to pass to Elasticsearch bulk. classmethod from_texts(texts: List[str], embedding: Optional[Embeddings] = None, metadatas: Optional[List[Dict[str, Any]]] = None, bulk_kwargs: Optional[Dict] = None, **kwargs: Any)  ElasticsearchStore[source] Construct ElasticsearchStore wrapper from raw documents. Example from langchain.vectorstores import ElasticsearchStore from langchain.embeddings.openai import OpenAIEmbeddings db = ElasticsearchStore.from_texts( texts, // embeddings optional if using // a strategy that doesn\'t require inference embeddings, index_name=""langchain-demo"", es_url="" ) Parameters texts  List of texts to add to the Elasticsearch index. embedding  Embedding function to use to embed the texts. metadatas  Optional list of metadatas associated with the texts. index_name  Name of the Elasticsearch index to create. es_url  URL of the Elasticsearch instance to connect to. cloud_id  Cloud ID of the Elasticsearch instance to connect to. es_user  Username to use when connecting to Elasticsearch. es_password  Password to use when connecting to Elasticsearch. es_api_key  API key to use when connecting to Elasticsearch. es_connection  Optional pre-existing Elasticsearch connection. vector_query_field  Optional. Name of the field to store the embedding vectors in. query_field  Optional. Name of the field to store the texts in. distance_strategy  Optional. Name of the distance strategy to use. Defaults to COSINE. can be one of COSINE, EUCLIDEAN_DISTANCE, DOT_PRODUCT. bulk_kwargs  Optional. Additional arguments to pass to Elasticsearch bulk. static get_user_agent()  str[source] max_marginal_relevance_search(query: str, k: int = 4, fetch_k: int = 20, lambda_mult: float = 0.5, fields: Optional[List[str]] = None, **kwargs: Any)  List[Document][source] Return docs selected using the maximal marginal relevance. Maximal marginal relevance optimizes for similarity to query AND diversityamong selected documents. Parameters query (str)  Text to look up documents similar to. k (int)  Number of Documents to return. Defaults to 4. fetch_k (int)  Number of Documents to fetch to pass to MMR algorithm. lambda_mult (float)  Number between 0 and 1 that determines the degree of diversity among the results with 0 corresponding to maximum diversity and 1 to minimum diversity. Defaults to 0.5. fields  Other fields to get from elasticsearch source. These fields will be added to the document metadata. Returns A list of Documents selected by maximal marginal relevance. Return type List[Document] max_marginal_relevance_search_by_vector(embedding: List[float], k: int = 4, fetch_k: int = 20, lambda_mult: float = 0.5, **kwargs: Any)  List[Document] Return docs selected using the maximal marginal relevance. Maximal marginal relevance optimizes for similarity to query AND diversity among selected documents. Parameters embedding  Embedding to look up documents similar to. k  Number of Documents to return. Defaults to 4. fetch_k  Number of Documents to fetch to pass to MMR algorithm. lambda_mult  Number between 0 and 1 that determines the degree of diversity among the results with 0 corresponding to maximum diversity and 1 to minimum diversity. Defaults to 0.5. Returns List of Documents selected by maximal marginal relevance. search(query: str, search_type: str, **kwargs: Any)  List[Document] Return docs most similar to query using specified search type. similarity_search(query: str, k: int = 4, fetch_k: int = 50, filter: Optional[List[dict]] = None, **kwargs: Any)  List[Document][source] Return Elasticsearch documents most similar to query. Parameters query  Text to look up documents similar to. k  Number of Documents to return. Defaults to 4. fetch_k (int)  Number of Documents to fetch to pass to knn num_candidates. filter  Array of Elasticsearch filter clauses to apply to the query. Returns List of Documents most similar to the query, in descending order of similarity. similarity_search_by_vector(embedding: List[float], k: int = 4, **kwargs: Any)  List[Document] Return docs most similar to embedding vector. Parameters embedding  Embedding to look up documents similar to. k  Number of Documents to return. Defaults to 4. Returns List of Documents most similar to the query vector. similarity_search_by_vector_with_relevance_scores(embedding: List[float], k: int = 4, filter: Optional[List[Dict]] = None, **kwargs: Any)  List[Tuple[Document, float]][source] Return Elasticsearch documents most similar to query, along with scores. Parameters embedding  Embedding to look up documents similar to. k  Number of Documents to return. Defaults to 4. filter  Array of Elasticsearch filter clauses to apply to the query. Returns List of Documents most similar to the embedding and score for each similarity_search_with_relevance_scores(query: str, k: int = 4, **kwargs: Any)  List[Tuple[Document, float]] Return docs and relevance scores in the range [0, 1]. 0 is dissimilar, 1 is most similar. Parameters query  input text k  Number of Documents to return. Defaults to 4. **kwargs  kwargs to be passed to similarity search. Should include: score_threshold: Optional, a floating point value between 0 to 1 to filter the resulting set of retrieved docs Returns List of Tuples of (doc, similarity_score) similarity_search_with_score(query: str, k: int = 4, filter: Optional[List[dict]] = None, **kwargs: Any)  List[Tuple[Document, float]][source] Return Elasticsearch documents most similar to query, along with scores. Parameters query  Text to look up documents similar to. k  Number of Documents to return. Defaults to 4. filter  Array of Elasticsearch filter clauses to apply to the query. Returns List of Documents most similar to the query and score for each Examples using ElasticsearchStore Elasticsearch Indexing  2023, Harrison Chase. Last updated on Nov 17, 2023. Show this page source', 'langchain.vectorstores.redis.base.Redis LangChain 0.0.337 API Experimental Python Docs Toggle Menu PrevUp Next LangChain 0.0.337 langchain.vectorstores.redis.base.Redis langchain.vectorstores.redis.base.Redis class langchain.vectorstores.redis.base.Redis(redis_url: str, index_name: str, embedding: Embeddings, index_schema: Optional[Union[Dict[str, str], str, PathLike]] = None, vector_schema: Optional[Dict[str, Union[int, str]]] = None, relevance_score_fn: Optional[Callable[[float], float]] = None, key_prefix: Optional[str] = None, **kwargs: Any)[source] Redis vector database. To use, you should have the redis python package installed and have a running Redis Enterprise or Redis-Stack server For production use cases, it is recommended to use Redis Enterprise as the scaling, performance, stability and availability is much better than Redis-Stack. For testing and prototyping, however, this is not required. Redis-Stack is available as a docker container the full vector search API available. # to run redis stack in docker locally docker run -d -p 6379:6379 -p 8001:8001 redis/redis-stack:latest Once running, you can connect to the redis server with the following url schemas: - redis://: # simple connection - redis://:@: # connection with authentication - rediss://: # connection with SSL - rediss://:@: # connection with SSL and auth Examples: The following examples show various ways to use the Redis VectorStore with LangChain. For all the following examples assume we have the following imports: from langchain.vectorstores import Redis from langchain.embeddings import OpenAIEmbeddings Initialize, create index, and load Documentsfrom langchain.vectorstores import Redis from langchain.embeddings import OpenAIEmbeddings rds = Redis.from_documents( documents, # a list of Document objects from loaders or created embeddings, # an Embeddings object redis_url=""redis://localhost:6379"", ) Initialize, create index, and load Documents with metadatards = Redis.from_texts( texts, # a list of strings metadata, # a list of metadata dicts embeddings, # an Embeddings object redis_url=""redis://localhost:6379"", ) Initialize, create index, and load Documents with metadata and return keys rds, keys = Redis.from_texts_return_keys( texts, # a list of strings metadata, # a list of metadata dicts embeddings, # an Embeddings object redis_url=""redis://localhost:6379"", ) For use cases where the index needs to stay alive, you can initialize with an index name such that it\'s easier to reference later rds = Redis.from_texts( texts, # a list of strings metadata, # a list of metadata dicts embeddings, # an Embeddings object index_name=""my-index"", redis_url=""redis://localhost:6379"", ) Initialize and connect to an existing index (from above) rds = Redis.from_existing_index( embeddings, # an Embeddings object index_name=""my-index"", redis_url=""redis://localhost:6379"", ) Advanced examples: Custom vector schema can be supplied to change the way that Redis creates the underlying vector schema. This is useful for production use cases where you want to optimize the vector schema for your use case. ex. using HNSW instead of FLAT (knn) which is the default vector_schema = { ""algorithm"": ""HNSW"" } rds = Redis.from_texts( texts, # a list of strings metadata, # a list of metadata dicts embeddings, # an Embeddings object vector_schema=vector_schema, redis_url=""redis://localhost:6379"", ) Custom index schema can be supplied to change the way that the metadata is indexed. This is useful for you would like to use the hybrid querying (filtering) capability of Redis. By default, this implementation will automatically generate the index schema according to the following rules: All strings are indexed as text fields All numbers are indexed as numeric fields All lists of strings are indexed as tag fields (joined bylangchain.vectorstores.redis.constants.REDIS_TAG_SEPARATOR) All None values are not indexed but still stored in Redis these arenot retrievable through the interface here, but the raw Redis client can be used to retrieve them. All other types are not indexed To override these rules, you can pass in a custom index schema like the following tag: - name: credit_score text: - name: user - name: job Typically, the credit_score field would be a text field since it\'s a string, however, we can override this behavior by specifying the field type as shown with the yaml config (can also be a dictionary) above and the code below. rds = Redis.from_texts( texts, # a list of strings metadata, # a list of metadata dicts embeddings, # an Embeddings object index_schema=""path/to/index_schema.yaml"", # can also be a dictionary redis_url=""redis://localhost:6379"", ) When connecting to an existing index where a custom schema has been applied, it\'s important to pass in the same schema to the from_existing_index method. Otherwise, the schema for newly added samples will be incorrect and metadata will not be returned. Initialize with necessary components. Attributes DEFAULT_VECTOR_SCHEMA embeddings Access the query embedding object if available. schema Return the schema of the index. Methods __init__(redis_url,index_name,embedding[,...]) Initialize with necessary components. aadd_documents(documents,**kwargs) Run more documents through the embeddings and add to the vectorstore. aadd_texts(texts[,metadatas]) Run more texts through the embeddings and add to the vectorstore. add_documents(documents,**kwargs) Run more documents through the embeddings and add to the vectorstore. add_texts(texts[,metadatas,embeddings,...]) Add more texts to the vectorstore. adelete([ids]) Delete by vector ID or other criteria. afrom_documents(documents,embedding,**kwargs) Return VectorStore initialized from documents and embeddings. afrom_texts(texts,embedding[,metadatas]) Return VectorStore initialized from texts and embeddings. amax_marginal_relevance_search(query[,k,...]) Return docs selected using the maximal marginal relevance. amax_marginal_relevance_search_by_vector(...) Return docs selected using the maximal marginal relevance. as_retriever(**kwargs) Return VectorStoreRetriever initialized from this VectorStore. asearch(query,search_type,**kwargs) Return docs most similar to query using specified search type. asimilarity_search(query[,k]) Return docs most similar to query. asimilarity_search_by_vector(embedding[,k]) Return docs most similar to embedding vector. asimilarity_search_with_relevance_scores(query) Return docs and relevance scores in the range [0, 1], asynchronously. asimilarity_search_with_score(*args,**kwargs) Run similarity search with distance asynchronously. delete([ids]) Delete a Redis entry. drop_index(index_name,delete_documents,...) Drop a Redis search index. from_documents(documents,embedding,**kwargs) Return VectorStore initialized from documents and embeddings. from_existing_index(embedding,index_name,...) Connect to an existing Redis index. from_texts(texts,embedding[,metadatas,...]) Create a Redis vectorstore from a list of texts. from_texts_return_keys(texts, embedding[, ...]) Create a Redis vectorstore from raw documents. max_marginal_relevance_search(query[, k, ...]) Return docs selected using the maximal marginal relevance. max_marginal_relevance_search_by_vector(...) Return docs selected using the maximal marginal relevance. search(query, search_type, **kwargs) Return docs most similar to query using specified search type. similarity_search(query[, k, filter, ...]) Run similarity search similarity_search_by_vector(embedding[, k, ...]) Run similarity search between a query vector and the indexed vectors. similarity_search_limit_score(query[, k, ...]) [Deprecated] Returns the most similar indexed documents to the query text within the score_threshold range. similarity_search_with_relevance_scores(query) Return docs and relevance scores in the range [0, 1]. similarity_search_with_score(query[, k, ...]) Run similarity search with vector distance. write_schema(path) Write the schema to a yaml file. __init__(redis_url: str, index_name: str, embedding: Embeddings, index_schema: Optional[Union[Dict[str, str], str, PathLike]] = None, vector_schema: Optional[Dict[str, Union[int, str]]] = None, relevance_score_fn: Optional[Callable[[float], float]] = None, key_prefix: Optional[str] = None, **kwargs: Any)[source] Initialize with necessary components. async aadd_documents(documents: List[Document], **kwargs: Any)  List[str] Run more documents through the embeddings and add to the vectorstore. Parameters (List[Document] (documents)  Documents to add to the vectorstore. Returns List of IDs of the added texts. Return type List[str] async aadd_texts(texts: Iterable[str], metadatas: Optional[List[dict]] = None, **kwargs: Any)  List[str] Run more texts through the embeddings and add to the vectorstore. add_documents(documents: List[Document], **kwargs: Any)  List[str] Run more documents through the embeddings and add to the vectorstore. Parameters (List[Document] (documents)  Documents to add to the vectorstore. Returns List of IDs of the added texts. Return type List[str] add_texts(texts: Iterable[str], metadatas: Optional[List[dict]] = None, embeddings: Optional[List[List[float]]] = None, batch_size: int = 1000, clean_metadata: bool = True, **kwargs: Any)  List[str][source] Add more texts to the vectorstore. Parameters texts (Iterable[str])  Iterable of strings/text to add to the vectorstore. metadatas (Optional[List[dict]], optional)  Optional list of metadatas. Defaults to None. embeddings (Optional[List[List[float]]], optional)  Optional pre-generated embeddings. Defaults to None. keys (List[str]) or ids (List[str])  Identifiers of entries. Defaults to None. batch_size (int, optional)  Batch size to use for writes. Defaults to 1000. Returns List of ids added to the vectorstore Return type List[str] async adelete(ids: Optional[List[str]] = None, **kwargs: Any)  Optional[bool] Delete by vector ID or other criteria. Parameters ids  List of ids to delete. **kwargs  Other keyword arguments that subclasses might use. Returns True if deletion is successful, False otherwise, None if not implemented. Return type Optional[bool] async classmethod afrom_documents(documents: List[Document], embedding: Embeddings, **kwargs: Any)  VST Return VectorStore initialized from documents and embeddings. async classmethod afrom_texts(texts: List[str], embedding: Embeddings, metadatas: Optional[List[dict]] = None, **kwargs: Any)  VST Return VectorStore initialized from texts and embeddings. async amax_marginal_relevance_search(query: str, k: int = 4, fetch_k: int = 20, lambda_mult: float = 0.5, **kwargs: Any)  List[Document] Return docs selected using the maximal marginal relevance. async amax_marginal_relevance_search_by_vector(embedding: List[float], k: int = 4, fetch_k: int = 20, lambda_mult: float = 0.5, **kwargs: Any)  List[Document] Return docs selected using the maximal marginal relevance. as_retriever(**kwargs: Any)  RedisVectorStoreRetriever[source] Return VectorStoreRetriever initialized from this VectorStore. Parameters search_type (Optional[str])  Defines the type of search that the Retriever should perform. Can be similarity (default), mmr, or similarity_score_threshold. search_kwargs (Optional[Dict])  Keyword arguments to pass to the search function. Can include things like: k: Amount of documents to return (Default: 4) score_threshold: Minimum relevance threshold for similarity_score_threshold fetch_k: Amount of documents to pass to MMR algorithm (Default: 20) lambda_mult: Diversity of results returned by MMR; 1 for minimum diversity and 0 for maximum. (Default: 0.5) filter: Filter by document metadata Returns Retriever class for VectorStore. Return type VectorStoreRetriever Examples: # Retrieve more documents with higher diversity # Useful if your dataset has many similar documents docsearch.as_retriever( search_type=""mmr"", search_kwargs={\'k\': 6, \'lambda_mult\': 0.25} ) # Fetch more documents for the MMR algorithm to consider # But only return the top 5 docsearch.as_retriever( search_type=""mmr"", search_kwargs={\'k\': 5, \'fetch_k\': 50} ) # Only retrieve documents that have a relevance score # Above a certain threshold docsearch.as_retriever( search_type=""similarity_score_threshold"", search_kwargs={\'score_threshold\': 0.8} ) # Only get the single most similar document from the dataset docsearch.as_retriever(search_kwargs={\'k\': 1}) # Use a filter to only retrieve documents from a specific paper docsearch.as_retriever( search_kwargs={\'filter\': {\'paper_title\':\'GPT-4 Technical Report\'}} ) async asearch(query: str, search_type: str, **kwargs: Any)  List[Document] Return docs most similar to query using specified search type. async asimilarity_search(query: str, k: int = 4, **kwargs: Any)  List[Document] Return docs most similar to query. async asimilarity_search_by_vector(embedding: List[float], k: int = 4, **kwargs: Any)  List[Document] Return docs most similar to embedding vector. async asimilarity_search_with_relevance_scores(query: str, k: int = 4, **kwargs: Any)  List[Tuple[Document, float]] Return docs and relevance scores in the range [0, 1], asynchronously. 0 is dissimilar, 1 is most similar. Parameters query  input text k  Number of Documents to return. Defaults to 4. **kwargs  kwargs to be passed to similarity search. Should include: score_threshold: Optional, a floating point value between 0 to 1 to filter the resulting set of retrieved docs Returns List of Tuples of (doc, similarity_score) async asimilarity_search_with_score(*args: Any, **kwargs: Any)  List[Tuple[Document, float]] Run similarity search with distance asynchronously. static delete(ids: Optional[List[str]] = None, **kwargs: Any)  bool[source] Delete a Redis entry. Parameters ids  List of ids (keys in redis) to delete. redis_url  Redis connection url. This should be passed in the kwargs or set as an environment variable: REDIS_URL. Returns Whether or not the deletions were successful. Return type bool Raises ValueError  If the redis python package is not installed. ValueError  If the ids (keys in redis) are not provided static drop_index(index_name: str, delete_documents: bool, **kwargs: Any)  bool[source] Drop a Redis search index. Parameters index_name (str)  Name of the index to drop. delete_documents (bool)  Whether to drop the associated documents. Returns Whether or not the drop was successful. Return type bool classmethod from_documents(documents: List[Document], embedding: Embeddings, **kwargs: Any)  VST Return VectorStore initialized from documents and embeddings. classmethod from_existing_index(embedding: Embeddings, index_name: str, schema: Union[Dict[str, str], str, PathLike], **kwargs: Any)  Redis[source] Connect to an existing Redis index. Example from langchain.vectorstores import Redis from langchain.embeddings import OpenAIEmbeddings embeddings = OpenAIEmbeddings() redisearch = Redis.from_existing_index( embeddings, index_name=""my-index"", redis_url=""redis://username:password@localhost:6379"" ) Parameters embedding (Embeddings)  Embedding model class (i.e. OpenAIEmbeddings) for embedding queries. index_name (str)  Name of the index to connect to. schema (Union[Dict[str, str], str, os.PathLike])  Schema of the index and the vector schema. Can be a dict, or path to yaml file **kwargs (Any)  Additional keyword arguments to pass to the Redis client. Returns Redis VectorStore instance. Return type Redis Raises ValueError  If the index does not exist. ImportError  If the redis python package is not installed. classmethod from_texts(texts: List[str], embedding: Embeddings, metadatas: Optional[List[dict]] = None, index_name: Optional[str] = None, index_schema: Optional[Union[Dict[str, str], str, PathLike]] = None, vector_schema: Optional[Dict[str, Union[int, str]]] = None, **kwargs: Any)  Redis[source] Create a Redis vectorstore from a list of texts. This is a user-friendly interface that: Embeds documents. Creates a new Redis index if it doesn\'t already exist Adds the documents to the newly created Redis index. This method will generate schema based on the metadata passed in if the index_schema is not defined. If the index_schema is defined, it will compare against the generated schema and warn if there are differences. If you are purposefully defining the schema for the metadata, then you can ignore that warning. To examine the schema options, initialize an instance of this class and print out the schema using the Redis.schema` property. This will include the content and content_vector classes which are always present in the langchain schema. Example from langchain.vectorstores import Redis from langchain.embeddings import OpenAIEmbeddings embeddings = OpenAIEmbeddings() redisearch = RediSearch.from_texts( texts, embeddings, redis_url=""redis://username:password@localhost:6379"" ) Parameters texts (List[str])  List of texts to add to the vectorstore. embedding (Embeddings)  Embedding model class (i.e. OpenAIEmbeddings) for embedding queries. metadatas (Optional[List[dict]], optional)  Optional list of metadata dicts to add to the vectorstore. Defaults to None. index_name (Optional[str], optional)  Optional name of the index to create or add to. Defaults to None. index_schema (Optional[Union[Dict[str, str], str, os.PathLike]], optional)  Optional fields to index within the metadata. Overrides generated schema. Defaults to None. vector_schema (Optional[Dict[str, Union[str, int]]], optional)  Optional vector schema to use. Defaults to None. **kwargs (Any)  Additional keyword arguments to pass to the Redis client. Returns Redis VectorStore instance. Return type Redis Raises ValueError  If the number of metadatas does not match the number of texts. ImportError  If the redis python package is not installed. classmethod from_texts_return_keys(texts: List[str], embedding: Embeddings, metadatas: Optional[List[dict]] = None, index_name: Optional[str] = None, index_schema: Optional[Union[Dict[str, str], str, PathLike]] = None, vector_schema: Optional[Dict[str, Union[int, str]]] = None, **kwargs: Any)  Tuple[Redis, List[str]][source] Create a Redis vectorstore from raw documents. This is a user-friendly interface that: Embeds documents. Creates a new Redis index if it doesn\'t already exist Adds the documents to the newly created Redis index. Returns the keys of the newly created documents once stored. This method will generate schema based on the metadata passed in if the index_schema is not defined. If the index_schema is defined, it will compare against the generated schema and warn if there are differences. If you are purposefully defining the schema for the metadata, then you can ignore that warning. To examine the schema options, initialize an instance of this class and print out the schema using the Redis.schema` property. This will include the content and content_vector classes which are always present in the langchain schema. Example from langchain.vectorstores import Redis from langchain.embeddings import OpenAIEmbeddings embeddings = OpenAIEmbeddings() redis, keys = Redis.from_texts_return_keys( texts, embeddings, redis_url=""redis://localhost:6379"" ) Parameters texts (List[str])  List of texts to add to the vectorstore. embedding (Embeddings)  Embeddings to use for the vectorstore. metadatas (Optional[List[dict]], optional)  Optional list of metadata dicts to add to the vectorstore. Defaults to None. index_name (Optional[str], optional)  Optional name of the index to create or add to. Defaults to None. index_schema (Optional[Union[Dict[str, str], str, os.PathLike]], optional)  Optional fields to index within the metadata. Overrides generated schema. Defaults to None. vector_schema (Optional[Dict[str, Union[str, int]]], optional)  Optional vector schema to use. Defaults to None. **kwargs (Any)  Additional keyword arguments to pass to the Redis client. Returns Tuple of the Redis instance and the keys ofthe newly created documents. Return type Tuple[Redis, List[str]] Raises ValueError  If the number of metadatas does not match the number of texts. max_marginal_relevance_search(query: str, k: int = 4, fetch_k: int = 20, lambda_mult: float = 0.5, filter: Optional[RedisFilterExpression] = None, return_metadata: bool = True, distance_threshold: Optional[float] = None, **kwargs: Any)  List[Document][source] Return docs selected using the maximal marginal relevance. Maximal marginal relevance optimizes for similarity to query AND diversityamong selected documents. Parameters query (str)  Text to look up documents similar to. k (int)  Number of Documents to return. Defaults to 4. fetch_k (int)  Number of Documents to fetch to pass to MMR algorithm. lambda_mult (float)  Number between 0 and 1 that determines the degree of diversity among the results with 0 corresponding to maximum diversity and 1 to minimum diversity. Defaults to 0.5. filter (RedisFilterExpression, optional)  Optional metadata filter. Defaults to None. return_metadata (bool, optional)  Whether to return metadata. Defaults to True. distance_threshold (Optional[float], optional)  Maximum vector distance between selected documents and the query vector. Defaults to None. Returns A list of Documents selected by maximal marginal relevance. Return type List[Document] max_marginal_relevance_search_by_vector(embedding: List[float], k: int = 4, fetch_k: int = 20, lambda_mult: float = 0.5, **kwargs: Any)  List[Document] Return docs selected using the maximal marginal relevance. Maximal marginal relevance optimizes for similarity to query AND diversity among selected documents. Parameters embedding  Embedding to look up documents similar to. k  Number of Documents to return. Defaults to 4. fetch_k  Number of Documents to fetch to pass to MMR algorithm. lambda_mult  Number between 0 and 1 that determines the degree of diversity among the results with 0 corresponding to maximum diversity and 1 to minimum diversity. Defaults to 0.5. Returns List of Documents selected by maximal marginal relevance. search(query: str, search_type: str, **kwargs: Any)  List[Document] Return docs most similar to query using specified search type. similarity_search(query: str, k: int = 4, filter: Optional[RedisFilterExpression] = None, return_metadata: bool = True, distance_threshold: Optional[float] = None, **kwargs: Any)  List[Document][source] Run similarity search Parameters query (str)  The query text for which to find similar documents. k (int)  The number of documents to return. Default is 4. filter (RedisFilterExpression, optional)  Optional metadata filter. Defaults to None. return_metadata (bool, optional)  Whether to return metadata. Defaults to True. distance_threshold (Optional[float], optional)  Maximum vector distance between selected documents and the query vector. Defaults to None. Returns A list of documents that are most similar to the querytext. Return type List[Document] similarity_search_by_vector(embedding: List[float], k: int = 4, filter: Optional[RedisFilterExpression] = None, return_metadata: bool = True, distance_threshold: Optional[float] = None, **kwargs: Any)  List[Document][source] Run similarity search between a query vector and the indexed vectors. Parameters embedding (List[float])  The query vector for which to find similar documents. k (int)  The number of documents to return. Default is 4. filter (RedisFilterExpression, optional)  Optional metadata filter. Defaults to None. return_metadata (bool, optional)  Whether to return metadata. Defaults to True. distance_threshold (Optional[float], optional)  Maximum vector distance between selected documents and the query vector. Defaults to None. Returns A list of documents that are most similar to the querytext. Return type List[Document] similarity_search_limit_score(query: str, k: int = 4, score_threshold: float = 0.2, **kwargs: Any)  List[Document][source] [Deprecated] Returns the most similar indexed documents to the query text within the score_threshold range. Deprecated: Use similarity_search with distance_threshold instead. Parameters query (str)  The query text for which to find similar documents. k (int)  The number of documents to return. Default is 4. score_threshold (float)  The minimum matching distance required for a document to be considered a match. Defaults to 0.2. Returns A list of documents that are most similar to the query textincluding the match score for each document. Return type List[Document] Note If there are no documents that satisfy the score_threshold value, an empty list is returned.[Deprecated] Returns the most similar indexed documents to the query text within the score_threshold range. Deprecated: Use similarity_search with distance_threshold instead. Parameters query (str)  The query text for which to find similar documents. k (int)  The number of documents to return. Default is 4. score_threshold (float)  The minimum matching distance required for a document to be considered a match. Defaults to 0.2. Returns A list of documents that are most similar to the query textincluding the match score for each document. Return type List[Document] Note If there are no documents that satisfy the score_threshold value, an empty list is returned. Notes Deprecated since version 0.0.272: Use similarity_search(distance_threshold=0.1) instead. similarity_search_with_relevance_scores(query: str, k: int = 4, **kwargs: Any)  List[Tuple[Document, float]] Return docs and relevance scores in the range [0, 1]. 0 is dissimilar, 1 is most similar. Parameters query  input text k  Number of Documents to return. Defaults to 4. **kwargs  kwargs to be passed to similarity search. Should include: score_threshold: Optional, a floating point value between 0 to 1 to filter the resulting set of retrieved docs Returns List of Tuples of (doc, similarity_score) similarity_search_with_score(query: str, k: int = 4, filter: Optional[RedisFilterExpression] = None, return_metadata: bool = True, **kwargs: Any)  List[Tuple[Document, float]][source] Run similarity search with vector distance. The scores returned from this function are the raw vector distances from the query vector. For similarity scores, use similarity_search_with_relevance_scores. Parameters query (str)  The query text for which to find similar documents. k (int)  The number of documents to return. Default is 4. filter (RedisFilterExpression, optional)  Optional metadata filter. Defaults to None. return_metadata (bool, optional)  Whether to return metadata. Defaults to True. Returns A list of documents that aremost similar to the query with the distance for each document. Return type List[Tuple[Document, float]] write_schema(path: Union[str, PathLike])  None[source] Write the schema to a yaml file. Examples using Redis Redis  2023, Harrison Chase. Last updated on Nov 17, 2023. Show this page source']","To create a vectorstore from a list of texts, you can use the `ElasticsearchStore` or `Redis` classes provided in the context. Here's a brief example using `ElasticsearchStore`:

```python
from langchain.vectorstores import ElasticsearchStore
from langchain.embeddings import OpenAIEmbeddings

# Initialize embeddings
embeddings = OpenAIEmbeddings()

# List of texts
texts = [""Text 1"", ""Text 2"", ""Text 3""]

# Create ElasticsearchStore
vectorstore = ElasticsearchStore(
    embedding=embeddings,
    index_name=""my_vectorstore"",
    es_url=""http://localhost:9200""  # Replace with your Elasticsearch URL
)

# Add texts to the vectorstore
vectorstore.add_texts(texts)
```

For `Redis`, the process is similar:

```python
from langchain.vectorstores import Redis
from langchain.embeddings import OpenAIEmbeddings

# Initialize embeddings
embeddings = OpenAIEmbeddings()

# List of texts
texts = [""Text 1"", ""Text 2"", ""Text 3""]

# Create Redis vectorstore
vectorstore = Redis(
    embeddings=embeddings,
    redis_url=""redis://localhost:6379""  # Replace with your Redis URL
)

# Add texts to the vectorstore
vectorstore.add_texts(texts)
```

Replace the URLs with your actual Elasticsearch or Redis instance URLs.","Each LangChain vectorstore class implements a ""from_texts"" class method that accepts a list of string texts, an instance of an `Embeddings` object to embed those texts, and an optional list of metadata dictionaries to add to the resulting documents.",0.9999999999666667,1.0,,0.035332789922126386,0.16842105263157894
73,what does on_tool_start mean?,"['Defining Custom Tools | Defining Custom Tools When constructing your own agent, you will need to provide it with a list of Tools that it can use. Besides the actual function that is called, the Tool consists of several components: - `name` (str), is required and must be unique within a set of tools provided to an agent - `description` (str), is optional but recommended, as it is used by an agent to determine tool use - `return_direct` (bool), defaults to False - `args_schema` (Pydantic BaseModel), is optional but recommended, can be used to provide more information (e.g., few-shot examples) or validation for expected parameters. There are two main ways to define a tool, we will cover both in the example below. ```python # Import things that are needed generically from langchain.agents import AgentType, initialize_agent from langchain.chains import LLMMathChain from langchain.chat_models import ChatOpenAI from langchain.tools import BaseTool, StructuredTool, Tool, tool from langchain.utilities import SerpAPIWrapper ``` Initialize the LLM to use for the agent. ```python llm = ChatOpenAI(temperature=0) ``` ## Completely New Tools - String Input and Output The simplest tools accept a single query string and return a string output. If your tool function requires multiple arguments, you might want to skip down to the `StructuredTool` section below. There are two ways to do this: either by using the Tool dataclass, or by subclassing the BaseTool class. ### Tool dataclass The \'Tool\' dataclass wraps functions that accept a single string input and returns a string output. ```python # Load the tool configs that are needed. search = SerpAPIWrapper() llm_math_chain = LLMMathChain(llm=llm, verbose=True) tools = [ Tool.from_function( func=search.run, name=""Search"", description=""useful for when you need to answer questions about current events"", # coroutine= ... <- you can specify an async method if desired as well ), ] ``` ```text /Users/wfh/code/lc/lckg/langchain/chains/llm_math/base.py:50: UserWarning: Directly instantiating an LLMMathChain with an llm is deprecated. Please instantiate with llm_chain argument or using the from_llm class method. warnings.warn( ``` You can also define a custom `args_schema` to provide more information about inputs. ```python from pydantic import BaseModel, Field class CalculatorInput(BaseModel): question: str = Field() tools.append( Tool.from_function( func=llm_math_chain.run, name=""Calculator"", description=""useful for when you need to answer questions about math"", args_schema=CalculatorInput, # coroutine= ... <- you can specify an async method if desired as well ) ) ``` ```python # Construct the agent. We will use the default agent type here. # See documentation for a full list of options. agent = initialize_agent( tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True ) ``` ```python agent.run( ""Who is Leo DiCaprio\'s girlfriend? What is her current age raised to the 0.43 power?"" ) ``` ```text > Entering new AgentExecutor chain... I need to find out Leo DiCaprio\'s girlfriend\'s name and her age Action: Search Action Input: ""Leo DiCaprio girlfriend"" Observation: After rumours of a romance with Gigi Hadid, the Oscar winner has seemingly moved on. First being linked to the television personality in September 2022, it appears as if his ""age bracket"" has moved up. This follows his rumoured relationship with mere 19-year-old Eden Polani. Thought:I still need to find out his current girlfriend\'s name and age Action: Search Action Input: ""Leo DiCaprio current girlfriend"" Observation: Just Jared on Instagram: Leonardo DiCaprio & girlfriend Camila Morrone couple up for a lunch date! Thought:Now that I know his girlfriend\'s name is Camila Morrone, I need to find her current age Action: Search Action Input: ""Camila Morrone age"" Observation: 25 years Thought:Now that I have her age, I need to calculate her age raised to the 0.43 power Action: Calculator Action Input: 25^(0.43) > Entering new LLMMathChain chain... 25^(0.43)```text 25**(0.43) ``` ...numexpr.evaluate(""25**(0.43)"")... Answer: 3.991298452658078 > Finished chain. Observation: Answer: 3.991298452658078 Thought:I now know the final answer Final Answer: Camila Morrone\'s current age raised to the 0.43 power is approximately 3.99. > Finished chain. ""Camila Morrone\'s current age raised to the 0.43 power is approximately 3.99."" ``` ### Subclassing the BaseTool You can also directly subclass `BaseTool`. This is useful if you want more control over the instance variables or if you want to propagate callbacks to nested chains or other tools. ```python from typing import Optional, Type from langchain.callbacks.manager import ( AsyncCallbackManagerForToolRun, CallbackManagerForToolRun, ) class CustomSearchTool(BaseTool): name = ""custom_search"" description = ""useful for when you need to answer questions about current events"" def _run( self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None ) -> str: """"""Use the tool."""""" return search.run(query) async def _arun( self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None ) -> str: """"""Use the tool asynchronously."""""" raise NotImplementedError(""custom_search does not support async"") class CustomCalculatorTool(BaseTool): name = ""Calculator"" description = ""useful for when you need to answer questions about math"" args_schema: Type[BaseModel] = CalculatorInput def _run( self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None ) -> str: """"""Use the tool."""""" return llm_math_chain.run(query) async def _arun( self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None ) -> str: """"""Use the tool asynchronously."""""" raise NotImplementedError(""Calculator does not support async"") ``` ```python tools = [CustomSearchTool(), CustomCalculatorTool()] agent = initialize_agent( tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True ) ``` ```python agent.run( ""Who is Leo DiCaprio\'s girlfriend? What is her current age raised to the 0.43 power?"" ) ``` ```text > Entering new AgentExecutor chain... I need to use custom_search to find out who Leo DiCaprio\'s girlfriend is, and then use the Calculator to raise her age to the 0.43 power. Action: custom_search Action Input: ""Leo DiCaprio girlfriend"" Observation: After rumours of a romance with Gigi Hadid, the Oscar winner has seemingly moved on. First being linked to the television personality in September 2022, it appears as if his ""age bracket"" has moved up. This follows his rumoured relationship with mere 19-year-old Eden Polani. Thought:I need to find out the current age of Eden Polani. Action: custom_search Action Input: ""Eden Polani age"" Observation: 19 years old Thought:Now I can use the Calculator to raise her age to the 0.43 power. Action: Calculator Action Input: 19 ^ 0.43 > Entering new LLMMathChain chain... 19 ^ 0.43```text 19 ** 0.43 ``` ...numexpr.evaluate(""19 ** 0.43"")... Answer: 3.547023357958959 > Finished chain. Observation: Answer: 3.547023357958959 Thought:I now know the final answer. Final Answer: 3.547023357958959 > Finished chain. \'3.547023357958959\' ``` ### Using the decorator To make it easier to define custom tools, a `@tool` decorator is provided. This decorator can be used to quickly create a `Tool` from a simple function. The decorator uses the function name as the tool name by default, but this can be overridden by passing a string as the first argument. Additionally, the decorator will use the function\'s docstring as the tool\'s description. ```python from langchain.tools import tool @tool def search_api(query: str) -> str: """"""Searches the API for the query."""""" return f""Results for query {query}"" search_api ``` You can also provide arguments like the tool name and whether to return directly. ```python @tool(""search"", return_direct=True) def search_api(query: str) -> str: """"""Searches the API for the query."""""" return ""Results"" ``` ```python search_api ``` ```text Tool(name=\'search\', description=\'search(query: str) -> str - Searches the API for the query.\', args_schema=, return_direct=True, verbose=False, callback_manager=, func=, coroutine=None) ``` You can also provide `args_schema` to provide more information about the argument. ```python class SearchInput(BaseModel): query: str = Field(description=""should be a search query"") @tool(""search"", return_direct=True, args_schema=SearchInput) def search_api(query: str) -> str: """"""Searches the API for the query."""""" return ""Results"" ``` ```python search_api ``` ```text Tool(name=\'search\', description=\'search(query: str) -> str - Searches the API for the query.\', args_schema=, return_direct=True, verbose=False, callback_manager=, func=, coroutine=None) ``` ## Custom Structured Tools If your functions require more structured arguments, you can use the `StructuredTool` class directly, or still subclass the `BaseTool` class. ### StructuredTool dataclass To dynamically generate a structured tool from a given function, the fastest way to get started is with `StructuredTool.from_function()`. ```python import requests from langchain.tools import StructuredTool def post_message(url: str, body: dict, parameters: Optional[dict] = None) -> str: """"""Sends a POST request to the given url with the given body and parameters."""""" result = requests.post(url, json=body, params=parameters) return f""Status: {result.status_code} - {result.text}"" tool = StructuredTool.from_function(post_message) ``` ### Subclassing the BaseTool The BaseTool automatically infers the schema from the `_run` method\'s signature. ```python from typing import Optional, Type from langchain.callbacks.manager import ( AsyncCallbackManagerForToolRun, CallbackManagerForToolRun, ) class CustomSearchTool(BaseTool): name = ""custom_search"" description = ""useful for when you need to answer questions about current events"" def _run( self, query: str, engine: str = ""google"", gl: str = ""us"", hl: str = ""en"", run_manager: Optional[CallbackManagerForToolRun] = None, ) -> str: """"""Use the tool."""""" search_wrapper = SerpAPIWrapper(params={""engine"": engine, ""gl"": gl, ""hl"": hl}) return search_wrapper.run(query) async def _arun( self, query: str, engine: str = ""google"", gl: str = ""us"", hl: str = ""en"", run_manager: Optional[AsyncCallbackManagerForToolRun] = None, ) -> str: """"""Use the tool asynchronously."""""" raise NotImplementedError(""custom_search does not support async"") # You can provide a custom args schema to add descriptions or custom validation class SearchSchema(BaseModel): query: str = Field(description=""should be a search query"") engine: str = Field(description=""should be a search engine"") gl: str = Field(description=""should be a country code"") hl: str = Field(description=""should be a language code"") class CustomSearchTool(BaseTool): name = ""custom_search"" description = ""useful for when you need to answer questions about current events"" args_schema: Type[SearchSchema] = SearchSchema def _run( self, query: str, engine: str = ""google"", gl: str = ""us"", hl: str = ""en"", run_manager: Optional[CallbackManagerForToolRun] = None, ) -> str: """"""Use the tool."""""" search_wrapper = SerpAPIWrapper(params={""engine"": engine, ""gl"": gl, ""hl"": hl}) return search_wrapper.run(query) async def _arun( self, query: str, engine: str = ""google"", gl: str = ""us"", hl: str = ""en"", run_manager: Optional[AsyncCallbackManagerForToolRun] = None, ) -> str: """"""Use the tool asynchronously."""""" raise NotImplementedError(""custom_search does not support async"") ``` ### Using the decorator The `tool` decorator creates a structured tool automatically if the signature has multiple arguments. ```python import requests from langchain.tools import tool @tool def post_message(url: str, body: dict, parameters: Optional[dict] = None) -> str: """"""Sends a POST request to the given url with the given body and parameters."""""" result = requests.post(url, json=body, params=parameters) return f""Status: {result.status_code} - {result.text}"" ``` ## Modify existing tools Now, we show how to load existing tools and modify them directly. In the example below, we do something really simple and change the Search tool to have the name `Google Search`. ```python from langchain.agents import load_tools ``` ```python tools = load_tools([""serpapi"", ""llm-math""], llm=llm) ``` ```python tools[0].name = ""Google Search"" ``` ```python agent = initialize_agent( tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True ) ``` ```python agent.run( ""Who is Leo DiCaprio\'s girlfriend? What is her current age raised to the 0.43 power?"" ) ``` ```text > Entering new AgentExecutor chain... I need to find out Leo DiCaprio\'s girlfriend\'s name and her age. Action: Google Search Action Input: ""Leo DiCaprio girlfriend"" Observation: After rumours of a romance with Gigi Hadid, the Oscar winner has seemingly moved on. First being linked to the television personality in September 2022, it appears as if his ""age bracket"" has moved up. This follows his rumoured relationship with mere 19-year-old Eden Polani. Thought:I still need to find out his current girlfriend\'s name and her age. Action: Google Search Action Input: ""Leo DiCaprio current girlfriend age"" Observation: Leonardo DiCaprio has been linked with 19-year-old model Eden Polani, continuing the rumour that he doesn\'t date any women over the age of ... Thought:I need to find out the age of Eden Polani. Action: Calculator Action Input: 19^(0.43) Observation: Answer: 3.547023357958959 Thought:I now know the final answer. Final Answer: The age of Leo DiCaprio\'s girlfriend raised to the 0.43 power is approximately 3.55. > Finished chain. ""The age of Leo DiCaprio\'s girlfriend raised to the 0.43 power is approximately 3.55."" ``` ## Defining the priorities among Tools When you made a Custom tool, you may want the Agent to use the custom tool more than normal tools. For example, you made a custom tool, which gets information on music from your database. When a user wants information on songs, You want the Agent to use `the custom tool` more than the normal `Search tool`. But the Agent might prioritize a normal Search tool. This can be accomplished by adding a statement such as `Use this more than the normal search if the question is about Music, like \'who is the singer of yesterday?\' or \'what is the most popular song in 2022?\'` to the description. An example is below. ```python # Import things that are needed generically from langchain.agents import AgentType, Tool, initialize_agent from langchain.chains import LLMMathChain from langchain.llms import OpenAI from langchain.utilities import SerpAPIWrapper search = SerpAPIWrapper() tools = [ Tool( name=""Search"", func=search.run, description=""useful for when you need to answer questions about current events"", ), Tool( name=""Music Search"", func=lambda x: ""\'All I Want For Christmas Is You\' by Mariah Carey."", # Mock Function description=""A Music search engine. Use this more than the normal search if the question is about Music, like \'who is the singer of yesterday?\' or \'what is the most popular song in 2022?\'"", ), ] agent = initialize_agent( tools, OpenAI(temperature=0), agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True, ) ``` ```python agent.run(""what is the most famous song of christmas"") ``` ```text > Entering new AgentExecutor chain... I should use a music search engine to find the answer Action: Music Search Action Input: most famous song of christmas\'All I Want For Christmas Is You\' by Mariah Carey. I now know the final answer Final Answer: \'All I Want For Christmas Is You\' by Mariah Carey. > Finished chain. ""\'All I Want For Christmas Is You\' by Mariah Carey."" ``` ## Using tools to return directly Often, it can be desirable to have a tool output returned directly to the user, if it\'s called. You can do this easily with LangChain by setting the `return_direct` flag for a tool to be True. ```python llm_math_chain = LLMMathChain(llm=llm) tools = [ Tool( name=""Calculator"", func=llm_math_chain.run, description=""useful for when you need to answer questions about math"", return_direct=True, ) ] ``` ```python llm = OpenAI(temperature=0) agent = initialize_agent( tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True ) ``` ```python agent.run(""whats 2**.12"") ``` ```text > Entering new AgentExecutor chain... I need to calculate this Action: Calculator Action Input: 2**.12Answer: 1.086734862526058 > Finished chain. \'Answer: 1.086734862526058\' ``` ## Handling Tool Errors When a tool encounters an error and the exception is not caught, the agent will stop executing. If you want the agent to continue execution, you can raise a `ToolException` and set `handle_tool_error` accordingly. When `ToolException` is thrown, the agent will not stop working, but will handle the exception according to the `handle_tool_error` variable of the tool, and the processing result will be returned to the agent as observation, and printed in red. You can set `handle_tool_error` to `True`, set it a unified string value, or set it as a function. If it\'s set as a function, the function should take a `ToolException` as a parameter and return a `str` value. Please note that only raising a `ToolException` won\'t be effective. You need to first set the `handle_tool_error` of the tool because its default value is `False`. ```python from langchain.agents import AgentType, initialize_agent from langchain.chat_models import ChatOpenAI from langchain.tools import Tool from langchain.tools.base import ToolException from langchain.utilities import SerpAPIWrapper def _handle_error(error: ToolException) -> str: return ( ""The following errors occurred during tool execution:"" + error.args[0] + ""Please try another tool."" ) def search_tool1(s: str): raise ToolException(""The search tool1 is not available."") def search_tool2(s: str): raise ToolException(""The search tool2 is not available."") search_tool3 = SerpAPIWrapper() ``` ```python description = ""useful for when you need to answer questions about current events.You should give priority to using it."" tools = [ Tool.from_function( func=search_tool1, name=""Search_tool1"", description=description, handle_tool_error=True, ), Tool.from_function( func=search_tool2, name=""Search_tool2"", description=description, handle_tool_error=_handle_error, ), Tool.from_function( func=search_tool3.run, name=""Search_tool3"", description=""useful for when you need to answer questions about current events"", ), ] agent = initialize_agent( tools, ChatOpenAI(temperature=0), agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True, ) ``` ```python agent.run(""Who is Leo DiCaprio\'s girlfriend?"") ``` ```text > Entering new AgentExecutor chain... I should use Search_tool1 to find recent news articles about Leo DiCaprio\'s personal life. Action: Search_tool1 Action Input: ""Leo DiCaprio girlfriend"" Observation: The search tool1 is not available. Thought:I should try using Search_tool2 instead. Action: Search_tool2 Action Input: ""Leo DiCaprio girlfriend"" Observation: The following errors occurred during tool execution:The search tool2 is not available.Please try another tool. Thought:I should try using Search_tool3 as a last resort. Action: Search_tool3 Action Input: ""Leo DiCaprio girlfriend"" Observation: Leonardo DiCaprio and Gigi Hadid were recently spotted at a pre-Oscars party, sparking interest once again in their rumored romance. The Revenant actor and the model first made headlines when they were spotted together at a New York Fashion Week afterparty in September 2022. Thought:Based on the information from Search_tool3, it seems that Gigi Hadid is currently rumored to be Leo DiCaprio\'s girlfriend. Final Answer: Gigi Hadid is currently rumored to be Leo DiCaprio\'s girlfriend. > Finished chain. ""Gigi Hadid is currently rumored to be Leo DiCaprio\'s girlfriend."" ``` - [Completely New Tools - String Input and Output](#completely-new-tools---string-input-and-output)- [Tool dataclass](#tool-dataclass) - [Subclassing the BaseTool](#subclassing-the-basetool) - [Using the decorator](#using-the-decorator) - [Custom Structured Tools](#custom-structured-tools)- [StructuredTool dataclass](#structuredtool-dataclass) - [Subclassing the BaseTool](#subclassing-the-basetool-1) - [Using the decorator](#using-the-decorator-1) - [Modify existing tools](#modify-existing-tools) - [Defining the priorities among Tools](#defining-the-priorities-among-tools) - [Using tools to return directly](#using-tools-to-return-directly) - [Handling Tool Errors](#handling-tool-errors)', 'Pairwise string comparison | Pairwise string comparison []( Often you will want to compare predictions of an LLM, Chain, or Agent for a given input. The `StringComparison` evaluators facilitate this so you can answer questions like: - Which LLM or prompt produces a preferred output for a given question? - Which examples should I include for few-shot example selection? - Which output is better to include for fine-tuning? The simplest and often most reliable automated way to choose a preferred prediction for a given input is to use the `pairwise_string` evaluator. Check out the reference docs for the [PairwiseStringEvalChain]( for more info. ```python from langchain.evaluation import load_evaluator evaluator = load_evaluator(""labeled_pairwise_string"") ``` ```python evaluator.evaluate_string_pairs( prediction=""there are three dogs"", prediction_b=""4"", input=""how many dogs are in the park?"", reference=""four"", ) ``` ```text {\'reasoning\': \'Both responses are relevant to the question asked, as they both provide a numerical answer to the question about the number of dogs in the park. However, Response A is incorrect according to the reference answer, which states that there are four dogs. Response B, on the other hand, is correct as it matches the reference answer. Neither response demonstrates depth of thought, as they both simply provide a numerical answer without any additional information or context. \\n\\nBased on these criteria, Response B is the better response.\\n\', \'value\': \'B\', \'score\': 0} ``` ## Methods The pairwise string evaluator can be called using [evaluate_string_pairs]( (or async [aevaluate_string_pairs]( methods, which accept: - prediction (str) The predicted response of the first model, chain, or prompt. - prediction_b (str) The predicted response of the second model, chain, or prompt. - input (str) The input question, prompt, or other text. - reference (str) (Only for the labeled_pairwise_string variant) The reference response. They return a dictionary with the following values: - value: \'A\' or \'B\', indicating whether `prediction` or `prediction_b` is preferred, respectively - score: Integer 0 or 1 mapped from the \'value\', where a score of 1 would mean that the first `prediction` is preferred, and a score of 0 would mean `prediction_b` is preferred. - reasoning: String ""chain of thought reasoning"" from the LLM generated prior to creating the score ## Without References When references aren\'t available, you can still predict the preferred response. The results will reflect the evaluation model\'s preference, which is less reliable and may result in preferences that are factually incorrect. ```python from langchain.evaluation import load_evaluator evaluator = load_evaluator(""pairwise_string"") ``` ```python evaluator.evaluate_string_pairs( prediction=""Addition is a mathematical operation."", prediction_b=""Addition is a mathematical operation that adds two numbers to create a third number, the \'sum\'."", input=""What is addition?"", ) ``` ```text {\'reasoning\': \'Both responses are correct and relevant to the question. However, Response B is more helpful and insightful as it provides a more detailed explanation of what addition is. Response A is correct but lacks depth as it does not explain what the operation of addition entails. \\n\\nFinal Decision: [[B]]\', \'value\': \'B\', \'score\': 0} ``` ## Defining the Criteria By default, the LLM is instructed to select the \'preferred\' response based on helpfulness, relevance, correctness, and depth of thought. You can customize the criteria by passing in a `criteria` argument, where the criteria could take any of the following forms: - [Criteria]( enum or its string value - to use one of the default criteria and their descriptions - [Constitutional principal]( - use one any of the constitutional principles defined in langchain - Dictionary: a list of custom criteria, where the key is the name of the criteria, and the value is the description. - A list of criteria or constitutional principles - to combine multiple criteria in one. Below is an example for determining preferred writing responses based on a custom style. ```python custom_criteria = { ""simplicity"": ""Is the language straightforward and unpretentious?"", ""clarity"": ""Are the sentences clear and easy to understand?"", ""precision"": ""Is the writing precise, with no unnecessary words or details?"", ""truthfulness"": ""Does the writing feel honest and sincere?"", ""subtext"": ""Does the writing suggest deeper meanings or themes?"", } evaluator = load_evaluator(""pairwise_string"", criteria=custom_criteria) ``` ```python evaluator.evaluate_string_pairs( prediction=""Every cheerful household shares a similar rhythm of joy; but sorrow, in each household, plays a unique, haunting melody."", prediction_b=""Where one finds a symphony of joy, every domicile of happiness resounds in harmonious,"" "" identical notes; yet, every abode of despair conducts a dissonant orchestra, each"" "" playing an elegy of grief that is peculiar and profound to its own existence."", input=""Write some prose about families."", ) ``` ```text {\'reasoning\': \'Response A is simple, clear, and precise. It uses straightforward language to convey a deep and sincere message about families. The metaphor of joy and sorrow as music is effective and easy to understand.\\n\\nResponse B, on the other hand, is more complex and less clear. The language is more pretentious, with words like ""domicile,"" ""resounds,"" ""abode,"" ""dissonant,"" and ""elegy."" While it conveys a similar message to Response A, it does so in a more convoluted way. The precision is also lacking due to the use of unnecessary words and details.\\n\\nBoth responses suggest deeper meanings or themes about the shared joy and unique sorrow in families. However, Response A does so in a more effective and accessible way.\\n\\nTherefore, the better response is [[A]].\', \'value\': \'A\', \'score\': 1} ``` ## Customize the LLM By default, the loader uses `gpt-4` in the evaluation chain. You can customize this when loading. ```python from langchain.chat_models import ChatAnthropic llm = ChatAnthropic(temperature=0) evaluator = load_evaluator(""labeled_pairwise_string"", llm=llm) ``` ```python evaluator.evaluate_string_pairs( prediction=""there are three dogs"", prediction_b=""4"", input=""how many dogs are in the park?"", reference=""four"", ) ``` ```text {\'reasoning\': \'Here is my assessment:\\n\\nResponse B is more helpful, insightful, and accurate than Response A. Response B simply states ""4"", which directly answers the question by providing the exact number of dogs mentioned in the reference answer. In contrast, Response A states ""there are three dogs"", which is incorrect according to the reference answer. \\n\\nIn terms of helpfulness, Response B gives the precise number while Response A provides an inaccurate guess. For relevance, both refer to dogs in the park from the question. However, Response B is more correct and factual based on the reference answer. Response A shows some attempt at reasoning but is ultimately incorrect. Response B requires less depth of thought to simply state the factual number.\\n\\nIn summary, Response B is superior in terms of helpfulness, relevance, correctness, and depth. My final decision is: [[B]]\\n\', \'value\': \'B\', \'score\': 0} ``` ## Customize the Evaluation Prompt You can use your own custom evaluation prompt to add more task-specific instructions or to instruct the evaluator to score the output. *Note: If you use a prompt that expects generates a result in a unique format, you may also have to pass in a custom output parser (`output_parser=your_parser()`) instead of the default `PairwiseStringResultOutputParser` ```python from langchain.prompts import PromptTemplate prompt_template = PromptTemplate.from_template( """"""Given the input context, which do you prefer: A or B? Evaluate based on the following criteria: {criteria} Reason step by step and finally, respond with either [[A]] or [[B]] on its own line. DATA ---- input: {input} reference: {reference} A: {prediction} B: {prediction_b} --- Reasoning: """""" ) evaluator = load_evaluator(""labeled_pairwise_string"", prompt=prompt_template) ``` ```python # The prompt was assigned to the evaluator print(evaluator.prompt) ``` ```text input_variables=[\'prediction\', \'reference\', \'prediction_b\', \'input\'] output_parser=None partial_variables={\'criteria\': \'helpfulness: Is the submission helpful, insightful, and appropriate?\\nrelevance: Is the submission referring to a real quote from the text?\\ncorrectness: Is the submission correct, accurate, and factual?\\ndepth: Does the submission demonstrate depth of thought?\'} template=\'Given the input context, which do you prefer: A or B?\\nEvaluate based on the following criteria:\\n{criteria}\\nReason step by step and finally, respond with either [[A]] or [[B]] on its own line.\\n\\nDATA\\n----\\ninput: {input}\\nreference: {reference}\\nA: {prediction}\\nB: {prediction_b}\\n---\\nReasoning:\\n\\n\' template_format=\'f-string\' validate_template=True ``` ```python evaluator.evaluate_string_pairs( prediction=""The dog that ate the ice cream was named fido."", prediction_b=""The dog\'s name is spot"", input=""What is the name of the dog that ate the ice cream?"", reference=""The dog\'s name is fido"", ) ``` ```text {\'reasoning\': \'Helpfulness: Both A and B are helpful as they provide a direct answer to the question.\\nRelevance: A is relevant as it refers to the correct name of the dog from the text. B is not relevant as it provides a different name.\\nCorrectness: A is correct as it accurately states the name of the dog. B is incorrect as it provides a different name.\\nDepth: Both A and B demonstrate a similar level of depth as they both provide a straightforward answer to the question.\\n\\nGiven these evaluations, the preferred response is:\\n\', \'value\': \'A\', \'score\': 1} ``` - [Methods](#methods) - [Without References](#without-references) - [Defining the Criteria](#defining-the-criteria) - [Customize the LLM](#customize-the-llm) - [Customize the Evaluation Prompt](#customize-the-evaluation-prompt)', 'Backed by a Vector Store | Backed by a Vector Store `VectorStoreRetrieverMemory` stores memories in a vector store and queries the top-K most ""salient"" docs every time it is called. This differs from most of the other Memory classes in that it doesn\'t explicitly track the order of interactions. In this case, the ""docs"" are previous conversation snippets. This can be useful to refer to relevant pieces of information that the AI was told earlier in the conversation. ```python from datetime import datetime from langchain.embeddings.openai import OpenAIEmbeddings from langchain.llms import OpenAI from langchain.memory import VectorStoreRetrieverMemory from langchain.chains import ConversationChain from langchain.prompts import PromptTemplate ``` ### Initialize your vector store Depending on the store you choose, this step may look different. Consult the relevant vector store documentation for more details. ```python import faiss from langchain.docstore import InMemoryDocstore from langchain.vectorstores import FAISS embedding_size = 1536 # Dimensions of the OpenAIEmbeddings index = faiss.IndexFlatL2(embedding_size) embedding_fn = OpenAIEmbeddings().embed_query vectorstore = FAISS(embedding_fn, index, InMemoryDocstore({}), {}) ``` ### Create your VectorStoreRetrieverMemory The memory object is instantiated from any vector store retriever. ```python # In actual usage, you would set `k` to be a higher value, but we use k=1 to show that # the vector lookup still returns the semantically relevant information retriever = vectorstore.as_retriever(search_kwargs=dict(k=1)) memory = VectorStoreRetrieverMemory(retriever=retriever) # When added to an agent, the memory object can save pertinent information from conversations or used tools memory.save_context({""input"": ""My favorite food is pizza""}, {""output"": ""that\'s good to know""}) memory.save_context({""input"": ""My favorite sport is soccer""}, {""output"": ""...""}) memory.save_context({""input"": ""I don\'t the Celtics""}, {""output"": ""ok""}) # ``` ```python # Notice the first result returned is the memory pertaining to tax help, which the language model deems more semantically relevant # to a 1099 than the other documents, despite them both containing numbers. print(memory.load_memory_variables({""prompt"": ""what sport should i watch?""})[""history""]) ``` ```text input: My favorite sport is soccer output: ... ``` ## Using in a chain Let\'s walk through an example, again setting `verbose=True` so we can see the prompt. ```python llm = OpenAI(temperature=0) # Can be any valid LLM _DEFAULT_TEMPLATE = """"""The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Relevant pieces of previous conversation: {history} (You do not need to use these pieces of information if not relevant) Current conversation: Human: {input} AI:"""""" PROMPT = PromptTemplate( input_variables=[""history"", ""input""], template=_DEFAULT_TEMPLATE ) conversation_with_summary = ConversationChain( llm=llm, prompt=PROMPT, # We set a very low max_token_limit for the purposes of testing. memory=memory, verbose=True ) conversation_with_summary.predict(input=""Hi, my name is Perry, what\'s up?"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Relevant pieces of previous conversation: input: My favorite food is pizza output: that\'s good to know (You do not need to use these pieces of information if not relevant) Current conversation: Human: Hi, my name is Perry, what\'s up? AI: > Finished chain. "" Hi Perry, I\'m doing well. How about you?"" ``` ```python # Here, the basketball related content is surfaced conversation_with_summary.predict(input=""what\'s my favorite sport?"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Relevant pieces of previous conversation: input: My favorite sport is soccer output: ... (You do not need to use these pieces of information if not relevant) Current conversation: Human: what\'s my favorite sport? AI: > Finished chain. \' You told me earlier that your favorite sport is soccer.\' ``` ```python # Even though the language model is stateless, since relevant memory is fetched, it can ""reason"" about the time. # Timestamping memories and data is useful in general to let the agent determine temporal relevance conversation_with_summary.predict(input=""Whats my favorite food"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Relevant pieces of previous conversation: input: My favorite food is pizza output: that\'s good to know (You do not need to use these pieces of information if not relevant) Current conversation: Human: Whats my favorite food AI: > Finished chain. \' You said your favorite food is pizza.\' ``` ```python # The memories from the conversation are automatically stored, # since this query best matches the introduction chat above, # the agent is able to \'remember\' the user\'s name. conversation_with_summary.predict(input=""What\'s my name?"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Relevant pieces of previous conversation: input: Hi, my name is Perry, what\'s up? response: Hi Perry, I\'m doing well. How about you? (You do not need to use these pieces of information if not relevant) Current conversation: Human: What\'s my name? AI: > Finished chain. \' Your name is Perry.\' ``` - [Initialize your vector store](#initialize-your-vector-store) - [Create your VectorStoreRetrieverMemory](#create-your-vectorstoreretrievermemory) - [Using in a chain](#using-in-a-chain)']","`on_tool_start` is a callback function that can be defined to execute code when a tool is initiated in an agent. This function allows you to perform actions or logging right when a tool is about to run, providing insight into the tool execution flow. 

Here's a simple example of how it might be used:

```python
from langchain.tools import Tool

class CustomTool(Tool):
    def __init__(self, *args, on_tool_start=None, **kwargs):
        super().__init__(*args, **kwargs)
        self.on_tool_start = on_tool_start

    def _run(self, *args, **kwargs):
        if self.on_tool_start:
            self.on_tool_start()  # Execute the on_tool_start callback
        # Tool execution logic here
        return ""Tool result""

# Define a callback function
def tool_start_callback():
    print(""Tool is starting..."")

# Create a tool instance with the callback
custom_tool = CustomTool(name=""custom_tool"", on_tool_start=tool_start_callback)

# Run the tool
custom_tool._run()
```

In this example, when `custom_tool._run()` is called, the `tool_start_callback` function will be executed first, printing ""Tool is starting..."" to the console.","The on_tool_start method is defined classes that implement LangChain's BaseCallbackHandler interface. When a ""Tool"" object is invoked, and callbacks are provided, the 'on_tool_start' method is called prior to executing the tool itself.",0.0,0.0,,0.14431799202687987,0.16981132075471697
74,"what does this do? return RunnableBranch(
        (
            RunnableLambda(lambda x: bool(x.get(""chat_history""))).with_config(
                run_name=""HasChatHistoryCheck""
            ),
            conversation_chain.with_config(run_name=""RetrievalChainWithHistory""),
        ),
        (
            RunnableLambda(itemgetter(""question"")).with_config(
                run_name=""Itemgetter:question""
            )
            | retriever
        ).with_config(run_name=""RetrievalChainWithNoHistory""),
    ).with_config(run_name=""RouteDependingOnChatHistory"")","['RAG | RAG Let\'s look at adding in a retrieval step to a prompt and LLM, which adds up to a ""retrieval-augmented generation"" chain ```bash pip install langchain openai faiss-cpu tiktoken ``` ```python from operator import itemgetter from langchain.chat_models import ChatOpenAI from langchain.embeddings import OpenAIEmbeddings from langchain.prompts import ChatPromptTemplate from langchain.schema.output_parser import StrOutputParser from langchain.schema.runnable import RunnableLambda, RunnablePassthrough from langchain.vectorstores import FAISS ``` ```python vectorstore = FAISS.from_texts( [""harrison worked at kensho""], embedding=OpenAIEmbeddings() ) retriever = vectorstore.as_retriever() template = """"""Answer the question based only on the following context: {context} Question: {question} """""" prompt = ChatPromptTemplate.from_template(template) model = ChatOpenAI() ``` ```python chain = ( {""context"": retriever, ""question"": RunnablePassthrough()} | prompt | model | StrOutputParser() ) ``` ```python chain.invoke(""where did harrison work?"") ``` ```text \'Harrison worked at Kensho.\' ``` ```python template = """"""Answer the question based only on the following context: {context} Question: {question} Answer in the following language: {language} """""" prompt = ChatPromptTemplate.from_template(template) chain = ( { ""context"": itemgetter(""question"") | retriever, ""question"": itemgetter(""question""), ""language"": itemgetter(""language""), } | prompt | model | StrOutputParser() ) ``` ```python chain.invoke({""question"": ""where did harrison work"", ""language"": ""italian""}) ``` ```text \'Harrison ha lavorato a Kensho.\' ``` ## Conversational Retrieval Chain We can easily add in conversation history. This primarily means adding in chat_message_history ```python from langchain.schema import format_document from langchain.schema.runnable import RunnableMap ``` ```python from langchain.prompts.prompt import PromptTemplate _template = """"""Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language. Chat History: {chat_history} Follow Up Input: {question} Standalone question:"""""" CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template) ``` ```python template = """"""Answer the question based only on the following context: {context} Question: {question} """""" ANSWER_PROMPT = ChatPromptTemplate.from_template(template) ``` ```python DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=""{page_content}"") def _combine_documents( docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=""\\n\\n"" ): doc_strings = [format_document(doc, document_prompt) for doc in docs] return document_separator.join(doc_strings) ``` ```python from typing import List, Tuple def _format_chat_history(chat_history: List[Tuple]) -> str: buffer = """" for dialogue_turn in chat_history: human = ""Human: "" + dialogue_turn[0] ai = ""Assistant: "" + dialogue_turn[1] buffer += ""\\n"" + ""\\n"".join([human, ai]) return buffer ``` ```python _inputs = RunnableMap( standalone_question=RunnablePassthrough.assign( chat_history=lambda x: _format_chat_history(x[""chat_history""]) ) | CONDENSE_QUESTION_PROMPT | ChatOpenAI(temperature=0) | StrOutputParser(), ) _context = { ""context"": itemgetter(""standalone_question"") | retriever | _combine_documents, ""question"": lambda x: x[""standalone_question""], } conversational_qa_chain = _inputs | _context | ANSWER_PROMPT | ChatOpenAI() ``` ```python conversational_qa_chain.invoke( { ""question"": ""where did harrison work?"", ""chat_history"": [], } ) ``` ```text AIMessage(content=\'Harrison was employed at Kensho.\', additional_kwargs={}, example=False) ``` ```python conversational_qa_chain.invoke( { ""question"": ""where did he work?"", ""chat_history"": [(""Who wrote this notebook?"", ""Harrison"")], } ) ``` ```text AIMessage(content=\'Harrison worked at Kensho.\', additional_kwargs={}, example=False) ``` ### With Memory and returning source documents This shows how to use memory with the above. For memory, we need to manage that outside at the memory. For returning the retrieved documents, we just need to pass them through all the way. ```python from operator import itemgetter from langchain.memory import ConversationBufferMemory ``` ```python memory = ConversationBufferMemory( return_messages=True, output_key=""answer"", input_key=""question"" ) ``` ```python # First we add a step to load memory # This adds a ""memory"" key to the input object loaded_memory = RunnablePassthrough.assign( chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(""history""), ) # Now we calculate the standalone question standalone_question = { ""standalone_question"": { ""question"": lambda x: x[""question""], ""chat_history"": lambda x: _format_chat_history(x[""chat_history""]), } | CONDENSE_QUESTION_PROMPT | ChatOpenAI(temperature=0) | StrOutputParser(), } # Now we retrieve the documents retrieved_documents = { ""docs"": itemgetter(""standalone_question"") | retriever, ""question"": lambda x: x[""standalone_question""], } # Now we construct the inputs for the final prompt final_inputs = { ""context"": lambda x: _combine_documents(x[""docs""]), ""question"": itemgetter(""question""), } # And finally, we do the part that returns the answers answer = { ""answer"": final_inputs | ANSWER_PROMPT | ChatOpenAI(), ""docs"": itemgetter(""docs""), } # And now we put it all together! final_chain = loaded_memory | standalone_question | retrieved_documents | answer ``` ```python inputs = {""question"": ""where did harrison work?""} result = final_chain.invoke(inputs) result ``` ```text {\'answer\': AIMessage(content=\'Harrison was employed at Kensho.\', additional_kwargs={}, example=False), \'docs\': [Document(page_content=\'harrison worked at kensho\', metadata={})]} ``` ```python # Note that the memory does not save automatically # This will be improved in the future # For now you need to save it yourself memory.save_context(inputs, {""answer"": result[""answer""].content}) ``` ```python memory.load_memory_variables({}) ``` ```text {\'history\': [HumanMessage(content=\'where did harrison work?\', additional_kwargs={}, example=False), AIMessage(content=\'Harrison was employed at Kensho.\', additional_kwargs={}, example=False)]} ``` - [Conversational Retrieval Chain](#conversational-retrieval-chain)- [With Memory and returning source documents](#with-memory-and-returning-source-documents)', 'Add message history (memory) | Add message history (memory) The `RunnableWithMessageHistory` let\'s us add message history to certain types of chains. Specifically, it can be used for any Runnable that takes as input one of - a sequence of `BaseMessage` - a dict with a key that takes a sequence of `BaseMessage` - a dict with a key that takes the latest message(s) as a string or sequence of `BaseMessage`, and a separate key that takes historical messages And returns as output one of - a string that can be treated as the contents of an `AIMessage` - a sequence of `BaseMessage` - a dict with a key that contains a sequence of `BaseMessage` Let\'s take a look at some examples to see how it works. ## Setup We\'ll use Redis to store our chat message histories and Anthropic\'s claude-2 model so we\'ll need to install the following dependencies: ```bash pip install -U langchain redis anthropic ``` Set your [Anthropic API key]( ```python import getpass import os os.environ[""ANTHROPIC_API_KEY""] = getpass.getpass() ``` Start a local Redis Stack server if we don\'t have an existing Redis deployment to connect to: ```bash docker run -d -p 6379:6379 -p 8001:8001 redis/redis-stack:latest ``` ```python REDIS_URL = ""redis://localhost:6379/0"" ``` ### LangSmith LangSmith is especially useful for something like message history injection, where it can be hard to otherwise understand what the inputs are to various parts of the chain. Note that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to uncoment the below and set your environment variables to start logging traces: ```python # os.environ[""LANGCHAIN_TRACING_V2""] = ""true"" # os.environ[""LANGCHAIN_API_KEY""] = getpass.getpass() ``` ## Example: Dict input, message output Let\'s create a simple chain that takes a dict as input and returns a BaseMessage. In this case the `""question""` key in the input represents our input message, and the `""history""` key is where our historical messages will be injected. ```python from typing import Optional from langchain.chat_models import ChatAnthropic from langchain.memory.chat_message_histories import RedisChatMessageHistory from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder from langchain.schema.chat_history import BaseChatMessageHistory from langchain.schema.runnable.history import RunnableWithMessageHistory ``` ```python prompt = ChatPromptTemplate.from_messages( [ (""system"", ""You\'re an assistant who\'s good at {ability}""), MessagesPlaceholder(variable_name=""history""), (""human"", ""{question}""), ] ) chain = prompt | ChatAnthropic(model=""claude-2"") ``` ### Adding message history To add message history to our original chain we wrap it in the `RunnableWithMessageHistory` class. Crucially, we also need to define a method that takes a session_id string and based on it returns a `BaseChatMessageHistory`. Given the same input, this method should return an equivalent output. In this case we\'ll also want to specify `input_messages_key` (the key to be treated as the latest input message) and `history_messages_key` (the key to add historical messages to). ```python chain_with_history = RunnableWithMessageHistory( chain, lambda session_id: RedisChatMessageHistory(session_id, url=REDIS_URL), input_messages_key=""question"", history_messages_key=""history"", ) ``` ## Invoking with config Whenever we call our chain with message history, we need to include a config that contains the `session_id` ```python config={""configurable"": {""session_id"": """"}} ``` Given the same configuration, our chain should be pulling from the same chat message history. ```python chain_with_history.invoke( {""ability"": ""math"", ""question"": ""What does cosine mean?""}, config={""configurable"": {""session_id"": ""foobar""}}, ) ``` ```text AIMessage(content=\' Cosine is one of the basic trigonometric functions in mathematics. It is defined as the ratio of the adjacent side to the hypotenuse in a right triangle.\\n\\nSome key properties and facts about cosine:\\n\\n- It is denoted by cos(), where is the angle in a right triangle. \\n\\n- The cosine of an acute angle is always positive. For angles greater than 90 degrees, cosine can be negative.\\n\\n- Cosine is one of the three main trig functions along with sine and tangent.\\n\\n- The cosine of 0 degrees is 1. As the angle increases towards 90 degrees, the cosine value decreases towards 0.\\n\\n- The range of values for cosine is -1 to 1.\\n\\n- The cosine function maps angles in a circle to the x-coordinate on the unit circle.\\n\\n- Cosine is used to find adjacent side lengths in right triangles, and has many other applications in mathematics, physics, engineering and more.\\n\\n- Key cosine identities include: cos(A+B) = cosAcosB sinAsinB and cos(2A) = cos^2(A) sin^2(A)\\n\\nSo in summary, cosine is a fundamental trig\') ``` ```python chain_with_history.invoke( {""ability"": ""math"", ""question"": ""What\'s its inverse""}, config={""configurable"": {""session_id"": ""foobar""}}, ) ``` ```text AIMessage(content=\' The inverse of the cosine function is called the arccosine or inverse cosine, often denoted as cos-1(x) or arccos(x).\\n\\nThe key properties and facts about arccosine:\\n\\n- It is defined as the angle between 0 and radians whose cosine is x. So arccos(x) = such that cos() = x.\\n\\n- The range of arccosine is 0 to radians (0 to 180 degrees).\\n\\n- The domain of arccosine is -1 to 1. \\n\\n- arccos(cos()) = for values of from 0 to radians.\\n\\n- arccos(x) is the angle in a right triangle whose adjacent side is x and hypotenuse is 1.\\n\\n- arccos(0) = 90 degrees. As x increases from 0 to 1, arccos(x) decreases from 90 to 0 degrees.\\n\\n- arccos(1) = 0 degrees. arccos(-1) = 180 degrees.\\n\\n- The graph of y = arccos(x) is part of the unit circle, restricted to x\') ``` [Langsmith trace]( at the Langsmith trace for the second call, we can see that when constructing the prompt, a ""history"" variable has been injected which is a list of two messages (our first input and first output). ## Example: messages input, dict output ```python from langchain.schema.messages import HumanMessage from langchain.schema.runnable import RunnableMap chain = RunnableMap({""output_message"": ChatAnthropic(model=""claude-2"")}) chain_with_history = RunnableWithMessageHistory( chain, lambda session_id: RedisChatMessageHistory(session_id, url=REDIS_URL), output_messages_key=""output_message"", ) chain_with_history.invoke( [HumanMessage(content=""What did Simone de Beauvoir believe about free will"")], config={""configurable"": {""session_id"": ""baz""}}, ) ``` ```text {\'output_message\': AIMessage(content=\' Here is a summary of Simone de Beauvoir\\\'s views on free will:\\n\\n- De Beauvoir was an existentialist philosopher and believed strongly in the concept of free will. She rejected the idea that human nature or instincts determine behavior.\\n\\n- Instead, de Beauvoir argued that human beings define their own essence or nature through their actions and choices. As she famously wrote, ""One is not born, but rather becomes, a woman.""\\n\\n- De Beauvoir believed that while individuals are situated in certain cultural contexts and social conditions, they still have agency and the ability to transcend these situations. Freedom comes from choosing one\\\'s attitude toward these constraints.\\n\\n- She emphasized the radical freedom and responsibility of the individual. We are ""condemned to be free"" because we cannot escape making choices and taking responsibility for our choices. \\n\\n- De Beauvoir felt that many people evade their freedom and responsibility by adopting rigid mindsets, ideologies, or conforming uncritically to social roles.\\n\\n- She advocated for the recognition of ambiguity in the human condition and warned against the quest for absolute rules that deny freedom and responsibility. Authentic living involves embracing ambiguity.\\n\\nIn summary, de Beauvoir promoted an existential ethics\')} ``` ```python chain_with_history.invoke( [HumanMessage(content=""How did this compare to Sartre"")], config={""configurable"": {""session_id"": ""baz""}}, ) ``` ```text {\'output_message\': AIMessage(content="" There are many similarities between Simone de Beauvoir\'s views on free will and those of Jean-Paul Sartre, though some key differences emerge as well:\\n\\nSimilarities with Sartre:\\n\\n- Both were existentialist thinkers who rejected determinism and emphasized human freedom and responsibility.\\n\\n- They agreed that existence precedes essence - there is no predefined human nature that determines who we are.\\n\\n- Individuals must define themselves through their choices and actions. This leads to anxiety but also freedom.\\n\\n- The human condition is characterized by ambiguity and uncertainty, rather than fixed meanings/values.\\n\\n- Both felt that most people evade their freedom through self-deception, conformity, or adopting collective identities/values uncritically.\\n\\nDifferences from Sartre: \\n\\n- Sartre placed more emphasis on the burden and anguish of radical freedom. De Beauvoir focused more on its positive potential.\\n\\n- De Beauvoir critiqued Sartre\'s premise that human relations are necessarily conflictual. She saw more potential for mutual recognition.\\n\\n- Sartre saw the Other\'s gaze as a threat to freedom. De Beauvoir put more stress on how the Other\'s gaze can confirm"")} ``` [LangSmith trace]( More examples We could also do any of the below: ```python from operator import itemgetter # messages in, messages out RunnableWithMessageHistory( ChatAnthropic(model=""claude-2""), lambda session_id: RedisChatMessageHistory(session_id, url=REDIS_URL), ) # dict with single key for all messages in, messages out RunnableWithMessageHistory( itemgetter(""input_messages"") | ChatAnthropic(model=""claude-2""), lambda session_id: RedisChatMessageHistory(session_id, url=REDIS_URL), input_messages_key=""input_messages"", ) ``` - [Setup](#setup)- [LangSmith](#langsmith) - [Example: Dict input, message output](#example-dict-input-message-output)- [Adding message history](#adding-message-history) - [Invoking with config](#invoking-with-config) - [Example: messages input, dict output](#example-messages-input-dict-output) - [More examples](#more-examples)', 'Map reduce | Map reduce The map reduce documents chain first applies an LLM chain to each document individually (the Map step), treating the chain output as a new document. It then passes all the new documents to a separate combine documents chain to get a single output (the Reduce step). It can optionally first compress, or collapse, the mapped documents to make sure that they fit in the combine documents chain (which will often pass them to an LLM). This compression step is performed recursively if necessary. ![map_reduce_diagram](/assets/images/map_reduce-c65525a871b62f5cacef431625c4d133.jpg) ## Recreating with LCEL With [LangChain Expression Language](/docs/expression_language) we can recreate the `MapReduceDocumentsChain` functionality, with the additional benefit of getting all the built-in LCEL features (batch, async, etc.) and with much more ability to customize specific parts of the chain. ```python from functools import partial from langchain.chains.combine_documents import collapse_docs, split_list_of_docs from langchain.chat_models import ChatAnthropic from langchain.prompts import PromptTemplate from langchain.schema import StrOutputParser from langchain.schema.prompt_template import format_document from langchain.schema.runnable import RunnableParallel, RunnablePassthrough ``` ```python llm = ChatAnthropic() # Prompt and method for converting Document -> str. document_prompt = PromptTemplate.from_template(""{page_content}"") partial_format_document = partial(format_document, prompt=document_prompt) ``` ```python # The chain we\'ll apply to each individual document. # Returns a summary of the document. map_chain = ( {""context"": partial_format_document} | PromptTemplate.from_template(""Summarize this content:\\n\\n{context}"") | llm | StrOutputParser() ) # A wrapper chain to keep the original Document metadata map_as_doc_chain = ( RunnableParallel({""doc"": RunnablePassthrough(), ""content"": map_chain}) | (lambda x: Document(page_content=x[""content""], metadata=x[""doc""].metadata)) ).with_config(run_name=""Summarize (return doc)"") ``` ```python # The chain we\'ll repeatedly apply to collapse subsets of the documents # into a consolidate document until the total token size of our # documents is below some max size. def format_docs(docs): return ""\\n\\n"".join(partial_format_document(doc) for doc in docs) collapse_chain = ( {""context"": format_docs} | PromptTemplate.from_template(""Collapse this content:\\n\\n{context}"") | llm | StrOutputParser() ) def get_num_tokens(docs): return llm.get_num_tokens(format_docs(docs)) def collapse( docs, config, token_max=4000, ): collapse_ct = 1 while get_num_tokens(docs) > token_max: config[""run_name""] = f""Collapse {collapse_ct}"" invoke = partial(collapse_chain.invoke, config=config) split_docs = split_list_of_docs(docs, get_num_tokens, token_max) docs = [collapse_docs(_docs, invoke) for _docs in split_docs] collapse_ct += 1 return docs ``` ```python # The chain we\'ll use to combine our individual document summaries # (or summaries over subset of documents if we had to collapse the map results) # into a final summary. reduce_chain = ( {""context"": format_docs} | PromptTemplate.from_template(""Combine these summaries:\\n\\n{context}"") | llm | StrOutputParser() ).with_config(run_name=""Reduce"") ``` ```python # The final full chain map_reduce = (map_as_doc_chain.map() | collapse | reduce_chain).with_config( run_name=""Map reduce"" ) ``` ## Example run ```python from langchain.schema import Document text = """"""Nuclear power in space is the use of nuclear power in outer space, typically either small fission systems or radioactive decay for electricity or heat. Another use is for scientific observation, as in a Mssbauer spectrometer. The most common type is a radioisotope thermoelectric generator, which has been used on many space probes and on crewed lunar missions. Small fission reactors for Earth observation satellites, such as the TOPAZ nuclear reactor, have also been flown.[1] A radioisotope heater unit is powered by radioactive decay and can keep components from becoming too cold to function, potentially over a span of decades.[2] The United States tested the SNAP-10A nuclear reactor in space for 43 days in 1965,[3] with the next test of a nuclear reactor power system intended for space use occurring on 13 September 2012 with the Demonstration Using Flattop Fission (DUFF) test of the Kilopower reactor.[4] After a ground-based test of the experimental 1965 Romashka reactor, which used uranium and direct thermoelectric conversion to electricity,[5] the USSR sent about 40 nuclear-electric satellites into space, mostly powered by the BES-5 reactor. The more powerful TOPAZ-II reactor produced 10 kilowatts of electricity.[3] Examples of concepts that use nuclear power for space propulsion systems include the nuclear electric rocket (nuclear powered ion thruster(s)), the radioisotope rocket, and radioisotope electric propulsion (REP).[6] One of the more explored concepts is the nuclear thermal rocket, which was ground tested in the NERVA program. Nuclear pulse propulsion was the subject of Project Orion.[7] Regulation and hazard prevention[edit] After the ban of nuclear weapons in space by the Outer Space Treaty in 1967, nuclear power has been discussed at least since 1972 as a sensitive issue by states.[8] Particularly its potential hazards to Earth\'s environment and thus also humans has prompted states to adopt in the U.N. General Assembly the Principles Relevant to the Use of Nuclear Power Sources in Outer Space (1992), particularly introducing safety principles for launches and to manage their traffic.[8] Benefits Both the Viking 1 and Viking 2 landers used RTGs for power on the surface of Mars. (Viking launch vehicle pictured) While solar power is much more commonly used, nuclear power can offer advantages in some areas. Solar cells, although efficient, can only supply energy to spacecraft in orbits where the solar flux is sufficiently high, such as low Earth orbit and interplanetary destinations close enough to the Sun. Unlike solar cells, nuclear power systems function independently of sunlight, which is necessary for deep space exploration. Nuclear-based systems can have less mass than solar cells of equivalent power, allowing more compact spacecraft that are easier to orient and direct in space. In the case of crewed spaceflight, nuclear power concepts that can power both life support and propulsion systems may reduce both cost and flight time.[9] Selected applications and/or technologies for space include: Radioisotope thermoelectric generator Radioisotope heater unit Radioisotope piezoelectric generator Radioisotope rocket Nuclear thermal rocket Nuclear pulse propulsion Nuclear electric rocket """""" docs = [ Document( page_content=split, metadata={""source"": "" ) for split in text.split(""\\n\\n"") ] ``` ```python print(map_reduce.invoke(docs[0:1], config={""max_concurrency"": 5})) ``` ```text Here is a summary that combines the key points about nuclear power in space: Nuclear power is used in space for electricity, heat, and scientific observation. The most common type is a radioisotope thermoelectric generator, which has powered space probes and lunar missions using the heat from radioactive decay. Small nuclear fission reactors have also been used to generate electricity for Earth observation satellites like the TOPAZ reactor. In addition, radioisotope heater units use radioactive decay to provide reliable heat that can keep components functioning properly over decades in the harsh space environment. Overall, nuclear power has proven useful for providing long-lasting power for space applications where solar power is not practical. Technologies like radioisotope decay heat and small fission reactors allow probes, satellites, and missions to operate far from the Sun and for extended periods by generating electricity and heat without reliance on solar energy. ``` - [Recreating with LCEL](#recreating-with-lcel) - [Example run](#example-run)']","This code creates a branching chain that routes the input based on whether there is a chat history present. Here's a breakdown of what each part does:

1. **RunnableBranch**: This constructs a branching chain that decides which sub-chain to execute based on the input.

2. **First Branch**:
    - **RunnableLambda**: This checks if the input dictionary `x` has a key `chat_history` and returns `True` if it exists.
        - `.with_config(run_name=""HasChatHistoryCheck"")`: This sets the run name for this lambda function.
    - **conversation_chain.with_config(run_name=""RetrievalChainWithHistory"")**: This is the chain that will be executed if there is a chat history. It is configured with a run name.

3. **Second Branch**:
    - **RunnableLambda**: This extracts the value associated with the key `question` from the input dictionary `x`.
        - `.with_config(run_name=""Itemgetter:question"")`: This sets the run name for this lambda function.
    - **| retriever**: This passes the extracted `question` to the retriever.
    - `.with_config(run_name=""RetrievalChainWithNoHistory"")`: This sets the run name for the chain that retrieves information without considering chat history.

4. **Outer .with_config(run_name=""RouteDependingOnChatHistory"")**: This sets the run name for the entire branching chain.

In summary, this code creates a chain that decides whether to use a retrieval chain with chat history (`RetrievalChainWithHistory`) or a retrieval chain without chat history (`RetrievalChainWithNoHistory`) based on the presence of `chat_history` in the input. If `chat_history` is present, it uses the chain configured for history; otherwise, it uses the chain without history.","This code defines a runnable chain with two branches. The first branch checks if the input has a 'chat_history' key and runs the conversation_chain if it does. Otherwise uses python stdlib's ""itemgetter"" to get the ""question"" item from the input dictionary. Each ""with_config"" call assigns a friendly ""run_name"" to that step in the traced DAG.",0.0,1.0,,0.06585833693600902,0.2426229508196721
75,How can I use OpenAI functions to get structured outputs in a chain?,"['Using OpenAI functions | Using OpenAI functions This walkthrough demonstrates how to incorporate OpenAI function-calling API\'s in a chain. We\'ll go over: 1. How to use functions to get structured outputs from ChatOpenAI 2. How to create a generic chain that uses (multiple) functions 3. How to create a chain that actually executes the chosen function ```python from typing import Optional from langchain.chains.openai_functions import ( create_openai_fn_chain, create_openai_fn_runnable, create_structured_output_chain, create_structured_output_runnable, ) from langchain.chat_models import ChatOpenAI from langchain.prompts import ChatPromptTemplate ``` ## Getting structured outputs We can take advantage of OpenAI functions to try and force the model to return a particular kind of structured output. We\'ll use `create_structured_output_runnable` to create our chain, which takes the desired structured output either as a Pydantic class or as JsonSchema. ### Using Pydantic classes When passing in Pydantic classes to structure our text, we need to make sure to have a docstring description for the class. It also helps to have descriptions for each of the classes attributes. ```python from langchain.pydantic_v1 import BaseModel, Field class Person(BaseModel): """"""Identifying information about a person."""""" name: str = Field(..., description=""The person\'s name"") age: int = Field(..., description=""The person\'s age"") fav_food: Optional[str] = Field(None, description=""The person\'s favorite food"") ``` ```python # If we pass in a model explicitly, we need to make sure it supports the OpenAI function-calling API. llm = ChatOpenAI(model=""gpt-4"", temperature=0) prompt = ChatPromptTemplate.from_messages( [ ( ""system"", ""You are a world class algorithm for extracting information in structured formats."", ), ( ""human"", ""Use the given format to extract information from the following input: {input}"", ), (""human"", ""Tip: Make sure to answer in the correct format""), ] ) runnable = create_structured_output_runnable(Person, llm, prompt) runnable.invoke({""input"": ""Sally is 13""}) ``` ```text Person(name=\'Sally\', age=13, fav_food=\'Unknown\') ``` To extract arbitrarily many structured outputs of a given format, we can just create a wrapper Pydantic class that takes a sequence of the original class. ```python from typing import Sequence class People(BaseModel): """"""Identifying information about all people in a text."""""" people: Sequence[Person] = Field(..., description=""The people in the text"") runnable = create_structured_output_runnable(People, llm, prompt) runnable.invoke( { ""input"": ""Sally is 13, Joey just turned 12 and loves spinach. Caroline is 10 years older than Sally."" } ) ``` ```text People(people=[Person(name=\'Sally\', age=13, fav_food=\'\'), Person(name=\'Joey\', age=12, fav_food=\'spinach\'), Person(name=\'Caroline\', age=23, fav_food=\'\')]) ``` ### Using JsonSchema We can also pass in JsonSchema instead of Pydantic classes to specify the desired structure. When we do this, our chain will output JSON corresponding to the properties described in the JsonSchema, instead of a Pydantic class. ```python json_schema = { ""title"": ""Person"", ""description"": ""Identifying information about a person."", ""type"": ""object"", ""properties"": { ""name"": {""title"": ""Name"", ""description"": ""The person\'s name"", ""type"": ""string""}, ""age"": {""title"": ""Age"", ""description"": ""The person\'s age"", ""type"": ""integer""}, ""fav_food"": { ""title"": ""Fav Food"", ""description"": ""The person\'s favorite food"", ""type"": ""string"", }, }, ""required"": [""name"", ""age""], } ``` ```python runnable = create_structured_output_runnable(json_schema, llm, prompt) runnable.invoke({""input"": ""Sally is 13""}) ``` ```text {\'name\': \'Sally\', \'age\': 13} ``` ### [Legacy] LLMChain-based approach ```python chain = create_structured_output_chain(Person, llm, prompt, verbose=True) chain.run(""Sally is 13"") ``` ```text > Entering new LLMChain chain... Prompt after formatting: System: You are a world class algorithm for extracting information in structured formats. Human: Use the given format to extract information from the following input: Sally is 13 Human: Tip: Make sure to answer in the correct format > Finished chain. Person(name=\'Sally\', age=13, fav_food=\'Unknown\') ``` ## Creating a generic OpenAI functions chain To create a generic OpenAI functions chain, we can use the `create_openai_fn_runnable` method. This is the same as `create_structured_output_runnable` except that instead of taking a single output schema, it takes a sequence of function definitions. Functions can be passed in as: - dicts conforming to OpenAI functions spec, - Pydantic classes, in which case they should have docstring descriptions of the function they represent and descriptions for each of the parameters, - Python functions, in which case they should have docstring descriptions of the function and args, along with type hints. ### Using Pydantic classes ```python class RecordPerson(BaseModel): """"""Record some identifying information about a pe."""""" name: str = Field(..., description=""The person\'s name"") age: int = Field(..., description=""The person\'s age"") fav_food: Optional[str] = Field(None, description=""The person\'s favorite food"") class RecordDog(BaseModel): """"""Record some identifying information about a dog."""""" name: str = Field(..., description=""The dog\'s name"") color: str = Field(..., description=""The dog\'s color"") fav_food: Optional[str] = Field(None, description=""The dog\'s favorite food"") ``` ```python from langchain.chains.openai_functions import ( convert_to_openai_function, get_openai_output_parser, ) prompt = ChatPromptTemplate.from_messages( [ (""system"", ""You are a world class algorithm for recording entities.""), ( ""human"", ""Make calls to the relevant function to record the entities in the following input: {input}"", ), (""human"", ""Tip: Make sure to answer in the correct format""), ] ) openai_functions = [convert_to_openai_function(f) for f in (RecordPerson, RecordDog)] llm_kwargs = {""functions"": openai_functions} if len(openai_functions) == 1: llm_kwargs[""function_call""] = {""name"": openai_functions[0][""name""]} output_parser = get_openai_output_parser((RecordPerson, RecordDog)) runnable = prompt | llm.bind(**llm_kwargs) | output_parser ``` ```python runnable.invoke({""input"": ""Harry was a chubby brown beagle who loved chicken""}) ``` ```text RecordDog(name=\'Harry\', color=\'brown\', fav_food=\'chicken\') ``` For convenience we can use the `create_openai_fn_runnable` method to help build our Runnable ```python runnable = create_openai_fn_runnable([RecordPerson, RecordDog], llm, prompt) runnable.invoke({""input"": ""Harry was a chubby brown beagle who loved chicken""}) ``` ```text RecordDog(name=\'Harry\', color=\'brown\', fav_food=\'chicken\') ``` ### Using Python functions We can pass in functions as Pydantic classes, directly as OpenAI function dicts, or Python functions. To pass Python function in directly, we\'ll want to make sure our parameters have type hints, we have a docstring, and we use [Google Python style docstrings]( to describe the parameters. **NOTE**: To use Python functions, make sure the function arguments are of primitive types (str, float, int, bool) or that they are Pydantic objects. ```python class OptionalFavFood(BaseModel): """"""Either a food or null."""""" food: Optional[str] = Field( None, description=""Either the name of a food or null. Should be null if the food isn\'t known."", ) def record_person(name: str, age: int, fav_food: OptionalFavFood) -> str: """"""Record some basic identifying information about a person. Args: name: The person\'s name. age: The person\'s age in years. fav_food: An OptionalFavFood object that either contains the person\'s favorite food or a null value. Food should be null if it\'s not known. """""" return f""Recording person {name} of age {age} with favorite food {fav_food.food}!"" runnable = create_openai_fn_runnable([record_person], llm, prompt) runnable.invoke( { ""input"": ""The most important thing to remember about Tommy, my 12 year old, is that he\'ll do anything for apple pie."" } ) ``` ```text {\'name\': \'Tommy\', \'age\': 12, \'fav_food\': {\'food\': \'apple pie\'}} ``` If we pass in multiple Python functions or OpenAI functions, then the returned output will be of the form: ```python {""name"": "">"", ""arguments"": {>}} ``` ```python def record_dog(name: str, color: str, fav_food: OptionalFavFood) -> str: """"""Record some basic identifying information about a dog. Args: name: The dog\'s name. color: The dog\'s color. fav_food: An OptionalFavFood object that either contains the dog\'s favorite food or a null value. Food should be null if it\'s not known. """""" return f""Recording dog {name} of color {color} with favorite food {fav_food}!"" runnable = create_openai_fn_runnable([record_person, record_dog], llm, prompt) runnable.invoke( { ""input"": ""I can\'t find my dog Henry anywhere, he\'s a small brown beagle. Could you send a message about him?"" } ) ``` ```text {\'name\': \'record_dog\', \'arguments\': {\'name\': \'Henry\', \'color\': \'brown\', \'fav_food\': {\'food\': None}}} ``` ## [Legacy] LLMChain-based approach ```python chain = create_openai_fn_chain([RecordPerson, RecordDog], llm, prompt, verbose=True) chain.run(""Harry was a chubby brown beagle who loved chicken"") ``` ```text > Entering new LLMChain chain... Prompt after formatting: System: You are a world class algorithm for recording entities. Human: Make calls to the relevant function to record the entities in the following input: Harry was a chubby brown beagle who loved chicken Human: Tip: Make sure to answer in the correct format > Finished chain. RecordDog(name=\'Harry\', color=\'brown\', fav_food=\'chicken\') ``` ## Other Chains using OpenAI functions There are a number of more specific chains that use OpenAI functions. - [Extraction](/docs/modules/chains/additional/extraction): very similar to structured output chain, intended for information/entity extraction specifically. - [Tagging](/docs/use_cases/tagging): tag inputs. - [OpenAPI](/docs/use_cases/apis/openapi_openai): take an OpenAPI spec and create + execute valid requests against the API, using OpenAI functions under the hood. - [QA with citations](/docs/use_cases/question_answering/qa_citations): use OpenAI functions ability to extract citations from text. - [Getting structured outputs](#getting-structured-outputs)- [Using Pydantic classes](#using-pydantic-classes) - [Using JsonSchema](#using-jsonschema) - [Legacy LLMChain-based approach](#legacy-llmchain-based-approach) - [Creating a generic OpenAI functions chain](#creating-a-generic-openai-functions-chain)- [Using Pydantic classes](#using-pydantic-classes-1) - [Using Python functions](#using-python-functions) - [Legacy LLMChain-based approach](#legacy-llmchain-based-approach-1) - [Other Chains using OpenAI functions](#other-chains-using-openai-functions)', 'Logical Fallacy chain | Logical Fallacy chain This example shows how to remove logical fallacies from model output. ## Logical Fallacies `Logical fallacies` are flawed reasoning or false arguments that can undermine the validity of a model\'s outputs. Examples include circular reasoning, false dichotomies, ad hominem attacks, etc. Machine learning models are optimized to perform well on specific metrics like accuracy, perplexity, or loss. However, optimizing for metrics alone does not guarantee logically sound reasoning. Language models can learn to exploit flaws in reasoning to generate plausible-sounding but logically invalid arguments. When models rely on fallacies, their outputs become unreliable and untrustworthy, even if they achieve high scores on metrics. Users cannot depend on such outputs. Propagating logical fallacies can spread misinformation, confuse users, and lead to harmful real-world consequences when models are deployed in products or services. Monitoring and testing specifically for logical flaws is challenging unlike other quality issues. It requires reasoning about arguments rather than pattern matching. Therefore, it is crucial that model developers proactively address logical fallacies after optimizing metrics. Specialized techniques like causal modeling, robustness testing, and bias mitigation can help avoid flawed reasoning. Overall, allowing logical flaws to persist makes models less safe and ethical. Eliminating fallacies ensures model outputs remain logically valid and aligned with human reasoning. This maintains user trust and mitigates risks. ## Example ```python # Imports from langchain.llms import OpenAI from langchain.prompts import PromptTemplate from langchain.chains.llm import LLMChain from langchain_experimental.fallacy_removal.base import FallacyChain ``` ```python # Example of a model output being returned with a logical fallacy misleading_prompt = PromptTemplate( template=""""""You have to respond by using only logical fallacies inherent in your answer explanations. Question: {question} Bad answer:"""""", input_variables=[""question""], ) llm = OpenAI(temperature=0) misleading_chain = LLMChain(llm=llm, prompt=misleading_prompt) misleading_chain.run(question=""How do I know the earth is round?"") ``` ```text \'The earth is round because my professor said it is, and everyone believes my professor\' ``` ```python fallacies = FallacyChain.get_fallacies([""correction""]) fallacy_chain = FallacyChain.from_llm( chain=misleading_chain, logical_fallacies=fallacies, llm=llm, verbose=True, ) fallacy_chain.run(question=""How do I know the earth is round?"") ``` ```text > Entering new FallacyChain chain... Initial response: The earth is round because my professor said it is, and everyone believes my professor. Applying correction... Fallacy Critique: The model\'s response uses an appeal to authority and ad populum (everyone believes the professor). Fallacy Critique Needed. Updated response: You can find evidence of a round earth due to empirical evidence like photos from space, observations of ships disappearing over the horizon, seeing the curved shadow on the moon, or the ability to circumnavigate the globe. > Finished chain. \'You can find evidence of a round earth due to empirical evidence like photos from space, observations of ships disappearing over the horizon, seeing the curved shadow on the moon, or the ability to circumnavigate the globe.\' ``` - [Logical Fallacies](#logical-fallacies) - [Example](#example)', ""How to | How to [ Bind runtime argsSometimes we want to invoke a Runnable within a Runnable sequence with constant arguments that are not part of the output of the preceding Runnable in the sequence, and which are not part of the user input. We can use Runnable.bind() to easily pass these arguments in.](/docs/expression_language/how_to/binding)[ Configure chain internals at runtimeOftentimes you may want to experiment with, or even expose to the end user, multiple different ways of doing things.](/docs/expression_language/how_to/configure)[ Add fallbacksThere are many possible points of failure in an LLM application, whether that be issues with LLM API's, poor model outputs, issues with other integrations, etc. Fallbacks help you gracefully handle and isolate these issues.](/docs/expression_language/how_to/fallbacks)[ Run custom functionsYou can use arbitrary functions in the pipeline](/docs/expression_language/how_to/functions)[ Stream custom generator functionsYou can use generator functions (ie. functions that use the yield keyword, and behave like iterators) in a LCEL pipeline.](/docs/expression_language/how_to/generators)[ Parallelize stepsRunnableParallel (aka. RunnableMap) makes it easy to execute multiple Runnables in parallel, and to return the output of these Runnables as a map.](/docs/expression_language/how_to/map)[ Add message history (memory)The RunnableWithMessageHistory let's us add message history to certain types of chains.](/docs/expression_language/how_to/message_history)[ Dynamically route logic based on inputThis notebook covers how to do routing in the LangChain Expression Language.](/docs/expression_language/how_to/routing)""]","To get structured outputs using OpenAI functions in a chain, use `create_structured_output_runnable`. Define a Pydantic class or JsonSchema for the desired output format. Pass the class or schema, a ChatOpenAI instance, and a prompt to `create_structured_output_runnable`. Invoke the runnable with input data.","You can use OpenAI functions to get structured outputs in a chain by using the create_structured_output_runnable function. This function takes the desired structured output either as a Pydantic class or as JsonSchema. If you pass in a model explicitly, make sure it supports the OpenAI function-calling API. You can then invoke the runnable with the input data, and it will return the structured output. You can also use the bind_functions() method on the ChatOpenAI class to do this directly from an instance of ChatOpenAI.",0.8333333332916666,0.8,1.0,0.4029726553344524,0.36496350364963503
76,whats the diff between a docstore and a vector store in langchain?,"['Fleet AI Libraries Context | Fleet AI Libraries Context The Fleet AI team is on a mission to embed the world\'s most important data. They\'ve started by embedding the top 1200 Python libraries to enable code generation with up-to-date knowledge. They\'ve been kind enough to share their embeddings of the [LangChain docs]( and [API reference]( Let\'s take a look at how we can use these embeddings to power a docs retrieval system and ultimately a simple code generating chain! ```bash pip install langchain fleet-context openai pandas faiss-cpu # faiss-gpu for CUDA supported GPU ``` ```python from operator import itemgetter from typing import Any, Optional, Type import pandas as pd from langchain.embeddings import OpenAIEmbeddings from langchain.retrievers import MultiVectorRetriever from langchain.schema import Document from langchain.schema.storage import BaseStore from langchain.schema.vectorstore import VectorStore from langchain.vectorstores import FAISS def load_fleet_retriever( df: pd.DataFrame, *, vectorstore_cls: Type[VectorStore] = FAISS, docstore: Optional[BaseStore] = None, **kwargs: Any, ): vectorstore = _populate_vectorstore(df, vectorstore_cls) if docstore is None: return vectorstore.as_retriever(**kwargs) else: _populate_docstore(df, docstore) return MultiVectorRetriever( vectorstore=vectorstore, docstore=docstore, id_key=""parent"", **kwargs ) def _populate_vectorstore( df: pd.DataFrame, vectorstore_cls: Type[VectorStore], ) -> VectorStore: if not hasattr(vectorstore_cls, ""from_embeddings""): raise ValueError( f""Incompatible vector store class {vectorstore_cls}."" ""Must implement `from_embeddings` class method."" ) texts_embeddings = [] metadatas = [] for _, row in df.iterrows(): texts_embeddings.append((row.metadata[""text""], row[""dense_embeddings""])) metadatas.append(row.metadata) return vectorstore_cls.from_embeddings( texts_embeddings, OpenAIEmbeddings(model=""text-embedding-ada-002""), metadatas=metadatas, ) def _populate_docstore(df: pd.DataFrame, docstore: BaseStore) -> None: parent_docs = [] df = df.copy() df[""parent""] = df.metadata.apply(itemgetter(""parent"")) for parent_id, group in df.groupby(""parent""): sorted_group = group.iloc[ group.metadata.apply(itemgetter(""section_index"")).argsort() ] text = """".join(sorted_group.metadata.apply(itemgetter(""text""))) metadata = { k: sorted_group.iloc[0].metadata[k] for k in (""title"", ""type"", ""url"") } text = metadata[""title""] + ""\\n"" + text metadata[""id""] = parent_id parent_docs.append(Document(page_content=text, metadata=metadata)) docstore.mset(((d.metadata[""id""], d) for d in parent_docs)) ``` ## Retriever chunks As part of their embedding process, the Fleet AI team first chunked long documents before embedding them. This means the vectors correspond to sections of pages in the LangChain docs, not entire pages. By default, when we spin up a retriever from these embeddings, we\'ll be retrieving these embedded chunks. We will be using Fleet Context\'s `download_embeddings()` to grab Langchain\'s documentation embeddings. You can view all supported libraries\' documentation at [ ```python from context import download_embeddings df = download_embeddings(""langchain"") vecstore_retriever = load_fleet_retriever(df) ``` ```python vecstore_retriever.get_relevant_documents(""How does the multi vector retriever work"") ``` ```text [Document(page_content=""# Vector store-backed retriever A vector store retriever is a retriever that uses a vector store to retrieve documents. It is a lightweight wrapper around the vector store class to make it conform to the retriever interface. It uses the search methods implemented by a vector store, like similarity search and MMR, to query the texts in the vector store. Once you construct a vector store, it\'s very easy to construct a retriever. Let\'s walk through an example."", metadata={\'id\': \'f509f20d-4c63-4a5a-a40a-5c4c0f099839\', \'library_id\': \'4506492b-70de-49f1-ba2e-d65bd7048a28\', \'page_id\': \'d78cf422-2dab-4860-80fe-d71a3619b02f\', \'parent\': \'c153ebd9-2611-4a43-9db6-daa1f5f214f6\', \'section_id\': \'\', \'section_index\': 0, \'text\': ""# Vector store-backed retriever A vector store retriever is a retriever that uses a vector store to retrieve documents. It is a lightweight wrapper around the vector store class to make it conform to the retriever interface. It uses the search methods implemented by a vector store, like similarity search and MMR, to query the texts in the vector store. Once you construct a vector store, it\'s very easy to construct a retriever. Let\'s walk through an example."", \'title\': \'Vector store-backed retriever | Langchain\', \'type\': None, \'url\': \' Document(page_content=\'# MultiVector Retriever It can often be beneficial to store multiple vectors per document. There are multiple use cases where this is beneficial. LangChain has a base `MultiVectorRetriever` which makes querying this type of setup easy. A lot of the complexity lies in how to create the multiple vectors per document. This notebook covers some of the common ways to create those vectors and use the `MultiVectorRetriever`. The methods to create multiple vectors per document include: - Smaller chunks: split a document into smaller chunks, and embed those (this is ParentDocumentRetriever). - Summary: create a summary for each document, embed that along with (or instead of) the document. - Hypothetical questions: create hypothetical questions that each document would be appropriate to answer, embed those along with (or instead of) the document. Note that this also enables another method of adding embeddings - manually.\', metadata={\'id\': \'e06e6bb5-127a-49f7-9511-247d279d0d83\', \'library_id\': \'4506492b-70de-49f1-ba2e-d65bd7048a28\', \'page_id\': \'7c7dd0de-25e0-4e1d-9237-cfd2674c29d4\', \'parent\': \'beec5531-16a7-453c-80ab-c5628e0236ce\', \'section_id\': \'\', \'section_index\': 0, \'text\': \'# MultiVector Retriever It can often be beneficial to store multiple vectors per document. There are multiple use cases where this is beneficial. LangChain has a base `MultiVectorRetriever` which makes querying this type of setup easy. A lot of the complexity lies in how to create the multiple vectors per document. This notebook covers some of the common ways to create those vectors and use the `MultiVectorRetriever`. The methods to create multiple vectors per document include: - Smaller chunks: split a document into smaller chunks, and embed those (this is ParentDocumentRetriever). - Summary: create a summary for each document, embed that along with (or instead of) the document. - Hypothetical questions: create hypothetical questions that each document would be appropriate to answer, embed those along with (or instead of) the document. Note that this also enables another method of adding embeddings - manually.\', \'title\': \'MultiVector Retriever | Langchain\', \'type\': None, \'url\': \' Document(page_content=\'# MultiQueryRetriever Distance-based vector database retrieval embeds (represents) queries in high-dimensional space and finds similar embedded documents based on ""distance"". But, retrieval may produce different results with subtle changes in query wording or if the embeddings do not capture the semantics of the data well. Prompt engineering / tuning is sometimes done to manually address these problems, but can be tedious. The `MultiQueryRetriever` automates the process of prompt tuning by using an LLM to generate multiple queries from different perspectives for a given user input query. For each query, it retrieves a set of relevant documents and takes the unique union across all queries to get a larger set of potentially relevant documents. By generating multiple perspectives on the same question, the `MultiQueryRetriever` might be able to overcome some of the limitations of the distance-based retrieval and get a richer set of results.\', metadata={\'id\': \'dc3b8e5b-a591-4a46-bfeb-a91b36affae1\', \'library_id\': \'4506492b-70de-49f1-ba2e-d65bd7048a28\', \'page_id\': \'31f80e84-c5db-4da2-939c-bccf519864a3\', \'parent\': \'f7c20633-6a60-4ca3-96b1-13fee66e321d\', \'section_id\': \'\', \'section_index\': 0, \'text\': \'# MultiQueryRetriever Distance-based vector database retrieval embeds (represents) queries in high-dimensional space and finds similar embedded documents based on ""distance"". But, retrieval may produce different results with subtle changes in query wording or if the embeddings do not capture the semantics of the data well. Prompt engineering / tuning is sometimes done to manually address these problems, but can be tedious. The `MultiQueryRetriever` automates the process of prompt tuning by using an LLM to generate multiple queries from different perspectives for a given user input query. For each query, it retrieves a set of relevant documents and takes the unique union across all queries to get a larger set of potentially relevant documents. By generating multiple perspectives on the same question, the `MultiQueryRetriever` might be able to overcome some of the limitations of the distance-based retrieval and get a richer set of results.\', \'title\': \'MultiQueryRetriever | Langchain\', \'type\': None, \'url\': \' Document(page_content=\'# `langchain.retrievers.multi_vector`.MultiVectorRetriever[](#langchain-retrievers-multi-vector-multivectorretriever) *class *langchain.retrievers.multi_vector.MultiVectorRetriever[[source]](../_modules/langchain/retrievers/multi_vector.html#MultiVectorRetriever)[](#langchain.retrievers.multi_vector.MultiVectorRetriever) # Examples using MultiVectorRetriever[](#langchain-retrievers-multi-vector-multivectorretriever) - [MultiVector Retriever]( metadata={\'id\': \'1f4ca702-35b8-44c0-b33b-18c09a1f787d\', \'library_id\': \'6254c672-7aa0-4233-b0a6-804bd273752b\', \'page_id\': \'87f5d1fb-a1c6-4080-9d2b-7b88794eb6bf\', \'parent\': \'1820c44d-7783-4846-a11c-106b18da015d\', \'section_id\': \'langchain-retrievers-multi-vector-multivectorretriever\', \'section_index\': 0, \'text\': \'# `langchain.retrievers.multi_vector`.MultiVectorRetriever[](#langchain-retrievers-multi-vector-multivectorretriever) *class *langchain.retrievers.multi_vector.MultiVectorRetriever[[source]](../_modules/langchain/retrievers/multi_vector.html#MultiVectorRetriever)[](#langchain.retrievers.multi_vector.MultiVectorRetriever) # Examples using MultiVectorRetriever[](#langchain-retrievers-multi-vector-multivectorretriever) - [MultiVector Retriever]( \'title\': \'langchain.retrievers.multi_vector.MultiVectorRetriever LangChain 0.0.322\', \'type\': None, \'url\': \' ``` ## Other packages You can download and use other embeddings from [this Dropbox link]( ## Retrieve parent docs The embeddings provided by Fleet AI contain metadata that indicates which embedding chunks correspond to the same original document page. If we\'d like we can use this information to retrieve whole parent documents, and not just embedded chunks. Under the hood, we\'ll use a MultiVectorRetriever and a BaseStore object to search for relevant chunks and then map them to their parent document. ```python from langchain.storage import InMemoryStore parent_retriever = load_fleet_retriever( "" docstore=InMemoryStore(), ) ``` ```python parent_retriever.get_relevant_documents(""How does the multi vector retriever work"") ``` ```text [Document(page_content=\'Vector store-backed retriever | Langchain\\n# Vector store-backed retriever A vector store retriever is a retriever that uses a vector store to retrieve documents. It is a lightweight wrapper around the vector store class to make it conform to the retriever interface. It uses the search methods implemented by a vector store, like similarity search and MMR, to query the texts in the vector store. Once you construct a vector store, it\\\'s very easy to construct a retriever. Let\\\'s walk through an example.Once you construct a vector store, it\\\'s very easy to construct a retriever. Let\\\'s walk through an example. ``` from langchain.document_loaders import TextLoaderloader = TextLoader(\\\'../../../state_of_the_union.txt\\\') ``` ``` from langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import FAISSfrom langchain.embeddings import OpenAIEmbeddingsdocuments = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)texts = text_splitter.split_documents(documents)embeddings = OpenAIEmbeddings()db = FAISS.from_documents(texts, embeddings) ``` ``` Exiting: Cleaning up .chroma directory ``` ``` retriever = db.as_retriever() ``` ``` docs = retriever.get_relevant_documents(""what did he say about ketanji brown jackson"") ``` ## Maximum marginal relevance retrieval[\\u200b](#maximum-marginal-relevance-retrieval) By default, the vector store retriever uses similarity search.If the underlying vector store supports maximum marginal relevance search, you can specify that as the search type. ``` retriever = db.as_retriever(search_type=""mmr"") ``` ``` docs = retriever.get_relevant_documents(""what did he say about ketanji brown jackson"") ``` ## Similarity score threshold retrieval[\\u200b](#similarity-score-threshold-retrieval) You can also a retrieval method that sets a similarity score threshold and only returns documents with a score above that threshold. ``` retriever = db.as_retriever(search_type=""similarity_score_threshold"", search_kwargs={""score_threshold"": .5}) ``` ``` docs = retriever.get_relevant_documents(""what did he say about ketanji brown jackson"") ``` ## Specifying top k[\\u200b](#specifying-top-k) You can also specify search kwargs like `k` to use when doing retrieval.``` retriever = db.as_retriever(search_kwargs={""k"": 1}) ``` ``` docs = retriever.get_relevant_documents(""what did he say about ketanji brown jackson"") ``` ``` len(docs) ``` ``` 1 ```\', metadata={\'title\': \'Vector store-backed retriever | Langchain\', \'type\': None, \'url\': \' \'id\': \'c153ebd9-2611-4a43-9db6-daa1f5f214f6\'}), Document(page_content=\'MultiVector Retriever | Langchain\\n# MultiVector Retriever It can often be beneficial to store multiple vectors per document. There are multiple use cases where this is beneficial. LangChain has a base `MultiVectorRetriever` which makes querying this type of setup easy. A lot of the complexity lies in how to create the multiple vectors per document. This notebook covers some of the common ways to create those vectors and use the `MultiVectorRetriever`. The methods to create multiple vectors per document include: - Smaller chunks: split a document into smaller chunks, and embed those (this is ParentDocumentRetriever). - Summary: create a summary for each document, embed that along with (or instead of) the document. - Hypothetical questions: create hypothetical questions that each document would be appropriate to answer, embed those along with (or instead of) the document. Note that this also enables another method of adding embeddings - manually.Note that this also enables another method of adding embeddings - manually. This is great because you can explicitly add questions or queries that should lead to a document being recovered, giving you more control. ``` from langchain.retrievers.multi_vector import MultiVectorRetriever ``` ``` from langchain.vectorstores import Chromafrom langchain.embeddings import OpenAIEmbeddingsfrom langchain.text_splitter import RecursiveCharacterTextSplitterfrom langchain.storage import InMemoryStorefrom langchain.document_loaders import TextLoader ``` ``` loaders = [ TextLoader(\\\'../../paul_graham_essay.txt\\\'), TextLoader(\\\'../../state_of_the_union.txt\\\'),]docs = []for l in loaders: docs.extend(l.load())text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000)docs = text_splitter.split_documents(docs) ``` ## Smaller chunks[\\u200b](#smaller-chunks) Often times it can be useful to retrieve larger chunks of information, but embed smaller chunks.This allows for embeddings to capture the semantic meaning as closely as possible, but for as much context as possible to be passed downstream. Note that this is what the `ParentDocumentRetriever` does. Here we show what is going on under the hood.``` # The vectorstore to use to index the child chunksvectorstore = Chroma( collection_name=""full_documents"", embedding_function=OpenAIEmbeddings())# The storage layer for the parent documentsstore = InMemoryStore()id_key = ""doc_id""# The retriever (empty to start)retriever = MultiVectorRetriever( vectorstore=vectorstore, docstore=store, id_key=id_key,)import uuiddoc_ids = [str(uuid.uuid4()) for _ in docs] ``` ``` # The splitter to use to create smaller chunkschild_text_splitter = RecursiveCharacterTextSplitter(chunk_size=400) ``` ``` sub_docs = []for i, doc in enumerate(docs): _id = doc_ids[i] _sub_docs = child_text_splitter.split_documents([doc]) for _doc in _sub_docs: _doc.metadata[id_key] = _id sub_docs.extend(_sub_docs) ``` ``` retriever.vectorstore.add_documents(sub_docs)retriever.docstore.mset(list(zip(doc_ids, docs))) ``` ``` # Vectorstore alone retrieves the small chunksretriever.vectorstore.similarity_search(""justice breyer"")[0] ``` ``` Document(page_content=\\\'Tonight, I\'d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyeran Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court.Justice Breyer, thank you for your service. \\\\n\\\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\\', metadata={\\\'doc_id\\\': \\\'10e9cbc0-4ba5-4d79-a09b-c033d1ba7b01\\\', \\\'source\\\': \\\'../../state_of_the_union.txt\\\'}) ``` ``` # Retriever returns larger chunkslen(retriever.get_relevant_documents(""justice breyer"")[0].page_content) ``` ``` 9874 ``` ## Summary[\\u200b](#summary) Oftentimes a summary may be able to distill more accurately what a chunk is about, leading to better retrieval. Here we show how to create summaries, and then embed those.``` from langchain.chat_models import ChatOpenAIfrom langchain.prompts import ChatPromptTemplatefrom langchain.schema.output_parser import StrOutputParserimport uuidfrom langchain.schema.document import Document ``` ``` chain = ( {""doc"": lambda x: x.page_content} | ChatPromptTemplate.from_template(""Summarize the following document:\\\\n\\\\n{doc}"") | ChatOpenAI(max_retries=0) | StrOutputParser()) ``` ``` summaries = chain.batch(docs, {""max_concurrency"": 5}) ``` ``` # The vectorstore to use to index the child chunksvectorstore = Chroma( collection_name=""summaries"", embedding_function=OpenAIEmbeddings())# The storage layer for the parent documentsstore = InMemoryStore()id_key = ""doc_id""# The retriever (empty to start)retriever = MultiVectorRetriever( vectorstore=vectorstore, docstore=store, id_key=id_key,)doc_ids = [str(uuid.uuid4()) for _ in docs] ``` ``` summary_docs = [Document(page_content=s,metadata={id_key: doc_ids[i]}) for i, s in enumerate(summaries)] ``` ``` retriever.vectorstore.add_documents(summary_docs)retriever.docstore.mset(list(zip(doc_ids, docs))) ``` ``` # # We can also add the original chunks to the vectorstore if we so want# for i, doc in enumerate(docs):# doc.metadata[id_key] = doc_ids[i]# retriever.vectorstore.add_documents(docs) ``` ``` sub_docs = vectorstore.similarity_search(""justice breyer"") ``` ``` sub_docs[0] ``` ``` Document(page_content=""The document is a transcript of a speech given by the President of the United States.The President discusses several important issues and initiatives, including the nomination of a Supreme Court Justice, border security and immigration reform, protecting women\\\'s rights, advancing LGBTQ+ equality, bipartisan legislation, addressing the opioid epidemic and mental health, supporting veterans, investigating the health effects of burn pits on military personnel, ending cancer, and the strength and resilience of the American people. "", metadata={\\\'doc_id\\\': \\\'79fa2e9f-28d9-4372-8af3-2caf4f1de312\\\'}) ``` ``` retrieved_docs = retriever.get_relevant_documents(""justice breyer"") ``` ``` len(retrieved_docs[0].page_content) ``` ``` 9194 ``` ## Hypothetical Queries[\\u200b](#hypothetical-queries) An LLM can also be used to generate a list of hypothetical questions that could be asked of a particular document.These questions can then be embedded ``` functions = [ { ""name"": ""hypothetical_questions"", ""description"": ""Generate hypothetical questions"", ""parameters"": { ""type"": ""object"", ""properties"": { ""questions"": { ""type"": ""array"", ""items"": { ""type"": ""string"" }, }, }, ""required"": [""questions""] } } ] ``` ``` from langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParserchain = ( {""doc"": lambda x: x.page_content} # Only asking for 3 hypothetical questions, but this could be adjusted | ChatPromptTemplate.from_template(""Generate a list of 3 hypothetical questions that the below document could be used to answer:\\\\n\\\\n{doc}"") | ChatOpenAI(max_retries=0, model=""gpt-4"").bind(functions=functions, function_call={""name"": ""hypothetical_questions""}) | JsonKeyOutputFunctionsParser(key_name=""questions"")) ``` ``` chain.invoke(docs[0]) ``` ``` [""What was the author\\\'s initial impression of philosophy as a field of study, and how did it change when they got to college?"", \\\'Why did the author decide to switch their focus to Artificial Intelligence (AI)? \\\', ""What led to the author\\\'s disillusionment with the field of AI as it was practiced at the time?""]``` ``` hypothetical_questions = chain.batch(docs, {""max_concurrency"": 5}) ``` ``` # The vectorstore to use to index the child chunksvectorstore = Chroma( collection_name=""hypo-questions"", embedding_function=OpenAIEmbeddings())# The storage layer for the parent documentsstore = InMemoryStore()id_key = ""doc_id""# The retriever (empty to start)retriever = MultiVectorRetriever( vectorstore=vectorstore, docstore=store, id_key=id_key,)doc_ids = [str(uuid.uuid4()) for _ in docs] ``` ``` question_docs = []for i, question_list in enumerate(hypothetical_questions): question_docs.extend([Document(page_content=s,metadata={id_key: doc_ids[i]}) for s in question_list]) ``` ``` retriever.vectorstore.add_documents(question_docs)retriever.docstore.mset(list(zip(doc_ids, docs))) ``` ``` sub_docs = vectorstore.similarity_search(""justice breyer"") ``` ``` sub_docs ``` ``` [Document(page_content=""What is the President\\\'s stance on immigration reform?"", metadata={\\\'doc_id\\\': \\\'505d73e3-8350-46ec-a58e-3af032f04ab3\\\'}), Document(page_content=""What is the President\\\'s stance on immigration reform? "", metadata={\\\'doc_id\\\': \\\'1c9618f0-7660-4b4f-a37c-509cbbbf6dba\\\'}), Document(page_content=""What is the President\\\'s stance on immigration reform? "", metadata={\\\'doc_id\\\': \\\'82c08209-b904-46a8-9532-edd2380950b7\\\'}), Document(page_content=\\\'What measures is the President proposing to protect the rights of LGBTQ+ Americans? \\\', metadata={\\\'doc_id\\\': \\\'82c08209-b904-46a8-9532-edd2380950b7\\\'})] ``` ``` retrieved_docs = retriever.get_relevant_documents(""justice breyer"") ``` ``` len(retrieved_docs[0].page_content) ``` ``` 9194 ```\', metadata={\'title\': \'MultiVector Retriever | Langchain\', \'type\': None, \'url\': \' \'id\': \'beec5531-16a7-453c-80ab-c5628e0236ce\'}), Document(page_content=\'MultiQueryRetriever | Langchain\\n# MultiQueryRetriever Distance-based vector database retrieval embeds (represents) queries in high-dimensional space and finds similar embedded documents based on ""distance"". But, retrieval may produce different results with subtle changes in query wording or if the embeddings do not capture the semantics of the data well. Prompt engineering / tuning is sometimes done to manually address these problems, but can be tedious. The `MultiQueryRetriever` automates the process of prompt tuning by using an LLM to generate multiple queries from different perspectives for a given user input query. For each query, it retrieves a set of relevant documents and takes the unique union across all queries to get a larger set of potentially relevant documents. By generating multiple perspectives on the same question, the `MultiQueryRetriever` might be able to overcome some of the limitations of the distance-based retrieval and get a richer set of results.By generating multiple perspectives on the same question, the `MultiQueryRetriever` might be able to overcome some of the limitations of the distance-based retrieval and get a richer set of results. ``` # Build a sample vectorDBfrom langchain.vectorstores import Chromafrom langchain.document_loaders import WebBaseLoaderfrom langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.text_splitter import RecursiveCharacterTextSplitter# Load blog postloader = WebBaseLoader("" = loader.load()# Splittext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)splits = text_splitter.split_documents(data)# VectorDBembedding = OpenAIEmbeddings()vectordb = Chroma.from_documents(documents=splits, embedding=embedding) ``` #### Simple usage[\\u200b](#simple-usage) Specify the LLM to use for query generation, and the retriever will do the rest.``` from langchain.chat_models import ChatOpenAIfrom langchain.retrievers.multi_query import MultiQueryRetrieverquestion = ""What are the approaches to Task Decomposition? ""llm = ChatOpenAI(temperature=0)retriever_from_llm = MultiQueryRetriever.from_llm( retriever=vectordb.as_retriever(), llm=llm) ``` ``` # Set logging for the queriesimport logginglogging.basicConfig()logging.getLogger(""langchain.retrievers.multi_query"").setLevel(logging.INFO) ``` ``` unique_docs = retriever_from_llm.get_relevant_documents(query=question)len(unique_docs) ``` ``` INFO:langchain.retrievers.multi_query:Generated queries: [\\\'1. How can Task Decomposition be approached? \\\', \\\'2. What are the different methods for Task Decomposition? \\\', \\\'3. What are the various approaches to decomposing tasks?\\\'] 5 ``` #### Supplying your own prompt[\\u200b](#supplying-your-own-prompt) You can also supply a prompt along with an output parser to split the results into a list of queries.5 ``` #### Supplying your own prompt[\\u200b](#supplying-your-own-prompt) You can also supply a prompt along with an output parser to split the results into a list of queries. ``` from typing import Listfrom langchain.chains import LLMChainfrom pydantic import BaseModel, Fieldfrom langchain.prompts import PromptTemplatefrom langchain.output_parsers import PydanticOutputParser# Output parser will split the LLM result into a list of queriesclass LineList(BaseModel): # ""lines"" is the key (attribute name) of the parsed output lines: List[str] = Field(description=""Lines of text"")class LineListOutputParser(PydanticOutputParser): def __init__(self) -> None: super().__init__(pydantic_object=LineList) def parse(self, text: str) -> LineList: lines = text.strip().split(""\\\\n"") return LineList(lines=lines)output_parser = LineListOutputParser()QUERY_PROMPT = PromptTemplate( input_variables=[""question""], template=""""""You are an AI language model assistant.Your task is to generate five different versions of the given user question to retrieve relevant documents from a vector database. By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based similarity search. Provide these alternative questions separated by newlines. Original question: {question}"""""",)llm = ChatOpenAI(temperature=0)# Chainllm_chain = LLMChain(llm=llm, prompt=QUERY_PROMPT, output_parser=output_parser)# Other inputsquestion = ""What are the approaches to Task Decomposition?"" ``` ``` # Runretriever = MultiQueryRetriever( retriever=vectordb.as_retriever(), llm_chain=llm_chain, parser_key=""lines"") # ""lines"" is the key (attribute name) of the parsed output# Resultsunique_docs = retriever.get_relevant_documents( query=""What does the course say about regression? "")len(unique_docs) ``` ``` INFO:langchain.retrievers.multi_query:Generated queries: [""1."")len(unique_docs) ``` ``` INFO:langchain.retrievers.multi_query:Generated queries: [""1. What is the course\\\'s perspective on regression? "", \\\'2. Can you provide information on regression as discussed in the course? \\\', \\\'3. How does the course cover the topic of regression? \\\', ""4. What are the course\\\'s teachings on regression? "", \\\'5. In relation to the course, what is mentioned about regression?\\\'] 11 ```\', metadata={\'title\': \'MultiQueryRetriever | Langchain\', \'type\': None, \'url\': \' \'id\': \'f7c20633-6a60-4ca3-96b1-13fee66e321d\'}), Document(page_content=\'langchain.retrievers.multi_vector.MultiVectorRetriever LangChain 0.0.322\\n# `langchain.retrievers.multi_vector`.MultiVectorRetriever[](#langchain-retrievers-multi-vector-multivectorretriever) *class *langchain.retrievers.multi_vector.MultiVectorRetriever[[source]](../_modules/langchain/retrievers/multi_vector.html#MultiVectorRetriever)[](#langchain.retrievers.multi_vector.MultiVectorRetriever) # Examples using MultiVectorRetriever[](#langchain-retrievers-multi-vector-multivectorretriever) - [MultiVector Retriever]( metadata={\'title\': \'langchain.retrievers.multi_vector.MultiVectorRetriever LangChain 0.0.322\', \'type\': None, \'url\': \' \'id\': \'1820c44d-7783-4846-a11c-106b18da015d\'})] ``` ## Putting it in a chain Let\'s try using our retrieval systems in a simple chain! ```python from langchain.chat_models import ChatOpenAI from langchain.prompts import ChatPromptTemplate from langchain.schema import StrOutputParser from langchain.schema.runnable import RunnablePassthrough prompt = ChatPromptTemplate.from_messages( [ ( ""system"", """"""You are a great software engineer who is very familiar \\ with Python. Given a user question or request about a new Python library called LangChain and \\ parts of the LangChain documentation, answer the question or generate the requested code. \\ Your answers must be accurate, should include code whenever possible, and should assume anything \\ about LangChain which is note explicitly stated in the LangChain documentation. If the required \\ information is not available, just say so. LangChain Documentation ------------------ {context}"""""", ), (""human"", ""{question}""), ] ) model = ChatOpenAI(model=""gpt-3.5-turbo-16k"") chain = ( { ""question"": RunnablePassthrough(), ""context"": parent_retriever | (lambda docs: ""\\n\\n"".join(d.page_content for d in docs)), } | prompt | model | StrOutputParser() ) ``` ```python for chunk in chain.invoke( ""How do I create a FAISS vector store retriever that returns 10 documents per search query"" ): print(chunk, end="""", flush=True) ``` ```text To create a FAISS vector store retriever that returns 10 documents per search query, you can use the following code: ```python from langchain.embeddings import OpenAIEmbeddings from langchain.vectorstores import FAISS # Assuming you have already loaded and split your documents # into `texts` and initialized your `embeddings` object # Create the FAISS vector store db = FAISS.from_documents(texts, embeddings) # Create the retriever with the desired search kwargs retriever = db.as_retriever(search_kwargs={""k"": 10}) ``` Now, you can use the `retriever` object to get relevant documents using the `get_relevant_documents` method. For example: ```python docs = retriever.get_relevant_documents(""your search query"") ``` This will return a list of 10 documents that are most relevant to the given search query. ``` - [Retriever chunks](#retriever-chunks) - [Other packages](#other-packages) - [Retrieve parent docs](#retrieve-parent-docs) - [Putting it in a chain](#putting-it-in-a-chain)', 'Chatbots | Chatbots []( ## Use case Chatbots are one of the central LLM use-cases. The core features of chatbots are that they can have long-running conversations and have access to information that users want to know about. Aside from basic prompting and LLMs, memory and retrieval are the core components of a chatbot. Memory allows a chatbot to remember past interactions, and retrieval provides a chatbot with up-to-date, domain-specific information. ![Image description](/assets/images/chat_use_case-eb8a4883931d726e9f23628a0d22e315.png) ## Overview The chat model interface is based around messages rather than raw text. Several components are important to consider for chat: - `chat model`: See [here](/docs/integrations/chat) for a list of chat model integrations and [here](/docs/modules/model_io/chat) for documentation on the chat model interface in LangChain. You can use `LLMs` (see [here](/docs/modules/model_io/llms)) for chatbots as well, but chat models have a more conversational tone and natively support a message interface. - `prompt template`: Prompt templates make it easy to assemble prompts that combine default messages, user input, chat history, and (optionally) additional retrieved context. - `memory`: [See here](/docs/modules/memory/) for in-depth documentation on memory types - `retriever` (optional): [See here](/docs/modules/data_connection/retrievers) for in-depth documentation on retrieval systems. These are useful if you want to build a chatbot with domain-specific knowledge. ## Quickstart Here\'s a quick preview of how we can create chatbot interfaces. First let\'s install some dependencies and set the required credentials: ```bash pip install langchain openai # Set env var OPENAI_API_KEY or load from a .env file: # import dotenv # dotenv.load_dotenv() ``` With a plain chat model, we can get chat completions by [passing one or more messages](/docs/modules/model_io/chat) to the model. The chat model will respond with a message. ```python from langchain.chat_models import ChatOpenAI from langchain.schema import HumanMessage, SystemMessage chat = ChatOpenAI() chat( [ HumanMessage( content=""Translate this sentence from English to French: I love programming."" ) ] ) ``` ```text AIMessage(content=""J\'adore la programmation."", additional_kwargs={}, example=False) ``` And if we pass in a list of messages: ```python messages = [ SystemMessage( content=""You are a helpful assistant that translates English to French."" ), HumanMessage(content=""I love programming.""), ] chat(messages) ``` ```text AIMessage(content=""J\'adore la programmation."", additional_kwargs={}, example=False) ``` We can then wrap our chat model in a `ConversationChain`, which has built-in memory for remembering past user inputs and model outputs. ```python from langchain.chains import ConversationChain conversation = ConversationChain(llm=chat) conversation.run(""Translate this sentence from English to French: I love programming."") ``` ```text \'Je adore la programmation.\' ``` ```python conversation.run(""Translate it to German."") ``` ```text \'Ich liebe Programmieren.\' ``` ## Memory As we mentioned above, the core component of chatbots is the memory system. One of the simplest and most commonly used forms of memory is `ConversationBufferMemory`: - This memory allows for storing of messages in a `buffer` - When called in a chain, it returns all of the messages it has stored LangChain comes with many other types of memory, too. [See here](/docs/modules/memory/) for in-depth documentation on memory types. For now let\'s take a quick look at ConversationBufferMemory. We can manually add a few chat messages to the memory like so: ```python from langchain.memory import ConversationBufferMemory memory = ConversationBufferMemory() memory.chat_memory.add_user_message(""hi!"") memory.chat_memory.add_ai_message(""whats up?"") ``` And now we can load from our memory. The key method exposed by all `Memory` classes is `load_memory_variables`. This takes in any initial chain input and returns a list of memory variables which are added to the chain input. Since this simple memory type doesn\'t actually take into account the chain input when loading memory, we can pass in an empty input for now: ```python memory.load_memory_variables({}) ``` ```text {\'history\': \'Human: hi!\\nAI: whats up?\'} ``` We can also keep a sliding window of the most recent `k` interactions using `ConversationBufferWindowMemory`. ```python from langchain.memory import ConversationBufferWindowMemory memory = ConversationBufferWindowMemory(k=1) memory.save_context({""input"": ""hi""}, {""output"": ""whats up""}) memory.save_context({""input"": ""not much you""}, {""output"": ""not much""}) memory.load_memory_variables({}) ``` ```text {\'history\': \'Human: not much you\\nAI: not much\'} ``` `ConversationSummaryMemory` is an extension of this theme. It creates a summary of the conversation over time. This memory is most useful for longer conversations where the full message history would consume many tokens. ```python from langchain.llms import OpenAI from langchain.memory import ConversationSummaryMemory llm = OpenAI(temperature=0) memory = ConversationSummaryMemory(llm=llm) memory.save_context({""input"": ""hi""}, {""output"": ""whats up""}) memory.save_context( {""input"": ""im working on better docs for chatbots""}, {""output"": ""oh, that sounds like a lot of work""}, ) memory.save_context( {""input"": ""yes, but it\'s worth the effort""}, {""output"": ""agreed, good docs are important!""}, ) ``` ```python memory.load_memory_variables({}) ``` ```text {\'history\': \'\\nThe human greets the AI, to which the AI responds. The human then mentions they are working on better docs for chatbots, to which the AI responds that it sounds like a lot of work. The human agrees that it is worth the effort, and the AI agrees that good docs are important.\'} ``` `ConversationSummaryBufferMemory` extends this a bit further: It uses token length rather than number of interactions to determine when to flush interactions. ```python from langchain.memory import ConversationSummaryBufferMemory memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=10) memory.save_context({""input"": ""hi""}, {""output"": ""whats up""}) memory.save_context({""input"": ""not much you""}, {""output"": ""not much""}) ``` ## Conversation We can unpack what goes under the hood with `ConversationChain`. We can specify our memory, `ConversationSummaryMemory` and we can specify the prompt. ```python from langchain.chains import LLMChain from langchain.prompts import ( ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate, ) # LLM llm = ChatOpenAI() # Prompt prompt = ChatPromptTemplate( messages=[ SystemMessagePromptTemplate.from_template( ""You are a nice chatbot having a conversation with a human."" ), # The `variable_name` here is what must align with memory MessagesPlaceholder(variable_name=""chat_history""), HumanMessagePromptTemplate.from_template(""{question}""), ] ) # Notice that we `return_messages=True` to fit into the MessagesPlaceholder # Notice that `""chat_history""` aligns with the MessagesPlaceholder name memory = ConversationBufferMemory(memory_key=""chat_history"", return_messages=True) conversation = LLMChain(llm=llm, prompt=prompt, verbose=True, memory=memory) # Notice that we just pass in the `question` variables - `chat_history` gets populated by memory conversation({""question"": ""hi""}) ``` ```text > Entering new LLMChain chain... Prompt after formatting: System: You are a nice chatbot having a conversation with a human. Human: hi > Finished chain. {\'question\': \'hi\', \'chat_history\': [HumanMessage(content=\'hi\', additional_kwargs={}, example=False), AIMessage(content=\'Hello! How can I assist you today?\', additional_kwargs={}, example=False)], \'text\': \'Hello! How can I assist you today?\'} ``` ```python conversation( {""question"": ""Translate this sentence from English to French: I love programming.""} ) ``` ```text > Entering new LLMChain chain... Prompt after formatting: System: You are a nice chatbot having a conversation with a human. Human: hi AI: Hello! How can I assist you today? Human: Translate this sentence from English to French: I love programming. > Finished chain. {\'question\': \'Translate this sentence from English to French: I love programming.\', \'chat_history\': [HumanMessage(content=\'hi\', additional_kwargs={}, example=False), AIMessage(content=\'Hello! How can I assist you today?\', additional_kwargs={}, example=False), HumanMessage(content=\'Translate this sentence from English to French: I love programming.\', additional_kwargs={}, example=False), AIMessage(content=\'Sure! The translation of ""I love programming"" from English to French is ""J\\\'adore programmer.""\', additional_kwargs={}, example=False)], \'text\': \'Sure! The translation of ""I love programming"" from English to French is ""J\\\'adore programmer.""\'} ``` ```python conversation({""question"": ""Now translate the sentence to German.""}) ``` ```text > Entering new LLMChain chain... Prompt after formatting: System: You are a nice chatbot having a conversation with a human. Human: hi AI: Hello! How can I assist you today? Human: Translate this sentence from English to French: I love programming. AI: Sure! The translation of ""I love programming"" from English to French is ""J\'adore programmer."" Human: Now translate the sentence to German. > Finished chain. {\'question\': \'Now translate the sentence to German.\', \'chat_history\': [HumanMessage(content=\'hi\', additional_kwargs={}, example=False), AIMessage(content=\'Hello! How can I assist you today?\', additional_kwargs={}, example=False), HumanMessage(content=\'Translate this sentence from English to French: I love programming.\', additional_kwargs={}, example=False), AIMessage(content=\'Sure! The translation of ""I love programming"" from English to French is ""J\\\'adore programmer.""\', additional_kwargs={}, example=False), HumanMessage(content=\'Now translate the sentence to German.\', additional_kwargs={}, example=False), AIMessage(content=\'Certainly! The translation of ""I love programming"" from English to German is ""Ich liebe das Programmieren.""\', additional_kwargs={}, example=False)], \'text\': \'Certainly! The translation of ""I love programming"" from English to German is ""Ich liebe das Programmieren.""\'} ``` We can see the chat history preserved in the prompt using the [LangSmith trace]( ![Image description](/assets/images/chat_use_case_2-a76871806149f125d08ff149363e0c2c.png) ## Chat Retrieval Now, suppose we want to [chat with documents]( or some other source of knowledge. This is popular use case, combining chat with [document retrieval](/docs/use_cases/question_answering). It allows us to chat with specific information that the model was not trained on. ```bash pip install tiktoken chromadb ``` Load a blog post. ```python from langchain.document_loaders import WebBaseLoader loader = WebBaseLoader("" data = loader.load() ``` Split and store this in a vector. ```python from langchain.text_splitter import RecursiveCharacterTextSplitter text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0) all_splits = text_splitter.split_documents(data) from langchain.embeddings import OpenAIEmbeddings from langchain.vectorstores import Chroma vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings()) ``` Create our memory, as before, but\'s let\'s use `ConversationSummaryMemory`. ```python memory = ConversationSummaryMemory( llm=llm, memory_key=""chat_history"", return_messages=True ) ``` ```python from langchain.chains import ConversationalRetrievalChain from langchain.chat_models import ChatOpenAI llm = ChatOpenAI() retriever = vectorstore.as_retriever() qa = ConversationalRetrievalChain.from_llm(llm, retriever=retriever, memory=memory) ``` ```python qa(""How do agents use Task decomposition?"") ``` ```text {\'question\': \'How do agents use Task decomposition?\', \'chat_history\': [SystemMessage(content=\'\', additional_kwargs={})], \'answer\': \'Agents can use task decomposition in several ways:\\n\\n1. Simple prompting: Agents can use Language Model based prompting to break down tasks into subgoals. For example, by providing prompts like ""Steps for XYZ"" or ""What are the subgoals for achieving XYZ?"", the agent can generate a sequence of smaller steps that lead to the completion of the overall task.\\n\\n2. Task-specific instructions: Agents can be given task-specific instructions to guide their planning process. For example, if the task is to write a novel, the agent can be instructed to ""Write a story outline."" This provides a high-level structure for the task and helps in breaking it down into smaller components.\\n\\n3. Human inputs: Agents can also take inputs from humans to decompose tasks. This can be done through direct communication or by leveraging human expertise. Humans can provide guidance and insights to help the agent break down complex tasks into manageable subgoals.\\n\\nOverall, task decomposition allows agents to break down large tasks into smaller, more manageable subgoals, enabling them to plan and execute complex tasks efficiently.\'} ``` ```python qa(""What are the various ways to implement memory to support it?"") ``` ```text {\'question\': \'What are the various ways to implement memory to support it?\', \'chat_history\': [SystemMessage(content=\'The human asks how agents use task decomposition. The AI explains that agents can use task decomposition in several ways, including simple prompting, task-specific instructions, and human inputs. Task decomposition allows agents to break down large tasks into smaller, more manageable subgoals, enabling them to plan and execute complex tasks efficiently.\', additional_kwargs={})], \'answer\': \'There are several ways to implement memory to support task decomposition:\\n\\n1. Long-Term Memory Management: This involves storing and organizing information in a long-term memory system. The agent can retrieve past experiences, knowledge, and learned strategies to guide the task decomposition process.\\n\\n2. Internet Access: The agent can use internet access to search for relevant information and gather resources to aid in task decomposition. This allows the agent to access a vast amount of information and utilize it in the decomposition process.\\n\\n3. GPT-3.5 Powered Agents: The agent can delegate simple tasks to GPT-3.5 powered agents. These agents can perform specific tasks or provide assistance in task decomposition, allowing the main agent to focus on higher-level planning and decision-making.\\n\\n4. File Output: The agent can store the results of task decomposition in files or documents. This allows for easy retrieval and reference during the execution of the task.\\n\\nThese memory resources help the agent in organizing and managing information, making informed decisions, and effectively decomposing complex tasks into smaller, manageable subgoals.\'} ``` Again, we can use the [LangSmith trace]( to explore the prompt structure. ### Going deeper - Agents, such as the [conversational retrieval agent](/docs/use_cases/question_answering/conversational_retrieval_agents), can be used for retrieval when necessary while also holding a conversation. - [Use case](#use-case) - [Overview](#overview) - [Quickstart](#quickstart) - [Memory](#memory) - [Conversation](#conversation) - [Chat Retrieval](#chat-retrieval)- [Going deeper](#going-deeper)', 'Rockset | Rockset [Rockset]( is a real-time analytics database service for serving low latency, high concurrency analytical queries at scale. It builds a Converged Index on structured and semi-structured data with an efficient store for vector embeddings. Its support for running SQL on schemaless data makes it a perfect choice for running vector search with metadata filters. This notebook goes over how to use [Rockset]( to store chat message history. ## Setting up ```bash pip install rockset ``` To begin, with get your API key from the [Rockset console]( Find your API region for the Rockset [API reference]( ## Example ```python from langchain.memory.chat_message_histories import RocksetChatMessageHistory from rockset import Regions, RocksetClient history = RocksetChatMessageHistory( session_id=""MySession"", client=RocksetClient( api_key=""YOUR API KEY"", host=Regions.usw2a1, # us-west-2 Oregon ), collection=""langchain_demo"", sync=True, ) history.add_user_message(""hi!"") history.add_ai_message(""whats up?"") print(history.messages) ``` The output should be something like: ```python [ HumanMessage(content=\'hi!\', additional_kwargs={\'id\': \'2e62f1c2-e9f7-465e-b551-49bae07fe9f0\'}, example=False), AIMessage(content=\'whats up?\', additional_kwargs={\'id\': \'b9be8eda-4c18-4cf8-81c3-e91e876927d0\'}, example=False) ] ``` - [Setting up](#setting-up) - [Example](#example)']","In LangChain, a `docstore` is used to store and manage documents, typically including metadata and content. It allows for the retrieval of entire documents based on their metadata. 

A `vector store`, on the other hand, is used to store vectors (embeddings) of documents, enabling similarity searches and other vector-based retrieval methods. It focuses on the vector representations rather than the full document content.

In summary:
- `docstore`: Manages full documents and their metadata.
- `vector store`: Stores embeddings for similarity search and vector-based retrieval.","Vector stores implement the VectorStore interface, which exposes a way to search using similarity search (based on embedding or vector similarity). Doc stores implement the DocStore interface, which allows a search() method to return documents, but this needn't use vector similarity.",0.8333333332916666,1.0,1.0,0.039421654936713776,0.2222222222222222
77,"Will this work?

```
from langchain.chat_models import ChatOpenAI

llm = ChatOpenAI(model=""claude-2"")
llm.predict(""Hi"")
```","['Log10 | Log10 This page covers how to use the [Log10]( within LangChain. ## What is Log10? Log10 is an [open-source]( proxiless LLM data management and application development platform that lets you log, debug and tag your Langchain calls. ## Quick start 1. Create your free account at [log10.io]( 2. Add your `LOG10_TOKEN` and `LOG10_ORG_ID` from the Settings and Organization tabs respectively as environment variables. 3. Also add `LOG10_URL= and your usual LLM API key: for e.g. `OPENAI_API_KEY` or `ANTHROPIC_API_KEY` to your environment ## How to enable Log10 data management for Langchain Integration with log10 is a simple one-line `log10_callback` integration as shown below: ```python from langchain.chat_models import ChatOpenAI from langchain.schema import HumanMessage from log10.langchain import Log10Callback from log10.llm import Log10Config log10_callback = Log10Callback(log10_config=Log10Config()) messages = [ HumanMessage(content=""You are a ping pong machine""), HumanMessage(content=""Ping?""), ] llm = ChatOpenAI(model_name=""gpt-3.5-turbo"", callbacks=[log10_callback]) ``` [Log10 + Langchain + Logs docs]( [More details + screenshots]( including instructions for self-hosting logs ## How to use tags with Log10 ```python from langchain.llms import OpenAI from langchain.chat_models import ChatAnthropic from langchain.chat_models import ChatOpenAI from langchain.schema import HumanMessage from log10.langchain import Log10Callback from log10.llm import Log10Config log10_callback = Log10Callback(log10_config=Log10Config()) messages = [ HumanMessage(content=""You are a ping pong machine""), HumanMessage(content=""Ping?""), ] llm = ChatOpenAI(model_name=""gpt-3.5-turbo"", callbacks=[log10_callback], temperature=0.5, tags=[""test""]) completion = llm.predict_messages(messages, tags=[""foobar""]) print(completion) llm = ChatAnthropic(model=""claude-2"", callbacks=[log10_callback], temperature=0.7, tags=[""baz""]) llm.predict_messages(messages) print(completion) llm = OpenAI(model_name=""text-davinci-003"", callbacks=[log10_callback], temperature=0.5) completion = llm.predict(""You are a ping pong machine.\\nPing?\\n"") print(completion) ``` You can also intermix direct OpenAI calls and Langchain LLM calls: ```python import os from log10.load import log10, log10_session import openai from langchain.llms import OpenAI log10(openai) with log10_session(tags=[""foo"", ""bar""]): # Log a direct OpenAI call response = openai.Completion.create( model=""text-ada-001"", prompt=""Where is the Eiffel Tower?"", temperature=0, max_tokens=1024, top_p=1, frequency_penalty=0, presence_penalty=0, ) print(response) # Log a call via Langchain llm = OpenAI(model_name=""text-ada-001"", temperature=0.5) response = llm.predict(""You are a ping pong machine.\\nPing?\\n"") print(response) ``` ## How to debug Langchain calls [Example of debugging]( [More Langchain examples]( - [What is Log10?](#what-is-log10) - [Quick start](#quick-start) - [How to enable Log10 data management for Langchain](#how-to-enable-log10-data-management-for-langchain) - [How to use tags with Log10](#how-to-use-tags-with-log10) - [How to debug Langchain calls](#how-to-debug-langchain-calls)', 'Memory in LLMChain | Memory in LLMChain This notebook goes over how to use the Memory class with an `LLMChain`. We will add the [ConversationBufferMemory]( class, although this can be any memory class. ```python from langchain.chains import LLMChain from langchain.llms import OpenAI from langchain.memory import ConversationBufferMemory from langchain.prompts import PromptTemplate ``` The most important step is setting up the prompt correctly. In the below prompt, we have two input keys: one for the actual input, another for the input from the Memory class. Importantly, we make sure the keys in the `PromptTemplate` and the `ConversationBufferMemory` match up (`chat_history`). ```python template = """"""You are a chatbot having a conversation with a human. {chat_history} Human: {human_input} Chatbot:"""""" prompt = PromptTemplate( input_variables=[""chat_history"", ""human_input""], template=template ) memory = ConversationBufferMemory(memory_key=""chat_history"") ``` ```python llm = OpenAI() llm_chain = LLMChain( llm=llm, prompt=prompt, verbose=True, memory=memory, ) ``` ```python llm_chain.predict(human_input=""Hi there my friend"") ``` ```text > Entering new LLMChain chain... Prompt after formatting: You are a chatbot having a conversation with a human. Human: Hi there my friend Chatbot: > Finished chain. \' Hi there! How can I help you today?\' ``` ```python llm_chain.predict(human_input=""Not too bad - how are you?"") ``` ```text > Entering new LLMChain chain... Prompt after formatting: You are a chatbot having a conversation with a human. Human: Hi there my friend AI: Hi there! How can I help you today? Human: Not too bad - how are you? Chatbot: > Finished chain. "" I\'m doing great, thanks for asking! How are you doing?"" ``` ## Adding Memory to a chat model-based LLMChain The above works for completion-style `LLM`s, but if you are using a chat model, you will likely get better performance using structured chat messages. Below is an example. ```python from langchain.chat_models import ChatOpenAI from langchain.prompts import ( ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder, ) from langchain.schema import SystemMessage ``` We will use the [ChatPromptTemplate]( class to set up the chat prompt. The [from_messages]( method creates a `ChatPromptTemplate` from a list of messages (e.g., `SystemMessage`, `HumanMessage`, `AIMessage`, `ChatMessage`, etc.) or message templates, such as the [MessagesPlaceholder]( below. The configuration below makes it so the memory will be injected to the middle of the chat prompt, in the `chat_history` key, and the user\'s inputs will be added in a human/user message to the end of the chat prompt. ```python prompt = ChatPromptTemplate.from_messages( [ SystemMessage( content=""You are a chatbot having a conversation with a human."" ), # The persistent system prompt MessagesPlaceholder( variable_name=""chat_history"" ), # Where the memory will be stored. HumanMessagePromptTemplate.from_template( ""{human_input}"" ), # Where the human input will injected ] ) memory = ConversationBufferMemory(memory_key=""chat_history"", return_messages=True) ``` ```python llm = ChatOpenAI() chat_llm_chain = LLMChain( llm=llm, prompt=prompt, verbose=True, memory=memory, ) ``` ```python chat_llm_chain.predict(human_input=""Hi there my friend"") ``` ```text > Entering new LLMChain chain... Prompt after formatting: System: You are a chatbot having a conversation with a human. Human: Hi there my friend > Finished chain. \'Hello! How can I assist you today, my friend?\' ``` ```python chat_llm_chain.predict(human_input=""Not too bad - how are you?"") ``` ```text > Entering new LLMChain chain... Prompt after formatting: System: You are a chatbot having a conversation with a human. Human: Hi there my friend AI: Hello! How can I assist you today, my friend? Human: Not too bad - how are you? > Finished chain. ""I\'m an AI chatbot, so I don\'t have feelings, but I\'m here to help and chat with you! Is there something specific you would like to talk about or any questions I can assist you with?"" ``` - [Adding Memory to a chat model-based LLMChain](#adding-memory-to-a-chat-model-based-llmchain)', 'RePhraseQuery | RePhraseQuery `RePhraseQuery` is a simple retriever that applies an LLM between the user input and the query passed by the retriever. It can be used to pre-process the user input in any way. ## Example ### Setting up Create a vector store. ```python import logging from langchain.chat_models import ChatOpenAI from langchain.document_loaders import WebBaseLoader from langchain.embeddings import OpenAIEmbeddings from langchain.retrievers import RePhraseQueryRetriever from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain.vectorstores import Chroma ``` ```python logging.basicConfig() logging.getLogger(""langchain.retrievers.re_phraser"").setLevel(logging.INFO) loader = WebBaseLoader("" data = loader.load() text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0) all_splits = text_splitter.split_documents(data) vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings()) ``` ### Using the default prompt The default prompt used in the `from_llm` classmethod: ```text DEFAULT_TEMPLATE = """"""You are an assistant tasked with taking a natural language \\ query from a user and converting it into a query for a vectorstore. \\ In this process, you strip out information that is not relevant for \\ the retrieval task. Here is the user query: {question}"""""" ``` ```python llm = ChatOpenAI(temperature=0) retriever_from_llm = RePhraseQueryRetriever.from_llm( retriever=vectorstore.as_retriever(), llm=llm ) ``` ```python docs = retriever_from_llm.get_relevant_documents( ""Hi I\'m Lance. What are the approaches to Task Decomposition?"" ) ``` ```text INFO:langchain.retrievers.re_phraser:Re-phrased question: The user query can be converted into a query for a vectorstore as follows: ""approaches to Task Decomposition"" ``` ```python docs = retriever_from_llm.get_relevant_documents( ""I live in San Francisco. What are the Types of Memory?"" ) ``` ```text INFO:langchain.retrievers.re_phraser:Re-phrased question: Query for vectorstore: ""Types of Memory"" ``` ### Custom prompt ```python from langchain.chains import LLMChain from langchain.prompts import PromptTemplate QUERY_PROMPT = PromptTemplate( input_variables=[""question""], template=""""""You are an assistant tasked with taking a natural languge query from a user and converting it into a query for a vectorstore. In the process, strip out all information that is not relevant for the retrieval task and return a new, simplified question for vectorstore retrieval. The new user query should be in pirate speech. Here is the user query: {question} """""", ) llm = ChatOpenAI(temperature=0) llm_chain = LLMChain(llm=llm, prompt=QUERY_PROMPT) ``` ```python retriever_from_llm_chain = RePhraseQueryRetriever( retriever=vectorstore.as_retriever(), llm_chain=llm_chain ) ``` ```python docs = retriever_from_llm_chain.get_relevant_documents( ""Hi I\'m Lance. What is Maximum Inner Product Search?"" ) ``` ```text INFO:langchain.retrievers.re_phraser:Re-phrased question: Ahoy matey! What be Maximum Inner Product Search, ye scurvy dog? ``` - [Example](#example)- [Setting up](#setting-up) - [Using the default prompt](#using-the-default-prompt) - [Custom prompt](#custom-prompt)']","Yes, this will work. The code initializes a `ChatOpenAI` model with the specified ""claude-2"" model and then calls the `predict` method with the input ""Hi"". The model will generate a response based on the input.",No. Claude-2 is a closed-source model developed by Anthropic. The ChatOpenAI class interfaces with OpenAI's chat models.,0.0,0.0,0.5,0.0,0.21428571428571427
78,hw do i create a prompt template for my chat bot,"['Conversational | Conversational This walkthrough demonstrates how to use an agent optimized for conversation. Other agents are often optimized for using tools to figure out the best response, which is not ideal in a conversational setting where you may want the agent to be able to chat with the user as well. If we compare it to the standard ReAct agent, the main difference is the prompt. We want it to be much more conversational. ```python from langchain.agents import AgentType, Tool, initialize_agent from langchain.llms import OpenAI from langchain.memory import ConversationBufferMemory from langchain.utilities import SerpAPIWrapper ``` ```python search = SerpAPIWrapper() tools = [ Tool( name=""Current Search"", func=search.run, description=""useful for when you need to answer questions about current events or the current state of the world"", ), ] ``` ```python llm = OpenAI(temperature=0) ``` ## Using LCEL We will first show how to create this agent using LCEL ```python from langchain import hub from langchain.agents.format_scratchpad import format_log_to_str from langchain.agents.output_parsers import ReActSingleInputOutputParser from langchain.tools.render import render_text_description ``` ```python prompt = hub.pull(""hwchase17/react-chat"") ``` ```python prompt = prompt.partial( tools=render_text_description(tools), tool_names="", "".join([t.name for t in tools]), ) ``` ```python llm_with_stop = llm.bind(stop=[""\\nObservation""]) ``` ```python agent = ( { ""input"": lambda x: x[""input""], ""agent_scratchpad"": lambda x: format_log_to_str(x[""intermediate_steps""]), ""chat_history"": lambda x: x[""chat_history""], } | prompt | llm_with_stop | ReActSingleInputOutputParser() ) ``` ```python from langchain.agents import AgentExecutor ``` ```python memory = ConversationBufferMemory(memory_key=""chat_history"") agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, memory=memory) ``` ```python agent_executor.invoke({""input"": ""hi, i am bob""})[""output""] ``` ```text > Entering new AgentExecutor chain... Thought: Do I need to use a tool? No Final Answer: Hi Bob, nice to meet you! How can I help you today? > Finished chain. \'Hi Bob, nice to meet you! How can I help you today?\' ``` ```python agent_executor.invoke({""input"": ""whats my name?""})[""output""] ``` ```text > Entering new AgentExecutor chain... Thought: Do I need to use a tool? No Final Answer: Your name is Bob. > Finished chain. \'Your name is Bob.\' ``` ```python agent_executor.invoke({""input"": ""what are some movies showing 9/21/2023?""})[""output""] ``` ```text > Entering new AgentExecutor chain... Thought: Do I need to use a tool? Yes Action: Current Search Action Input: Movies showing 9/21/2023[\'September 2023 Movies: The Creator Dumb Money Expend4bles The Kill Room The Inventor The Equalizer 3 PAW Patrol: The Mighty Movie, ...\'] Do I need to use a tool? No Final Answer: According to current search, some movies showing on 9/21/2023 are The Creator, Dumb Money, Expend4bles, The Kill Room, The Inventor, The Equalizer 3, and PAW Patrol: The Mighty Movie. > Finished chain. \'According to current search, some movies showing on 9/21/2023 are The Creator, Dumb Money, Expend4bles, The Kill Room, The Inventor, The Equalizer 3, and PAW Patrol: The Mighty Movie.\' ``` ## Use the off-the-shelf agent We can also create this agent using the off-the-shelf agent class ```python agent_executor = initialize_agent( tools, llm, agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION, verbose=True, memory=memory, ) ``` ## Use a chat model We can also use a chat model here. The main difference here is in the prompts used. ```python from langchain import hub from langchain.chat_models import ChatOpenAI ``` ```python prompt = hub.pull(""hwchase17/react-chat-json"") chat_model = ChatOpenAI(temperature=0, model=""gpt-4"") ``` ```python prompt = prompt.partial( tools=render_text_description(tools), tool_names="", "".join([t.name for t in tools]), ) ``` ```python chat_model_with_stop = chat_model.bind(stop=[""\\nObservation""]) ``` ```python from langchain.agents.format_scratchpad import format_log_to_messages from langchain.agents.output_parsers import JSONAgentOutputParser ``` ```python # We need some extra steering, or the chat model forgets how to respond sometimes TEMPLATE_TOOL_RESPONSE = """"""TOOL RESPONSE: --------------------- {observation} USER\'S INPUT -------------------- Okay, so what is the response to my last comment? If using information obtained from the tools you must mention it explicitly without mentioning the tool names - I have forgotten all TOOL RESPONSES! Remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else - even if you just want to respond to the user. Do NOT respond with anything except a JSON snippet no matter what!"""""" agent = ( { ""input"": lambda x: x[""input""], ""agent_scratchpad"": lambda x: format_log_to_messages( x[""intermediate_steps""], template_tool_response=TEMPLATE_TOOL_RESPONSE ), ""chat_history"": lambda x: x[""chat_history""], } | prompt | chat_model_with_stop | JSONAgentOutputParser() ) ``` ```python from langchain.agents import AgentExecutor ``` ```python memory = ConversationBufferMemory(memory_key=""chat_history"", return_messages=True) agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, memory=memory) ``` ```python agent_executor.invoke({""input"": ""hi, i am bob""})[""output""] ``` ```text > Entering new AgentExecutor chain... ```json { ""action"": ""Final Answer"", ""action_input"": ""Hello Bob, how can I assist you today?"" } ``` > Finished chain. \'Hello Bob, how can I assist you today?\' ``` ```python agent_executor.invoke({""input"": ""whats my name?""})[""output""] ``` ```text > Entering new AgentExecutor chain... ```json { ""action"": ""Final Answer"", ""action_input"": ""Your name is Bob."" } ``` > Finished chain. \'Your name is Bob.\' ``` ```python agent_executor.invoke({""input"": ""what are some movies showing 9/21/2023?""})[""output""] ``` ```text > Entering new AgentExecutor chain... ```json { ""action"": ""Current Search"", ""action_input"": ""movies showing on 9/21/2023"" } ```[\'September 2023 Movies: The Creator Dumb Money Expend4bles The Kill Room The Inventor The Equalizer 3 PAW Patrol: The Mighty Movie, ...\']```json { ""action"": ""Final Answer"", ""action_input"": ""Some movies that are showing on 9/21/2023 include \'The Creator\', \'Dumb Money\', \'Expend4bles\', \'The Kill Room\', \'The Inventor\', \'The Equalizer 3\', and \'PAW Patrol: The Mighty Movie\'."" } ``` > Finished chain. ""Some movies that are showing on 9/21/2023 include \'The Creator\', \'Dumb Money\', \'Expend4bles\', \'The Kill Room\', \'The Inventor\', \'The Equalizer 3\', and \'PAW Patrol: The Mighty Movie\'."" ``` We can also initialize the agent executor with a predefined agent type ```python from langchain.chat_models import ChatOpenAI from langchain.memory import ConversationBufferMemory ``` ```python memory = ConversationBufferMemory(memory_key=""chat_history"", return_messages=True) llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, temperature=0) agent_chain = initialize_agent( tools, llm, agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION, verbose=True, memory=memory, ) ``` - [Using LCEL](#using-lcel) - [Use the off-the-shelf agent](#use-the-off-the-shelf-agent) - [Use a chat model](#use-a-chat-model)', 'Chat Bot Feedback Template | Chat Bot Feedback Template This template shows how to evaluate your chat bot without explicit user feedback. It defines a simple chat bot in [chain.py](/docs/templates/chat_bot_feedback/chain.py) and custom evaluator that scores bot response effectiveness based on the subsequent user response. You can apply this run evaluator to your own chat bot by calling `with_config` on the chat bot before serving. You can also directly deploy your chat app using this template. [Chat bots]( are one of the most common interfaces for deploying LLMs. The quality of chat bots varies, making continuous development important. But users are wont to leave explicit feedback through mechanisms like thumbs-up or thumbs-down buttons. Furthermore, traditional analytics such as ""session length"" or ""conversation length"" often lack clarity. However, multi-turn conversations with a chat bot can provide a wealth of information, which we can transform into metrics for fine-tuning, evaluation, and product analytics. Taking [Chat Langchain]( as a case study, only about 0.04% of all queries receive explicit feedback. Yet, approximately 70% of the queries are follow-ups to previous questions. A significant portion of these follow-up queries continue useful information we can use to infer the quality of the previous AI response. This template helps solve this ""feedback scarcity"" problem. Below is an example invocation of this chat bot: []( When the user responds to this ([link]( the response evaluator is invoked, resulting in the following evaluationrun: []( As shown, the evaluator sees that the user is increasingly frustrated, indicating that the prior response was not effective ## LangSmith Feedback [LangSmith]( is a platform for building production-grade LLM applications. Beyond its debugging and offline evaluation features, LangSmith helps you capture both user and model-assisted feedback to refine your LLM application. This template uses an LLM to generate feedback for your application, which you can use to continuously improve your service. For more examples on collecting feedback using LangSmith, consult the [documentation]( ## Evaluator Implementation The user feedback is inferred by custom `RunEvaluator`. This evaluator is called using the `EvaluatorCallbackHandler`, which run it in a separate thread to avoid interfering with the chat bot\'s runtime. You can use this custom evaluator on any compatible chat bot by calling the following function on your LangChain object: ```python my_chain .with_config( callbacks=[ EvaluatorCallbackHandler( evaluators=[ ResponseEffectivenessEvaluator(evaluate_response_effectiveness) ] ) ], ) ``` The evaluator instructs an LLM, specifically `gpt-3.5-turbo`, to evaluate the AI\'s most recent chat message based on the user\'s followup response. It generates a score and accompanying reasoning that is converted to feedback in LangSmith, applied to the value provided as the `last_run_id`. The prompt used within the LLM [is available on the hub]( Feel free to customize it with things like additional app context (such as the goal of the app or the types of questions it should respond to) or ""symptoms"" you\'d like the LLM to focus on. This evaluator also utilizes OpenAI\'s function-calling API to ensure a more consistent, structured output for the grade. ## Environment Variables Ensure that `OPENAI_API_KEY` is set to use OpenAI models. Also, configure LangSmith by setting your `LANGSMITH_API_KEY`. ```bash export OPENAI_API_KEY=sk-... export LANGSMITH_API_KEY=... export LANGCHAIN_TRACING_V2=true export LANGCHAIN_PROJECT=my-project # Set to the project you want to save to ``` ## Usage If deploying via `LangServe`, we recommend configuring the server to return callback events as well. This will ensure the backend traces are included in whatever traces you generate using the `RemoteRunnable`. ```python from chat_bot_feedback.chain import chain add_routes(app, chain, path=""/chat-bot-feedback"", include_callback_events=True) ``` With the server running, you can use the following code snippet to stream the chat bot responses for a 2 turn conversation. ```python from functools import partial from typing import Dict, Optional, Callable, List from langserve import RemoteRunnable from langchain.callbacks.manager import tracing_v2_enabled from langchain.schema import BaseMessage, AIMessage, HumanMessage # Update with the URL provided by your LangServe server chain = RemoteRunnable("" def stream_content( text: str, chat_history: Optional[List[BaseMessage]] = None, last_run_id: Optional[str] = None, on_chunk: Callable = None, ): results = [] with tracing_v2_enabled() as cb: for chunk in chain.stream( {""text"": text, ""chat_history"": chat_history, ""last_run_id"": last_run_id}, ): on_chunk(chunk) results.append(chunk) last_run_id = cb.latest_run.id if cb.latest_run else None return last_run_id, """".join(results) chat_history = [] text = ""Where are my keys?"" last_run_id, response_message = stream_content(text, on_chunk=partial(print, end="""")) print() chat_history.extend([HumanMessage(content=text), AIMessage(content=response_message)]) text = ""I CAN\'T FIND THEM ANYWHERE"" # The previous response will likely receive a low score, # as the user\'s frustration appears to be escalating. last_run_id, response_message = stream_content( text, chat_history=chat_history, last_run_id=str(last_run_id), on_chunk=partial(print, end=""""), ) print() chat_history.extend([HumanMessage(content=text), AIMessage(content=response_message)]) ``` This uses the `tracing_v2_enabled` callback manager to get the run ID of the call, which we provide in subsequent calls in the same chat thread, so the evaluator can assign feedback to the appropriate trace. ## Conclusion This template provides a simple chat bot definition you can directly deploy using LangServe. It defines a custom evaluator to log evaluation feedback for the bot without any explicit user ratings. This is an effective way to augment your analytics and to better select data points for fine-tuning and evaluation. - [LangSmith Feedback](#langsmith-feedback) - [Evaluator Implementation](#evaluator-implementation) - [Environment Variables](#environment-variables) - [Usage](#usage) - [Conclusion](#conclusion)', 'Prompt templates | Prompt templates Prompt templates are pre-defined recipes for generating prompts for language models. A template may include instructions, few-shot examples, and specific context and questions appropriate for a given task. LangChain provides tooling to create and work with prompt templates. LangChain strives to create model agnostic templates to make it easy to reuse existing templates across different language models. Typically, language models expect the prompt to either be a string or else a list of chat messages. ## PromptTemplate Use `PromptTemplate` to create a template for a string prompt. By default, `PromptTemplate` uses [Python\'s str.format]( syntax for templating. ```python from langchain.prompts import PromptTemplate prompt_template = PromptTemplate.from_template( ""Tell me a {adjective} joke about {content}."" ) prompt_template.format(adjective=""funny"", content=""chickens"") ``` ```text \'Tell me a funny joke about chickens.\' ``` The template supports any number of variables, including no variables: ```python from langchain.prompts import PromptTemplate prompt_template = PromptTemplate.from_template(""Tell me a joke"") prompt_template.format() ``` ```text \'Tell me a joke\' ``` For additional validation, specify `input_variables` explicitly. These variables will be compared against the variables present in the template string during instantiation, **raising an exception if there is a mismatch**. For example: ```python from langchain.prompts import PromptTemplate invalid_prompt = PromptTemplate( input_variables=[""adjective""], template=""Tell me a {adjective} joke about {content}."", ) ``` ```text --------------------------------------------------------------------------- ValidationError Traceback (most recent call last) Cell In[19], line 3 1 from langchain.prompts import PromptTemplate ----> 3 invalid_prompt = PromptTemplate( 4 input_variables=[""adjective""], 5 template=""Tell me a {adjective} joke about {content}."" 6 ) File ~/langchain/libs/langchain/langchain/load/serializable.py:97, in Serializable.__init__(self, **kwargs) 96 def __init__(self, **kwargs: Any) -> None: ---> 97 super().__init__(**kwargs) 98 self._lc_kwargs = kwargs File ~/langchain/.venv/lib/python3.9/site-packages/pydantic/main.py:341, in pydantic.main.BaseModel.__init__() ValidationError: 1 validation error for PromptTemplate __root__ Invalid prompt schema; check for mismatched or missing input parameters. \'content\' (type=value_error) ``` You can create custom prompt templates that format the prompt in any way you want. For more information, see [Custom Prompt Templates](/docs/modules/model_io/prompts/prompt_templates/custom_prompt_template.html). ## ChatPromptTemplate The prompt to [chat models](/docs/modules/model_io/prompts/models/chat) is a list of chat messages. Each chat message is associated with content, and an additional parameter called `role`. For example, in the OpenAI [Chat Completions API]( a chat message can be associated with an AI assistant, a human or a system role. Create a chat prompt template like this: ```python from langchain.prompts import ChatPromptTemplate chat_template = ChatPromptTemplate.from_messages( [ (""system"", ""You are a helpful AI bot. Your name is {name}.""), (""human"", ""Hello, how are you doing?""), (""ai"", ""I\'m doing well, thanks!""), (""human"", ""{user_input}""), ] ) messages = chat_template.format_messages(name=""Bob"", user_input=""What is your name?"") ``` `ChatPromptTemplate.from_messages` accepts a variety of message representations. For example, in addition to using the 2-tuple representation of (type, content) used above, you could pass in an instance of `MessagePromptTemplate` or `BaseMessage`. ```python from langchain.chat_models import ChatOpenAI from langchain.prompts import HumanMessagePromptTemplate from langchain.schema.messages import SystemMessage chat_template = ChatPromptTemplate.from_messages( [ SystemMessage( content=( ""You are a helpful assistant that re-writes the user\'s text to "" ""sound more upbeat."" ) ), HumanMessagePromptTemplate.from_template(""{text}""), ] ) llm = ChatOpenAI() llm(chat_template.format_messages(text=""i dont like eating tasty things."")) ``` ```text AIMessage(content=\'I absolutely love indulging in delicious treats!\') ``` This provides you with a lot of flexibility in how you construct your chat prompts. ## LCEL `PromptTemplate` and `ChatPromptTemplate` implement the [Runnable interface](/docs/expression_language/interface), the basic building block of the [LangChain Expression Language (LCEL)](/docs/expression_language/). This means they support `invoke`, `ainvoke`, `stream`, `astream`, `batch`, `abatch`, `astream_log` calls. `PromptTemplate` accepts a dictionary (of the prompt variables) and returns a `StringPromptValue`. A `ChatPromptTemplate` accepts a dictionary and returns a `ChatPromptValue`. ```python prompt_val = prompt_template.invoke({""adjective"": ""funny"", ""content"": ""chickens""}) prompt_val ``` ```text StringPromptValue(text=\'Tell me a joke\') ``` ```python prompt_val.to_string() ``` ```text \'Tell me a joke\' ``` ```python prompt_val.to_messages() ``` ```text [HumanMessage(content=\'Tell me a joke\')] ``` ```python chat_val = chat_template.invoke({""text"": ""i dont like eating tasty things.""}) ``` ```python chat_val.to_messages() ``` ```text [SystemMessage(content=""You are a helpful assistant that re-writes the user\'s text to sound more upbeat.""), HumanMessage(content=\'i dont like eating tasty things.\')] ``` ```python chat_val.to_string() ``` ```text ""System: You are a helpful assistant that re-writes the user\'s text to sound more upbeat.\\nHuman: i dont like eating tasty things."" ``` - [PromptTemplate](#prompttemplate) - [ChatPromptTemplate](#chatprompttemplate) - [LCEL](#lcel)']","To create a prompt template for your chat bot, use `ChatPromptTemplate` from LangChain. Here's a simple example:

```python
from langchain.prompts import ChatPromptTemplate

# Define your chat template
chat_template = ChatPromptTemplate.from_messages([
    (""system"", ""You are a helpful AI bot.""),
    (""human"", ""Hello, how are you?"")
])

# Format the template with variables
messages = chat_template.format_messages()

# Use the messages in your chat bot
```

This sets up a basic chat template with a system message and a human input. Adjust the messages as needed for your bot's functionality.","To create a prompt template for a chat bot, you can use the `ChatPromptTemplate` class. You can initialize it with a list of chat messages, where each message has a role and content. The role can be 'system', 'human', or 'ai'. For example:

```python
from langchain.prompts import ChatPromptTemplate

chat_template = ChatPromptTemplate.from_messages(
    [
        ('system', 'You are a helpful AI bot. Your name is {name}.'),
        ('human', 'Hello, how are you doing?'),
        ('ai', 'I'm doing well, thanks!'),
        ('human', '{user_input}'),
    ]
)

messages = chat_template.format_messages(name='Bob', user_input='What is your name?')
```

You can also use different subclasses of the `ChatMessagePromptTemplate` class, such as `SystemMessagePromptTemplate` and `HumanMessagePromptTemplate`, to customize the role or content of specific messages in the template. Finally, you can use the ""MessagesPlaceholder"" class to format chat history as a sequence of messages.",0.9999999999666667,,1.0,0.4285920398668233,0.42152466367713004
79,what method should subclasses override if they can start producing output while input is still being generated,"[""langchain.schema.runnable.base.RunnableGenerator LangChain 0.0.337 API Experimental Python Docs Toggle Menu PrevUp Next LangChain 0.0.337 langchain.schema.runnable.base.RunnableGenerator langchain.schema.runnable.base.RunnableGenerator class langchain.schema.runnable.base.RunnableGenerator(transform: Union[Callable[[Iterator[Input]], Iterator[Output]], Callable[[AsyncIterator[Input]], AsyncIterator[Output]]], atransform: Optional[Callable[[AsyncIterator[Input]], AsyncIterator[Output]]] = None)[source] A runnable that runs a generator function. Attributes InputType The type of input this runnable accepts specified as a type annotation. OutputType The type of output this runnable produces specified as a type annotation. config_specs List configurable fields for this runnable. input_schema The type of input this runnable accepts specified as a pydantic model. output_schema The type of output this runnable produces specified as a pydantic model. Methods __init__(transform[,atransform]) abatch(inputs[,config,return_exceptions]) Default implementation runs ainvoke in parallel using asyncio.gather. ainvoke(input[,config]) Default implementation of ainvoke, calls invoke from a thread. astream(input[,config]) Default implementation of astream, which calls ainvoke. astream_log(input[,config,diff,...]) Stream all output from a runnable, as reported to the callback system. atransform(input[,config]) Default implementation of atransform, which buffers input and calls astream. batch(inputs[,config,return_exceptions]) Default implementation runs invoke in parallel using a thread pool executor. bind(**kwargs) Bind arguments to a Runnable, returning a new Runnable. config_schema(*[,include]) The type of config this runnable accepts specified as a pydantic model. get_input_schema([config]) Get a pydantic model that can be used to validate input to the runnable. get_output_schema([config]) Get a pydantic model that can be used to validate output to the runnable. invoke(input[,config]) Transform a single input into an output. map() Return a new Runnable that maps a list of inputs to a list of outputs, by calling invoke() with each input. stream(input[,config]) Default implementation of stream, which calls invoke. transform(input[,config]) Default implementation of transform, which buffers input and then calls stream. with_config([config]) Bind config to a Runnable, returning a new Runnable. with_fallbacks(fallbacks,*[,...]) Add fallbacks to a runnable, returning a new Runnable. with_listeners(*[,on_start,on_end,on_error]) Bind lifecycle listeners to a Runnable, returning a new Runnable. with_retry(*[,retry_if_exception_type,...]) Create a new Runnable that retries the original runnable on exceptions. with_types(*[,input_type,output_type]) Bind input and output types to a Runnable, returning a new Runnable. __init__(transform: Union[Callable[[Iterator[Input]], Iterator[Output]], Callable[[AsyncIterator[Input]], AsyncIterator[Output]]], atransform: Optional[Callable[[AsyncIterator[Input]], AsyncIterator[Output]]] = None) None[source] async abatch(inputs: List[Input], config: Optional[Union[RunnableConfig, List[RunnableConfig]]] = None, *, return_exceptions: bool = False, **kwargs: Optional[Any]) List[Output] Default implementation runs ainvoke in parallel using asyncio.gather. The default implementation of batch works well for IO bound runnables. Subclasses should override this method if they can batch more efficiently; e.g., if the underlying runnable uses an API which supports a batch mode. async ainvoke(input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any) Output[source] Default implementation of ainvoke, calls invoke from a thread. The default implementation allows usage of async code even if the runnable did not implement a native async version of invoke. Subclasses should override this method if they can run asynchronously. astream(input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any)  AsyncIterator[Output][source] Default implementation of astream, which calls ainvoke. Subclasses should override this method if they support streaming output. async astream_log(input: Any, config: Optional[RunnableConfig] = None, *, diff: bool = True, include_names: Optional[Sequence[str]] = None, include_types: Optional[Sequence[str]] = None, include_tags: Optional[Sequence[str]] = None, exclude_names: Optional[Sequence[str]] = None, exclude_types: Optional[Sequence[str]] = None, exclude_tags: Optional[Sequence[str]] = None, **kwargs: Optional[Any])  Union[AsyncIterator[RunLogPatch], AsyncIterator[RunLog]] Stream all output from a runnable, as reported to the callback system. This includes all inner runs of LLMs, Retrievers, Tools, etc. Output is streamed as Log objects, which include a list of jsonpatch ops that describe how the state of the run has changed in each step, and the final state of the run. The jsonpatch ops can be applied in order to construct state. atransform(input: AsyncIterator[Input], config: Optional[RunnableConfig] = None, **kwargs: Any)  AsyncIterator[Output][source] Default implementation of atransform, which buffers input and calls astream. Subclasses should override this method if they can start producing output while input is still being generated. batch(inputs: List[Input], config: Optional[Union[RunnableConfig, List[RunnableConfig]]] = None, *, return_exceptions: bool = False, **kwargs: Optional[Any])  List[Output] Default implementation runs invoke in parallel using a thread pool executor. The default implementation of batch works well for IO bound runnables. Subclasses should override this method if they can batch more efficiently; e.g., if the underlying runnable uses an API which supports a batch mode. bind(**kwargs: Any)  Runnable[Input, Output] Bind arguments to a Runnable, returning a new Runnable. config_schema(*, include: Optional[Sequence[str]] = None)  Type[BaseModel] The type of config this runnable accepts specified as a pydantic model. To mark a field as configurable, see the configurable_fields and configurable_alternatives methods. Parameters include  A list of fields to include in the config schema. Returns A pydantic model that can be used to validate config. get_input_schema(config: Optional[RunnableConfig] = None)  Type[BaseModel] Get a pydantic model that can be used to validate input to the runnable. Runnables that leverage the configurable_fields and configurable_alternatives methods will have a dynamic input schema that depends on which configuration the runnable is invoked with. This method allows to get an input schema for a specific configuration. Parameters config  A config to use when generating the schema. Returns A pydantic model that can be used to validate input. get_output_schema(config: Optional[RunnableConfig] = None)  Type[BaseModel] Get a pydantic model that can be used to validate output to the runnable. Runnables that leverage the configurable_fields and configurable_alternatives methods will have a dynamic output schema that depends on which configuration the runnable is invoked with. This method allows to get an output schema for a specific configuration. Parameters config  A config to use when generating the schema. Returns A pydantic model that can be used to validate output. invoke(input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any)  Output[source] Transform a single input into an output. Override to implement. Parameters input  The input to the runnable. config  A config to use when invoking the runnable. The config supports standard keys like 'tags', 'metadata' for tracing purposes, 'max_concurrency' for controlling how much work to do in parallel, and other keys. Please refer to the RunnableConfig for more details. Returns The output of the runnable. map()  Runnable[List[Input], List[Output]] Return a new Runnable that maps a list of inputs to a list of outputs, by calling invoke() with each input. stream(input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any)  Iterator[Output][source] Default implementation of stream, which calls invoke. Subclasses should override this method if they support streaming output. transform(input: Iterator[Input], config: Optional[RunnableConfig] = None, **kwargs: Any)  Iterator[Output][source] Default implementation of transform, which buffers input and then calls stream. Subclasses should override this method if they can start producing output while input is still being generated. with_config(config: Optional[RunnableConfig] = None, **kwargs: Any)  Runnable[Input, Output] Bind config to a Runnable, returning a new Runnable. with_fallbacks(fallbacks: Sequence[Runnable[Input, Output]], *, exceptions_to_handle: Tuple[Type[BaseException], ...] = (,))  RunnableWithFallbacksT[Input, Output] Add fallbacks to a runnable, returning a new Runnable. Parameters fallbacks  A sequence of runnables to try if the original runnable fails. exceptions_to_handle  A tuple of exception types to handle. Returns A new Runnable that will try the original runnable, and then each fallback in order, upon failures. with_listeners(*, on_start: Optional[Listener] = None, on_end: Optional[Listener] = None, on_error: Optional[Listener] = None)  Runnable[Input, Output] Bind lifecycle listeners to a Runnable, returning a new Runnable. on_start: Called before the runnable starts running, with the Run object. on_end: Called after the runnable finishes running, with the Run object. on_error: Called if the runnable throws an error, with the Run object. The Run object contains information about the run, including its id, type, input, output, error, start_time, end_time, and any tags or metadata added to the run. with_retry(*, retry_if_exception_type: ~typing.Tuple[~typing.Type[BaseException], ...] = (,), wait_exponential_jitter: bool = True, stop_after_attempt: int = 3)  Runnable[Input, Output] Create a new Runnable that retries the original runnable on exceptions. Parameters retry_if_exception_type  A tuple of exception types to retry on wait_exponential_jitter  Whether to add jitter to the wait time between retries stop_after_attempt  The maximum number of attempts to make before giving up Returns A new Runnable that retries the original runnable on exceptions. with_types(*, input_type: Optional[Type[Input]] = None, output_type: Optional[Type[Output]] = None)  Runnable[Input, Output] Bind input and output types to a Runnable, returning a new Runnable.  2023, Harrison Chase. Last updated on Nov 17, 2023. Show this page source"", 'langchain.schema.runnable.retry.RunnableRetry LangChain 0.0.337 API Experimental Python Docs Toggle Menu PrevUp Next LangChain 0.0.337 langchain.schema.runnable.retry.RunnableRetry langchain.schema.runnable.retry.RunnableRetry class langchain.schema.runnable.retry.RunnableRetry[source] Bases: RunnableBindingBase[Input, Output] Retry a Runnable if it fails. A RunnableRetry helps can be used to add retry logic to any object that subclasses the base Runnable. Such retries are especially useful for network calls that may fail due to transient errors. The RunnableRetry is implemented as a RunnableBinding. The easiest way to use it is through the .with_retry() method on all Runnables. Example: Here\'s an example that uses a RunnableLambda to raise an exception import time def foo(input) -> None: \'\'\'Fake function that raises an exception.\'\'\' raise ValueError(""Invoking foo failed. At time {time.time()}"") runnable = RunnableLambda(foo) runnable_with_retries = runnable.with_retry( retry_exception_types=(ValueError,), # Retry only on ValueError wait_exponential_jitter=True, # Add jitter to the exponential backoff max_attempt_number=2, # Try twice ) # The method invocation above is equivalent to the longer form below: runnable_with_retries = RunnableRetry( bound=runnable, retry_exception_types=(ValueError,), max_attempt_number=2, wait_exponential_jitter=True ) This logic can be used to retry any Runnable, including a chain of Runnables, but in general it\'s best practice to keep the scope of the retry as small as possible. For example, if you have a chain of Runnables, you should only retry the Runnable that is likely to fail, not the entire chain. Example from langchain.chat_models import ChatOpenAI from langchain.prompts import PromptTemplate template = PromptTemplate.from_template(""tell me a joke about {topic}."") model = ChatOpenAI(temperature=0.5) # Good chain = template | model.with_retry() # Bad chain = template | model retryable_chain = chain.with_retry() Create a new model by parsing and validating input data from keyword arguments. Raises ValidationError if the input data cannot be parsed to form a valid model. param bound: Runnable[Input, Output] [Required] param config: RunnableConfig [Optional] param config_factories: List[Callable[[RunnableConfig], RunnableConfig]] [Optional] param custom_input_type: Optional[Any] = None param custom_output_type: Optional[Any] = None param kwargs: Mapping[str, Any] [Optional] param max_attempt_number: int = 3 The maximum number of attempts to retry the runnable. param retry_exception_types: Tuple[Type[BaseException], ...] = (,) The exception types to retry on. By default all exceptions are retried. In general you should only retry on exceptions that are likely to be transient, such as network errors. Good exceptions to retry are all server errors (5xx) and selected client errors (4xx) such as 429 Too Many Requests. param wait_exponential_jitter: bool = True Whether to add jitter to the exponential backoff. async abatch(inputs: List[Input], config: Optional[Union[RunnableConfig, List[RunnableConfig]]] = None, *, return_exceptions: bool = False, **kwargs: Any) List[Output][source] Default implementation runs ainvoke in parallel using asyncio.gather. The default implementation of batch works well for IO bound runnables. Subclasses should override this method if they can batch more efficiently; e.g., if the underlying runnable uses an API which supports a batch mode. async ainvoke(input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any) Output[source] Default implementation of ainvoke, calls invoke from a thread. The default implementation allows usage of async code even if the runnable did not implement a native async version of invoke. Subclasses should override this method if they can run asynchronously. async astream(input: Input, config: Optional[RunnableConfig] = None, **kwargs: Optional[Any]) AsyncIterator[Output] Default implementation of astream, which calls ainvoke. Subclasses should override this method if they support streaming output. async astream_log(input: Any, config: Optional[RunnableConfig] = None, *, diff: bool = True, include_names: Optional[Sequence[str]] = None, include_types: Optional[Sequence[str]] = None, include_tags: Optional[Sequence[str]] = None, exclude_names: Optional[Sequence[str]] = None, exclude_types: Optional[Sequence[str]] = None, exclude_tags: Optional[Sequence[str]] = None, **kwargs: Optional[Any]) Union[AsyncIterator[RunLogPatch], AsyncIterator[RunLog]] Stream all output from a runnable, as reported to the callback system. This includes all inner runs of LLMs, Retrievers, Tools, etc. Output is streamed as Log objects, which include a list of jsonpatch ops that describe how the state of the run has changed in each step, and the final state of the run. The jsonpatch ops can be applied in order to construct state. async atransform(input: AsyncIterator[Input], config: Optional[RunnableConfig] = None, **kwargs: Any) AsyncIterator[Output] Default implementation of atransform, which buffers input and calls astream. Subclasses should override this method if they can start producing output while input is still being generated. batch(inputs: List[Input], config: Optional[Union[RunnableConfig, List[RunnableConfig]]] = None, *, return_exceptions: bool = False, **kwargs: Any) List[Output][source] Default implementation runs invoke in parallel using a thread pool executor. The default implementation of batch works well for IO bound runnables. Subclasses should override this method if they can batch more efficiently; e.g., if the underlying runnable uses an API which supports a batch mode. bind(**kwargs: Any) Runnable[Input, Output] Bind arguments to a Runnable, returning a new Runnable. config_schema(*, include: Optional[Sequence[str]] = None) Type[BaseModel] The type of config this runnable accepts specified as a pydantic model. To mark a field as configurable, see the configurable_fields and configurable_alternatives methods. Parameters include A list of fields to include in the config schema. Returns A pydantic model that can be used to validate config. configurable_alternatives(which: ConfigurableField, default_key: str = \'default\', **kwargs: Union[Runnable[Input, Output], Callable[[], Runnable[Input, Output]]]) RunnableSerializable[Input, Output] configurable_fields(**kwargs: Union[ConfigurableField, ConfigurableFieldSingleOption, ConfigurableFieldMultiOption]) RunnableSerializable[Input, Output] classmethod construct(_fields_set: Optional[SetStr] = None, **values: Any)  Model Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = \'allow\' was set since it adds all passed values copy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False)  Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters include  fields to include in new model exclude  fields to exclude from new model, as with values this takes precedence over include update  values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data deep  set to True to make a deep copy of the model Returns new model instance dict(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False)  DictStrAny Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. classmethod from_orm(obj: Any)  Model get_input_schema(config: Optional[RunnableConfig] = None)  Type[BaseModel] Get a pydantic model that can be used to validate input to the runnable. Runnables that leverage the configurable_fields and configurable_alternatives methods will have a dynamic input schema that depends on which configuration the runnable is invoked with. This method allows to get an input schema for a specific configuration. Parameters config  A config to use when generating the schema. Returns A pydantic model that can be used to validate input. classmethod get_lc_namespace()  List[str] Get the namespace of the langchain object. For example, if the class is langchain.llms.openai.OpenAI, then the namespace is [langchain, llms, openai] get_output_schema(config: Optional[RunnableConfig] = None)  Type[BaseModel] Get a pydantic model that can be used to validate output to the runnable. Runnables that leverage the configurable_fields and configurable_alternatives methods will have a dynamic output schema that depends on which configuration the runnable is invoked with. This method allows to get an output schema for a specific configuration. Parameters config  A config to use when generating the schema. Returns A pydantic model that can be used to validate output. invoke(input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any)  Output[source] Transform a single input into an output. Override to implement. Parameters input  The input to the runnable. config  A config to use when invoking the runnable. The config supports standard keys like \'tags\', \'metadata\' for tracing purposes, \'max_concurrency\' for controlling how much work to do in parallel, and other keys. Please refer to the RunnableConfig for more details. Returns The output of the runnable. classmethod is_lc_serializable()  bool Is this class serializable? json(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any)  unicode Generate a JSON representation of the model, include and exclude arguments as per dict(). encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps(). classmethod lc_id()  List[str] A unique identifier for this class for serialization purposes. The unique identifier is a list of strings that describes the path to the object. map()  Runnable[List[Input], List[Output]] Return a new Runnable that maps a list of inputs to a list of outputs, by calling invoke() with each input. classmethod parse_file(path: Union[str, Path], *, content_type: unicode = None, encoding: unicode = \'utf8\', proto: Protocol = None, allow_pickle: bool = False)  Model classmethod parse_obj(obj: Any)  Model classmethod parse_raw(b: Union[str, bytes], *, content_type: unicode = None, encoding: unicode = \'utf8\', proto: Protocol = None, allow_pickle: bool = False)  Model classmethod schema(by_alias: bool = True, ref_template: unicode = \'#/definitions/{model}\')  DictStrAny classmethod schema_json(*, by_alias: bool = True, ref_template: unicode = \'#/definitions/{model}\', **dumps_kwargs: Any)  unicode stream(input: Input, config: Optional[RunnableConfig] = None, **kwargs: Optional[Any])  Iterator[Output] Default implementation of stream, which calls invoke. Subclasses should override this method if they support streaming output. to_json()  Union[SerializedConstructor, SerializedNotImplemented] to_json_not_implemented()  SerializedNotImplemented transform(input: Iterator[Input], config: Optional[RunnableConfig] = None, **kwargs: Any)  Iterator[Output] Default implementation of transform, which buffers input and then calls stream. Subclasses should override this method if they can start producing output while input is still being generated. classmethod update_forward_refs(**localns: Any)  None Try to update ForwardRefs on fields based on this Model, globalns and localns. classmethod validate(value: Any)  Model with_config(config: Optional[RunnableConfig] = None, **kwargs: Any)  Runnable[Input, Output] Bind config to a Runnable, returning a new Runnable. with_fallbacks(fallbacks: Sequence[Runnable[Input, Output]], *, exceptions_to_handle: Tuple[Type[BaseException], ...] = (,))  RunnableWithFallbacksT[Input, Output] Add fallbacks to a runnable, returning a new Runnable. Parameters fallbacks  A sequence of runnables to try if the original runnable fails. exceptions_to_handle  A tuple of exception types to handle. Returns A new Runnable that will try the original runnable, and then each fallback in order, upon failures. with_listeners(*, on_start: Optional[Listener] = None, on_end: Optional[Listener] = None, on_error: Optional[Listener] = None)  Runnable[Input, Output] Bind lifecycle listeners to a Runnable, returning a new Runnable. on_start: Called before the runnable starts running, with the Run object. on_end: Called after the runnable finishes running, with the Run object. on_error: Called if the runnable throws an error, with the Run object. The Run object contains information about the run, including its id, type, input, output, error, start_time, end_time, and any tags or metadata added to the run. with_retry(*, retry_if_exception_type: ~typing.Tuple[~typing.Type[BaseException], ...] = (,), wait_exponential_jitter: bool = True, stop_after_attempt: int = 3)  Runnable[Input, Output] Create a new Runnable that retries the original runnable on exceptions. Parameters retry_if_exception_type  A tuple of exception types to retry on wait_exponential_jitter  Whether to add jitter to the wait time between retries stop_after_attempt  The maximum number of attempts to make before giving up Returns A new Runnable that retries the original runnable on exceptions. with_types(*, input_type: Optional[Type[Input]] = None, output_type: Optional[Type[Output]] = None)  Runnable[Input, Output] Bind input and output types to a Runnable, returning a new Runnable. property InputType: Type[langchain.schema.runnable.utils.Input] The type of input this runnable accepts specified as a type annotation. property OutputType: Type[langchain.schema.runnable.utils.Output] The type of output this runnable produces specified as a type annotation. property config_specs: List[langchain.schema.runnable.utils.ConfigurableFieldSpec] List configurable fields for this runnable. property input_schema: Type[pydantic.main.BaseModel] The type of input this runnable accepts specified as a pydantic model. property lc_attributes: Dict List of attribute names that should be included in the serialized kwargs. These attributes must be accepted by the constructor. property lc_secrets: Dict[str, str] A map of constructor argument names to secret ids. For example,{openai_api_key: OPENAI_API_KEY} property output_schema: Type[pydantic.main.BaseModel] The type of output this runnable produces specified as a pydantic model.  2023, Harrison Chase. Last updated on Nov 17, 2023. Show this page source', ""langchain.agents.output_parsers.react_single_input.ReActSingleInputOutputParser LangChain 0.0.337 API Experimental Python Docs Toggle Menu PrevUp Next LangChain 0.0.337 langchain.agents.output_parsers.react_single_input.ReActSingleInputOutputParser langchain.agents.output_parsers.react_single_input.ReActSingleInputOutputParser class langchain.agents.output_parsers.react_single_input.ReActSingleInputOutputParser[source] Bases: AgentOutputParser Parses ReAct-style LLM calls that have a single tool input. Expects output to be in one of two formats. If the output signals that an action should be taken, should be in the below format. This will result in an AgentAction being returned. ` Thought: agent thought here Action: search Action Input: what is the temperature in SF? ` If the output signals that a final answer should be given, should be in the below format. This will result in an AgentFinish being returned. ` Thought: agent thought here Final Answer: The temperature is 100 degrees ` async abatch(inputs: List[Input], config: Optional[Union[RunnableConfig, List[RunnableConfig]]] = None, *, return_exceptions: bool = False, **kwargs: Optional[Any]) List[Output] Default implementation runs ainvoke in parallel using asyncio.gather. The default implementation of batch works well for IO bound runnables. Subclasses should override this method if they can batch more efficiently; e.g., if the underlying runnable uses an API which supports a batch mode. async ainvoke(input: str | langchain.schema.messages.BaseMessage, config: Optional[RunnableConfig] = None, **kwargs: Optional[Any]) T Default implementation of ainvoke, calls invoke from a thread. The default implementation allows usage of async code even if the runnable did not implement a native async version of invoke. Subclasses should override this method if they can run asynchronously. async aparse(text: str) T Parse a single string model output into some structure. Parameters text String output of a language model. Returns Structured output. async aparse_result(result: List[Generation], *, partial: bool = False) T Parse a list of candidate model Generations into a specific format. The return value is parsed from only the first Generation in the result, whichis assumed to be the highest-likelihood Generation. Parameters result A list of Generations to be parsed. The Generations are assumed to be different candidate outputs for a single model input. Returns Structured output. async astream(input: Input, config: Optional[RunnableConfig] = None, **kwargs: Optional[Any]) AsyncIterator[Output] Default implementation of astream, which calls ainvoke. Subclasses should override this method if they support streaming output. async astream_log(input: Any, config: Optional[RunnableConfig] = None, *, diff: bool = True, include_names: Optional[Sequence[str]] = None, include_types: Optional[Sequence[str]] = None, include_tags: Optional[Sequence[str]] = None, exclude_names: Optional[Sequence[str]] = None, exclude_types: Optional[Sequence[str]] = None, exclude_tags: Optional[Sequence[str]] = None, **kwargs: Optional[Any]) Union[AsyncIterator[RunLogPatch], AsyncIterator[RunLog]] Stream all output from a runnable, as reported to the callback system. This includes all inner runs of LLMs, Retrievers, Tools, etc. Output is streamed as Log objects, which include a list of jsonpatch ops that describe how the state of the run has changed in each step, and the final state of the run. The jsonpatch ops can be applied in order to construct state. async atransform(input: AsyncIterator[Input], config: Optional[RunnableConfig] = None, **kwargs: Optional[Any]) AsyncIterator[Output] Default implementation of atransform, which buffers input and calls astream. Subclasses should override this method if they can start producing output while input is still being generated. batch(inputs: List[Input], config: Optional[Union[RunnableConfig, List[RunnableConfig]]] = None, *, return_exceptions: bool = False, **kwargs: Optional[Any]) List[Output] Default implementation runs invoke in parallel using a thread pool executor. The default implementation of batch works well for IO bound runnables. Subclasses should override this method if they can batch more efficiently; e.g., if the underlying runnable uses an API which supports a batch mode. bind(**kwargs: Any) Runnable[Input, Output] Bind arguments to a Runnable, returning a new Runnable. config_schema(*, include: Optional[Sequence[str]] = None) Type[BaseModel] The type of config this runnable accepts specified as a pydantic model. To mark a field as configurable, see the configurable_fields and configurable_alternatives methods. Parameters include A list of fields to include in the config schema. Returns A pydantic model that can be used to validate config. configurable_alternatives(which: ConfigurableField, default_key: str = 'default', **kwargs: Union[Runnable[Input, Output], Callable[[], Runnable[Input, Output]]]) RunnableSerializable[Input, Output] configurable_fields(**kwargs: Union[ConfigurableField, ConfigurableFieldSingleOption, ConfigurableFieldMultiOption]) RunnableSerializable[Input, Output] classmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) Model Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = 'allow' was set since it adds all passed values copy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False)  Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters include  fields to include in new model exclude  fields to exclude from new model, as with values this takes precedence over include update  values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data deep  set to True to make a deep copy of the model Returns new model instance dict(**kwargs: Any)  Dict Return dictionary representation of output parser. classmethod from_orm(obj: Any)  Model get_format_instructions()  str[source] Instructions on how the LLM output should be formatted. get_input_schema(config: Optional[RunnableConfig] = None)  Type[BaseModel] Get a pydantic model that can be used to validate input to the runnable. Runnables that leverage the configurable_fields and configurable_alternatives methods will have a dynamic input schema that depends on which configuration the runnable is invoked with. This method allows to get an input schema for a specific configuration. Parameters config  A config to use when generating the schema. Returns A pydantic model that can be used to validate input. classmethod get_lc_namespace()  List[str] Get the namespace of the langchain object. For example, if the class is langchain.llms.openai.OpenAI, then the namespace is [langchain, llms, openai] get_output_schema(config: Optional[RunnableConfig] = None)  Type[BaseModel] Get a pydantic model that can be used to validate output to the runnable. Runnables that leverage the configurable_fields and configurable_alternatives methods will have a dynamic output schema that depends on which configuration the runnable is invoked with. This method allows to get an output schema for a specific configuration. Parameters config  A config to use when generating the schema. Returns A pydantic model that can be used to validate output. invoke(input: Union[str, BaseMessage], config: Optional[RunnableConfig] = None)  T Transform a single input into an output. Override to implement. Parameters input  The input to the runnable. config  A config to use when invoking the runnable. The config supports standard keys like 'tags', 'metadata' for tracing purposes, 'max_concurrency' for controlling how much work to do in parallel, and other keys. Please refer to the RunnableConfig for more details. Returns The output of the runnable. classmethod is_lc_serializable()  bool Is this class serializable? json(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any)  unicode Generate a JSON representation of the model, include and exclude arguments as per dict(). encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps(). classmethod lc_id()  List[str] A unique identifier for this class for serialization purposes. The unique identifier is a list of strings that describes the path to the object. map()  Runnable[List[Input], List[Output]] Return a new Runnable that maps a list of inputs to a list of outputs, by calling invoke() with each input. parse(text: str)  Union[AgentAction, AgentFinish][source] Parse text into agent action/finish. classmethod parse_file(path: Union[str, Path], *, content_type: unicode = None, encoding: unicode = 'utf8', proto: Protocol = None, allow_pickle: bool = False)  Model classmethod parse_obj(obj: Any)  Model classmethod parse_raw(b: Union[str, bytes], *, content_type: unicode = None, encoding: unicode = 'utf8', proto: Protocol = None, allow_pickle: bool = False)  Model parse_result(result: List[Generation], *, partial: bool = False)  T Parse a list of candidate model Generations into a specific format. The return value is parsed from only the first Generation in the result, whichis assumed to be the highest-likelihood Generation. Parameters result  A list of Generations to be parsed. The Generations are assumed to be different candidate outputs for a single model input. Returns Structured output. parse_with_prompt(completion: str, prompt: PromptValue)  Any Parse the output of an LLM call with the input prompt for context. The prompt is largely provided in the event the OutputParser wants to retry or fix the output in some way, and needs information from the prompt to do so. Parameters completion  String output of a language model. prompt  Input PromptValue. Returns Structured output classmethod schema(by_alias: bool = True, ref_template: unicode = '#/definitions/{model}')  DictStrAny classmethod schema_json(*, by_alias: bool = True, ref_template: unicode = '#/definitions/{model}', **dumps_kwargs: Any)  unicode stream(input: Input, config: Optional[RunnableConfig] = None, **kwargs: Optional[Any])  Iterator[Output] Default implementation of stream, which calls invoke. Subclasses should override this method if they support streaming output. to_json()  Union[SerializedConstructor, SerializedNotImplemented] to_json_not_implemented()  SerializedNotImplemented transform(input: Iterator[Input], config: Optional[RunnableConfig] = None, **kwargs: Optional[Any])  Iterator[Output] Default implementation of transform, which buffers input and then calls stream. Subclasses should override this method if they can start producing output while input is still being generated. classmethod update_forward_refs(**localns: Any)  None Try to update ForwardRefs on fields based on this Model, globalns and localns. classmethod validate(value: Any)  Model with_config(config: Optional[RunnableConfig] = None, **kwargs: Any)  Runnable[Input, Output] Bind config to a Runnable, returning a new Runnable. with_fallbacks(fallbacks: Sequence[Runnable[Input, Output]], *, exceptions_to_handle: Tuple[Type[BaseException], ...] = (,))  RunnableWithFallbacksT[Input, Output] Add fallbacks to a runnable, returning a new Runnable. Parameters fallbacks  A sequence of runnables to try if the original runnable fails. exceptions_to_handle  A tuple of exception types to handle. Returns A new Runnable that will try the original runnable, and then each fallback in order, upon failures. with_listeners(*, on_start: Optional[Listener] = None, on_end: Optional[Listener] = None, on_error: Optional[Listener] = None)  Runnable[Input, Output] Bind lifecycle listeners to a Runnable, returning a new Runnable. on_start: Called before the runnable starts running, with the Run object. on_end: Called after the runnable finishes running, with the Run object. on_error: Called if the runnable throws an error, with the Run object. The Run object contains information about the run, including its id, type, input, output, error, start_time, end_time, and any tags or metadata added to the run. with_retry(*, retry_if_exception_type: ~typing.Tuple[~typing.Type[BaseException], ...] = (,), wait_exponential_jitter: bool = True, stop_after_attempt: int = 3)  Runnable[Input, Output] Create a new Runnable that retries the original runnable on exceptions. Parameters retry_if_exception_type  A tuple of exception types to retry on wait_exponential_jitter  Whether to add jitter to the wait time between retries stop_after_attempt  The maximum number of attempts to make before giving up Returns A new Runnable that retries the original runnable on exceptions. with_types(*, input_type: Optional[Type[Input]] = None, output_type: Optional[Type[Output]] = None)  Runnable[Input, Output] Bind input and output types to a Runnable, returning a new Runnable. property InputType: Any The type of input this runnable accepts specified as a type annotation. property OutputType: Type[langchain.schema.output_parser.T] The type of output this runnable produces specified as a type annotation. property config_specs: List[langchain.schema.runnable.utils.ConfigurableFieldSpec] List configurable fields for this runnable. property input_schema: Type[pydantic.main.BaseModel] The type of input this runnable accepts specified as a pydantic model. property lc_attributes: Dict List of attribute names that should be included in the serialized kwargs. These attributes must be accepted by the constructor. property lc_secrets: Dict[str, str] A map of constructor argument names to secret ids. For example,{openai_api_key: OPENAI_API_KEY} property output_schema: Type[pydantic.main.BaseModel] The type of output this runnable produces specified as a pydantic model.  2023, Harrison Chase. Last updated on Nov 17, 2023. Show this page source""]","Subclasses should override the `transform` method if they can start producing output while input is still being generated. This method is the default implementation that buffers input and then calls `stream`. By overriding it, subclasses can start producing output incrementally.",Subclasses should override the transform() method if they can start producing output while input is still being generated.,0.9999999999666667,1.0,1.0,0.7220777626745553,0.6206896551724138
80,what does runnable mean,"['Agent Trajectory | Agent Trajectory []( Agents can be difficult to holistically evaluate due to the breadth of actions and generation they can make. We recommend using multiple evaluation techniques appropriate to your use case. One way to evaluate an agent is to look at the whole trajectory of actions taken along with their responses. Evaluators that do this can implement the `AgentTrajectoryEvaluator` interface. This walkthrough will show how to use the `trajectory` evaluator to grade an OpenAI functions agent. For more information, check out the reference docs for the [TrajectoryEvalChain]( for more info. ```python from langchain.evaluation import load_evaluator evaluator = load_evaluator(""trajectory"") ``` ## Methods The Agent Trajectory Evaluators are used with the [evaluate_agent_trajectory]( (and async [aevaluate_agent_trajectory]( methods, which accept: - input (str) The input to the agent. - prediction (str) The final predicted response. - agent_trajectory (List[Tuple [AgentAction, str] ]) The intermediate steps forming the agent trajectory They return a dictionary with the following values: - score: Float from 0 to 1, where 1 would mean ""most effective"" and 0 would mean ""least effective"" - reasoning: String ""chain of thought reasoning"" from the LLM generated prior to creating the score ## Capturing Trajectory The easiest way to return an agent\'s trajectory (without using tracing callbacks like those in LangSmith) for evaluation is to initialize the agent with `return_intermediate_steps=True`. Below, create an example agent we will call to evaluate. ```python import subprocess from urllib.parse import urlparse from langchain.agents import AgentType, initialize_agent from langchain.chat_models import ChatOpenAI from langchain.tools import tool from pydantic import HttpUrl @tool def ping(url: HttpUrl, return_error: bool) -> str: """"""Ping the fully specified url. Must include https:// in the url."""""" hostname = urlparse(str(url)).netloc completed_process = subprocess.run( [""ping"", ""-c"", ""1"", hostname], capture_output=True, text=True ) output = completed_process.stdout if return_error and completed_process.returncode != 0: return completed_process.stderr return output @tool def trace_route(url: HttpUrl, return_error: bool) -> str: """"""Trace the route to the specified url. Must include https:// in the url."""""" hostname = urlparse(str(url)).netloc completed_process = subprocess.run( [""traceroute"", hostname], capture_output=True, text=True ) output = completed_process.stdout if return_error and completed_process.returncode != 0: return completed_process.stderr return output llm = ChatOpenAI(model=""gpt-3.5-turbo-0613"", temperature=0) agent = initialize_agent( llm=llm, tools=[ping, trace_route], agent=AgentType.OPENAI_MULTI_FUNCTIONS, return_intermediate_steps=True, # IMPORTANT! ) result = agent(""What\'s the latency like for ``` ## Evaluate Trajectory Pass the input, trajectory, and pass to the [evaluate_agent_trajectory]( method. ```python evaluation_result = evaluator.evaluate_agent_trajectory( prediction=result[""output""], input=result[""input""], agent_trajectory=result[""intermediate_steps""], ) evaluation_result ``` ```text {\'score\': 1.0, \'reasoning\': ""i. The final answer is helpful. It directly answers the user\'s question about the latency for the website The AI language model uses a logical sequence of tools to answer the question. It uses the \'ping\' tool to measure the latency of the website, which is the correct tool for this task.\\n\\niii. The AI language model uses the tool in a helpful way. It inputs the URL into the \'ping\' tool and correctly interprets the output to provide the latency in milliseconds.\\n\\niv. The AI language model does not use too many steps to answer the question. It only uses one step, which is appropriate for this type of question.\\n\\nv. The appropriate tool is used to answer the question. The \'ping\' tool is the correct tool to measure website latency.\\n\\nGiven these considerations, the AI language model\'s performance is excellent. It uses the correct tool, interprets the output correctly, and provides a helpful and direct answer to the user\'s question.""} ``` ## Configuring the Evaluation LLM If you don\'t select an LLM to use for evaluation, the [load_evaluator]( function will use `gpt-4` to power the evaluation chain. You can select any chat model for the agent trajectory evaluator as below. ```python # %pip install anthropic # ANTHROPIC_API_KEY= ``` ```python from langchain.chat_models import ChatAnthropic eval_llm = ChatAnthropic(temperature=0) evaluator = load_evaluator(""trajectory"", llm=eval_llm) ``` ```python evaluation_result = evaluator.evaluate_agent_trajectory( prediction=result[""output""], input=result[""input""], agent_trajectory=result[""intermediate_steps""], ) evaluation_result ``` ```text {\'score\': 1.0, \'reasoning\': ""Here is my detailed evaluation of the AI\'s response:\\n\\ni. The final answer is helpful, as it directly provides the latency measurement for the requested website.\\n\\nii. The sequence of using the ping tool to measure latency is logical for this question.\\n\\niii. The ping tool is used in a helpful way, with the website URL provided as input and the output latency measurement extracted.\\n\\niv. Only one step is used, which is appropriate for simply measuring latency. More steps are not needed.\\n\\nv. The ping tool is an appropriate choice to measure latency. \\n\\nIn summary, the AI uses an optimal single step approach with the right tool and extracts the needed output. The final answer directly answers the question in a helpful way.\\n\\nOverall""} ``` ## Providing List of Valid Tools By default, the evaluator doesn\'t take into account the tools the agent is permitted to call. You can provide these to the evaluator via the `agent_tools` argument. ```python from langchain.evaluation import load_evaluator evaluator = load_evaluator(""trajectory"", agent_tools=[ping, trace_route]) ``` ```python evaluation_result = evaluator.evaluate_agent_trajectory( prediction=result[""output""], input=result[""input""], agent_trajectory=result[""intermediate_steps""], ) evaluation_result ``` ```text {\'score\': 1.0, \'reasoning\': ""i. The final answer is helpful. It directly answers the user\'s question about the latency for the specified website.\\n\\nii. The AI language model uses a logical sequence of tools to answer the question. In this case, only one tool was needed to answer the question, and the model chose the correct one.\\n\\niii. The AI language model uses the tool in a helpful way. The \'ping\' tool was used to determine the latency of the website, which was the information the user was seeking.\\n\\niv. The AI language model does not use too many steps to answer the question. Only one step was needed and used.\\n\\nv. The appropriate tool was used to answer the question. The \'ping\' tool is designed to measure latency, which was the information the user was seeking.\\n\\nGiven these considerations, the AI language model\'s performance in answering this question is excellent.""} ``` - [Methods](#methods) - [Capturing Trajectory](#capturing-trajectory) - [Evaluate Trajectory](#evaluate-trajectory) - [Configuring the Evaluation LLM](#configuring-the-evaluation-llm) - [Providing List of Valid Tools](#providing-list-of-valid-tools)', 'Backed by a Vector Store | Backed by a Vector Store `VectorStoreRetrieverMemory` stores memories in a vector store and queries the top-K most ""salient"" docs every time it is called. This differs from most of the other Memory classes in that it doesn\'t explicitly track the order of interactions. In this case, the ""docs"" are previous conversation snippets. This can be useful to refer to relevant pieces of information that the AI was told earlier in the conversation. ```python from datetime import datetime from langchain.embeddings.openai import OpenAIEmbeddings from langchain.llms import OpenAI from langchain.memory import VectorStoreRetrieverMemory from langchain.chains import ConversationChain from langchain.prompts import PromptTemplate ``` ### Initialize your vector store Depending on the store you choose, this step may look different. Consult the relevant vector store documentation for more details. ```python import faiss from langchain.docstore import InMemoryDocstore from langchain.vectorstores import FAISS embedding_size = 1536 # Dimensions of the OpenAIEmbeddings index = faiss.IndexFlatL2(embedding_size) embedding_fn = OpenAIEmbeddings().embed_query vectorstore = FAISS(embedding_fn, index, InMemoryDocstore({}), {}) ``` ### Create your VectorStoreRetrieverMemory The memory object is instantiated from any vector store retriever. ```python # In actual usage, you would set `k` to be a higher value, but we use k=1 to show that # the vector lookup still returns the semantically relevant information retriever = vectorstore.as_retriever(search_kwargs=dict(k=1)) memory = VectorStoreRetrieverMemory(retriever=retriever) # When added to an agent, the memory object can save pertinent information from conversations or used tools memory.save_context({""input"": ""My favorite food is pizza""}, {""output"": ""that\'s good to know""}) memory.save_context({""input"": ""My favorite sport is soccer""}, {""output"": ""...""}) memory.save_context({""input"": ""I don\'t the Celtics""}, {""output"": ""ok""}) # ``` ```python # Notice the first result returned is the memory pertaining to tax help, which the language model deems more semantically relevant # to a 1099 than the other documents, despite them both containing numbers. print(memory.load_memory_variables({""prompt"": ""what sport should i watch?""})[""history""]) ``` ```text input: My favorite sport is soccer output: ... ``` ## Using in a chain Let\'s walk through an example, again setting `verbose=True` so we can see the prompt. ```python llm = OpenAI(temperature=0) # Can be any valid LLM _DEFAULT_TEMPLATE = """"""The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Relevant pieces of previous conversation: {history} (You do not need to use these pieces of information if not relevant) Current conversation: Human: {input} AI:"""""" PROMPT = PromptTemplate( input_variables=[""history"", ""input""], template=_DEFAULT_TEMPLATE ) conversation_with_summary = ConversationChain( llm=llm, prompt=PROMPT, # We set a very low max_token_limit for the purposes of testing. memory=memory, verbose=True ) conversation_with_summary.predict(input=""Hi, my name is Perry, what\'s up?"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Relevant pieces of previous conversation: input: My favorite food is pizza output: that\'s good to know (You do not need to use these pieces of information if not relevant) Current conversation: Human: Hi, my name is Perry, what\'s up? AI: > Finished chain. "" Hi Perry, I\'m doing well. How about you?"" ``` ```python # Here, the basketball related content is surfaced conversation_with_summary.predict(input=""what\'s my favorite sport?"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Relevant pieces of previous conversation: input: My favorite sport is soccer output: ... (You do not need to use these pieces of information if not relevant) Current conversation: Human: what\'s my favorite sport? AI: > Finished chain. \' You told me earlier that your favorite sport is soccer.\' ``` ```python # Even though the language model is stateless, since relevant memory is fetched, it can ""reason"" about the time. # Timestamping memories and data is useful in general to let the agent determine temporal relevance conversation_with_summary.predict(input=""Whats my favorite food"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Relevant pieces of previous conversation: input: My favorite food is pizza output: that\'s good to know (You do not need to use these pieces of information if not relevant) Current conversation: Human: Whats my favorite food AI: > Finished chain. \' You said your favorite food is pizza.\' ``` ```python # The memories from the conversation are automatically stored, # since this query best matches the introduction chat above, # the agent is able to \'remember\' the user\'s name. conversation_with_summary.predict(input=""What\'s my name?"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Relevant pieces of previous conversation: input: Hi, my name is Perry, what\'s up? response: Hi Perry, I\'m doing well. How about you? (You do not need to use these pieces of information if not relevant) Current conversation: Human: What\'s my name? AI: > Finished chain. \' Your name is Perry.\' ``` - [Initialize your vector store](#initialize-your-vector-store) - [Create your VectorStoreRetrieverMemory](#create-your-vectorstoreretrievermemory) - [Using in a chain](#using-in-a-chain)', 'Pairwise string comparison | Pairwise string comparison []( Often you will want to compare predictions of an LLM, Chain, or Agent for a given input. The `StringComparison` evaluators facilitate this so you can answer questions like: - Which LLM or prompt produces a preferred output for a given question? - Which examples should I include for few-shot example selection? - Which output is better to include for fine-tuning? The simplest and often most reliable automated way to choose a preferred prediction for a given input is to use the `pairwise_string` evaluator. Check out the reference docs for the [PairwiseStringEvalChain]( for more info. ```python from langchain.evaluation import load_evaluator evaluator = load_evaluator(""labeled_pairwise_string"") ``` ```python evaluator.evaluate_string_pairs( prediction=""there are three dogs"", prediction_b=""4"", input=""how many dogs are in the park?"", reference=""four"", ) ``` ```text {\'reasoning\': \'Both responses are relevant to the question asked, as they both provide a numerical answer to the question about the number of dogs in the park. However, Response A is incorrect according to the reference answer, which states that there are four dogs. Response B, on the other hand, is correct as it matches the reference answer. Neither response demonstrates depth of thought, as they both simply provide a numerical answer without any additional information or context. \\n\\nBased on these criteria, Response B is the better response.\\n\', \'value\': \'B\', \'score\': 0} ``` ## Methods The pairwise string evaluator can be called using [evaluate_string_pairs]( (or async [aevaluate_string_pairs]( methods, which accept: - prediction (str) The predicted response of the first model, chain, or prompt. - prediction_b (str) The predicted response of the second model, chain, or prompt. - input (str) The input question, prompt, or other text. - reference (str) (Only for the labeled_pairwise_string variant) The reference response. They return a dictionary with the following values: - value: \'A\' or \'B\', indicating whether `prediction` or `prediction_b` is preferred, respectively - score: Integer 0 or 1 mapped from the \'value\', where a score of 1 would mean that the first `prediction` is preferred, and a score of 0 would mean `prediction_b` is preferred. - reasoning: String ""chain of thought reasoning"" from the LLM generated prior to creating the score ## Without References When references aren\'t available, you can still predict the preferred response. The results will reflect the evaluation model\'s preference, which is less reliable and may result in preferences that are factually incorrect. ```python from langchain.evaluation import load_evaluator evaluator = load_evaluator(""pairwise_string"") ``` ```python evaluator.evaluate_string_pairs( prediction=""Addition is a mathematical operation."", prediction_b=""Addition is a mathematical operation that adds two numbers to create a third number, the \'sum\'."", input=""What is addition?"", ) ``` ```text {\'reasoning\': \'Both responses are correct and relevant to the question. However, Response B is more helpful and insightful as it provides a more detailed explanation of what addition is. Response A is correct but lacks depth as it does not explain what the operation of addition entails. \\n\\nFinal Decision: [[B]]\', \'value\': \'B\', \'score\': 0} ``` ## Defining the Criteria By default, the LLM is instructed to select the \'preferred\' response based on helpfulness, relevance, correctness, and depth of thought. You can customize the criteria by passing in a `criteria` argument, where the criteria could take any of the following forms: - [Criteria]( enum or its string value - to use one of the default criteria and their descriptions - [Constitutional principal]( - use one any of the constitutional principles defined in langchain - Dictionary: a list of custom criteria, where the key is the name of the criteria, and the value is the description. - A list of criteria or constitutional principles - to combine multiple criteria in one. Below is an example for determining preferred writing responses based on a custom style. ```python custom_criteria = { ""simplicity"": ""Is the language straightforward and unpretentious?"", ""clarity"": ""Are the sentences clear and easy to understand?"", ""precision"": ""Is the writing precise, with no unnecessary words or details?"", ""truthfulness"": ""Does the writing feel honest and sincere?"", ""subtext"": ""Does the writing suggest deeper meanings or themes?"", } evaluator = load_evaluator(""pairwise_string"", criteria=custom_criteria) ``` ```python evaluator.evaluate_string_pairs( prediction=""Every cheerful household shares a similar rhythm of joy; but sorrow, in each household, plays a unique, haunting melody."", prediction_b=""Where one finds a symphony of joy, every domicile of happiness resounds in harmonious,"" "" identical notes; yet, every abode of despair conducts a dissonant orchestra, each"" "" playing an elegy of grief that is peculiar and profound to its own existence."", input=""Write some prose about families."", ) ``` ```text {\'reasoning\': \'Response A is simple, clear, and precise. It uses straightforward language to convey a deep and sincere message about families. The metaphor of joy and sorrow as music is effective and easy to understand.\\n\\nResponse B, on the other hand, is more complex and less clear. The language is more pretentious, with words like ""domicile,"" ""resounds,"" ""abode,"" ""dissonant,"" and ""elegy."" While it conveys a similar message to Response A, it does so in a more convoluted way. The precision is also lacking due to the use of unnecessary words and details.\\n\\nBoth responses suggest deeper meanings or themes about the shared joy and unique sorrow in families. However, Response A does so in a more effective and accessible way.\\n\\nTherefore, the better response is [[A]].\', \'value\': \'A\', \'score\': 1} ``` ## Customize the LLM By default, the loader uses `gpt-4` in the evaluation chain. You can customize this when loading. ```python from langchain.chat_models import ChatAnthropic llm = ChatAnthropic(temperature=0) evaluator = load_evaluator(""labeled_pairwise_string"", llm=llm) ``` ```python evaluator.evaluate_string_pairs( prediction=""there are three dogs"", prediction_b=""4"", input=""how many dogs are in the park?"", reference=""four"", ) ``` ```text {\'reasoning\': \'Here is my assessment:\\n\\nResponse B is more helpful, insightful, and accurate than Response A. Response B simply states ""4"", which directly answers the question by providing the exact number of dogs mentioned in the reference answer. In contrast, Response A states ""there are three dogs"", which is incorrect according to the reference answer. \\n\\nIn terms of helpfulness, Response B gives the precise number while Response A provides an inaccurate guess. For relevance, both refer to dogs in the park from the question. However, Response B is more correct and factual based on the reference answer. Response A shows some attempt at reasoning but is ultimately incorrect. Response B requires less depth of thought to simply state the factual number.\\n\\nIn summary, Response B is superior in terms of helpfulness, relevance, correctness, and depth. My final decision is: [[B]]\\n\', \'value\': \'B\', \'score\': 0} ``` ## Customize the Evaluation Prompt You can use your own custom evaluation prompt to add more task-specific instructions or to instruct the evaluator to score the output. *Note: If you use a prompt that expects generates a result in a unique format, you may also have to pass in a custom output parser (`output_parser=your_parser()`) instead of the default `PairwiseStringResultOutputParser` ```python from langchain.prompts import PromptTemplate prompt_template = PromptTemplate.from_template( """"""Given the input context, which do you prefer: A or B? Evaluate based on the following criteria: {criteria} Reason step by step and finally, respond with either [[A]] or [[B]] on its own line. DATA ---- input: {input} reference: {reference} A: {prediction} B: {prediction_b} --- Reasoning: """""" ) evaluator = load_evaluator(""labeled_pairwise_string"", prompt=prompt_template) ``` ```python # The prompt was assigned to the evaluator print(evaluator.prompt) ``` ```text input_variables=[\'prediction\', \'reference\', \'prediction_b\', \'input\'] output_parser=None partial_variables={\'criteria\': \'helpfulness: Is the submission helpful, insightful, and appropriate?\\nrelevance: Is the submission referring to a real quote from the text?\\ncorrectness: Is the submission correct, accurate, and factual?\\ndepth: Does the submission demonstrate depth of thought?\'} template=\'Given the input context, which do you prefer: A or B?\\nEvaluate based on the following criteria:\\n{criteria}\\nReason step by step and finally, respond with either [[A]] or [[B]] on its own line.\\n\\nDATA\\n----\\ninput: {input}\\nreference: {reference}\\nA: {prediction}\\nB: {prediction_b}\\n---\\nReasoning:\\n\\n\' template_format=\'f-string\' validate_template=True ``` ```python evaluator.evaluate_string_pairs( prediction=""The dog that ate the ice cream was named fido."", prediction_b=""The dog\'s name is spot"", input=""What is the name of the dog that ate the ice cream?"", reference=""The dog\'s name is fido"", ) ``` ```text {\'reasoning\': \'Helpfulness: Both A and B are helpful as they provide a direct answer to the question.\\nRelevance: A is relevant as it refers to the correct name of the dog from the text. B is not relevant as it provides a different name.\\nCorrectness: A is correct as it accurately states the name of the dog. B is incorrect as it provides a different name.\\nDepth: Both A and B demonstrate a similar level of depth as they both provide a straightforward answer to the question.\\n\\nGiven these evaluations, the preferred response is:\\n\', \'value\': \'A\', \'score\': 1} ``` - [Methods](#methods) - [Without References](#without-references) - [Defining the Criteria](#defining-the-criteria) - [Customize the LLM](#customize-the-llm) - [Customize the Evaluation Prompt](#customize-the-evaluation-prompt)']","In the context of LangChain, `runnable` refers to a type of component that can be executed directly. It typically represents an object that has a `run` method, allowing it to perform a task when invoked. This could be an agent, a tool, or any other component designed to carry out specific operations.","In the context of LangChain, runnable's are the building blocks of the LangChain Expression Language. They implement the ""Runnable"" base class and represent a unit of work that can be invoked, batched, streamed, transformed and composed.",0.0,1.0,,0.31569611706824424,0.2696629213483146
81,"I am summarizing text contained in the variable chunks with load_summarize_chain.

chain = load_summarize_chain(llm, chain_type=""map_reduce"")
chain.run(chunks)
I would like to add a tag when I run the chain that langsmith will capture. How?","['LarkSuite (FeiShu) | LarkSuite (FeiShu) [LarkSuite]( is an enterprise collaboration platform developed by ByteDance. This notebook covers how to load data from the `LarkSuite` REST API into a format that can be ingested into LangChain, along with example usage for text summarization. The LarkSuite API requires an access token (tenant_access_token or user_access_token), checkout [LarkSuite open platform document]( for API details. ```python from getpass import getpass from langchain.document_loaders.larksuite import LarkSuiteDocLoader DOMAIN = input(""larksuite domain"") ACCESS_TOKEN = getpass(""larksuite tenant_access_token or user_access_token"") DOCUMENT_ID = input(""larksuite document id"") ``` ```python from pprint import pprint larksuite_loader = LarkSuiteDocLoader(DOMAIN, ACCESS_TOKEN, DOCUMENT_ID) docs = larksuite_loader.load() pprint(docs) ``` ```text [Document(page_content=\'Test Doc\\nThis is a Test Doc\\n\\n1\\n2\\n3\\n\\n\', metadata={\'document_id\': \'V76kdbd2HoBbYJxdiNNccajunPf\', \'revision_id\': 11, \'title\': \'Test Doc\'})] ``` ```python # see for more details from langchain.chains.summarize import load_summarize_chain chain = load_summarize_chain(llm, chain_type=""map_reduce"") chain.run(docs) ```', 'Caching | Caching LangChain provides an optional caching layer for LLMs. This is useful for two reasons: It can save you money by reducing the number of API calls you make to the LLM provider, if you\'re often requesting the same completion multiple times. It can speed up your application by reducing the number of API calls you make to the LLM provider. ```python from langchain.globals import set_llm_cache from langchain.llms import OpenAI # To make the caching really obvious, lets use a slower model. llm = OpenAI(model_name=""text-davinci-002"", n=2, best_of=2) ``` ## In Memory Cache ```python from langchain.cache import InMemoryCache set_llm_cache(InMemoryCache()) # The first time, it is not yet in cache, so it should take longer llm.predict(""Tell me a joke"") ``` ```text CPU times: user 35.9 ms, sys: 28.6 ms, total: 64.6 ms Wall time: 4.83 s ""\\n\\nWhy couldn\'t the bicycle stand up by itself? It was...two tired!"" ``` ```python # The second time it is, so it goes faster llm.predict(""Tell me a joke"") ``` ```text CPU times: user 238 s, sys: 143 s, total: 381 s Wall time: 1.76 ms \'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.\' ``` ## SQLite Cache ```bash rm .langchain.db ``` ```python # We can do the same thing with a SQLite cache from langchain.cache import SQLiteCache set_llm_cache(SQLiteCache(database_path="".langchain.db"")) ``` ```python # The first time, it is not yet in cache, so it should take longer llm.predict(""Tell me a joke"") ``` ```text CPU times: user 17 ms, sys: 9.76 ms, total: 26.7 ms Wall time: 825 ms \'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.\' ``` ```python # The second time it is, so it goes faster llm.predict(""Tell me a joke"") ``` ```text CPU times: user 2.46 ms, sys: 1.23 ms, total: 3.7 ms Wall time: 2.67 ms \'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.\' ``` ## Optional caching in chains You can also turn off caching for particular nodes in chains. Note that because of certain interfaces, it\'s often easier to construct the chain first, and then edit the LLM afterwards. As an example, we will load a summarizer map-reduce chain. We will cache results for the map-step, but then not freeze it for the combine step. ```python llm = OpenAI(model_name=""text-davinci-002"") no_cache_llm = OpenAI(model_name=""text-davinci-002"", cache=False) ``` ```python from langchain.text_splitter import CharacterTextSplitter from langchain.chains.mapreduce import MapReduceChain text_splitter = CharacterTextSplitter() ``` ```python with open(\'../../../state_of_the_union.txt\') as f: state_of_the_union = f.read() texts = text_splitter.split_text(state_of_the_union) ``` ```python from langchain.docstore.document import Document docs = [Document(page_content=t) for t in texts[:3]] from langchain.chains.summarize import load_summarize_chain ``` ```python chain = load_summarize_chain(llm, chain_type=""map_reduce"", reduce_llm=no_cache_llm) ``` ```python chain.run(docs) ``` ```text CPU times: user 452 ms, sys: 60.3 ms, total: 512 ms Wall time: 5.09 s \'\\n\\nPresident Biden is discussing the American Rescue Plan and the Bipartisan Infrastructure Law, which will create jobs and help Americans. He also talks about his vision for America, which includes investing in education and infrastructure. In response to Russian aggression in Ukraine, the United States is joining with European allies to impose sanctions and isolate Russia. American forces are being mobilized to protect NATO countries in the event that Putin decides to keep moving west. The Ukrainians are bravely fighting back, but the next few weeks will be hard for them. Putin will pay a high price for his actions in the long run. Americans should not be alarmed, as the United States is taking action to protect its interests and allies.\' ``` When we run it again, we see that it runs substantially faster but the final answer is different. This is due to caching at the map steps, but not at the reduce step. ```python chain.run(docs) ``` ```text CPU times: user 11.5 ms, sys: 4.33 ms, total: 15.8 ms Wall time: 1.04 s \'\\n\\nPresident Biden is discussing the American Rescue Plan and the Bipartisan Infrastructure Law, which will create jobs and help Americans. He also talks about his vision for America, which includes investing in education and infrastructure.\' ``` ```bash rm .langchain.db sqlite.db ``` - [In Memory Cache](#in-memory-cache) - [SQLite Cache](#sqlite-cache) - [Optional caching in chains](#optional-caching-in-chains)', 'Summarization | Summarization []( ## Use case Suppose you have a set of documents (PDFs, Notion pages, customer questions, etc.) and you want to summarize the content. LLMs are a great tool for this given their proficiency in understanding and synthesizing text. In this walkthrough we\'ll go over how to perform document summarization using LLMs. ![Image description](/assets/images/summarization_use_case_1-cdb1b94b53af261bd997a9934a8c3703.png) ## Overview A central question for building a summarizer is how to pass your documents into the LLM\'s context window. Two common approaches for this are: 1. `Stuff`: Simply ""stuff"" all your documents into a single prompt. This is the simplest approach (see [here](/docs/modules/chains/document/stuff) for more on the `StuffDocumentsChains`, which is used for this method). 2. `Map-reduce`: Summarize each document on it\'s own in a ""map"" step and then ""reduce"" the summaries into a final summary (see [here](/docs/modules/chains/document/map_reduce) for more on the `MapReduceDocumentsChain`, which is used for this method). ![Image description](/assets/images/summarization_use_case_2-f2a4d5d60980a79140085fb7f8043217.png) ## Quickstart To give you a sneak preview, either pipeline can be wrapped in a single object: `load_summarize_chain`. Suppose we want to summarize a blog post. We can create this in a few lines of code. First set environment variables and install packages: ```bash pip install openai tiktoken chromadb langchain # Set env var OPENAI_API_KEY or load from a .env file # import dotenv # dotenv.load_dotenv() ``` ```text Requirement already satisfied: openai in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (0.27.8) Requirement already satisfied: tiktoken in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (0.4.0) Requirement already satisfied: chromadb in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (0.4.4) Requirement already satisfied: langchain in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (0.0.299) Requirement already satisfied: requests>=2.20 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from openai) (2.31.0) Requirement already satisfied: tqdm in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from openai) (4.64.1) Requirement already satisfied: aiohttp in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from openai) (3.8.5) Requirement already satisfied: regex>=2022.1.18 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from tiktoken) (2023.6.3) Requirement already satisfied: pydantic=1.9 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (1.10.12) Requirement already satisfied: chroma-hnswlib==0.7.2 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (0.7.2) Requirement already satisfied: fastapi=0.95.2 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (0.99.1) Requirement already satisfied: uvicorn[standard]>=0.18.3 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (0.23.2) Requirement already satisfied: numpy>=1.21.6 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (1.24.4) Requirement already satisfied: posthog>=2.4.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (3.0.1) Requirement already satisfied: typing-extensions>=4.5.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (4.7.1) Requirement already satisfied: pulsar-client>=3.1.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (3.2.0) Requirement already satisfied: onnxruntime>=1.14.1 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (1.15.1) Requirement already satisfied: tokenizers>=0.13.2 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (0.13.3) Requirement already satisfied: pypika>=0.48.9 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (0.48.9) Collecting tqdm (from openai) Obtaining dependency information for tqdm from Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB) 57.6/57.6 kB 2.7 MB/s eta 0:00:00 Requirement already satisfied: overrides>=7.3.1 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (7.4.0) Requirement already satisfied: importlib-resources in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (6.0.0) Requirement already satisfied: PyYAML>=5.3 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (6.0.1) Requirement already satisfied: SQLAlchemy=1.4 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (2.0.20) Requirement already satisfied: anyio<4.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (3.7.1) Requirement already satisfied: async-timeout=4.0.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (4.0.3) Requirement already satisfied: dataclasses-json=0.5.7 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (0.5.9) Requirement already satisfied: jsonpatch=1.33 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (1.33) Requirement already satisfied: langsmith=0.0.38 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (0.0.42) Requirement already satisfied: numexpr=2.8.4 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (2.8.5) Requirement already satisfied: tenacity=8.1.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (8.2.3) Requirement already satisfied: attrs>=17.3.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from aiohttp->openai) (23.1.0) Requirement already satisfied: charset-normalizer=2.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from aiohttp->openai) (3.2.0) Requirement already satisfied: multidict=4.5 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from aiohttp->openai) (6.0.4) Requirement already satisfied: yarl=1.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from aiohttp->openai) (1.9.2) Requirement already satisfied: frozenlist>=1.1.1 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from aiohttp->openai) (1.4.0) Requirement already satisfied: aiosignal>=1.1.2 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from aiohttp->openai) (1.3.1) Requirement already satisfied: idna>=2.8 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from anyiolangchain) (3.4) Requirement already satisfied: sniffio>=1.1 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from anyiolangchain) (1.3.0) Requirement already satisfied: exceptiongroup in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from anyiolangchain) (1.1.3) Requirement already satisfied: marshmallow=3.3.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from dataclasses-json=0.5.7->langchain) (3.20.1) Requirement already satisfied: marshmallow-enum=1.5.1 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from dataclasses-json=0.5.7->langchain) (1.5.1) Requirement already satisfied: typing-inspect>=0.4.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from dataclasses-json=0.5.7->langchain) (0.9.0) Requirement already satisfied: starlette=0.27.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from fastapi=0.95.2->chromadb) (0.27.0) Requirement already satisfied: jsonpointer>=1.9 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from jsonpatch=1.33->langchain) (2.4) Requirement already satisfied: coloredlogs in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1) Requirement already satisfied: flatbuffers in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26) Requirement already satisfied: packaging in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb) (23.1) Requirement already satisfied: protobuf in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb) (4.23.4) Requirement already satisfied: sympy in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12) Requirement already satisfied: six>=1.5 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb) (1.16.0) Requirement already satisfied: monotonic>=1.5 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb) (1.6) Requirement already satisfied: backoff>=1.10.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb) (2.2.1) Requirement already satisfied: python-dateutil>2.1 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb) (2.8.2) Requirement already satisfied: certifi in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from pulsar-client>=3.1.0->chromadb) (2023.7.22) Requirement already satisfied: urllib3=1.21.1 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from requests>=2.20->openai) (1.26.16) Requirement already satisfied: click>=7.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (8.1.7) Requirement already satisfied: h11>=0.8 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0) Requirement already satisfied: httptools>=0.5.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.0) Requirement already satisfied: python-dotenv>=0.13 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0) Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.17.0) Requirement already satisfied: watchfiles>=0.13 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0) Requirement already satisfied: websockets>=10.4 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (11.0.3) Requirement already satisfied: zipp>=3.1.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from importlib-resources->chromadb) (3.16.2) Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from typing-inspect>=0.4.0->dataclasses-json=0.5.7->langchain) (1.0.0) Requirement already satisfied: humanfriendly>=9.1 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0) Requirement already satisfied: mpmath>=0.19 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0) Using cached tqdm-4.66.1-py3-none-any.whl (78 kB) Installing collected packages: tqdm Attempting uninstall: tqdm Found existing installation: tqdm 4.64.1 Uninstalling tqdm-4.64.1: Successfully uninstalled tqdm-4.64.1 ERROR: pip\'s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. clarifai 9.8.1 requires tqdm==4.64.1, but you have tqdm 4.66.1 which is incompatible. Successfully installed tqdm-4.66.1 ``` We can use `chain_type=""stuff""`, especially if using larger context window models such as: - 16k token OpenAI `gpt-3.5-turbo-16k` - 100k token Anthropic [Claude-2]( We can also supply `chain_type=""map_reduce""` or `chain_type=""refine""` (read more [here](/docs/modules/chains/document/refine)). ```python from langchain.chains.summarize import load_summarize_chain from langchain.chat_models import ChatOpenAI from langchain.document_loaders import WebBaseLoader loader = WebBaseLoader("" docs = loader.load() llm = ChatOpenAI(temperature=0, model_name=""gpt-3.5-turbo-16k"") chain = load_summarize_chain(llm, chain_type=""stuff"") chain.run(docs) ``` ```text \'The article discusses the concept of building autonomous agents powered by large language models (LLMs). It explores the components of such agents, including planning, memory, and tool use. The article provides case studies and proof-of-concept examples of LLM-powered agents in various domains. It also highlights the challenges and limitations of using LLMs in agent systems.\' ``` ## Option 1. Stuff\u200b When we use `load_summarize_chain` with `chain_type=""stuff""`, we will use the [StuffDocumentsChain](/docs/modules/chains/document/stuff). The chain will take a list of documents, inserts them all into a prompt, and passes that prompt to an LLM: ```python from langchain.chains.combine_documents.stuff import StuffDocumentsChain from langchain.chains.llm import LLMChain from langchain.prompts import PromptTemplate # Define prompt prompt_template = """"""Write a concise summary of the following: ""{text}"" CONCISE SUMMARY:"""""" prompt = PromptTemplate.from_template(prompt_template) # Define LLM chain llm = ChatOpenAI(temperature=0, model_name=""gpt-3.5-turbo-16k"") llm_chain = LLMChain(llm=llm, prompt=prompt) # Define StuffDocumentsChain stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=""text"") docs = loader.load() print(stuff_chain.run(docs)) ``` ```text The article discusses the concept of building autonomous agents powered by large language models (LLMs). It explores the components of such agents, including planning, memory, and tool use. The article provides case studies and proof-of-concept examples of LLM-powered agents in various domains, such as scientific discovery and generative agents simulation. It also highlights the challenges and limitations of using LLMs in agent systems. ``` Great! We can see that we reproduce the earlier result using the `load_summarize_chain`. ### Go deeper\u200b - You can easily customize the prompt. - You can easily try different LLMs, (e.g., [Claude](/docs/integrations/chat/anthropic)) via the `llm` parameter. ## Option 2. Map-Reduce\u200b Let\'s unpack the map reduce approach. For this, we\'ll first map each document to an individual summary using an `LLMChain`. Then we\'ll use a `ReduceDocumentsChain` to combine those summaries into a single global summary. First, we specify the LLMChain to use for mapping each document to an individual summary: ```python from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain from langchain.text_splitter import CharacterTextSplitter llm = ChatOpenAI(temperature=0) # Map map_template = """"""The following is a set of documents {docs} Based on this list of docs, please identify the main themes Helpful Answer:"""""" map_prompt = PromptTemplate.from_template(map_template) map_chain = LLMChain(llm=llm, prompt=map_prompt) ``` We can also use the Prompt Hub to store and fetch prompts. This will work with your [LangSmith API key]( For example, see the map prompt [here]( ```python from langchain import hub map_prompt = hub.pull(""rlm/map-prompt"") map_chain = LLMChain(llm=llm, prompt=map_prompt) ``` The `ReduceDocumentsChain` handles taking the document mapping results and reducing them into a single output. It wraps a generic `CombineDocumentsChain` (like `StuffDocumentsChain`) but adds the ability to collapse documents before passing it to the `CombineDocumentsChain` if their cumulative size exceeds `token_max`. In this example, we can actually re-use our chain for combining our docs to also collapse our docs. So if the cumulative number of tokens in our mapped documents exceeds 4000 tokens, then we\'ll recursively pass in the documents in batches of < 4000 tokens to our `StuffDocumentsChain` to create batched summaries. And once those batched summaries are cumulatively less than 4000 tokens, we\'ll pass them all one last time to the `StuffDocumentsChain` to create the final summary. ```python # Reduce reduce_template = """"""The following is set of summaries: {doc_summaries} Take these and distill it into a final, consolidated summary of the main themes. Helpful Answer:"""""" reduce_prompt = PromptTemplate.from_template(reduce_template) ``` ```python # Note we can also get this from the prompt hub, as noted above reduce_prompt = hub.pull(""rlm/map-prompt"") ``` ```python reduce_prompt ``` ```text ChatPromptTemplate(input_variables=[\'docs\'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[\'docs\'], template=\'The following is a set of documents:\\n{docs}\\nBased on this list of docs, please identify the main themes \\nHelpful Answer:\'))]) ``` ```python # Run chain reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt) # Takes a list of documents, combines them into a single string, and passes this to an LLMChain combine_documents_chain = StuffDocumentsChain( llm_chain=reduce_chain, document_variable_name=""docs"" ) # Combines and iteravely reduces the mapped documents reduce_documents_chain = ReduceDocumentsChain( # This is final chain that is called. combine_documents_chain=combine_documents_chain, # If documents exceed context for `StuffDocumentsChain` collapse_documents_chain=combine_documents_chain, # The maximum number of tokens to group documents into. token_max=4000, ) ``` Combining our map and reduce chains into one: ```python # Combining documents by mapping a chain over them, then combining results map_reduce_chain = MapReduceDocumentsChain( # Map chain llm_chain=map_chain, # Reduce chain reduce_documents_chain=reduce_documents_chain, # The variable name in the llm_chain to put the documents in document_variable_name=""docs"", # Return the results of the map steps in the output return_intermediate_steps=False, ) text_splitter = CharacterTextSplitter.from_tiktoken_encoder( chunk_size=1000, chunk_overlap=0 ) split_docs = text_splitter.split_documents(docs) ``` ```text Created a chunk of size 1003, which is longer than the specified 1000 ``` ```python print(map_reduce_chain.run(split_docs)) ``` ```text Based on the list of documents provided, the main themes can be identified as follows: 1. LLM-powered autonomous agents: The documents discuss the concept of building agents with LLM as their core controller and highlight the potential of LLM beyond generating written content. They explore the capabilities of LLM as a general problem solver. 2. Agent system overview: The documents provide an overview of the components that make up a LLM-powered autonomous agent system, including planning, memory, and tool use. Each component is explained in detail, highlighting its role in enhancing the agent\'s capabilities. 3. Planning: The documents discuss how the agent breaks down large tasks into smaller subgoals and utilizes self-reflection to improve the quality of its actions and results. 4. Memory: The documents explain the importance of both short-term and long-term memory in an agent system. Short-term memory is utilized for in-context learning, while long-term memory allows the agent to retain and recall information over extended periods. 5. Tool use: The documents highlight the agent\'s ability to call external APIs for additional information and resources that may be missing from its pre-trained model weights. This includes accessing current information, executing code, and retrieving proprietary information. 6. Case studies and proof-of-concept examples: The documents provide examples of how LLM-powered autonomous agents can be applied in various domains, such as scientific discovery and generative agent simulations. These case studies serve as examples of the capabilities and potential applications of such agents. 7. Challenges: The documents acknowledge the challenges associated with building and utilizing LLM-powered autonomous agents, although specific challenges are not mentioned in the given set of documents. 8. Citation and references: The documents include a citation and reference section, indicating that the information presented is based on existing research and sources. Overall, the main themes in the provided documents revolve around LLM-powered autonomous agents, their components and capabilities, planning, memory, tool use, case studies, and challenges. ``` ### Go deeper\u200b **Customization** - As shown above, you can customize the LLMs and prompts for map and reduce stages. **Real-world use-case** - See [this blog post]( case-study on analyzing user interactions (questions about LangChain documentation)! - The blog post and associated [repo]( also introduce clustering as a means of summarization. - This opens up a third path beyond the `stuff` or `map-reduce` approaches that is worth considering. ![Image description](/assets/images/summarization_use_case_3-4247e4011d0a52901288005bfaf20dc1.png) ## Option 3. Refine\u200b [Refine](/docs/modules/chains/document/refine) is similar to map-reduce: The refine documents chain constructs a response by looping over the input documents and iteratively updating its answer. For each document, it passes all non-document inputs, the current document, and the latest intermediate answer to an LLM chain to get a new answer. This can be easily run with the `chain_type=""refine""` specified. ```python chain = load_summarize_chain(llm, chain_type=""refine"") chain.run(split_docs) ``` ```text \'The article explores the concept of building autonomous agents powered by large language models (LLMs) and their potential as problem solvers. It discusses different approaches to task decomposition, the integration of self-reflection into LLM-based agents, and the use of external classical planners for long-horizon planning. The new context introduces the Chain of Hindsight (CoH) approach and Algorithm Distillation (AD) for training models to produce better outputs. It also discusses different types of memory and the use of external memory for fast retrieval. The article explores the concept of tool use and introduces the MRKL system and experiments on fine-tuning LLMs to use external tools. It introduces HuggingGPT, a framework that uses ChatGPT as a task planner, and discusses the challenges of using LLM-powered agents in real-world scenarios. The article concludes with case studies on scientific discovery agents and the use of LLM-powered agents in anticancer drug discovery. It also introduces the concept of generative agents that combine LLM with memory, planning, and reflection mechanisms. The conversation samples provided discuss the implementation of a game architecture and the challenges in building LLM-centered agents. The article provides references to related research papers and resources for further exploration.\' ``` It\'s also possible to supply a prompt and return intermediate steps. ```python prompt_template = """"""Write a concise summary of the following: {text} CONCISE SUMMARY:"""""" prompt = PromptTemplate.from_template(prompt_template) refine_template = ( ""Your job is to produce a final summary\\n"" ""We have provided an existing summary up to a certain point: {existing_answer}\\n"" ""We have the opportunity to refine the existing summary"" ""(only if needed) with some more context below.\\n"" ""------------\\n"" ""{text}\\n"" ""------------\\n"" ""Given the new context, refine the original summary in Italian"" ""If the context isn\'t useful, return the original summary."" ) refine_prompt = PromptTemplate.from_template(refine_template) chain = load_summarize_chain( llm=llm, chain_type=""refine"", question_prompt=prompt, refine_prompt=refine_prompt, return_intermediate_steps=True, input_key=""input_documents"", output_key=""output_text"", ) result = chain({""input_documents"": split_docs}, return_only_outputs=True) ``` ```python print(result[""output_text""]) ``` ```text Il presente articolo discute il concetto di costruire agenti autonomi utilizzando LLM (large language model) come controller principale. Esplora i diversi componenti di un sistema di agenti alimentato da LLM, tra cui la pianificazione, la memoria e l\'uso degli strumenti. Dimostrazioni di concetto come AutoGPT mostrano il potenziale di LLM come risolutore generale di problemi. Approcci come Chain of Thought, Tree of Thoughts, LLM+P, ReAct e Reflexion consentono agli agenti autonomi di pianificare, riflettere su se stessi e migliorarsi iterativamente. Tuttavia, ci sono sfide da affrontare, come la limitata capacit di contesto che limita l\'inclusione di informazioni storiche dettagliate e la difficolt di pianificazione a lungo termine e decomposizione delle attivit. Inoltre, l\'affidabilit dell\'interfaccia di linguaggio naturale tra LLM e componenti esterni come la memoria e gli strumenti  incerta, poich i LLM possono commettere errori di formattazione e mostrare comportamenti ribelli. Nonostante ci, il sistema AutoGPT viene menzionato come esempio di dimostrazione di concetto che utilizza LLM come controller principale per agenti autonomi. Questo articolo fa riferimento a diverse fonti che esplorano approcci e applicazioni specifiche di LLM nell\'ambito degli agenti autonomi. ``` ```python print(""\\n\\n"".join(result[""intermediate_steps""][:3])) ``` ```text This article discusses the concept of building autonomous agents using LLM (large language model) as the core controller. The article explores the different components of an LLM-powered agent system, including planning, memory, and tool use. It also provides examples of proof-of-concept demos and highlights the potential of LLM as a general problem solver. Questo articolo discute del concetto di costruire agenti autonomi utilizzando LLM (large language model) come controller principale. L\'articolo esplora i diversi componenti di un sistema di agenti alimentato da LLM, inclusa la pianificazione, la memoria e l\'uso degli strumenti. Vengono forniti anche esempi di dimostrazioni di proof-of-concept e si evidenzia il potenziale di LLM come risolutore generale di problemi. Inoltre, vengono presentati approcci come Chain of Thought, Tree of Thoughts, LLM+P, ReAct e Reflexion che consentono agli agenti autonomi di pianificare, riflettere su se stessi e migliorare iterativamente. Questo articolo discute del concetto di costruire agenti autonomi utilizzando LLM (large language model) come controller principale. L\'articolo esplora i diversi componenti di un sistema di agenti alimentato da LLM, inclusa la pianificazione, la memoria e l\'uso degli strumenti. Vengono forniti anche esempi di dimostrazioni di proof-of-concept e si evidenzia il potenziale di LLM come risolutore generale di problemi. Inoltre, vengono presentati approcci come Chain of Thought, Tree of Thoughts, LLM+P, ReAct e Reflexion che consentono agli agenti autonomi di pianificare, riflettere su se stessi e migliorare iterativamente. Il nuovo contesto riguarda l\'approccio Chain of Hindsight (CoH) che permette al modello di migliorare autonomamente i propri output attraverso un processo di apprendimento supervisionato. Viene anche presentato l\'approccio Algorithm Distillation (AD) che applica lo stesso concetto alle traiettorie di apprendimento per compiti di reinforcement learning. ``` ## Splitting and summarizing in a single chain\u200b For convenience, we can wrap both the text splitting of our long document and summarizing in a single `AnalyzeDocumentsChain`. ```python from langchain.chains import AnalyzeDocumentChain summarize_document_chain = AnalyzeDocumentChain( combine_docs_chain=chain, text_splitter=text_splitter ) summarize_document_chain.run(docs[0]) ``` ```text --------------------------------------------------------------------------- ValueError Traceback (most recent call last) Cell In[17], line 4 1 from langchain.chains import AnalyzeDocumentChain 3 summarize_document_chain = AnalyzeDocumentChain(combine_docs_chain=chain, text_splitter=text_splitter) ----> 4 summarize_document_chain.run(docs[0]) File ~/langchain/libs/langchain/langchain/chains/base.py:496, in Chain.run(self, callbacks, tags, metadata, *args, **kwargs) 459 """"""Convenience method for executing chain. 460 461 The main difference between this method and `Chain.__call__` is that this (...) 493 # -> ""The temperature in Boise is..."" 494 """""" 495 # Run at start to make sure this is possible/defined --> 496 _output_key = self._run_output_key 498 if args and not kwargs: 499 if len(args) != 1: File ~/langchain/libs/langchain/langchain/chains/base.py:445, in Chain._run_output_key(self) 442 @property 443 def _run_output_key(self) -> str: 444 if len(self.output_keys) != 1: --> 445 raise ValueError( 446 f""`run` not supported when there is not exactly "" 447 f""one output key. Got {self.output_keys}."" 448 ) 449 return self.output_keys[0] ValueError: `run` not supported when there is not exactly one output key. Got [\'output_text\', \'intermediate_steps\']. ``` - [Use case](#use-case) - [Overview](#overview) - [Quickstart](#quickstart) - [Option 1. Stuff](#option-1-stuff)- [Go deeper](#go-deeper) - [Option 2. Map-Reduce](#option-2-map-reduce)- [Go deeper](#go-deeper-1) - [Option 3. Refine](#option-3-refine) - [Splitting and summarizing in a single chain](#splitting-and-summarizing-in-a-single-chain)']","To add a tag when running the chain that LangSmith will capture, you can pass the `tags` parameter to the `run` method of the chain. Here's how you can do it:

```python
from langchain.chains.summarize import load_summarize_chain

# Load the summarize chain
chain = load_summarize_chain(llm, chain_type=""map_reduce"")

# Define your chunks
chunks = [...]

# Run the chain with a tag
result = chain.run(chunks, tags=[""your_custom_tag_here""])

# result will contain the output of the chain
```

In this example, replace `""your_custom_tag_here""` with the tag you want to add. This tag will be captured by LangSmith for tracking and analysis purposes.","all chains inherit from the runnable interface. So they support an invoke method. The invoke method takes the input and a config.
For the config you can pass in { 'tags': [ 'tag1' ] }.

chain.invoke( input, config={ 'tags': ['tags1'] }.

If you want to just tag that run (and not propagate the tags to child calls), you can pass the tags in as a list of strings to the call to `load_summarize_chain(llm, chain_type=""map_reduce"", tags=['tag1'])`.
For older methods in the LangChain API like run, __call__, etc. you can also add tags directly when calling, such as with `chain.run(""foo"", tags=['tag1])`.",0.5833333333041666,0.5,1.0,0.17798185213207657,0.24413145539906106
82,"Let's say I have a chain like:

model_call_1 = (
  RunnablePassthrough()
  | prompt
  | model_parser
)
# ...
chain = model_call_1 #| { ""attr"": model_call_2 } | model_call_3
How can I print out the filled out prompts for each model call?","['Prompt + LLM | Prompt + LLM The most common and valuable composition is taking: `PromptTemplate` / `ChatPromptTemplate` -> `LLM` / `ChatModel` -> `OutputParser` Almost any other chains you build will use this building block. ## PromptTemplate + LLM The simplest composition is just combining a prompt and model to create a chain that takes user input, adds it to a prompt, passes it to a model, and returns the raw model output. Note, you can mix and match PromptTemplate/ChatPromptTemplates and LLMs/ChatModels as you like here. ```python from langchain.chat_models import ChatOpenAI from langchain.prompts import ChatPromptTemplate prompt = ChatPromptTemplate.from_template(""tell me a joke about {foo}"") model = ChatOpenAI() chain = prompt | model ``` ```python chain.invoke({""foo"": ""bears""}) ``` ```text AIMessage(content=""Why don\'t bears wear shoes?\\n\\nBecause they have bear feet!"", additional_kwargs={}, example=False) ``` Often times we want to attach kwargs that\'ll be passed to each model call. Here are a few examples of that: ### Attaching Stop Sequences ```python chain = prompt | model.bind(stop=[""\\n""]) ``` ```python chain.invoke({""foo"": ""bears""}) ``` ```text AIMessage(content=\'Why did the bear never wear shoes?\', additional_kwargs={}, example=False) ``` ### Attaching Function Call information ```python functions = [ { ""name"": ""joke"", ""description"": ""A joke"", ""parameters"": { ""type"": ""object"", ""properties"": { ""setup"": {""type"": ""string"", ""description"": ""The setup for the joke""}, ""punchline"": { ""type"": ""string"", ""description"": ""The punchline for the joke"", }, }, ""required"": [""setup"", ""punchline""], }, } ] chain = prompt | model.bind(function_call={""name"": ""joke""}, functions=functions) ``` ```python chain.invoke({""foo"": ""bears""}, config={}) ``` ```text AIMessage(content=\'\', additional_kwargs={\'function_call\': {\'name\': \'joke\', \'arguments\': \'{\\n ""setup"": ""Why don\\\'t bears wear shoes?"",\\n ""punchline"": ""Because they have bear feet!""\\n}\'}}, example=False) ``` ## PromptTemplate + LLM + OutputParser We can also add in an output parser to easily transform the raw LLM/ChatModel output into a more workable format ```python from langchain.schema.output_parser import StrOutputParser chain = prompt | model | StrOutputParser() ``` Notice that this now returns a string - a much more workable format for downstream tasks ```python chain.invoke({""foo"": ""bears""}) ``` ```text ""Why don\'t bears wear shoes?\\n\\nBecause they have bear feet!"" ``` ### Functions Output Parser When you specify the function to return, you may just want to parse that directly ```python from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser chain = ( prompt | model.bind(function_call={""name"": ""joke""}, functions=functions) | JsonOutputFunctionsParser() ) ``` ```python chain.invoke({""foo"": ""bears""}) ``` ```text {\'setup\': ""Why don\'t bears like fast food?"", \'punchline\': ""Because they can\'t catch it!""} ``` ```python from langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParser chain = ( prompt | model.bind(function_call={""name"": ""joke""}, functions=functions) | JsonKeyOutputFunctionsParser(key_name=""setup"") ) ``` ```python chain.invoke({""foo"": ""bears""}) ``` ```text ""Why don\'t bears wear shoes?"" ``` ## Simplifying input To make invocation even simpler, we can add a `RunnableMap` to take care of creating the prompt input dict for us: ```python from langchain.schema.runnable import RunnableMap, RunnablePassthrough map_ = RunnableMap(foo=RunnablePassthrough()) chain = ( map_ | prompt | model.bind(function_call={""name"": ""joke""}, functions=functions) | JsonKeyOutputFunctionsParser(key_name=""setup"") ) ``` ```python chain.invoke(""bears"") ``` ```text ""Why don\'t bears wear shoes?"" ``` Since we\'re composing our map with another Runnable, we can even use some syntactic sugar and just use a dict: ```python chain = ( {""foo"": RunnablePassthrough()} | prompt | model.bind(function_call={""name"": ""joke""}, functions=functions) | JsonKeyOutputFunctionsParser(key_name=""setup"") ) ``` ```python chain.invoke(""bears"") ``` ```text ""Why don\'t bears like fast food?"" ``` - [PromptTemplate + LLM](#prompttemplate--llm)- [Attaching Stop Sequences](#attaching-stop-sequences) - [Attaching Function Call information](#attaching-function-call-information) - [PromptTemplate + LLM + OutputParser](#prompttemplate--llm--outputparser)- [Functions Output Parser](#functions-output-parser) - [Simplifying input](#simplifying-input)', 'Bind runtime args | Bind runtime args Sometimes we want to invoke a Runnable within a Runnable sequence with constant arguments that are not part of the output of the preceding Runnable in the sequence, and which are not part of the user input. We can use `Runnable.bind()` to easily pass these arguments in. Suppose we have a simple prompt + model sequence: ```python from langchain.chat_models import ChatOpenAI from langchain.prompts import ChatPromptTemplate from langchain.schema import StrOutputParser from langchain.schema.runnable import RunnablePassthrough ``` ```python prompt = ChatPromptTemplate.from_messages( [ ( ""system"", ""Write out the following equation using algebraic symbols then solve it. Use the format\\n\\nEQUATION:...\\nSOLUTION:...\\n\\n"", ), (""human"", ""{equation_statement}""), ] ) model = ChatOpenAI(temperature=0) runnable = ( {""equation_statement"": RunnablePassthrough()} | prompt | model | StrOutputParser() ) print(runnable.invoke(""x raised to the third plus seven equals 12"")) ``` ```text EQUATION: x^3 + 7 = 12 SOLUTION: Subtracting 7 from both sides of the equation, we get: x^3 = 12 - 7 x^3 = 5 Taking the cube root of both sides, we get: x = 5 Therefore, the solution to the equation x^3 + 7 = 12 is x = 5. ``` and want to call the model with certain `stop` words: ```python runnable = ( {""equation_statement"": RunnablePassthrough()} | prompt | model.bind(stop=""SOLUTION"") | StrOutputParser() ) print(runnable.invoke(""x raised to the third plus seven equals 12"")) ``` ```text EQUATION: x^3 + 7 = 12 ``` ## Attaching OpenAI functions One particularly useful application of binding is to attach OpenAI functions to a compatible OpenAI model: ```python function = { ""name"": ""solver"", ""description"": ""Formulates and solves an equation"", ""parameters"": { ""type"": ""object"", ""properties"": { ""equation"": { ""type"": ""string"", ""description"": ""The algebraic expression of the equation"", }, ""solution"": { ""type"": ""string"", ""description"": ""The solution to the equation"", }, }, ""required"": [""equation"", ""solution""], }, } ``` ```python # Need gpt-4 to solve this one correctly prompt = ChatPromptTemplate.from_messages( [ ( ""system"", ""Write out the following equation using algebraic symbols then solve it."", ), (""human"", ""{equation_statement}""), ] ) model = ChatOpenAI(model=""gpt-4"", temperature=0).bind( function_call={""name"": ""solver""}, functions=[function] ) runnable = {""equation_statement"": RunnablePassthrough()} | prompt | model runnable.invoke(""x raised to the third plus seven equals 12"") ``` ```text AIMessage(content=\'\', additional_kwargs={\'function_call\': {\'name\': \'solver\', \'arguments\': \'{\\n""equation"": ""x^3 + 7 = 12"",\\n""solution"": ""x = 5""\\n}\'}}, example=False) ``` ## Attaching OpenAI tools ```python tools = [ { ""type"": ""function"", ""function"": { ""name"": ""get_current_weather"", ""description"": ""Get the current weather in a given location"", ""parameters"": { ""type"": ""object"", ""properties"": { ""location"": { ""type"": ""string"", ""description"": ""The city and state, e.g. San Francisco, CA"", }, ""unit"": {""type"": ""string"", ""enum"": [""celsius"", ""fahrenheit""]}, }, ""required"": [""location""], }, }, } ] ``` ```python model = ChatOpenAI(model=""gpt-3.5-turbo-1106"").bind(tools=tools) model.invoke(""What\'s the weather in SF, NYC and LA?"") ``` ```text AIMessage(content=\'\', additional_kwargs={\'tool_calls\': [{\'id\': \'call_zHN0ZHwrxM7nZDdqTp6dkPko\', \'function\': {\'arguments\': \'{""location"": ""San Francisco, CA"", ""unit"": ""celsius""}\', \'name\': \'get_current_weather\'}, \'type\': \'function\'}, {\'id\': \'call_aqdMm9HBSlFW9c9rqxTa7eQv\', \'function\': {\'arguments\': \'{""location"": ""New York, NY"", ""unit"": ""celsius""}\', \'name\': \'get_current_weather\'}, \'type\': \'function\'}, {\'id\': \'call_cx8E567zcLzYV2WSWVgO63f1\', \'function\': {\'arguments\': \'{""location"": ""Los Angeles, CA"", ""unit"": ""celsius""}\', \'name\': \'get_current_weather\'}, \'type\': \'function\'}]}) ``` - [Attaching OpenAI functions](#attaching-openai-functions) - [Attaching OpenAI tools](#attaching-openai-tools)', 'Retrieval-augmented generation (RAG) | Retrieval-augmented generation (RAG) []( ## Overview ### What is RAG? RAG is a technique for augmenting LLM knowledge with additional, often private or real-time, data. LLMs can reason about wide-ranging topics, but their knowledge is limited to the public data up to a specific point in time that they were trained on. If you want to build AI applications that can reason about private data or data introduced after a model\'s cutoff date, you need to augment the knowledge of the model with the specific information it needs. The process of bringing the appropriate information and inserting it into the model prompt is known as Retrieval Augmented Generation (RAG). ### What\'s in this guide? LangChain has a number of components specifically designed to help build RAG applications. To familiarize ourselves with these, we\'ll build a simple question-answering application over a text data source. Specifically, we\'ll build a QA bot over the [LLM Powered Autonomous Agents]( blog post by Lilian Weng. Along the way we\'ll go over a typical QA architecture, discuss the relevant LangChain components, and highlight additional resources for more advanced QA techniques. We\'ll also see how LangSmith can help us trace and understand our application. LangSmith will become increasingly helpful as our application grows in complexity. **Note** Here we focus on RAG for unstructured data. Two RAG use cases which we cover elsewhere are: - [QA over structured data](/docs/use_cases/qa_structured/sql) (e.g., SQL) - [QA over code](/docs/use_cases/question_answering/code_understanding) (e.g., Python) ## Architecture A typical RAG application has two main components: **Indexing**: a pipeline for ingesting data from a source and indexing it. _This usually happen offline._ **Retrieval and generation**: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model. The most common full sequence from raw data to answer looks like: #### Indexing 1. **Load**: First we need to load our data. We\'ll use [DocumentLoaders](/docs/modules/data_connection/document_loaders/) for this. 2. **Split**: [Text splitters](/docs/modules/data_connection/document_transformers/) break large `Documents` into smaller chunks. This is useful both for indexing data and for passing it in to a model, since large chunks are harder to search over and won\'t in a model\'s finite context window. 3. **Store**: We need somewhere to store and index our splits, so that they can later be searched over. This is often done using a [VectorStore](/docs/modules/data_connection/vectorstores/) and [Embeddings](/docs/modules/data_connection/text_embedding/) model. #### Retrieval and generation 1. **Retrieve**: Given a user input, relevant splits are retrieved from storage using a [Retriever](/docs/modules/data_connection/retrievers/). 2. **Generate**: A [ChatModel](/docs/modules/model_io/chat_models) / [LLM](/docs/modules/model_io/llms/) produces an answer using a prompt that includes the question and the retrieved data ![flow.jpeg](/assets/images/qa_flow-9fbd91de9282eb806bda1c6db501ecec.jpeg) ## Setup ### Dependencies We\'ll use an OpenAI chat model and embeddings and a Chroma vector store in this walkthrough, but everything shown here works with any [ChatModel](/docs/integrations/chat/) or [LLM](/docs/integrations/llms/), [Embeddings](/docs/integrations/text_embedding/), and [VectorStore](/docs/integrations/vectorstores/) or [Retriever](/docs/integrations/retrievers). We\'ll use the following packages: ```bash pip install -U langchain openai chromadb langchainhub bs4 ``` We need to set environment variable `OPENAI_API_KEY`, which can be done directly or loaded from a `.env` file like so: ```python import getpass import os os.environ[""OPENAI_API_KEY""] = getpass.getpass() # import dotenv # dotenv.load_dotenv() ``` ### LangSmith Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with [LangSmith]( Note that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces: ```python os.environ[""LANGCHAIN_TRACING_V2""] = ""true"" os.environ[""LANGCHAIN_API_KEY""] = getpass.getpass() ``` ## Quickstart Suppose we want to build a QA app over the [LLM Powered Autonomous Agents]( blog post by Lilian Weng. We can create a simple pipeline for this in ~20 lines of code: ```python import bs4 from langchain import hub from langchain.chat_models import ChatOpenAI from langchain.document_loaders import WebBaseLoader from langchain.embeddings import OpenAIEmbeddings from langchain.schema import StrOutputParser from langchain.schema.runnable import RunnablePassthrough from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain.vectorstores import Chroma ``` ```python loader = WebBaseLoader( web_paths=("" bs_kwargs=dict( parse_only=bs4.SoupStrainer( class_=(""post-content"", ""post-title"", ""post-header"") ) ), ) docs = loader.load() text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200) splits = text_splitter.split_documents(docs) vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings()) retriever = vectorstore.as_retriever() prompt = hub.pull(""rlm/rag-prompt"") llm = ChatOpenAI(model_name=""gpt-3.5-turbo"", temperature=0) def format_docs(docs): return ""\\n\\n"".join(doc.page_content for doc in docs) rag_chain = ( {""context"": retriever | format_docs, ""question"": RunnablePassthrough()} | prompt | llm | StrOutputParser() ) ``` ```python rag_chain.invoke(""What is Task Decomposition?"") ``` ```text \'Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It can be done through prompting techniques like Chain of Thought or Tree of Thoughts, or by using task-specific instructions or human inputs. Task decomposition helps agents plan ahead and manage complicated tasks more effectively.\' ``` ```python # cleanup vectorstore.delete_collection() ``` Check out the [LangSmith trace]( ## Detailed walkthrough Let\'s go through the above code step-by-step to really understand what\'s going on. ## Step 1. Load We need to first load the blog post contents. We can use `DocumentLoader`s for this, which are objects that load in data from a source as `Documents`. A `Document` is an object with `page_content` (str) and `metadata` (dict) attributes. In this case we\'ll use the `WebBaseLoader`, which uses `urllib` and `BeautifulSoup` to load and parse the passed in web urls, returning one `Document` per url. We can customize the html -> text parsing by passing in parameters to the `BeautifulSoup` parser via `bs_kwargs` (see [BeautifulSoup docs]( In this case only HTML tags with class ""post-content"", ""post-title"", or ""post-header"" are relevant, so we\'ll remove all others. ```python from langchain.document_loaders import WebBaseLoader loader = WebBaseLoader( web_paths=("" bs_kwargs={ ""parse_only"": bs4.SoupStrainer( class_=(""post-content"", ""post-title"", ""post-header"") ) }, ) docs = loader.load() ``` ```python len(docs[0].page_content) ``` ```text 42824 ``` ```python print(docs[0].page_content[:500]) ``` ```text LLM Powered Autonomous Agents Date: June 23, 2023 | Estimated Reading Time: 31 min | Author: Lilian Weng Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver. Agent System Overview# In ``` ### Go deeper `DocumentLoader`: Object that load data from a source as `Documents`. - [Docs](/docs/modules/data_connection/document_loaders/): Further documentation on how to use `DocumentLoader`s. - [Integrations](/docs/integrations/document_loaders/): Find the relevant `DocumentLoader` integration (of the > 160 of them) for your use case. ## Step 2. Split Our loaded document is over 42k characters long. This is too long to fit in the context window of many models. And even for those models that could fit the full post in their context window, empirically models struggle to find the relevant context in very long prompts. So we\'ll split the `Document` into chunks for embedding and vector storage. This should help us retrieve only the most relevant bits of the blog post at run time. In this case we\'ll split our documents into chunks of 1000 characters with 200 characters of overlap between chunks. The overlap helps mitigate the possibility of separating a statement from important context related to it. We use the `RecursiveCharacterTextSplitter`, which will (recursively) split the document using common separators (like new lines) until each chunk is the appropriate size. ```python from langchain.text_splitter import RecursiveCharacterTextSplitter text_splitter = RecursiveCharacterTextSplitter( chunk_size=1000, chunk_overlap=200, add_start_index=True ) all_splits = text_splitter.split_documents(docs) ``` ```python len(all_splits) ``` ```text 66 ``` ```python len(all_splits[0].page_content) ``` ```text 969 ``` ```python all_splits[10].metadata ``` ```text {\'source\': \' \'start_index\': 7056} ``` ### Go deeper `DocumentSplitter`: Object that splits a list of `Document`s into smaller chunks. Subclass of `DocumentTransformer`s. - Explore `Context-aware splitters`, which keep the location (""context"") of each split in the original `Document`:- [Markdown files](/docs/use_cases/question_answering/document-context-aware-QA) - [Code (py or js)](/docs/use_cases/question_answering/docs/integrations/document_loaders/source_code) - [Scientific papers](/docs/integrations/document_loaders/grobid) `DocumentTransformer`: Object that performs a transformation on a list of `Document`s. - [Docs](/docs/modules/data_connection/document_transformers/): Further documentation on how to use `DocumentTransformer`s - [Integrations](/docs/integrations/document_transformers/) ## Step 3. Store Now that we\'ve got 66 text chunks in memory, we need to store and index them so that we can search them later in our RAG app. The most common way to do this is to embed the contents of each document split and upload those embeddings to a vector store. Then, when we want to search over our splits, we take the search query, embed it as well, and perform some sort of ""similarity"" search to identify the stored splits with the most similar embeddings to our query embedding. The simplest similarity measure is cosine similarity we measure the cosine of the angle between each pair of embeddings (which are just very high dimensional vectors). We can embed and store all of our document splits in a single command using the `Chroma` vector store and `OpenAIEmbeddings` model. ```python from langchain.embeddings import OpenAIEmbeddings from langchain.vectorstores import Chroma vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings()) ``` ### Go deeper `Embeddings`: Wrapper around a text embedding model, used for converting text to embeddings. - [Docs](/docs/modules/data_connection/text_embedding): Further documentation on the interface. - [Integrations](/docs/integrations/text_embedding/): Browse the > 30 text embedding integrations `VectorStore`: Wrapper around a vector database, used for storing and querying embeddings. - [Docs](/docs/modules/data_connection/vectorstores/): Further documentation on the interface. - [Integrations](/docs/integrations/vectorstores/): Browse the > 40 `VectorStore` integrations. This completes the **Indexing** portion of the pipeline. At this point we have an query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question: ## Step 4. Retrieve Now let\'s write the actual application logic. We want to create a simple application that let\'s the user ask a question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and finally returns an answer. LangChain defines a `Retriever` interface which wraps an index that can return relevant documents given a string query. All retrievers implement a common method `get_relevant_documents()` (and its asynchronous variant `aget_relevant_documents()`). The most common type of `Retriever` is the `VectorStoreRetriever`, which uses the similarity search capabilities of a vector store to facillitate retrieval. Any `VectorStore` can easily be turned into a `Retriever`: ```python retriever = vectorstore.as_retriever(search_type=""similarity"", search_kwargs={""k"": 6}) ``` ```python retrieved_docs = retriever.get_relevant_documents( ""What are the approaches to Task Decomposition?"" ) ``` ```python len(retrieved_docs) ``` ```text 6 ``` ```python print(retrieved_docs[0].page_content) ``` ```text Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote. Task decomposition can be done (1) by LLM with simple prompting like ""Steps for XYZ.\\n1."", ""What are the subgoals for achieving XYZ?"", (2) by using task-specific instructions; e.g. ""Write a story outline."" for writing a novel, or (3) with human inputs. ``` ### Go deeper Vector stores are commonly used for retrieval, but there are plenty of other ways to do retrieval. `Retriever`: An object that returns `Document`s given a text query - [Docs](/docs/modules/data_connection/retrievers/): Further documentation on the interface and built-in retrieval techniques. Some of which include:- `MultiQueryRetriever` [generates variants of the input question](/docs/modules/data_connection/retrievers/MultiQueryRetriever) to improve retrieval hit rate. - `MultiVectorRetriever` (diagram below) instead generates [variants of the embeddings](/docs/modules/data_connection/retrievers/multi_vector), also in order to improve retrieval hit rate. - `Max marginal relevance` selects for [relevance and diversity]( among the retrieved documents to avoid passing in duplicate context. - Documents can be filtered during vector store retrieval using [metadata filters](/docs/use_cases/question_answering/document-context-aware-QA). - [Integrations](/docs/integrations/retrievers/): Integrations with retrieval services. ## Step 5. Generate Let\'s put it all together into a chain that takes a question, retrieves relevant documents, constructs a prompt, passes that to a model, and parses the output. We\'ll use the gpt-3.5-turbo OpenAI chat model, but any LangChain `LLM` or `ChatModel` could be substituted in. ```python from langchain.chat_models import ChatOpenAI llm = ChatOpenAI(model_name=""gpt-3.5-turbo"", temperature=0) ``` We\'ll use a prompt for RAG that is checked into the LangChain prompt hub ([here]( ```python from langchain import hub prompt = hub.pull(""rlm/rag-prompt"") ``` ```python print( prompt.invoke( {""context"": ""filler context"", ""question"": ""filler question""} ).to_string() ) ``` ```text Human: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise. Question: filler question Context: filler context Answer: ``` We\'ll use the [LCEL Runnable]( protocol to define the chain, allowing us to - pipe together components and functions in a transparent way - automatically trace our chain in LangSmith - get streaming, async, and batched calling out of the box ```python from langchain.schema import StrOutputParser from langchain.schema.runnable import RunnablePassthrough def format_docs(docs): return ""\\n\\n"".join(doc.page_content for doc in docs) rag_chain = ( {""context"": retriever | format_docs, ""question"": RunnablePassthrough()} | prompt | llm | StrOutputParser() ) ``` ```python for chunk in rag_chain.stream(""What is Task Decomposition?""): print(chunk, end="""", flush=True) ``` ```text Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It can be done through methods like Chain of Thought (CoT) or Tree of Thoughts, which involve dividing the task into manageable subtasks and exploring multiple reasoning possibilities at each step. Task decomposition can be performed by AI models with prompting, task-specific instructions, or human inputs. ``` Check out the [LangSmith trace]( ### Go deeper #### Choosing LLMs `ChatModel`: An LLM-backed chat model wrapper. Takes in a sequence of messages and returns a message. - [Docs](/docs/modules/model_io/chat/) - [Integrations](/docs/integrations/chat/): Explore over 25 `ChatModel` integrations. `LLM`: A text-in-text-out LLM. Takes in a string and returns a string. - [Docs](/docs/modules/model_io/llms) - [Integrations](/docs/integrations/llms): Explore over 75 `LLM` integrations. See a guide on RAG with locally-running models [here](/docs/modules/use_cases/question_answering/local_retrieval_qa). #### Customizing the prompt As shown above, we can load prompts (e.g., [this RAG prompt]( from the prompt hub. The prompt can also be easily customized: ```python from langchain.prompts import PromptTemplate template = """"""Use the following pieces of context to answer the question at the end. If you don\'t know the answer, just say that you don\'t know, don\'t try to make up an answer. Use three sentences maximum and keep the answer as concise as possible. Always say ""thanks for asking!"" at the end of the answer. {context} Question: {question} Helpful Answer:"""""" rag_prompt_custom = PromptTemplate.from_template(template) rag_chain = ( {""context"": retriever | format_docs, ""question"": RunnablePassthrough()} | rag_prompt_custom | llm | StrOutputParser() ) rag_chain.invoke(""What is Task Decomposition?"") ``` ```text \'Task decomposition is the process of breaking down a complex task into smaller and simpler steps. It can be done through techniques like Chain of Thought (CoT) or Tree of Thoughts, which involve dividing the problem into multiple thought steps and generating multiple thoughts per step. Task decomposition helps in enhancing model performance and understanding the thinking process of the model. Thanks for asking!\' ``` Check out the [LangSmith trace]( ### Adding sources With LCEL it\'s easy to return the retrieved documents or certain source metadata from the documents: ```python from operator import itemgetter from langchain.schema.runnable import RunnableMap rag_chain_from_docs = ( { ""context"": lambda input: format_docs(input[""documents""]), ""question"": itemgetter(""question""), } | rag_prompt_custom | llm | StrOutputParser() ) rag_chain_with_source = RunnableMap( {""documents"": retriever, ""question"": RunnablePassthrough()} ) | { ""documents"": lambda input: [doc.metadata for doc in input[""documents""]], ""answer"": rag_chain_from_docs, } rag_chain_with_source.invoke(""What is Task Decomposition"") ``` ```text {\'documents\': [{\'source\': \' \'start_index\': 1585}, {\'source\': \' \'start_index\': 2192}, {\'source\': \' \'start_index\': 17804}, {\'source\': \' \'start_index\': 17414}, {\'source\': \' \'start_index\': 29630}, {\'source\': \' \'start_index\': 19373}], \'answer\': \'Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It involves transforming big tasks into multiple manageable tasks, allowing for a more systematic and organized approach to problem-solving. Thanks for asking!\'} ``` Check out the [LangSmith trace]( Adding memory Suppose we want to create a stateful application that remembers past user inputs. There are two main things we need to do to support this. 1. Add a messages placeholder to our chain which allows us to pass in historical messages 2. Add a chain that takes the latest user query and reformulates it in the context of the chat history into a standalone question that can be passed to our retriever. Let\'s start with 2. We can build a ""condense question"" chain that looks something like this: ```python from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder condense_q_system_prompt = """"""Given a chat history and the latest user question \\ which might reference the chat history, formulate a standalone question \\ which can be understood without the chat history. Do NOT answer the question, \\ just reformulate it if needed and otherwise return it as is."""""" condense_q_prompt = ChatPromptTemplate.from_messages( [ (""system"", condense_q_system_prompt), MessagesPlaceholder(variable_name=""chat_history""), (""human"", ""{question}""), ] ) condense_q_chain = condense_q_prompt | llm | StrOutputParser() ``` ```python from langchain.schema.messages import AIMessage, HumanMessage condense_q_chain.invoke( { ""chat_history"": [ HumanMessage(content=""What does LLM stand for?""), AIMessage(content=""Large language model""), ], ""question"": ""What is meant by large"", } ) ``` ```text \'What is the definition of ""large"" in the context of a language model?\' ``` ```python condense_q_chain.invoke( { ""chat_history"": [ HumanMessage(content=""What does LLM stand for?""), AIMessage(content=""Large language model""), ], ""question"": ""How do transformers work"", } ) ``` ```text \'How do transformer models function?\' ``` And now we can build our full QA chain. Notice we add some routing functionality to only run the ""condense question chain"" when our chat history isn\'t empty. ```python qa_system_prompt = """"""You are an assistant for question-answering tasks. \\ Use the following pieces of retrieved context to answer the question. \\ If you don\'t know the answer, just say that you don\'t know. \\ Use three sentences maximum and keep the answer concise.\\ {context}"""""" qa_prompt = ChatPromptTemplate.from_messages( [ (""system"", qa_system_prompt), MessagesPlaceholder(variable_name=""chat_history""), (""human"", ""{question}""), ] ) def condense_question(input: dict): if input.get(""chat_history""): return condense_q_chain else: return input[""question""] rag_chain = ( RunnablePassthrough.assign(context=condense_question | retriever | format_docs) | qa_prompt | llm ) ``` ```python chat_history = [] question = ""What is Task Decomposition?"" ai_msg = rag_chain.invoke({""question"": question, ""chat_history"": chat_history}) chat_history.extend([HumanMessage(content=question), ai_msg]) second_question = ""What are common ways of doing it?"" rag_chain.invoke({""question"": second_question, ""chat_history"": chat_history}) ``` ```text AIMessage(content=\'Common ways of task decomposition include:\\n\\n1. Using Chain of Thought (CoT): CoT is a prompting technique that instructs the model to ""think step by step"" and decompose complex tasks into smaller and simpler steps. It utilizes more test-time computation and sheds light on the model\\\'s thinking process.\\n\\n2. Prompting with LLM: Language Model (LLM) can be used to prompt the model with simple instructions like ""Steps for XYZ"" or ""What are the subgoals for achieving XYZ?"" This allows the model to generate a sequence of subtasks or thought steps.\\n\\n3. Task-specific instructions: For certain tasks, task-specific instructions can be provided to guide the model in decomposing the task. For example, for writing a novel, the instruction ""Write a story outline"" can be given to break down the task into manageable steps.\\n\\n4. Human inputs: In some cases, human inputs can be used to assist in task decomposition. Humans can provide their expertise and knowledge to identify and break down complex tasks into smaller subtasks.\') ``` Check out the [LangSmith trace]( Here we\'ve gone over how to add chain logic for incorporating historical outputs. But how do we actually store and retrieve historical outputs for different sessions? For that check out the LCEL [How to add message history (memory)](/docs/expression_language/how_to/message_history) page. ## Next steps That\'s a lot of content we\'ve covered in a short amount of time. There\'s plenty of nuances, features, integrations, etc to explore in each of the above sections. Aside from the sources mentioned above, good next steps include: - Reading up on more advanced retrieval techniques in the [Retrievers](/docs/modules/data_connection/retrievers/) section. - Learning about the LangChain [Indexing API](/docs/modules/data_connection/indexing), which helps repeatedly sync data sources and vector stores without redundant computation or storage. - Exploring RAG [LangChain Templates](/docs/templates/#-advanced-retrieval), which are reference applications that can easily be deployed with [LangServe](/docs/langserve). - Learning about [evaluating RAG applications with LangSmith]( - [Overview](#overview)- [What is RAG?](#what-is-rag) - [What\'s in this guide?](#whats-in-this-guide) - [Architecture](#architecture) - [Setup](#setup)- [Dependencies](#dependencies) - [LangSmith](#langsmith) - [Quickstart](#quickstart) - [Detailed walkthrough](#detailed-walkthrough) - [Step 1. Load](#step-1-load)- [Go deeper](#go-deeper) - [Step 2. Split](#step-2-split)- [Go deeper](#go-deeper-1) - [Step 3. Store](#step-3-store)- [Go deeper](#go-deeper-2) - [Step 4. Retrieve](#step-4-retrieve)- [Go deeper](#go-deeper-3) - [Step 5. Generate](#step-5-generate)- [Go deeper](#go-deeper-4) - [Adding sources](#adding-sources) - [Adding memory](#adding-memory) - [Next steps](#next-steps)']","To print out the filled out prompts for each model call in your chain, you can use the `invoke` method with the `return_values=True` parameter. This will return the values passed to each component in the chain. Here's how you can do it:

```python
results = chain.invoke({""input"": ""your input here""}, return_values=True)

for i, result in enumerate(results):
    print(f""Model call {i+1} output: {result}"")
```

In this example, `results` will contain the output of each model call in the order they appear in the chain. By iterating over `results`, you can print out the filled out prompts for each model call.

If you want to access the specific prompts used, you might need to modify your chain to store or return the prompts explicitly. For example:

```python
def print_prompts(chain, input_data):
    for i, component in enumerate(chain):
        if isinstance(component, RunnablePassthrough):
            output = component.invoke(input_data, return_values=True)
            print(f""Model call {i+1} output: {output}"")
        else:
            print(f""Model call {i+1} prompt: {component.get_prompt()}"")

print_prompts(chain, {""input"": ""your input here""})
```

This will print the prompts for each model call if they are available. Adjust the logic based on how your chain and components are structured.","There are a few options. You can use set_debug(True) to print out each run to the console:
```
from langchain.globals import set_debug

set_debug(True)
```
This uses the ConsoleCallbackHandler. You can also trace with LangSmith by configuring your environment:

```
export LANGCHAIN_API_KEY=""Your API Key""
export LANGCHAIN_TRACING_V2=true
```",0.0,1.0,,0.039795734590198195,0.10727969348659003
83,"soooo, is it possible to pass any kwargs for similarity thresholds or K-top documents in the multivectorretriever as we do in normal retrievers?","['Fleet AI Libraries Context | Fleet AI Libraries Context The Fleet AI team is on a mission to embed the world\'s most important data. They\'ve started by embedding the top 1200 Python libraries to enable code generation with up-to-date knowledge. They\'ve been kind enough to share their embeddings of the [LangChain docs]( and [API reference]( Let\'s take a look at how we can use these embeddings to power a docs retrieval system and ultimately a simple code generating chain! ```bash pip install langchain fleet-context openai pandas faiss-cpu # faiss-gpu for CUDA supported GPU ``` ```python from operator import itemgetter from typing import Any, Optional, Type import pandas as pd from langchain.embeddings import OpenAIEmbeddings from langchain.retrievers import MultiVectorRetriever from langchain.schema import Document from langchain.schema.storage import BaseStore from langchain.schema.vectorstore import VectorStore from langchain.vectorstores import FAISS def load_fleet_retriever( df: pd.DataFrame, *, vectorstore_cls: Type[VectorStore] = FAISS, docstore: Optional[BaseStore] = None, **kwargs: Any, ): vectorstore = _populate_vectorstore(df, vectorstore_cls) if docstore is None: return vectorstore.as_retriever(**kwargs) else: _populate_docstore(df, docstore) return MultiVectorRetriever( vectorstore=vectorstore, docstore=docstore, id_key=""parent"", **kwargs ) def _populate_vectorstore( df: pd.DataFrame, vectorstore_cls: Type[VectorStore], ) -> VectorStore: if not hasattr(vectorstore_cls, ""from_embeddings""): raise ValueError( f""Incompatible vector store class {vectorstore_cls}."" ""Must implement `from_embeddings` class method."" ) texts_embeddings = [] metadatas = [] for _, row in df.iterrows(): texts_embeddings.append((row.metadata[""text""], row[""dense_embeddings""])) metadatas.append(row.metadata) return vectorstore_cls.from_embeddings( texts_embeddings, OpenAIEmbeddings(model=""text-embedding-ada-002""), metadatas=metadatas, ) def _populate_docstore(df: pd.DataFrame, docstore: BaseStore) -> None: parent_docs = [] df = df.copy() df[""parent""] = df.metadata.apply(itemgetter(""parent"")) for parent_id, group in df.groupby(""parent""): sorted_group = group.iloc[ group.metadata.apply(itemgetter(""section_index"")).argsort() ] text = """".join(sorted_group.metadata.apply(itemgetter(""text""))) metadata = { k: sorted_group.iloc[0].metadata[k] for k in (""title"", ""type"", ""url"") } text = metadata[""title""] + ""\\n"" + text metadata[""id""] = parent_id parent_docs.append(Document(page_content=text, metadata=metadata)) docstore.mset(((d.metadata[""id""], d) for d in parent_docs)) ``` ## Retriever chunks As part of their embedding process, the Fleet AI team first chunked long documents before embedding them. This means the vectors correspond to sections of pages in the LangChain docs, not entire pages. By default, when we spin up a retriever from these embeddings, we\'ll be retrieving these embedded chunks. We will be using Fleet Context\'s `download_embeddings()` to grab Langchain\'s documentation embeddings. You can view all supported libraries\' documentation at [ ```python from context import download_embeddings df = download_embeddings(""langchain"") vecstore_retriever = load_fleet_retriever(df) ``` ```python vecstore_retriever.get_relevant_documents(""How does the multi vector retriever work"") ``` ```text [Document(page_content=""# Vector store-backed retriever A vector store retriever is a retriever that uses a vector store to retrieve documents. It is a lightweight wrapper around the vector store class to make it conform to the retriever interface. It uses the search methods implemented by a vector store, like similarity search and MMR, to query the texts in the vector store. Once you construct a vector store, it\'s very easy to construct a retriever. Let\'s walk through an example."", metadata={\'id\': \'f509f20d-4c63-4a5a-a40a-5c4c0f099839\', \'library_id\': \'4506492b-70de-49f1-ba2e-d65bd7048a28\', \'page_id\': \'d78cf422-2dab-4860-80fe-d71a3619b02f\', \'parent\': \'c153ebd9-2611-4a43-9db6-daa1f5f214f6\', \'section_id\': \'\', \'section_index\': 0, \'text\': ""# Vector store-backed retriever A vector store retriever is a retriever that uses a vector store to retrieve documents. It is a lightweight wrapper around the vector store class to make it conform to the retriever interface. It uses the search methods implemented by a vector store, like similarity search and MMR, to query the texts in the vector store. Once you construct a vector store, it\'s very easy to construct a retriever. Let\'s walk through an example."", \'title\': \'Vector store-backed retriever | Langchain\', \'type\': None, \'url\': \' Document(page_content=\'# MultiVector Retriever It can often be beneficial to store multiple vectors per document. There are multiple use cases where this is beneficial. LangChain has a base `MultiVectorRetriever` which makes querying this type of setup easy. A lot of the complexity lies in how to create the multiple vectors per document. This notebook covers some of the common ways to create those vectors and use the `MultiVectorRetriever`. The methods to create multiple vectors per document include: - Smaller chunks: split a document into smaller chunks, and embed those (this is ParentDocumentRetriever). - Summary: create a summary for each document, embed that along with (or instead of) the document. - Hypothetical questions: create hypothetical questions that each document would be appropriate to answer, embed those along with (or instead of) the document. Note that this also enables another method of adding embeddings - manually.\', metadata={\'id\': \'e06e6bb5-127a-49f7-9511-247d279d0d83\', \'library_id\': \'4506492b-70de-49f1-ba2e-d65bd7048a28\', \'page_id\': \'7c7dd0de-25e0-4e1d-9237-cfd2674c29d4\', \'parent\': \'beec5531-16a7-453c-80ab-c5628e0236ce\', \'section_id\': \'\', \'section_index\': 0, \'text\': \'# MultiVector Retriever It can often be beneficial to store multiple vectors per document. There are multiple use cases where this is beneficial. LangChain has a base `MultiVectorRetriever` which makes querying this type of setup easy. A lot of the complexity lies in how to create the multiple vectors per document. This notebook covers some of the common ways to create those vectors and use the `MultiVectorRetriever`. The methods to create multiple vectors per document include: - Smaller chunks: split a document into smaller chunks, and embed those (this is ParentDocumentRetriever). - Summary: create a summary for each document, embed that along with (or instead of) the document. - Hypothetical questions: create hypothetical questions that each document would be appropriate to answer, embed those along with (or instead of) the document. Note that this also enables another method of adding embeddings - manually.\', \'title\': \'MultiVector Retriever | Langchain\', \'type\': None, \'url\': \' Document(page_content=\'# MultiQueryRetriever Distance-based vector database retrieval embeds (represents) queries in high-dimensional space and finds similar embedded documents based on ""distance"". But, retrieval may produce different results with subtle changes in query wording or if the embeddings do not capture the semantics of the data well. Prompt engineering / tuning is sometimes done to manually address these problems, but can be tedious. The `MultiQueryRetriever` automates the process of prompt tuning by using an LLM to generate multiple queries from different perspectives for a given user input query. For each query, it retrieves a set of relevant documents and takes the unique union across all queries to get a larger set of potentially relevant documents. By generating multiple perspectives on the same question, the `MultiQueryRetriever` might be able to overcome some of the limitations of the distance-based retrieval and get a richer set of results.\', metadata={\'id\': \'dc3b8e5b-a591-4a46-bfeb-a91b36affae1\', \'library_id\': \'4506492b-70de-49f1-ba2e-d65bd7048a28\', \'page_id\': \'31f80e84-c5db-4da2-939c-bccf519864a3\', \'parent\': \'f7c20633-6a60-4ca3-96b1-13fee66e321d\', \'section_id\': \'\', \'section_index\': 0, \'text\': \'# MultiQueryRetriever Distance-based vector database retrieval embeds (represents) queries in high-dimensional space and finds similar embedded documents based on ""distance"". But, retrieval may produce different results with subtle changes in query wording or if the embeddings do not capture the semantics of the data well. Prompt engineering / tuning is sometimes done to manually address these problems, but can be tedious. The `MultiQueryRetriever` automates the process of prompt tuning by using an LLM to generate multiple queries from different perspectives for a given user input query. For each query, it retrieves a set of relevant documents and takes the unique union across all queries to get a larger set of potentially relevant documents. By generating multiple perspectives on the same question, the `MultiQueryRetriever` might be able to overcome some of the limitations of the distance-based retrieval and get a richer set of results.\', \'title\': \'MultiQueryRetriever | Langchain\', \'type\': None, \'url\': \' Document(page_content=\'# `langchain.retrievers.multi_vector`.MultiVectorRetriever[](#langchain-retrievers-multi-vector-multivectorretriever) *class *langchain.retrievers.multi_vector.MultiVectorRetriever[[source]](../_modules/langchain/retrievers/multi_vector.html#MultiVectorRetriever)[](#langchain.retrievers.multi_vector.MultiVectorRetriever) # Examples using MultiVectorRetriever[](#langchain-retrievers-multi-vector-multivectorretriever) - [MultiVector Retriever]( metadata={\'id\': \'1f4ca702-35b8-44c0-b33b-18c09a1f787d\', \'library_id\': \'6254c672-7aa0-4233-b0a6-804bd273752b\', \'page_id\': \'87f5d1fb-a1c6-4080-9d2b-7b88794eb6bf\', \'parent\': \'1820c44d-7783-4846-a11c-106b18da015d\', \'section_id\': \'langchain-retrievers-multi-vector-multivectorretriever\', \'section_index\': 0, \'text\': \'# `langchain.retrievers.multi_vector`.MultiVectorRetriever[](#langchain-retrievers-multi-vector-multivectorretriever) *class *langchain.retrievers.multi_vector.MultiVectorRetriever[[source]](../_modules/langchain/retrievers/multi_vector.html#MultiVectorRetriever)[](#langchain.retrievers.multi_vector.MultiVectorRetriever) # Examples using MultiVectorRetriever[](#langchain-retrievers-multi-vector-multivectorretriever) - [MultiVector Retriever]( \'title\': \'langchain.retrievers.multi_vector.MultiVectorRetriever LangChain 0.0.322\', \'type\': None, \'url\': \' ``` ## Other packages You can download and use other embeddings from [this Dropbox link]( ## Retrieve parent docs The embeddings provided by Fleet AI contain metadata that indicates which embedding chunks correspond to the same original document page. If we\'d like we can use this information to retrieve whole parent documents, and not just embedded chunks. Under the hood, we\'ll use a MultiVectorRetriever and a BaseStore object to search for relevant chunks and then map them to their parent document. ```python from langchain.storage import InMemoryStore parent_retriever = load_fleet_retriever( "" docstore=InMemoryStore(), ) ``` ```python parent_retriever.get_relevant_documents(""How does the multi vector retriever work"") ``` ```text [Document(page_content=\'Vector store-backed retriever | Langchain\\n# Vector store-backed retriever A vector store retriever is a retriever that uses a vector store to retrieve documents. It is a lightweight wrapper around the vector store class to make it conform to the retriever interface. It uses the search methods implemented by a vector store, like similarity search and MMR, to query the texts in the vector store. Once you construct a vector store, it\\\'s very easy to construct a retriever. Let\\\'s walk through an example.Once you construct a vector store, it\\\'s very easy to construct a retriever. Let\\\'s walk through an example. ``` from langchain.document_loaders import TextLoaderloader = TextLoader(\\\'../../../state_of_the_union.txt\\\') ``` ``` from langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores import FAISSfrom langchain.embeddings import OpenAIEmbeddingsdocuments = loader.load()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)texts = text_splitter.split_documents(documents)embeddings = OpenAIEmbeddings()db = FAISS.from_documents(texts, embeddings) ``` ``` Exiting: Cleaning up .chroma directory ``` ``` retriever = db.as_retriever() ``` ``` docs = retriever.get_relevant_documents(""what did he say about ketanji brown jackson"") ``` ## Maximum marginal relevance retrieval[\\u200b](#maximum-marginal-relevance-retrieval) By default, the vector store retriever uses similarity search.If the underlying vector store supports maximum marginal relevance search, you can specify that as the search type. ``` retriever = db.as_retriever(search_type=""mmr"") ``` ``` docs = retriever.get_relevant_documents(""what did he say about ketanji brown jackson"") ``` ## Similarity score threshold retrieval[\\u200b](#similarity-score-threshold-retrieval) You can also a retrieval method that sets a similarity score threshold and only returns documents with a score above that threshold. ``` retriever = db.as_retriever(search_type=""similarity_score_threshold"", search_kwargs={""score_threshold"": .5}) ``` ``` docs = retriever.get_relevant_documents(""what did he say about ketanji brown jackson"") ``` ## Specifying top k[\\u200b](#specifying-top-k) You can also specify search kwargs like `k` to use when doing retrieval.``` retriever = db.as_retriever(search_kwargs={""k"": 1}) ``` ``` docs = retriever.get_relevant_documents(""what did he say about ketanji brown jackson"") ``` ``` len(docs) ``` ``` 1 ```\', metadata={\'title\': \'Vector store-backed retriever | Langchain\', \'type\': None, \'url\': \' \'id\': \'c153ebd9-2611-4a43-9db6-daa1f5f214f6\'}), Document(page_content=\'MultiVector Retriever | Langchain\\n# MultiVector Retriever It can often be beneficial to store multiple vectors per document. There are multiple use cases where this is beneficial. LangChain has a base `MultiVectorRetriever` which makes querying this type of setup easy. A lot of the complexity lies in how to create the multiple vectors per document. This notebook covers some of the common ways to create those vectors and use the `MultiVectorRetriever`. The methods to create multiple vectors per document include: - Smaller chunks: split a document into smaller chunks, and embed those (this is ParentDocumentRetriever). - Summary: create a summary for each document, embed that along with (or instead of) the document. - Hypothetical questions: create hypothetical questions that each document would be appropriate to answer, embed those along with (or instead of) the document. Note that this also enables another method of adding embeddings - manually.Note that this also enables another method of adding embeddings - manually. This is great because you can explicitly add questions or queries that should lead to a document being recovered, giving you more control. ``` from langchain.retrievers.multi_vector import MultiVectorRetriever ``` ``` from langchain.vectorstores import Chromafrom langchain.embeddings import OpenAIEmbeddingsfrom langchain.text_splitter import RecursiveCharacterTextSplitterfrom langchain.storage import InMemoryStorefrom langchain.document_loaders import TextLoader ``` ``` loaders = [ TextLoader(\\\'../../paul_graham_essay.txt\\\'), TextLoader(\\\'../../state_of_the_union.txt\\\'),]docs = []for l in loaders: docs.extend(l.load())text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000)docs = text_splitter.split_documents(docs) ``` ## Smaller chunks[\\u200b](#smaller-chunks) Often times it can be useful to retrieve larger chunks of information, but embed smaller chunks.This allows for embeddings to capture the semantic meaning as closely as possible, but for as much context as possible to be passed downstream. Note that this is what the `ParentDocumentRetriever` does. Here we show what is going on under the hood.``` # The vectorstore to use to index the child chunksvectorstore = Chroma( collection_name=""full_documents"", embedding_function=OpenAIEmbeddings())# The storage layer for the parent documentsstore = InMemoryStore()id_key = ""doc_id""# The retriever (empty to start)retriever = MultiVectorRetriever( vectorstore=vectorstore, docstore=store, id_key=id_key,)import uuiddoc_ids = [str(uuid.uuid4()) for _ in docs] ``` ``` # The splitter to use to create smaller chunkschild_text_splitter = RecursiveCharacterTextSplitter(chunk_size=400) ``` ``` sub_docs = []for i, doc in enumerate(docs): _id = doc_ids[i] _sub_docs = child_text_splitter.split_documents([doc]) for _doc in _sub_docs: _doc.metadata[id_key] = _id sub_docs.extend(_sub_docs) ``` ``` retriever.vectorstore.add_documents(sub_docs)retriever.docstore.mset(list(zip(doc_ids, docs))) ``` ``` # Vectorstore alone retrieves the small chunksretriever.vectorstore.similarity_search(""justice breyer"")[0] ``` ``` Document(page_content=\\\'Tonight, I\'d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyeran Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court.Justice Breyer, thank you for your service. \\\\n\\\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\\', metadata={\\\'doc_id\\\': \\\'10e9cbc0-4ba5-4d79-a09b-c033d1ba7b01\\\', \\\'source\\\': \\\'../../state_of_the_union.txt\\\'}) ``` ``` # Retriever returns larger chunkslen(retriever.get_relevant_documents(""justice breyer"")[0].page_content) ``` ``` 9874 ``` ## Summary[\\u200b](#summary) Oftentimes a summary may be able to distill more accurately what a chunk is about, leading to better retrieval. Here we show how to create summaries, and then embed those.``` from langchain.chat_models import ChatOpenAIfrom langchain.prompts import ChatPromptTemplatefrom langchain.schema.output_parser import StrOutputParserimport uuidfrom langchain.schema.document import Document ``` ``` chain = ( {""doc"": lambda x: x.page_content} | ChatPromptTemplate.from_template(""Summarize the following document:\\\\n\\\\n{doc}"") | ChatOpenAI(max_retries=0) | StrOutputParser()) ``` ``` summaries = chain.batch(docs, {""max_concurrency"": 5}) ``` ``` # The vectorstore to use to index the child chunksvectorstore = Chroma( collection_name=""summaries"", embedding_function=OpenAIEmbeddings())# The storage layer for the parent documentsstore = InMemoryStore()id_key = ""doc_id""# The retriever (empty to start)retriever = MultiVectorRetriever( vectorstore=vectorstore, docstore=store, id_key=id_key,)doc_ids = [str(uuid.uuid4()) for _ in docs] ``` ``` summary_docs = [Document(page_content=s,metadata={id_key: doc_ids[i]}) for i, s in enumerate(summaries)] ``` ``` retriever.vectorstore.add_documents(summary_docs)retriever.docstore.mset(list(zip(doc_ids, docs))) ``` ``` # # We can also add the original chunks to the vectorstore if we so want# for i, doc in enumerate(docs):# doc.metadata[id_key] = doc_ids[i]# retriever.vectorstore.add_documents(docs) ``` ``` sub_docs = vectorstore.similarity_search(""justice breyer"") ``` ``` sub_docs[0] ``` ``` Document(page_content=""The document is a transcript of a speech given by the President of the United States.The President discusses several important issues and initiatives, including the nomination of a Supreme Court Justice, border security and immigration reform, protecting women\\\'s rights, advancing LGBTQ+ equality, bipartisan legislation, addressing the opioid epidemic and mental health, supporting veterans, investigating the health effects of burn pits on military personnel, ending cancer, and the strength and resilience of the American people. "", metadata={\\\'doc_id\\\': \\\'79fa2e9f-28d9-4372-8af3-2caf4f1de312\\\'}) ``` ``` retrieved_docs = retriever.get_relevant_documents(""justice breyer"") ``` ``` len(retrieved_docs[0].page_content) ``` ``` 9194 ``` ## Hypothetical Queries[\\u200b](#hypothetical-queries) An LLM can also be used to generate a list of hypothetical questions that could be asked of a particular document.These questions can then be embedded ``` functions = [ { ""name"": ""hypothetical_questions"", ""description"": ""Generate hypothetical questions"", ""parameters"": { ""type"": ""object"", ""properties"": { ""questions"": { ""type"": ""array"", ""items"": { ""type"": ""string"" }, }, }, ""required"": [""questions""] } } ] ``` ``` from langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParserchain = ( {""doc"": lambda x: x.page_content} # Only asking for 3 hypothetical questions, but this could be adjusted | ChatPromptTemplate.from_template(""Generate a list of 3 hypothetical questions that the below document could be used to answer:\\\\n\\\\n{doc}"") | ChatOpenAI(max_retries=0, model=""gpt-4"").bind(functions=functions, function_call={""name"": ""hypothetical_questions""}) | JsonKeyOutputFunctionsParser(key_name=""questions"")) ``` ``` chain.invoke(docs[0]) ``` ``` [""What was the author\\\'s initial impression of philosophy as a field of study, and how did it change when they got to college?"", \\\'Why did the author decide to switch their focus to Artificial Intelligence (AI)? \\\', ""What led to the author\\\'s disillusionment with the field of AI as it was practiced at the time?""]``` ``` hypothetical_questions = chain.batch(docs, {""max_concurrency"": 5}) ``` ``` # The vectorstore to use to index the child chunksvectorstore = Chroma( collection_name=""hypo-questions"", embedding_function=OpenAIEmbeddings())# The storage layer for the parent documentsstore = InMemoryStore()id_key = ""doc_id""# The retriever (empty to start)retriever = MultiVectorRetriever( vectorstore=vectorstore, docstore=store, id_key=id_key,)doc_ids = [str(uuid.uuid4()) for _ in docs] ``` ``` question_docs = []for i, question_list in enumerate(hypothetical_questions): question_docs.extend([Document(page_content=s,metadata={id_key: doc_ids[i]}) for s in question_list]) ``` ``` retriever.vectorstore.add_documents(question_docs)retriever.docstore.mset(list(zip(doc_ids, docs))) ``` ``` sub_docs = vectorstore.similarity_search(""justice breyer"") ``` ``` sub_docs ``` ``` [Document(page_content=""What is the President\\\'s stance on immigration reform?"", metadata={\\\'doc_id\\\': \\\'505d73e3-8350-46ec-a58e-3af032f04ab3\\\'}), Document(page_content=""What is the President\\\'s stance on immigration reform? "", metadata={\\\'doc_id\\\': \\\'1c9618f0-7660-4b4f-a37c-509cbbbf6dba\\\'}), Document(page_content=""What is the President\\\'s stance on immigration reform? "", metadata={\\\'doc_id\\\': \\\'82c08209-b904-46a8-9532-edd2380950b7\\\'}), Document(page_content=\\\'What measures is the President proposing to protect the rights of LGBTQ+ Americans? \\\', metadata={\\\'doc_id\\\': \\\'82c08209-b904-46a8-9532-edd2380950b7\\\'})] ``` ``` retrieved_docs = retriever.get_relevant_documents(""justice breyer"") ``` ``` len(retrieved_docs[0].page_content) ``` ``` 9194 ```\', metadata={\'title\': \'MultiVector Retriever | Langchain\', \'type\': None, \'url\': \' \'id\': \'beec5531-16a7-453c-80ab-c5628e0236ce\'}), Document(page_content=\'MultiQueryRetriever | Langchain\\n# MultiQueryRetriever Distance-based vector database retrieval embeds (represents) queries in high-dimensional space and finds similar embedded documents based on ""distance"". But, retrieval may produce different results with subtle changes in query wording or if the embeddings do not capture the semantics of the data well. Prompt engineering / tuning is sometimes done to manually address these problems, but can be tedious. The `MultiQueryRetriever` automates the process of prompt tuning by using an LLM to generate multiple queries from different perspectives for a given user input query. For each query, it retrieves a set of relevant documents and takes the unique union across all queries to get a larger set of potentially relevant documents. By generating multiple perspectives on the same question, the `MultiQueryRetriever` might be able to overcome some of the limitations of the distance-based retrieval and get a richer set of results.By generating multiple perspectives on the same question, the `MultiQueryRetriever` might be able to overcome some of the limitations of the distance-based retrieval and get a richer set of results. ``` # Build a sample vectorDBfrom langchain.vectorstores import Chromafrom langchain.document_loaders import WebBaseLoaderfrom langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.text_splitter import RecursiveCharacterTextSplitter# Load blog postloader = WebBaseLoader("" = loader.load()# Splittext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)splits = text_splitter.split_documents(data)# VectorDBembedding = OpenAIEmbeddings()vectordb = Chroma.from_documents(documents=splits, embedding=embedding) ``` #### Simple usage[\\u200b](#simple-usage) Specify the LLM to use for query generation, and the retriever will do the rest.``` from langchain.chat_models import ChatOpenAIfrom langchain.retrievers.multi_query import MultiQueryRetrieverquestion = ""What are the approaches to Task Decomposition? ""llm = ChatOpenAI(temperature=0)retriever_from_llm = MultiQueryRetriever.from_llm( retriever=vectordb.as_retriever(), llm=llm) ``` ``` # Set logging for the queriesimport logginglogging.basicConfig()logging.getLogger(""langchain.retrievers.multi_query"").setLevel(logging.INFO) ``` ``` unique_docs = retriever_from_llm.get_relevant_documents(query=question)len(unique_docs) ``` ``` INFO:langchain.retrievers.multi_query:Generated queries: [\\\'1. How can Task Decomposition be approached? \\\', \\\'2. What are the different methods for Task Decomposition? \\\', \\\'3. What are the various approaches to decomposing tasks?\\\'] 5 ``` #### Supplying your own prompt[\\u200b](#supplying-your-own-prompt) You can also supply a prompt along with an output parser to split the results into a list of queries.5 ``` #### Supplying your own prompt[\\u200b](#supplying-your-own-prompt) You can also supply a prompt along with an output parser to split the results into a list of queries. ``` from typing import Listfrom langchain.chains import LLMChainfrom pydantic import BaseModel, Fieldfrom langchain.prompts import PromptTemplatefrom langchain.output_parsers import PydanticOutputParser# Output parser will split the LLM result into a list of queriesclass LineList(BaseModel): # ""lines"" is the key (attribute name) of the parsed output lines: List[str] = Field(description=""Lines of text"")class LineListOutputParser(PydanticOutputParser): def __init__(self) -> None: super().__init__(pydantic_object=LineList) def parse(self, text: str) -> LineList: lines = text.strip().split(""\\\\n"") return LineList(lines=lines)output_parser = LineListOutputParser()QUERY_PROMPT = PromptTemplate( input_variables=[""question""], template=""""""You are an AI language model assistant.Your task is to generate five different versions of the given user question to retrieve relevant documents from a vector database. By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based similarity search. Provide these alternative questions separated by newlines. Original question: {question}"""""",)llm = ChatOpenAI(temperature=0)# Chainllm_chain = LLMChain(llm=llm, prompt=QUERY_PROMPT, output_parser=output_parser)# Other inputsquestion = ""What are the approaches to Task Decomposition?"" ``` ``` # Runretriever = MultiQueryRetriever( retriever=vectordb.as_retriever(), llm_chain=llm_chain, parser_key=""lines"") # ""lines"" is the key (attribute name) of the parsed output# Resultsunique_docs = retriever.get_relevant_documents( query=""What does the course say about regression? "")len(unique_docs) ``` ``` INFO:langchain.retrievers.multi_query:Generated queries: [""1."")len(unique_docs) ``` ``` INFO:langchain.retrievers.multi_query:Generated queries: [""1. What is the course\\\'s perspective on regression? "", \\\'2. Can you provide information on regression as discussed in the course? \\\', \\\'3. How does the course cover the topic of regression? \\\', ""4. What are the course\\\'s teachings on regression? "", \\\'5. In relation to the course, what is mentioned about regression?\\\'] 11 ```\', metadata={\'title\': \'MultiQueryRetriever | Langchain\', \'type\': None, \'url\': \' \'id\': \'f7c20633-6a60-4ca3-96b1-13fee66e321d\'}), Document(page_content=\'langchain.retrievers.multi_vector.MultiVectorRetriever LangChain 0.0.322\\n# `langchain.retrievers.multi_vector`.MultiVectorRetriever[](#langchain-retrievers-multi-vector-multivectorretriever) *class *langchain.retrievers.multi_vector.MultiVectorRetriever[[source]](../_modules/langchain/retrievers/multi_vector.html#MultiVectorRetriever)[](#langchain.retrievers.multi_vector.MultiVectorRetriever) # Examples using MultiVectorRetriever[](#langchain-retrievers-multi-vector-multivectorretriever) - [MultiVector Retriever]( metadata={\'title\': \'langchain.retrievers.multi_vector.MultiVectorRetriever LangChain 0.0.322\', \'type\': None, \'url\': \' \'id\': \'1820c44d-7783-4846-a11c-106b18da015d\'})] ``` ## Putting it in a chain Let\'s try using our retrieval systems in a simple chain! ```python from langchain.chat_models import ChatOpenAI from langchain.prompts import ChatPromptTemplate from langchain.schema import StrOutputParser from langchain.schema.runnable import RunnablePassthrough prompt = ChatPromptTemplate.from_messages( [ ( ""system"", """"""You are a great software engineer who is very familiar \\ with Python. Given a user question or request about a new Python library called LangChain and \\ parts of the LangChain documentation, answer the question or generate the requested code. \\ Your answers must be accurate, should include code whenever possible, and should assume anything \\ about LangChain which is note explicitly stated in the LangChain documentation. If the required \\ information is not available, just say so. LangChain Documentation ------------------ {context}"""""", ), (""human"", ""{question}""), ] ) model = ChatOpenAI(model=""gpt-3.5-turbo-16k"") chain = ( { ""question"": RunnablePassthrough(), ""context"": parent_retriever | (lambda docs: ""\\n\\n"".join(d.page_content for d in docs)), } | prompt | model | StrOutputParser() ) ``` ```python for chunk in chain.invoke( ""How do I create a FAISS vector store retriever that returns 10 documents per search query"" ): print(chunk, end="""", flush=True) ``` ```text To create a FAISS vector store retriever that returns 10 documents per search query, you can use the following code: ```python from langchain.embeddings import OpenAIEmbeddings from langchain.vectorstores import FAISS # Assuming you have already loaded and split your documents # into `texts` and initialized your `embeddings` object # Create the FAISS vector store db = FAISS.from_documents(texts, embeddings) # Create the retriever with the desired search kwargs retriever = db.as_retriever(search_kwargs={""k"": 10}) ``` Now, you can use the `retriever` object to get relevant documents using the `get_relevant_documents` method. For example: ```python docs = retriever.get_relevant_documents(""your search query"") ``` This will return a list of 10 documents that are most relevant to the given search query. ``` - [Retriever chunks](#retriever-chunks) - [Other packages](#other-packages) - [Retrieve parent docs](#retrieve-parent-docs) - [Putting it in a chain](#putting-it-in-a-chain)', ""Retrievers | Retrievers [ Activeloop Deep MemoryActiveloop Deep Memory is a suite of tools that enables you to optimize your Vector Store for your use-case and achieve higher accuracy in your LLM apps.](/docs/integrations/retrievers/Activeloop DeepMemory+LangChain)[ Amazon KendraAmazon Kendra is an intelligent search service provided by Amazon Web Services (AWS). It utilizes advanced natural language processing (NLP) and machine learning algorithms to enable powerful search capabilities across various data sources within an organization. Kendra is designed to help users find the information they need quickly and accurately, improving productivity and decision-making.](/docs/integrations/retrievers/amazon_kendra_retriever)[ Arcee RetrieverThis notebook demonstrates how to use the ArceeRetriever class to retrieve relevant document(s) for Arcee's Domain Adapted Language Models (DALMs).](/docs/integrations/retrievers/arcee)[ ArxivarXiv is an open-access archive for 2 million scholarly articles in the fields of physics, mathematics, computer science, quantitative biology, quantitative finance, statistics, electrical engineering and systems science, and economics.](/docs/integrations/retrievers/arxiv)[ Azure Cognitive SearchAzure Cognitive Search (formerly known as Azure Search) is a cloud search service that gives developers infrastructure, APIs, and tools for building a rich search experience over private, heterogeneous content in web, mobile, and enterprise applications.](/docs/integrations/retrievers/azure_cognitive_search)[ BM25BM25 also known as the Okapi BM25, is a ranking function used in information retrieval systems to estimate the relevance of documents to a given search query.](/docs/integrations/retrievers/bm25)[ ChaindeskChaindesk platform brings data from anywhere (Datsources: Text, PDF, Word, PowerPpoint, Excel, Notion, Airtable, Google Sheets, etc..) into Datastores (container of multiple Datasources).](/docs/integrations/retrievers/chaindesk)[ ChatGPT PluginOpenAI plugins connect ChatGPT to third-party applications. These plugins enable ChatGPT to interact with APIs defined by developers, enhancing ChatGPT's capabilities and allowing it to perform a wide range of actions.](/docs/integrations/retrievers/chatgpt-plugin)[ Cohere RerankerCohere is a Canadian startup that provides natural language processing models that help companies improve human-machine interactions.](/docs/integrations/retrievers/cohere-reranker)[ Cohere RAG retrieverThis notebook covers how to get started with Cohere RAG retriever. This allows you to leverage the ability to search documents over various connectors or by supplying your own.](/docs/integrations/retrievers/cohere)[ DocArrayDocArray is a versatile, open-source tool for managing your multi-modal data. It lets you shape your data however you want, and offers the flexibility to store and search it using various document index backends. Plus, it gets even better - you can utilize your DocArray document index to create a DocArrayRetriever, and build awesome Langchain apps!](/docs/integrations/retrievers/docarray_retriever)[ ElasticSearch BM25Elasticsearch is a distributed, RESTful search and analytics engine. It provides a distributed, multitenant-capable full-text search engine with an HTTP web interface and schema-free JSON documents.](/docs/integrations/retrievers/elastic_search_bm25)[ Fleet AI Libraries ContextThe Fleet AI team is on a mission to embed the world's most important data. They've started by embedding the top 1200 Python libraries to enable code generation with up-to-date knowledge. They've been kind enough to share their embeddings of the LangChain docs and API reference.](/docs/integrations/retrievers/fleet_context)[ Google DriveThis notebook covers how to retrieve documents from Google Drive.](/docs/integrations/retrievers/google_drive)[ Google Vertex AI SearchVertex AI Search (formerly known as Enterprise Search on Generative AI App Builder) is a part of the Vertex AI machine learning platform offered by Google Cloud.](/docs/integrations/retrievers/google_vertex_ai_search)[ Kay.aiData API built for RAG We are curating the world's largest datasets as high-quality embeddings so your AI agents can retrieve context on the fly. Latest models, fast retrieval, and zero infra.](/docs/integrations/retrievers/kay)[ kNNIn statistics, the k-nearest neighbors algorithm (k-NN) is a non-parametric supervised learning method first developed by Evelyn Fix and Joseph Hodges in 1951, and later expanded by Thomas Cover. It is used for classification and regression.](/docs/integrations/retrievers/knn)[ LOTR (Merger Retriever)Lord of the Retrievers, also known as MergerRetriever, takes a list of retrievers as input and merges the results of their getrelevantdocuments() methods into a single list. The merged results will be a list of documents that are relevant to the query and that have been ranked by the different retrievers.](/docs/integrations/retrievers/merger_retriever)[ MetalMetal is a managed service for ML Embeddings.](/docs/integrations/retrievers/metal)[ Pinecone Hybrid SearchPinecone is a vector database with broad functionality.](/docs/integrations/retrievers/pinecone_hybrid_search)[ PubMedPubMed by The National Center for Biotechnology Information, National Library of Medicine comprises more than 35 million citations for biomedical literature from MEDLINE, life science journals, and online books. Citations may include links to full text content from PubMed Central and publisher web sites.](/docs/integrations/retrievers/pubmed)[ RePhraseQueryRePhraseQuery is a simple retriever that applies an LLM between the user input and the query passed by the retriever.](/docs/integrations/retrievers/re_phrase)[ SEC filingThe SEC filing is a financial statement or other formal document submitted to the U.S. Securities and Exchange Commission (SEC). Public companies, certain insiders, and broker-dealers are required to make regular SEC filings. Investors and financial professionals rely on these filings for information about companies they are evaluating for investment purposes.](/docs/integrations/retrievers/sec_filings)[ Self-querying retriever14 items](/docs/integrations/retrievers/self_query/)[ SingleStoreDBSingleStoreDB is a high-performance distributed SQL database that supports deployment both in the cloud and on-premises. It provides vector storage, and vector functions including dotproduct and euclideandistance, thereby supporting AI applications that require text similarity matching.](/docs/integrations/retrievers/singlestoredb)[ SVMSupport vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.](/docs/integrations/retrievers/svm)[ Tavily Search APITavily's Search API is a search engine built specifically for AI agents (LLMs), delivering real-time, accurate, and factual results at speed.](/docs/integrations/retrievers/tavily)[ TF-IDFTF-IDF means term-frequency times inverse document-frequency.](/docs/integrations/retrievers/tf_idf)[ VespaVespa is a fully featured search engine and vector database. It supports vector search (ANN), lexical search, and search in structured data, all in the same query.](/docs/integrations/retrievers/vespa)[ Weaviate Hybrid SearchWeaviate is an open-source vector database.](/docs/integrations/retrievers/weaviate-hybrid)[ WikipediaWikipedia is a multilingual free online encyclopedia written and maintained by a community of volunteers, known as Wikipedians, through open collaboration and using a wiki-based editing system called MediaWiki. Wikipedia is the largest and most-read reference work in history.](/docs/integrations/retrievers/wikipedia)[ you-retrieverUsing the You.com Retriever](/docs/integrations/retrievers/you-retriever)[ ZepRetriever Example for Zep](/docs/integrations/retrievers/zep_memorystore)"", 'Contextual compression | Contextual compression One challenge with retrieval is that usually you don\'t know the specific queries your document storage system will face when you ingest data into the system. This means that the information most relevant to a query may be buried in a document with a lot of irrelevant text. Passing that full document through your application can lead to more expensive LLM calls and poorer responses. Contextual compression is meant to fix this. The idea is simple: instead of immediately returning retrieved documents as-is, you can compress them using the context of the given query, so that only the relevant information is returned. Compressing here refers to both compressing the contents of an individual document and filtering out documents wholesale. To use the Contextual Compression Retriever, you\'ll need: - a base retriever - a Document Compressor The Contextual Compression Retriever passes queries to the base retriever, takes the initial documents and passes them through the Document Compressor. The Document Compressor takes a list of documents and shortens it by reducing the contents of documents or dropping documents altogether. ![]( ## Get started ```python # Helper function for printing docs def pretty_print_docs(docs): print(f""\\n{\'-\' * 100}\\n"".join([f""Document {i+1}:\\n\\n"" + d.page_content for i, d in enumerate(docs)])) ``` ## Using a vanilla vector store retriever Let\'s start by initializing a simple vector store retriever and storing the 2023 State of the Union speech (in chunks). We can see that given an example question our retriever returns one or two relevant docs and a few irrelevant docs. And even the relevant docs have a lot of irrelevant information in them. ```python from langchain.text_splitter import CharacterTextSplitter from langchain.embeddings import OpenAIEmbeddings from langchain.document_loaders import TextLoader from langchain.vectorstores import FAISS documents = TextLoader(\'../../../state_of_the_union.txt\').load() text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0) texts = text_splitter.split_documents(documents) retriever = FAISS.from_documents(texts, OpenAIEmbeddings()).as_retriever() docs = retriever.get_relevant_documents(""What did the president say about Ketanji Brown Jackson"") pretty_print_docs(docs) ``` ```text Document 1: Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you\'re at it, pass the Disclose Act so Americans can know who is funding our elections. Tonight, I\'d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyeran Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation\'s top legal minds, who will continue Justice Breyer\'s legacy of excellence. ---------------------------------------------------------------------------------------------------- Document 2: A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she\'s been nominated, she\'s received a broad range of supportfrom the Fraternal Order of Police to former judges appointed by Democrats and Republicans. And if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. We can do both. At our border, we\'ve installed new technology like cutting-edge scanners to better detect drug smuggling. We\'ve set up joint patrols with Mexico and Guatemala to catch more human traffickers. We\'re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. We\'re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders. ---------------------------------------------------------------------------------------------------- Document 3: And for our LGBTQ+ Americans, let\'s finally get the bipartisan Equality Act to my desk. The onslaught of state laws targeting transgender Americans and their families is wrong. As I said last year, especially to our younger transgender Americans, I will always have your back as your President, so you can be yourself and reach your God-given potential. While it often appears that we never agree, that isn\'t true. I signed 80 bipartisan bills into law last year. From preventing government shutdowns to protecting Asian-Americans from still-too-common hate crimes to reforming military justice. And soon, we\'ll strengthen the Violence Against Women Act that I first wrote three decades ago. It is important for us to show the nation that we can come together and do big things. So tonight I\'m offering a Unity Agenda for the Nation. Four big things we can do together. First, beat the opioid epidemic. ---------------------------------------------------------------------------------------------------- Document 4: Tonight, I\'m announcing a crackdown on these companies overcharging American businesses and consumers. And as Wall Street firms take over more nursing homes, quality in those homes has gone down and costs have gone up. That ends on my watch. Medicare is going to set higher standards for nursing homes and make sure your loved ones get the care they deserve and expect. We\'ll also cut costs and keep the economy going strong by giving workers a fair shot, provide more training and apprenticeships, hire them based on their skills not degrees. Let\'s pass the Paycheck Fairness Act and paid leave. Raise the minimum wage to $15 an hour and extend the Child Tax Credit, so no one has to raise a family in poverty. Let\'s increase Pell Grants and increase our historic support of HBCUs, and invest in what Jillour First Lady who teaches full-timecalls America\'s best-kept secret: community colleges. ``` ## Adding contextual compression with an LLMChainExtractor Now let\'s wrap our base retriever with a `ContextualCompressionRetriever`. We\'ll add an `LLMChainExtractor`, which will iterate over the initially returned documents and extract from each only the content that is relevant to the query. ```python from langchain.llms import OpenAI from langchain.retrievers import ContextualCompressionRetriever from langchain.retrievers.document_compressors import LLMChainExtractor llm = OpenAI(temperature=0) compressor = LLMChainExtractor.from_llm(llm) compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=retriever) compressed_docs = compression_retriever.get_relevant_documents(""What did the president say about Ketanji Jackson Brown"") pretty_print_docs(compressed_docs) ``` ```text Document 1: ""One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation\'s top legal minds, who will continue Justice Breyer\'s legacy of excellence."" ---------------------------------------------------------------------------------------------------- Document 2: ""A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she\'s been nominated, she\'s received a broad range of supportfrom the Fraternal Order of Police to former judges appointed by Democrats and Republicans."" ``` ## More built-in compressors: filters ### LLMChainFilter The `LLMChainFilter` is slightly simpler but more robust compressor that uses an LLM chain to decide which of the initially retrieved documents to filter out and which ones to return, without manipulating the document contents. ```python from langchain.retrievers.document_compressors import LLMChainFilter _filter = LLMChainFilter.from_llm(llm) compression_retriever = ContextualCompressionRetriever(base_compressor=_filter, base_retriever=retriever) compressed_docs = compression_retriever.get_relevant_documents(""What did the president say about Ketanji Jackson Brown"") pretty_print_docs(compressed_docs) ``` ```text Document 1: Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you\'re at it, pass the Disclose Act so Americans can know who is funding our elections. Tonight, I\'d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyeran Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation\'s top legal minds, who will continue Justice Breyer\'s legacy of excellence. ``` ### EmbeddingsFilter Making an extra LLM call over each retrieved document is expensive and slow. The `EmbeddingsFilter` provides a cheaper and faster option by embedding the documents and query and only returning those documents which have sufficiently similar embeddings to the query. ```python from langchain.embeddings import OpenAIEmbeddings from langchain.retrievers.document_compressors import EmbeddingsFilter embeddings = OpenAIEmbeddings() embeddings_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.76) compression_retriever = ContextualCompressionRetriever(base_compressor=embeddings_filter, base_retriever=retriever) compressed_docs = compression_retriever.get_relevant_documents(""What did the president say about Ketanji Jackson Brown"") pretty_print_docs(compressed_docs) ``` ```text Document 1: Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you\'re at it, pass the Disclose Act so Americans can know who is funding our elections. Tonight, I\'d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyeran Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation\'s top legal minds, who will continue Justice Breyer\'s legacy of excellence. ---------------------------------------------------------------------------------------------------- Document 2: A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she\'s been nominated, shes received a broad range of supportfrom the Fraternal Order of Police to former judges appointed by Democrats and Republicans. And if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. We can do both. At our border, weve installed new technology like cutting-edge scanners to better detect drug smuggling. Weve set up joint patrols with Mexico and Guatemala to catch more human traffickers. Were putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. Were securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders. ---------------------------------------------------------------------------------------------------- Document 3: And for our LGBTQ+ Americans, lets finally get the bipartisan Equality Act to my desk. The onslaught of state laws targeting transgender Americans and their families is wrong. As I said last year, especially to our younger transgender Americans, I will always have your back as your President, so you can be yourself and reach your God-given potential. While it often appears that we never agree, that isnt true. I signed 80 bipartisan bills into law last year. From preventing government shutdowns to protecting Asian-Americans from still-too-common hate crimes to reforming military justice. And soon, well strengthen the Violence Against Women Act that I first wrote three decades ago. It is important for us to show the nation that we can come together and do big things. So tonight Im offering a Unity Agenda for the Nation. Four big things we can do together. First, beat the opioid epidemic. ``` # Stringing compressors and document transformers together Using the `DocumentCompressorPipeline` we can also easily combine multiple compressors in sequence. Along with compressors we can add `BaseDocumentTransformer`s to our pipeline, which don\'t perform any contextual compression but simply perform some transformation on a set of documents. For example `TextSplitter`s can be used as document transformers to split documents into smaller pieces, and the `EmbeddingsRedundantFilter` can be used to filter out redundant documents based on embedding similarity between documents. Below we create a compressor pipeline by first splitting our docs into smaller chunks, then removing redundant documents, and then filtering based on relevance to the query. ```python from langchain.document_transformers import EmbeddingsRedundantFilter from langchain.retrievers.document_compressors import DocumentCompressorPipeline from langchain.text_splitter import CharacterTextSplitter splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0, separator="". "") redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings) relevant_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.76) pipeline_compressor = DocumentCompressorPipeline( transformers=[splitter, redundant_filter, relevant_filter] ) ``` ```python compression_retriever = ContextualCompressionRetriever(base_compressor=pipeline_compressor, base_retriever=retriever) compressed_docs = compression_retriever.get_relevant_documents(""What did the president say about Ketanji Jackson Brown"") pretty_print_docs(compressed_docs) ``` ```text Document 1: One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson ---------------------------------------------------------------------------------------------------- Document 2: As I said last year, especially to our younger transgender Americans, I will always have your back as your President, so you can be yourself and reach your God-given potential. While it often appears that we never agree, that isnt true. I signed 80 bipartisan bills into law last year ---------------------------------------------------------------------------------------------------- Document 3: A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder ``` - [Get started](#get-started) - [Using a vanilla vector store retriever](#using-a-vanilla-vector-store-retriever) - [Adding contextual compression with an LLMChainExtractor](#adding-contextual-compression-with-an-llmchainextractor) - [More built-in compressors: filters](#more-built-in-compressors-filters)- [LLMChainFilter](#llmchainfilter) - [EmbeddingsFilter](#embeddingsfilter)']","Yes, it is possible to pass certain keyword arguments (kwargs) to the `MultiVectorRetriever` to customize its behavior, similar to how you would with normal retrievers. Specifically, you can specify the number of top documents to retrieve by using the `k` parameter. Here's an example:

```python
retriever = MultiVectorRetriever(
    vectorstore=vectorstore,
    docstore=store,
    id_key=id_key,
    k=10  # Retrieve top 10 documents
)
```

For similarity thresholds, the `MultiVectorRetriever` may not directly support this feature out of the box. However, you can implement custom logic to filter documents based on similarity scores after retrieval. This would involve retrieving the documents first and then applying a threshold filter based on the similarity scores obtained.

Always refer to the latest documentation for the most current features and capabilities.",You can assign search_kwargs (such as k) to the instance of the MultiVectorRetriever,0.9999999999,1.0,,0.0530918756028263,0.08695652173913042
84,whats the stuff documents chain?,"['Conversation Buffer | Conversation Buffer This notebook shows how to use `ConversationBufferMemory`. This memory allows for storing messages and then extracts the messages in a variable. We can first extract it as a string. ```python from langchain.memory import ConversationBufferMemory ``` ```python memory = ConversationBufferMemory() memory.save_context({""input"": ""hi""}, {""output"": ""whats up""}) ``` ```python memory.load_memory_variables({}) ``` ```text {\'history\': \'Human: hi\\nAI: whats up\'} ``` We can also get the history as a list of messages (this is useful if you are using this with a chat model). ```python memory = ConversationBufferMemory(return_messages=True) memory.save_context({""input"": ""hi""}, {""output"": ""whats up""}) ``` ```python memory.load_memory_variables({}) ``` ```text {\'history\': [HumanMessage(content=\'hi\', additional_kwargs={}), AIMessage(content=\'whats up\', additional_kwargs={})]} ``` ## Using in a chain Finally, let\'s take a look at using this in a chain (setting `verbose=True` so we can see the prompt). ```python from langchain.llms import OpenAI from langchain.chains import ConversationChain llm = OpenAI(temperature=0) conversation = ConversationChain( llm=llm, verbose=True, memory=ConversationBufferMemory() ) ``` ```python conversation.predict(input=""Hi there!"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: Human: Hi there! AI: > Finished chain. "" Hi there! It\'s nice to meet you. How can I help you today?"" ``` ```python conversation.predict(input=""I\'m doing well! Just having a conversation with an AI."") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: Human: Hi there! AI: Hi there! It\'s nice to meet you. How can I help you today? Human: I\'m doing well! Just having a conversation with an AI. AI: > Finished chain. "" That\'s great! It\'s always nice to have a conversation with someone new. What would you like to talk about?"" ``` ```python conversation.predict(input=""Tell me about yourself."") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: Human: Hi there! AI: Hi there! It\'s nice to meet you. How can I help you today? Human: I\'m doing well! Just having a conversation with an AI. AI: That\'s great! It\'s always nice to have a conversation with someone new. What would you like to talk about? Human: Tell me about yourself. AI: > Finished chain. "" Sure! I\'m an AI created to help people with their everyday tasks. I\'m programmed to understand natural language and provide helpful information. I\'m also constantly learning and updating my knowledge base so I can provide more accurate and helpful answers."" ``` - [Using in a chain](#using-in-a-chain)', 'Chat Messages | Chat Messages infoHead to [Integrations](/docs/integrations/memory/) for documentation on built-in memory integrations with 3rd-party databases and tools. One of the core utility classes underpinning most (if not all) memory modules is the `ChatMessageHistory` class. This is a super lightweight wrapper that provides convenience methods for saving HumanMessages, AIMessages, and then fetching them all. You may want to use this class directly if you are managing memory outside of a chain. ```python from langchain.memory import ChatMessageHistory history = ChatMessageHistory() history.add_user_message(""hi!"") history.add_ai_message(""whats up?"") ``` ```python history.messages ``` ```text [HumanMessage(content=\'hi!\', additional_kwargs={}), AIMessage(content=\'whats up?\', additional_kwargs={})] ```', 'Motrhead | Motrhead [Motrhead]( is a memory server implemented in Rust. It automatically handles incremental summarization in the background and allows for stateless applications. ## Setup See instructions at [Motrhead]( for running the server locally. ```python from langchain.memory.motorhead_memory import MotorheadMemory ``` ## Example ```python from langchain.chains import LLMChain from langchain.llms import OpenAI from langchain.prompts import PromptTemplate template = """"""You are a chatbot having a conversation with a human. {chat_history} Human: {human_input} AI:"""""" prompt = PromptTemplate( input_variables=[""chat_history"", ""human_input""], template=template ) memory = MotorheadMemory( session_id=""testing-1"", url="" memory_key=""chat_history"" ) await memory.init() # loads previous state from Motrhead llm_chain = LLMChain( llm=OpenAI(), prompt=prompt, verbose=True, memory=memory, ) ``` ```python llm_chain.run(""hi im bob"") ``` ```text > Entering new LLMChain chain... Prompt after formatting: You are a chatbot having a conversation with a human. Human: hi im bob AI: > Finished chain. \' Hi Bob, nice to meet you! How are you doing today?\' ``` ```python llm_chain.run(""whats my name?"") ``` ```text > Entering new LLMChain chain... Prompt after formatting: You are a chatbot having a conversation with a human. Human: hi im bob AI: Hi Bob, nice to meet you! How are you doing today? Human: whats my name? AI: > Finished chain. \' You said your name is Bob. Is that correct?\' ``` ```python llm_chain.run(""whats for dinner?"") ``` ```text > Entering new LLMChain chain... Prompt after formatting: You are a chatbot having a conversation with a human. Human: hi im bob AI: Hi Bob, nice to meet you! How are you doing today? Human: whats my name? AI: You said your name is Bob. Is that correct? Human: whats for dinner? AI: > Finished chain. "" I\'m sorry, I\'m not sure what you\'re asking. Could you please rephrase your question?"" ``` - [Setup](#setup) - [Example](#example)']","The `ConversationBufferMemory` stores messages in a buffer and can be used in a chain. Here's a brief example:

```python
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain
from langchain.llms import OpenAI

memory = ConversationBufferMemory()
llm = OpenAI(temperature=0)
conversation = ConversationChain(llm=llm, verbose=True, memory=memory)

response = conversation.predict(input=""Hi there!"")
print(response)

response = conversation.predict(input=""I'm doing well! Just having a conversation with an AI."")
print(response)
```

This setup keeps the conversation history in memory.","The stuff documents chain (""stuff"" as in ""to stuff"" or ""to fill"") is the most straightforward of the document chains. It takes a list of documents, inserts them all into a prompt and passes that prompt to an LLM.

This chain is well-suited for applications where documents are small and only a few are passed in for most calls.",0.0,0.0,,0.018080119651675462,0.11267605633802817
85,Does AlephAlpha support token-level streaming natively in LangChain?,"['Aleph Alpha | Aleph Alpha [The Luminous series]( is a family of large language models. This example goes over how to use LangChain to interact with Aleph Alpha models ```bash # Install the package pip install aleph-alpha-client ``` ```python # create a new token: from getpass import getpass ALEPH_ALPHA_API_KEY = getpass() ``` ```text ``` ```python from langchain.chains import LLMChain from langchain.llms import AlephAlpha from langchain.prompts import PromptTemplate ``` ```python template = """"""Q: {question} A:"""""" prompt = PromptTemplate(template=template, input_variables=[""question""]) ``` ```python llm = AlephAlpha( model=""luminous-extended"", maximum_tokens=20, stop_sequences=[""Q:""], aleph_alpha_api_key=ALEPH_ALPHA_API_KEY, ) ``` ```python llm_chain = LLMChain(prompt=prompt, llm=llm) ``` ```python question = ""What is AI?"" llm_chain.run(question) ``` ```text \' Artificial Intelligence (AI) is the simulation of human intelligence processes by machines, especially computer systems.\\n\' ```', 'Aleph Alpha | Aleph Alpha [Aleph Alpha]( was founded in 2019 with the mission to research and build the foundational technology for an era of strong AI. The team of international scientists, engineers, and innovators researches, develops, and deploys transformative AI like large language and multimodal models and runs the fastest European commercial AI cluster. [The Luminous series]( is a family of large language models. ## Installation and Setup ```bash pip install aleph-alpha-client ``` You have to create a new token. Please, see [instructions]( ```python from getpass import getpass ALEPH_ALPHA_API_KEY = getpass() ``` ## LLM See a [usage example](/docs/integrations/llms/aleph_alpha). ```python from langchain.llms import AlephAlpha ``` ## Text Embedding Models See a [usage example](/docs/integrations/text_embedding/aleph_alpha). ```python from langchain.embeddings import AlephAlphaSymmetricSemanticEmbedding, AlephAlphaAsymmetricSemanticEmbedding ``` - [Installation and Setup](#installation-and-setup) - [LLM](#llm) - [Text Embedding Models](#text-embedding-models)', 'langchain.embeddings.aleph_alpha.AlephAlphaSymmetricSemanticEmbedding LangChain 0.0.337 API Experimental Python Docs Toggle Menu PrevUp Next LangChain 0.0.337 langchain.embeddings.aleph_alpha.AlephAlphaSymmetricSemanticEmbedding langchain.embeddings.aleph_alpha.AlephAlphaSymmetricSemanticEmbedding class langchain.embeddings.aleph_alpha.AlephAlphaSymmetricSemanticEmbedding[source] Bases: AlephAlphaAsymmetricSemanticEmbedding The symmetric version of the Aleph Alpha\'s semantic embeddings. The main difference is that here, both the documents and queries are embedded with a SemanticRepresentation.Symmetric .. rubric:: Example from aleph_alpha import AlephAlphaSymmetricSemanticEmbedding embeddings = AlephAlphaAsymmetricSemanticEmbedding( normalize=True, compress_to_size=128 ) text = ""This is a test text"" doc_result = embeddings.embed_documents([text]) query_result = embeddings.embed_query(text) Create a new model by parsing and validating input data from keyword arguments. Raises ValidationError if the input data cannot be parsed to form a valid model. param aleph_alpha_api_key: Optional[str] = None API key for Aleph Alpha API. param compress_to_size: Optional[int] = None Should the returned embeddings come back as an original 5120-dim vector, or should it be compressed to 128-dim. param contextual_control_threshold: Optional[int] = None Attention control parameters only apply to those tokens that have explicitly been set in the request. param control_log_additive: bool = True Apply controls on prompt items by adding the log(control_factor) to attention scores. param host: str = \' The hostname of the API host. The default one is param hosting: Optional[str] = None Determines in which datacenters the request may be processed. You can either set the parameter to aleph-alpha or omit it (defaulting to None). Not setting this value, or setting it to None, gives us maximal flexibility in processing your request in our own datacenters and on servers hosted with other providers. Choose this option for maximal availability. Setting it to aleph-alpha allows us to only process the request in our own datacenters. Choose this option for maximal data privacy. param model: str = \'luminous-base\' Model name to use. param nice: bool = False Setting this to True, will signal to the API that you intend to be nice to other users by de-prioritizing your request below concurrent ones. param normalize: Optional[bool] = None Should returned embeddings be normalized param request_timeout_seconds: int = 305 Client timeout that will be set for HTTP requests in the requests library\'s API calls. Server will close all requests after 300 seconds with an internal server error. param total_retries: int = 8 The number of retries made in case requests fail with certain retryable status codes. If the last retry fails a corresponding exception is raised. Note, that between retries an exponential backoff is applied, starting with 0.5 s after the first retry and doubling for each retry made. So with the default setting of 8 retries a total wait time of 63.5 s is added between the retries. async aembed_documents(texts: List[str]) List[List[float]] Asynchronous Embed search docs. async aembed_query(text: str) List[float] Asynchronous Embed query text. classmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) Model Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data. Default values are respected, but no other validation is performed. Behaves as if Config.extra = \'allow\' was set since it adds all passed values copy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) Model Duplicate a model, optionally choose which fields to include, exclude and change. Parameters include fields to include in new model exclude fields to exclude from new model, as with values this takes precedence over include update values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data deep set to True to make a deep copy of the model Returns new model instance dict(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False)  DictStrAny Generate a dictionary representation of the model, optionally specifying which fields to include or exclude. embed_documents(texts: List[str])  List[List[float]][source] Call out to Aleph Alpha\'s Document endpoint. Parameters texts  The list of texts to embed. Returns List of embeddings, one for each text. embed_query(text: str)  List[float][source] Call out to Aleph Alpha\'s asymmetric, query embedding endpoint :param text: The text to embed. Returns Embeddings for the text. classmethod from_orm(obj: Any)  Model json(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any)  unicode Generate a JSON representation of the model, include and exclude arguments as per dict(). encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps(). classmethod parse_file(path: Union[str, Path], *, content_type: unicode = None, encoding: unicode = \'utf8\', proto: Protocol = None, allow_pickle: bool = False)  Model classmethod parse_obj(obj: Any)  Model classmethod parse_raw(b: Union[str, bytes], *, content_type: unicode = None, encoding: unicode = \'utf8\', proto: Protocol = None, allow_pickle: bool = False)  Model classmethod schema(by_alias: bool = True, ref_template: unicode = \'#/definitions/{model}\')  DictStrAny classmethod schema_json(*, by_alias: bool = True, ref_template: unicode = \'#/definitions/{model}\', **dumps_kwargs: Any)  unicode classmethod update_forward_refs(**localns: Any)  None Try to update ForwardRefs on fields based on this Model, globalns and localns. classmethod validate(value: Any)  Model Examples using AlephAlphaSymmetricSemanticEmbedding Aleph Alpha  2023, Harrison Chase. Last updated on Nov 17, 2023. Show this page source']","AlephAlpha does not natively support token-level streaming in LangChain. For streaming responses, consider using other models that support this feature.","No, while the stream() method is provided in all langchain runnables, AlephAlpha doesn't natively implement streaming on the token level.",0.9999999999,0.0,0.0,0.011395817319981894,0.19047619047619047
86,How to use a prompt template with chat history,"['Streamlit | Streamlit [Streamlit]( is an open-source Python library that makes it easy to create and share beautiful, custom web apps for machine learning and data science. This notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in [Streamlit session state]( at the specified `key=`. The default key is `""langchain_messages""`. - Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app. - You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit) for LangChain. - For more on Streamlit check out their [getting started documentation]( You can see the [full app example running here]( and more examples in [github.com/langchain-ai/streamlit-agent]( ```python from langchain.memory import StreamlitChatMessageHistory history = StreamlitChatMessageHistory(key=""chat_messages"") history.add_user_message(""hi!"") history.add_ai_message(""whats up?"") ``` ```python history.messages ``` You can integrate `StreamlitChatMessageHistory` into `ConversationBufferMemory` and chains or agents as usual. The history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions. ```python from langchain.memory import ConversationBufferMemory from langchain.memory.chat_message_histories import StreamlitChatMessageHistory # Optionally, specify your own session_state key for storing messages msgs = StreamlitChatMessageHistory(key=""special_app_key"") memory = ConversationBufferMemory(memory_key=""history"", chat_memory=msgs) if len(msgs.messages) == 0: msgs.add_ai_message(""How can I help you?"") ``` ```python from langchain.chains import LLMChain from langchain.llms import OpenAI from langchain.prompts import PromptTemplate template = """"""You are an AI chatbot having a conversation with a human. {history} Human: {human_input} AI: """""" prompt = PromptTemplate(input_variables=[""history"", ""human_input""], template=template) # Add the memory to an LLMChain as usual llm_chain = LLMChain(llm=OpenAI(), prompt=prompt, memory=memory) ``` Conversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`: ```python import streamlit as st for msg in msgs.messages: st.chat_message(msg.type).write(msg.content) if prompt := st.chat_input(): st.chat_message(""human"").write(prompt) # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called. response = llm_chain.run(prompt) st.chat_message(""ai"").write(response) ``` **View the final app.**', 'Elasticsearch | Elasticsearch [Elasticsearch]( is a distributed, RESTful search and analytics engine, capable of performing both vector and lexical search. It is built on top of the Apache Lucene library. This notebook shows how to use chat message history functionality with `Elasticsearch`. ## Set up Elasticsearch There are two main ways to set up an Elasticsearch instance: 1. **Elastic Cloud.** Elastic Cloud is a managed Elasticsearch service. Sign up for a [free trial]( 2. **Local Elasticsearch installation.** Get started with Elasticsearch by running it locally. The easiest way is to use the official Elasticsearch Docker image. See the [Elasticsearch Docker documentation]( for more information. ## Install dependencies ```python %pip install elasticsearch langchain ``` ## Authentication ### How to obtain a password for the default ""elastic"" user To obtain your Elastic Cloud password for the default ""elastic"" user: 1. Log in to the [Elastic Cloud console]( 2. Go to ""Security"" > ""Users"" 3. Locate the ""elastic"" user and click ""Edit"" 4. Click ""Reset password"" 5. Follow the prompts to reset the password ### Use the Username/password ```python es_username = os.environ.get(""ES_USERNAME"", ""elastic"") es_password = os.environ.get(""ES_PASSWORD"", ""change me..."") history = ElasticsearchChatMessageHistory( es_url=es_url, es_user=es_username, es_password=es_password, index=""test-history"", session_id=""test-session"" ) ``` ### How to obtain an API key To obtain an API key: 1. Log in to the [Elastic Cloud console]( 2. Open `Kibana` and go to Stack Management > API Keys 3. Click ""Create API key"" 4. Enter a name for the API key and click ""Create"" ### Use the API key ```python es_api_key = os.environ.get(""ES_API_KEY"") history = ElasticsearchChatMessageHistory( es_api_key=es_api_key, index=""test-history"", session_id=""test-session"" ) ``` ## Initialize Elasticsearch client and chat message history ```python import os from langchain.memory import ElasticsearchChatMessageHistory es_url = os.environ.get(""ES_URL"", "" # If using Elastic Cloud: # es_cloud_id = os.environ.get(""ES_CLOUD_ID"") # Note: see Authentication section for various authentication methods history = ElasticsearchChatMessageHistory( es_url=es_url, index=""test-history"", session_id=""test-session"" ) ``` ## Use the chat message history ```python history.add_user_message(""hi!"") history.add_ai_message(""whats up?"") ``` ```text indexing message content=\'hi!\' additional_kwargs={} example=False indexing message content=\'whats up?\' additional_kwargs={} example=False ``` - [Set up Elasticsearch](#set-up-elasticsearch) - [Install dependencies](#install-dependencies) - [Authentication](#authentication)- [How to obtain a password for the default ""elastic"" user](#how-to-obtain-a-password-for-the-default-elastic-user) - [Use the Username/password](#use-the-usernamepassword) - [How to obtain an API key](#how-to-obtain-an-api-key) - [Use the API key](#use-the-api-key) - [Initialize Elasticsearch client and chat message history](#initialize-elasticsearch-client-and-chat-message-history) - [Use the chat message history](#use-the-chat-message-history)', 'Chatbots | Chatbots []( ## Use case Chatbots are one of the central LLM use-cases. The core features of chatbots are that they can have long-running conversations and have access to information that users want to know about. Aside from basic prompting and LLMs, memory and retrieval are the core components of a chatbot. Memory allows a chatbot to remember past interactions, and retrieval provides a chatbot with up-to-date, domain-specific information. ![Image description](/assets/images/chat_use_case-eb8a4883931d726e9f23628a0d22e315.png) ## Overview The chat model interface is based around messages rather than raw text. Several components are important to consider for chat: - `chat model`: See [here](/docs/integrations/chat) for a list of chat model integrations and [here](/docs/modules/model_io/chat) for documentation on the chat model interface in LangChain. You can use `LLMs` (see [here](/docs/modules/model_io/llms)) for chatbots as well, but chat models have a more conversational tone and natively support a message interface. - `prompt template`: Prompt templates make it easy to assemble prompts that combine default messages, user input, chat history, and (optionally) additional retrieved context. - `memory`: [See here](/docs/modules/memory/) for in-depth documentation on memory types - `retriever` (optional): [See here](/docs/modules/data_connection/retrievers) for in-depth documentation on retrieval systems. These are useful if you want to build a chatbot with domain-specific knowledge. ## Quickstart Here\'s a quick preview of how we can create chatbot interfaces. First let\'s install some dependencies and set the required credentials: ```bash pip install langchain openai # Set env var OPENAI_API_KEY or load from a .env file: # import dotenv # dotenv.load_dotenv() ``` With a plain chat model, we can get chat completions by [passing one or more messages](/docs/modules/model_io/chat) to the model. The chat model will respond with a message. ```python from langchain.chat_models import ChatOpenAI from langchain.schema import HumanMessage, SystemMessage chat = ChatOpenAI() chat( [ HumanMessage( content=""Translate this sentence from English to French: I love programming."" ) ] ) ``` ```text AIMessage(content=""J\'adore la programmation."", additional_kwargs={}, example=False) ``` And if we pass in a list of messages: ```python messages = [ SystemMessage( content=""You are a helpful assistant that translates English to French."" ), HumanMessage(content=""I love programming.""), ] chat(messages) ``` ```text AIMessage(content=""J\'adore la programmation."", additional_kwargs={}, example=False) ``` We can then wrap our chat model in a `ConversationChain`, which has built-in memory for remembering past user inputs and model outputs. ```python from langchain.chains import ConversationChain conversation = ConversationChain(llm=chat) conversation.run(""Translate this sentence from English to French: I love programming."") ``` ```text \'Je adore la programmation.\' ``` ```python conversation.run(""Translate it to German."") ``` ```text \'Ich liebe Programmieren.\' ``` ## Memory As we mentioned above, the core component of chatbots is the memory system. One of the simplest and most commonly used forms of memory is `ConversationBufferMemory`: - This memory allows for storing of messages in a `buffer` - When called in a chain, it returns all of the messages it has stored LangChain comes with many other types of memory, too. [See here](/docs/modules/memory/) for in-depth documentation on memory types. For now let\'s take a quick look at ConversationBufferMemory. We can manually add a few chat messages to the memory like so: ```python from langchain.memory import ConversationBufferMemory memory = ConversationBufferMemory() memory.chat_memory.add_user_message(""hi!"") memory.chat_memory.add_ai_message(""whats up?"") ``` And now we can load from our memory. The key method exposed by all `Memory` classes is `load_memory_variables`. This takes in any initial chain input and returns a list of memory variables which are added to the chain input. Since this simple memory type doesn\'t actually take into account the chain input when loading memory, we can pass in an empty input for now: ```python memory.load_memory_variables({}) ``` ```text {\'history\': \'Human: hi!\\nAI: whats up?\'} ``` We can also keep a sliding window of the most recent `k` interactions using `ConversationBufferWindowMemory`. ```python from langchain.memory import ConversationBufferWindowMemory memory = ConversationBufferWindowMemory(k=1) memory.save_context({""input"": ""hi""}, {""output"": ""whats up""}) memory.save_context({""input"": ""not much you""}, {""output"": ""not much""}) memory.load_memory_variables({}) ``` ```text {\'history\': \'Human: not much you\\nAI: not much\'} ``` `ConversationSummaryMemory` is an extension of this theme. It creates a summary of the conversation over time. This memory is most useful for longer conversations where the full message history would consume many tokens. ```python from langchain.llms import OpenAI from langchain.memory import ConversationSummaryMemory llm = OpenAI(temperature=0) memory = ConversationSummaryMemory(llm=llm) memory.save_context({""input"": ""hi""}, {""output"": ""whats up""}) memory.save_context( {""input"": ""im working on better docs for chatbots""}, {""output"": ""oh, that sounds like a lot of work""}, ) memory.save_context( {""input"": ""yes, but it\'s worth the effort""}, {""output"": ""agreed, good docs are important!""}, ) ``` ```python memory.load_memory_variables({}) ``` ```text {\'history\': \'\\nThe human greets the AI, to which the AI responds. The human then mentions they are working on better docs for chatbots, to which the AI responds that it sounds like a lot of work. The human agrees that it is worth the effort, and the AI agrees that good docs are important.\'} ``` `ConversationSummaryBufferMemory` extends this a bit further: It uses token length rather than number of interactions to determine when to flush interactions. ```python from langchain.memory import ConversationSummaryBufferMemory memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=10) memory.save_context({""input"": ""hi""}, {""output"": ""whats up""}) memory.save_context({""input"": ""not much you""}, {""output"": ""not much""}) ``` ## Conversation We can unpack what goes under the hood with `ConversationChain`. We can specify our memory, `ConversationSummaryMemory` and we can specify the prompt. ```python from langchain.chains import LLMChain from langchain.prompts import ( ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate, ) # LLM llm = ChatOpenAI() # Prompt prompt = ChatPromptTemplate( messages=[ SystemMessagePromptTemplate.from_template( ""You are a nice chatbot having a conversation with a human."" ), # The `variable_name` here is what must align with memory MessagesPlaceholder(variable_name=""chat_history""), HumanMessagePromptTemplate.from_template(""{question}""), ] ) # Notice that we `return_messages=True` to fit into the MessagesPlaceholder # Notice that `""chat_history""` aligns with the MessagesPlaceholder name memory = ConversationBufferMemory(memory_key=""chat_history"", return_messages=True) conversation = LLMChain(llm=llm, prompt=prompt, verbose=True, memory=memory) # Notice that we just pass in the `question` variables - `chat_history` gets populated by memory conversation({""question"": ""hi""}) ``` ```text > Entering new LLMChain chain... Prompt after formatting: System: You are a nice chatbot having a conversation with a human. Human: hi > Finished chain. {\'question\': \'hi\', \'chat_history\': [HumanMessage(content=\'hi\', additional_kwargs={}, example=False), AIMessage(content=\'Hello! How can I assist you today?\', additional_kwargs={}, example=False)], \'text\': \'Hello! How can I assist you today?\'} ``` ```python conversation( {""question"": ""Translate this sentence from English to French: I love programming.""} ) ``` ```text > Entering new LLMChain chain... Prompt after formatting: System: You are a nice chatbot having a conversation with a human. Human: hi AI: Hello! How can I assist you today? Human: Translate this sentence from English to French: I love programming. > Finished chain. {\'question\': \'Translate this sentence from English to French: I love programming.\', \'chat_history\': [HumanMessage(content=\'hi\', additional_kwargs={}, example=False), AIMessage(content=\'Hello! How can I assist you today?\', additional_kwargs={}, example=False), HumanMessage(content=\'Translate this sentence from English to French: I love programming.\', additional_kwargs={}, example=False), AIMessage(content=\'Sure! The translation of ""I love programming"" from English to French is ""J\\\'adore programmer.""\', additional_kwargs={}, example=False)], \'text\': \'Sure! The translation of ""I love programming"" from English to French is ""J\\\'adore programmer.""\'} ``` ```python conversation({""question"": ""Now translate the sentence to German.""}) ``` ```text > Entering new LLMChain chain... Prompt after formatting: System: You are a nice chatbot having a conversation with a human. Human: hi AI: Hello! How can I assist you today? Human: Translate this sentence from English to French: I love programming. AI: Sure! The translation of ""I love programming"" from English to French is ""J\'adore programmer."" Human: Now translate the sentence to German. > Finished chain. {\'question\': \'Now translate the sentence to German.\', \'chat_history\': [HumanMessage(content=\'hi\', additional_kwargs={}, example=False), AIMessage(content=\'Hello! How can I assist you today?\', additional_kwargs={}, example=False), HumanMessage(content=\'Translate this sentence from English to French: I love programming.\', additional_kwargs={}, example=False), AIMessage(content=\'Sure! The translation of ""I love programming"" from English to French is ""J\\\'adore programmer.""\', additional_kwargs={}, example=False), HumanMessage(content=\'Now translate the sentence to German.\', additional_kwargs={}, example=False), AIMessage(content=\'Certainly! The translation of ""I love programming"" from English to German is ""Ich liebe das Programmieren.""\', additional_kwargs={}, example=False)], \'text\': \'Certainly! The translation of ""I love programming"" from English to German is ""Ich liebe das Programmieren.""\'} ``` We can see the chat history preserved in the prompt using the [LangSmith trace]( ![Image description](/assets/images/chat_use_case_2-a76871806149f125d08ff149363e0c2c.png) ## Chat Retrieval Now, suppose we want to [chat with documents]( or some other source of knowledge. This is popular use case, combining chat with [document retrieval](/docs/use_cases/question_answering). It allows us to chat with specific information that the model was not trained on. ```bash pip install tiktoken chromadb ``` Load a blog post. ```python from langchain.document_loaders import WebBaseLoader loader = WebBaseLoader("" data = loader.load() ``` Split and store this in a vector. ```python from langchain.text_splitter import RecursiveCharacterTextSplitter text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0) all_splits = text_splitter.split_documents(data) from langchain.embeddings import OpenAIEmbeddings from langchain.vectorstores import Chroma vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings()) ``` Create our memory, as before, but\'s let\'s use `ConversationSummaryMemory`. ```python memory = ConversationSummaryMemory( llm=llm, memory_key=""chat_history"", return_messages=True ) ``` ```python from langchain.chains import ConversationalRetrievalChain from langchain.chat_models import ChatOpenAI llm = ChatOpenAI() retriever = vectorstore.as_retriever() qa = ConversationalRetrievalChain.from_llm(llm, retriever=retriever, memory=memory) ``` ```python qa(""How do agents use Task decomposition?"") ``` ```text {\'question\': \'How do agents use Task decomposition?\', \'chat_history\': [SystemMessage(content=\'\', additional_kwargs={})], \'answer\': \'Agents can use task decomposition in several ways:\\n\\n1. Simple prompting: Agents can use Language Model based prompting to break down tasks into subgoals. For example, by providing prompts like ""Steps for XYZ"" or ""What are the subgoals for achieving XYZ?"", the agent can generate a sequence of smaller steps that lead to the completion of the overall task.\\n\\n2. Task-specific instructions: Agents can be given task-specific instructions to guide their planning process. For example, if the task is to write a novel, the agent can be instructed to ""Write a story outline."" This provides a high-level structure for the task and helps in breaking it down into smaller components.\\n\\n3. Human inputs: Agents can also take inputs from humans to decompose tasks. This can be done through direct communication or by leveraging human expertise. Humans can provide guidance and insights to help the agent break down complex tasks into manageable subgoals.\\n\\nOverall, task decomposition allows agents to break down large tasks into smaller, more manageable subgoals, enabling them to plan and execute complex tasks efficiently.\'} ``` ```python qa(""What are the various ways to implement memory to support it?"") ``` ```text {\'question\': \'What are the various ways to implement memory to support it?\', \'chat_history\': [SystemMessage(content=\'The human asks how agents use task decomposition. The AI explains that agents can use task decomposition in several ways, including simple prompting, task-specific instructions, and human inputs. Task decomposition allows agents to break down large tasks into smaller, more manageable subgoals, enabling them to plan and execute complex tasks efficiently.\', additional_kwargs={})], \'answer\': \'There are several ways to implement memory to support task decomposition:\\n\\n1. Long-Term Memory Management: This involves storing and organizing information in a long-term memory system. The agent can retrieve past experiences, knowledge, and learned strategies to guide the task decomposition process.\\n\\n2. Internet Access: The agent can use internet access to search for relevant information and gather resources to aid in task decomposition. This allows the agent to access a vast amount of information and utilize it in the decomposition process.\\n\\n3. GPT-3.5 Powered Agents: The agent can delegate simple tasks to GPT-3.5 powered agents. These agents can perform specific tasks or provide assistance in task decomposition, allowing the main agent to focus on higher-level planning and decision-making.\\n\\n4. File Output: The agent can store the results of task decomposition in files or documents. This allows for easy retrieval and reference during the execution of the task.\\n\\nThese memory resources help the agent in organizing and managing information, making informed decisions, and effectively decomposing complex tasks into smaller, manageable subgoals.\'} ``` Again, we can use the [LangSmith trace]( to explore the prompt structure. ### Going deeper - Agents, such as the [conversational retrieval agent](/docs/use_cases/question_answering/conversational_retrieval_agents), can be used for retrieval when necessary while also holding a conversation. - [Use case](#use-case) - [Overview](#overview) - [Quickstart](#quickstart) - [Memory](#memory) - [Conversation](#conversation) - [Chat Retrieval](#chat-retrieval)- [Going deeper](#going-deeper)']","To use a prompt template with chat history in LangChain, you can combine a `ChatPromptTemplate` with `ConversationBufferMemory`. Here's a simple example:

```python
from langchain.chat_models import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate

# Initialize memory
memory = ConversationBufferMemory(memory_key=""chat_history"", return_messages=True)

# Define the prompt template
prompt = ChatPromptTemplate.from_messages([
    SystemMessagePromptTemplate.from_template(""You are a nice chatbot having a conversation with a human.""),
    MessagesPlaceholder(variable_name=""chat_history""),
    HumanMessagePromptTemplate.from_template(""{question}"")
])

# Initialize the chat model
chat = ChatOpenAI()

# Create a chain with the prompt and memory
conversation = LLMChain(llm=chat, prompt=prompt, verbose=True, memory=memory)

# Run the chain with a question
response = conversation.run({""question"": ""hi""})
print(response)
```

In this example, `ConversationBufferMemory` stores the chat history, and `MessagesPlaceholder` inserts the chat history into the prompt template. The `LLMChain` uses the chat model to generate responses based on the prompt and memory.","To add chat history to a prompt template, you can use the MessagesPlaceholder class.

```
from langchain.prompts import MessagesPlaceholder, ChatPromptTemplate

chat_template = ChatPromptTemplate.from_messages(
    [
        (""system"", ""You are a helpful AI bot.""),
        MessagesPlaceholder(variable_name=""chat_history"")
        (""human"", ""{input}""),
    ]
)
```
",0.5833333333041666,1.0,,0.009130318956332041,0.2487046632124352
87,How do i run llama 2 in langchain,"['WebResearchRetriever | WebResearchRetriever Given a query, this retriever will: - Formulate a set of relate Google searches - Search for each - Load all the resulting URLs - Then embed and perform similarity search with the query on the consolidate page content ```python from langchain.retrievers.web_research import WebResearchRetriever ``` ### Simple usage Specify the LLM to use for Google search query generation. ```python import os from langchain.chat_models.openai import ChatOpenAI from langchain.embeddings import OpenAIEmbeddings from langchain.utilities import GoogleSearchAPIWrapper from langchain.vectorstores import Chroma # Vectorstore vectorstore = Chroma( embedding_function=OpenAIEmbeddings(), persist_directory=""./chroma_db_oai"" ) # LLM llm = ChatOpenAI(temperature=0) # Search os.environ[""GOOGLE_CSE_ID""] = ""xxx"" os.environ[""GOOGLE_API_KEY""] = ""xxx"" search = GoogleSearchAPIWrapper() ``` ```python # Initialize web_research_retriever = WebResearchRetriever.from_llm( vectorstore=vectorstore, llm=llm, search=search, ) ``` #### Run with citations We can use `RetrievalQAWithSourcesChain` to retrieve docs and provide citations. ```python from langchain.chains import RetrievalQAWithSourcesChain user_input = ""How do LLM Powered Autonomous Agents work?"" qa_chain = RetrievalQAWithSourcesChain.from_chain_type( llm, retriever=web_research_retriever ) result = qa_chain({""question"": user_input}) result ``` ```text Fetching pages: 100%|###################################################################################################################################| 1/1 [00:00<00:00, 3.33it/s] {\'question\': \'How do LLM Powered Autonomous Agents work?\', \'answer\': ""LLM Powered Autonomous Agents work by using LLM (large language model) as the core controller of the agent\'s brain. It is complemented by several key components, including planning, memory, and tool use. The agent system is designed to be a powerful general problem solver. \\n"", \'sources\': \' ``` #### Run with logging Here, we use `get_relevant_documents` method to return docs. ```python # Run import logging logging.basicConfig() logging.getLogger(""langchain.retrievers.web_research"").setLevel(logging.INFO) user_input = ""What is Task Decomposition in LLM Powered Autonomous Agents?"" docs = web_research_retriever.get_relevant_documents(user_input) ``` ```text INFO:langchain.retrievers.web_research:Generating questions for Google Search ... INFO:langchain.retrievers.web_research:Questions for Google Search (raw): {\'question\': \'What is Task Decomposition in LLM Powered Autonomous Agents?\', \'text\': LineList(lines=[\'1. How do LLM powered autonomous agents utilize task decomposition?\\n\', \'2. Can you explain the concept of task decomposition in LLM powered autonomous agents?\\n\', \'3. What role does task decomposition play in the functioning of LLM powered autonomous agents?\\n\', \'4. Why is task decomposition important for LLM powered autonomous agents?\\n\'])} INFO:langchain.retrievers.web_research:Questions for Google Search: [\'1. How do LLM powered autonomous agents utilize task decomposition?\\n\', \'2. Can you explain the concept of task decomposition in LLM powered autonomous agents?\\n\', \'3. What role does task decomposition play in the functioning of LLM powered autonomous agents?\\n\', \'4. Why is task decomposition important for LLM powered autonomous agents?\\n\'] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{\'title\': ""LLM Powered Autonomous Agents | Lil\'Log"", \'link\': \' \'snippet\': \'Jun 23, 2023 ... Task decomposition can be done (1) by LLM with simple prompting like ""Steps for XYZ.\\\\n1."" , ""What are the subgoals for achieving XYZ?\'}] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{\'title\': ""LLM Powered Autonomous Agents | Lil\'Log"", \'link\': \' \'snippet\': \'Jun 23, 2023 ... Task decomposition can be done (1) by LLM with simple prompting like ""Steps for XYZ.\\\\n1."" , ""What are the subgoals for achieving XYZ?"" , (2)\\xa0...\'}] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{\'title\': ""LLM Powered Autonomous Agents | Lil\'Log"", \'link\': \' \'snippet\': \'Jun 23, 2023 ... In a LLM-powered autonomous agent system, LLM functions as the ... Task decomposition can be done (1) by LLM with simple prompting like\\xa0...\'}] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{\'title\': ""LLM Powered Autonomous Agents | Lil\'Log"", \'link\': \' \'snippet\': \'Jun 23, 2023 ... Agent System Overview In a LLM-powered autonomous agent system, ... Task decomposition can be done (1) by LLM with simple prompting like\\xa0...\'}] INFO:langchain.retrievers.web_research:New URLs to load: [] ``` #### Generate answer using retrieved docs We can use `load_qa_chain` for QA using the retrieved docs. ```python from langchain.chains.question_answering import load_qa_chain chain = load_qa_chain(llm, chain_type=""stuff"") output = chain( {""input_documents"": docs, ""question"": user_input}, return_only_outputs=True ) output[""output_text""] ``` ```text \'Task decomposition in LLM-powered autonomous agents refers to the process of breaking down a complex task into smaller, more manageable subgoals. This allows the agent to efficiently handle and execute the individual steps required to complete the overall task. By decomposing the task, the agent can prioritize and organize its actions, making it easier to plan and execute the necessary steps towards achieving the desired outcome.\' ``` ### More flexibility Pass an LLM chain with custom prompt and output parsing. ```python import os import re from typing import List from langchain.chains import LLMChain from langchain.output_parsers.pydantic import PydanticOutputParser from langchain.prompts import PromptTemplate from pydantic import BaseModel, Field # LLMChain search_prompt = PromptTemplate( input_variables=[""question""], template=""""""You are an assistant tasked with improving Google search results. Generate FIVE Google search queries that are similar to this question. The output should be a numbered list of questions and each should have a question mark at the end: {question}"""""", ) class LineList(BaseModel): """"""List of questions."""""" lines: List[str] = Field(description=""Questions"") class QuestionListOutputParser(PydanticOutputParser): """"""Output parser for a list of numbered questions."""""" def __init__(self) -> None: super().__init__(pydantic_object=LineList) def parse(self, text: str) -> LineList: lines = re.findall(r""\\d+\\..*?\\n"", text) return LineList(lines=lines) llm_chain = LLMChain( llm=llm, prompt=search_prompt, output_parser=QuestionListOutputParser(), ) ``` ```python # Initialize web_research_retriever_llm_chain = WebResearchRetriever( vectorstore=vectorstore, llm_chain=llm_chain, search=search, ) # Run docs = web_research_retriever_llm_chain.get_relevant_documents(user_input) ``` ```text INFO:langchain.retrievers.web_research:Generating questions for Google Search ... INFO:langchain.retrievers.web_research:Questions for Google Search (raw): {\'question\': \'What is Task Decomposition in LLM Powered Autonomous Agents?\', \'text\': LineList(lines=[\'1. How do LLM powered autonomous agents use task decomposition?\\n\', \'2. Why is task decomposition important for LLM powered autonomous agents?\\n\', \'3. Can you explain the concept of task decomposition in LLM powered autonomous agents?\\n\', \'4. What are the benefits of task decomposition in LLM powered autonomous agents?\\n\'])} INFO:langchain.retrievers.web_research:Questions for Google Search: [\'1. How do LLM powered autonomous agents use task decomposition?\\n\', \'2. Why is task decomposition important for LLM powered autonomous agents?\\n\', \'3. Can you explain the concept of task decomposition in LLM powered autonomous agents?\\n\', \'4. What are the benefits of task decomposition in LLM powered autonomous agents?\\n\'] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{\'title\': ""LLM Powered Autonomous Agents | Lil\'Log"", \'link\': \' \'snippet\': \'Jun 23, 2023 ... Task decomposition can be done (1) by LLM with simple prompting like ""Steps for XYZ.\\\\n1."" , ""What are the subgoals for achieving XYZ?\'}] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{\'title\': ""LLM Powered Autonomous Agents | Lil\'Log"", \'link\': \' \'snippet\': \'Jun 23, 2023 ... Task decomposition can be done (1) by LLM with simple prompting like ""Steps for XYZ.\\\\n1."" , ""What are the subgoals for achieving XYZ?"" , (2)\\xa0...\'}] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{\'title\': ""LLM Powered Autonomous Agents | Lil\'Log"", \'link\': \' \'snippet\': \'Jun 23, 2023 ... Task decomposition can be done (1) by LLM with simple prompting like ""Steps for XYZ.\\\\n1."" , ""What are the subgoals for achieving XYZ?\'}] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{\'title\': ""LLM Powered Autonomous Agents | Lil\'Log"", \'link\': \' \'snippet\': \'Jun 23, 2023 ... Task decomposition can be done (1) by LLM with simple prompting like ""Steps for XYZ.\\\\n1."" , ""What are the subgoals for achieving XYZ?\'}] INFO:langchain.retrievers.web_research:New URLs to load: [\' INFO:langchain.retrievers.web_research:Grabbing most relevant splits from urls ... Fetching pages: 100%|###################################################################################################################################| 1/1 [00:00<00:00, 6.32it/s] ``` ```python len(docs) ``` ```text 1 ``` ### Run locally Specify LLM and embeddings that will run locally (e.g., on your laptop). ```python from langchain.callbacks.manager import CallbackManager from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler from langchain.embeddings import GPT4AllEmbeddings from langchain.llms import LlamaCpp n_gpu_layers = 1 # Metal set to 1 is enough. n_batch = 512 # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip. callback_manager = CallbackManager([StreamingStdOutCallbackHandler()]) llama = LlamaCpp( model_path=""/Users/rlm/Desktop/Code/llama.cpp/llama-2-13b-chat.ggmlv3.q4_0.bin"", n_gpu_layers=n_gpu_layers, n_batch=n_batch, n_ctx=4096, # Context window max_tokens=1000, # Max tokens to generate f16_kv=True, # MUST set to True, otherwise you will run into problem after a couple of calls callback_manager=callback_manager, verbose=True, ) vectorstore_llama = Chroma( embedding_function=GPT4AllEmbeddings(), persist_directory=""./chroma_db_llama"" ) ``` ```text llama.cpp: loading model from /Users/rlm/Desktop/Code/llama.cpp/llama-2-13b-chat.ggmlv3.q4_0.bin llama_model_load_internal: format = ggjt v3 (latest) llama_model_load_internal: n_vocab = 32000 llama_model_load_internal: n_ctx = 4096 llama_model_load_internal: n_embd = 5120 llama_model_load_internal: n_mult = 256 llama_model_load_internal: n_head = 40 llama_model_load_internal: n_layer = 40 llama_model_load_internal: n_rot = 128 llama_model_load_internal: freq_base = 10000.0 llama_model_load_internal: freq_scale = 1 llama_model_load_internal: ftype = 2 (mostly Q4_0) llama_model_load_internal: n_ff = 13824 llama_model_load_internal: model size = 13B llama_model_load_internal: ggml ctx size = 0.09 MB llama_model_load_internal: mem required = 9132.71 MB (+ 1608.00 MB per state) llama_new_context_with_model: kv self size = 3200.00 MB ggml_metal_init: allocating Found model file at /Users/rlm/.cache/gpt4all/ggml-all-MiniLM-L6-v2-f16.bin llama_new_context_with_model: max tensor size = 87.89 MB ggml_metal_init: using MPS ggml_metal_init: loading \'/Users/rlm/miniforge3/envs/llama/lib/python3.9/site-packages/llama_cpp/ggml-metal.metal\' ggml_metal_init: loaded kernel_add 0x110fbd600 ggml_metal_init: loaded kernel_mul 0x110fbeb30 ggml_metal_init: loaded kernel_mul_row 0x110fbf350 ggml_metal_init: loaded kernel_scale 0x110fbf9e0 ggml_metal_init: loaded kernel_silu 0x110fc0150 ggml_metal_init: loaded kernel_relu 0x110fbd950 ggml_metal_init: loaded kernel_gelu 0x110fbdbb0 ggml_metal_init: loaded kernel_soft_max 0x110fc14d0 ggml_metal_init: loaded kernel_diag_mask_inf 0x110fc1980 ggml_metal_init: loaded kernel_get_rows_f16 0x110fc22a0 ggml_metal_init: loaded kernel_get_rows_q4_0 0x110fc2ad0 ggml_metal_init: loaded kernel_get_rows_q4_1 0x110fc3260 ggml_metal_init: loaded kernel_get_rows_q2_K 0x110fc3ad0 ggml_metal_init: loaded kernel_get_rows_q3_K 0x110fc41c0 ggml_metal_init: loaded kernel_get_rows_q4_K 0x110fc48c0 ggml_metal_init: loaded kernel_get_rows_q5_K 0x110fc4fa0 ggml_metal_init: loaded kernel_get_rows_q6_K 0x110fc56a0 ggml_metal_init: loaded kernel_rms_norm 0x110fc5da0 ggml_metal_init: loaded kernel_norm 0x110fc64d0 ggml_metal_init: loaded kernel_mul_mat_f16_f32 0x2a5c19990 ggml_metal_init: loaded kernel_mul_mat_q4_0_f32 0x2a5c1d4a0 ggml_metal_init: loaded kernel_mul_mat_q4_1_f32 0x2a5c19fc0 ggml_metal_init: loaded kernel_mul_mat_q2_K_f32 0x2a5c1dcc0 ggml_metal_init: loaded kernel_mul_mat_q3_K_f32 0x2a5c1e420 ggml_metal_init: loaded kernel_mul_mat_q4_K_f32 0x2a5c1edc0 ggml_metal_init: loaded kernel_mul_mat_q5_K_f32 0x2a5c1fd90 ggml_metal_init: loaded kernel_mul_mat_q6_K_f32 0x2a5c20540 ggml_metal_init: loaded kernel_rope 0x2a5c20d40 ggml_metal_init: loaded kernel_alibi_f32 0x2a5c21730 ggml_metal_init: loaded kernel_cpy_f32_f16 0x2a5c21ab0 ggml_metal_init: loaded kernel_cpy_f32_f32 0x2a5c22080 ggml_metal_init: loaded kernel_cpy_f16_f16 0x2a5c231d0 ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB ggml_metal_init: hasUnifiedMemory = true ggml_metal_init: maxTransferRate = built-in GPU ggml_metal_add_buffer: allocated \'data \' buffer, size = 6984.06 MB, ( 6984.52 / 21845.34) ggml_metal_add_buffer: allocated \'eval \' buffer, size = 1040.00 MB, ( 8024.52 / 21845.34) ggml_metal_add_buffer: allocated \'kv \' buffer, size = 3202.00 MB, (11226.52 / 21845.34) ggml_metal_add_buffer: allocated \'scr0 \' buffer, size = 597.00 MB, (11823.52 / 21845.34) AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | ggml_metal_add_buffer: allocated \'scr1 \' buffer, size = 512.00 MB, (12335.52 / 21845.34) objc[33471]: Class GGMLMetalClass is implemented in both /Users/rlm/miniforge3/envs/llama/lib/python3.9/site-packages/llama_cpp/libllama.dylib (0x2c7368208) and /Users/rlm/miniforge3/envs/llama/lib/python3.9/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libreplit-mainline-metal.dylib (0x5ebf48208). One of the two will be used. Which one is undefined. objc[33471]: Class GGMLMetalClass is implemented in both /Users/rlm/miniforge3/envs/llama/lib/python3.9/site-packages/llama_cpp/libllama.dylib (0x2c7368208) and /Users/rlm/miniforge3/envs/llama/lib/python3.9/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libllamamodel-mainline-metal.dylib (0x5ec374208). One of the two will be used. Which one is undefined. ``` We supplied `StreamingStdOutCallbackHandler()`, so model outputs (e.g., generated questions) are streamed. We also have logging on, so we seem them there too. ```python from langchain.chains import RetrievalQAWithSourcesChain # Initialize web_research_retriever = WebResearchRetriever.from_llm( vectorstore=vectorstore_llama, llm=llama, search=search, ) # Run user_input = ""What is Task Decomposition in LLM Powered Autonomous Agents?"" qa_chain = RetrievalQAWithSourcesChain.from_chain_type( llama, retriever=web_research_retriever ) result = qa_chain({""question"": user_input}) result ``` ```text INFO:langchain.retrievers.web_research:Generating questions for Google Search ... Sure, here are five Google search queries that are similar to ""What is Task Decomposition in LLM Powered Autonomous Agents?"": 1. How does Task Decomposition work in LLM Powered Autonomous Agents? 2. What are the benefits of using Task Decomposition in LLM Powered Autonomous Agents? 3. Can you provide examples of Task Decomposition in LLM Powered Autonomous Agents? 4. How does Task Decomposition improve the performance of LLM Powered Autonomous Agents? 5. What are some common challenges or limitations of using Task Decomposition in LLM Powered Autonomous Agents, and how can they be addressed? llama_print_timings: load time = 8585.01 ms llama_print_timings: sample time = 124.24 ms / 164 runs ( 0.76 ms per token, 1320.04 tokens per second) llama_print_timings: prompt eval time = 8584.83 ms / 101 tokens ( 85.00 ms per token, 11.76 tokens per second) llama_print_timings: eval time = 7268.55 ms / 163 runs ( 44.59 ms per token, 22.43 tokens per second) llama_print_timings: total time = 16236.13 ms INFO:langchain.retrievers.web_research:Questions for Google Search (raw): {\'question\': \'What is Task Decomposition in LLM Powered Autonomous Agents?\', \'text\': LineList(lines=[\'1. How does Task Decomposition work in LLM Powered Autonomous Agents? \\n\', \'2. What are the benefits of using Task Decomposition in LLM Powered Autonomous Agents? \\n\', \'3. Can you provide examples of Task Decomposition in LLM Powered Autonomous Agents? \\n\', \'4. How does Task Decomposition improve the performance of LLM Powered Autonomous Agents? \\n\'])} INFO:langchain.retrievers.web_research:Questions for Google Search: [\'1. How does Task Decomposition work in LLM Powered Autonomous Agents? \\n\', \'2. What are the benefits of using Task Decomposition in LLM Powered Autonomous Agents? \\n\', \'3. Can you provide examples of Task Decomposition in LLM Powered Autonomous Agents? \\n\', \'4. How does Task Decomposition improve the performance of LLM Powered Autonomous Agents? \\n\'] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{\'title\': ""LLM Powered Autonomous Agents | Lil\'Log"", \'link\': \' \'snippet\': \'Jun 23, 2023 ... Task decomposition can be done (1) by LLM with simple prompting like ""Steps for XYZ.\\\\n1."" , ""What are the subgoals for achieving XYZ?\'}] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{\'title\': ""LLM Powered Autonomous Agents | Lil\'Log"", \'link\': \' \'snippet\': \'Jun 23, 2023 ... Task decomposition can be done (1) by LLM with simple prompting like ""Steps for XYZ.\\\\n1."" , ""What are the subgoals for achieving XYZ?"" , (2)\\xa0...\'}] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{\'title\': ""LLM Powered Autonomous Agents | Lil\'Log"", \'link\': \' \'snippet\': \'Jun 23, 2023 ... A complicated task usually involves many steps. An agent needs to know what they are and plan ahead. Task Decomposition#. Chain of thought (CoT;\\xa0...\'}] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{\'title\': ""LLM Powered Autonomous Agents | Lil\'Log"", \'link\': \' \'snippet\': \'Jun 23, 2023 ... Agent System Overview In a LLM-powered autonomous agent system, ... Task decomposition can be done (1) by LLM with simple prompting like\\xa0...\'}] INFO:langchain.retrievers.web_research:New URLs to load: [\' INFO:langchain.retrievers.web_research:Grabbing most relevant splits from urls ... Fetching pages: 100%|###################################################################################################################################| 1/1 [00:00<00:00, 10.49it/s] Llama.generate: prefix-match hit The content discusses Task Decomposition in LLM Powered Autonomous Agents, which involves breaking down large tasks into smaller, manageable subgoals for efficient handling of complex tasks. SOURCES: llama_print_timings: load time = 8585.01 ms llama_print_timings: sample time = 52.88 ms / 72 runs ( 0.73 ms per token, 1361.55 tokens per second) llama_print_timings: prompt eval time = 125925.13 ms / 2358 tokens ( 53.40 ms per token, 18.73 tokens per second) llama_print_timings: eval time = 3504.16 ms / 71 runs ( 49.35 ms per token, 20.26 tokens per second) llama_print_timings: total time = 129584.60 ms {\'question\': \'What is Task Decomposition in LLM Powered Autonomous Agents?\', \'answer\': \' The content discusses Task Decomposition in LLM Powered Autonomous Agents, which involves breaking down large tasks into smaller, manageable subgoals for efficient handling of complex tasks.\\n\', \'sources\': \' ``` - [Simple usage](#simple-usage) - [More flexibility](#more-flexibility) - [Run locally](#run-locally)', 'RAG using local models | RAG using local models The popularity of projects like [PrivateGPT]( [llama.cpp]( and [GPT4All]( underscore the importance of running LLMs locally. LangChain has [integrations]( with many open-source LLMs that can be run locally. See [here](/docs/use_cases/question_answering/docs/guides/local_llms) for setup instructions for these LLMs. For example, here we show how to run `GPT4All` or `LLaMA2` locally (e.g., on your laptop) using local embeddings and a local LLM. ## Document Loading First, install packages needed for local embeddings and vector storage. ```python pip install gpt4all chromadb langchainhub ``` Load and split an example document. We\'ll use a blog post on agents as an example. ```python from langchain.document_loaders import WebBaseLoader loader = WebBaseLoader("" data = loader.load() from langchain.text_splitter import RecursiveCharacterTextSplitter text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0) all_splits = text_splitter.split_documents(data) ``` Next, the below steps will download the `GPT4All` embeddings locally (if you don\'t already have them). ```python from langchain.embeddings import GPT4AllEmbeddings from langchain.vectorstores import Chroma vectorstore = Chroma.from_documents(documents=all_splits, embedding=GPT4AllEmbeddings()) ``` ```text Found model file at /Users/rlm/.cache/gpt4all/ggml-all-MiniLM-L6-v2-f16.bin objc[49534]: Class GGMLMetalClass is implemented in both /Users/rlm/miniforge3/envs/llama2/lib/python3.9/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libreplit-mainline-metal.dylib (0x131614208) and /Users/rlm/miniforge3/envs/llama2/lib/python3.9/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libllamamodel-mainline-metal.dylib (0x131988208). One of the two will be used. Which one is undefined. ``` Test similarity search is working with our local embeddings. ```python question = ""What are the approaches to Task Decomposition?"" docs = vectorstore.similarity_search(question) len(docs) ``` ```text 4 ``` ```python docs[0] ``` ```text Document(page_content=\'Task decomposition can be done (1) by LLM with simple prompting like ""Steps for XYZ.\\\\n1."", ""What are the subgoals for achieving XYZ?"", (2) by using task-specific instructions; e.g. ""Write a story outline."" for writing a novel, or (3) with human inputs.\', metadata={\'description\': \'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent\'s brain, complemented by several key components:\', \'language\': \'en\', \'source\': \' \'title\': ""LLM Powered Autonomous Agents | Lil\'Log""}) ``` ## Model ### LLaMA2 Note: new versions of `llama-cpp-python` use GGUF model files (see [here]( If you have an existing GGML model, see [here](/docs/use_cases/question_answering/docs/integrations/llms/llamacpp) for instructions for conversion for GGUF. And / or, you can download a GGUF converted model (e.g., [here]( Finally, as noted in detail [here](/docs/use_cases/question_answering/docs/guides/local_llms) install `llama-cpp-python` ```python pip install llama-cpp-python ``` To enable use of GPU on Apple Silicon, follow the steps [here]( to use the Python binding `with Metal support`. In particular, ensure that `conda` is using the correct virtual environment that you created (`miniforge3`). E.g., for me: ```text conda activate /Users/rlm/miniforge3/envs/llama ``` With this confirmed: ```bash CMAKE_ARGS=""-DLLAMA_METAL=on"" FORCE_CMAKE=1 /Users/rlm/miniforge3/envs/llama/bin/pip install -U llama-cpp-python --no-cache-dir ``` ```python from langchain.callbacks.manager import CallbackManager from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler from langchain.llms import LlamaCpp ``` Setting model parameters as noted in the [llama.cpp docs]( ```python n_gpu_layers = 1 # Metal set to 1 is enough. n_batch = 512 # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip. callback_manager = CallbackManager([StreamingStdOutCallbackHandler()]) # Make sure the model path is correct for your system! llm = LlamaCpp( model_path=""/Users/rlm/Desktop/Code/llama.cpp/models/llama-2-13b-chat.ggufv3.q4_0.bin"", n_gpu_layers=n_gpu_layers, n_batch=n_batch, n_ctx=2048, f16_kv=True, # MUST set to True, otherwise you will run into problem after a couple of calls callback_manager=callback_manager, verbose=True, ) ``` Note that these indicate that [Metal was enabled properly]( ```text ggml_metal_init: allocating ggml_metal_init: using MPS ``` ```python llm(""Simulate a rap battle between Stephen Colbert and John Oliver"") ``` ```text Llama.generate: prefix-match hit by jonathan Here\'s the hypothetical rap battle: [Stephen Colbert]: Yo, this is Stephen Colbert, known for my comedy show. I\'m here to put some sense in your mind, like an enema do-go. Your opponent? A man of laughter and witty quips, John Oliver! Now let\'s see who gets the most laughs while taking shots at each other [John Oliver]: Yo, this is John Oliver, known for my own comedy show. I\'m here to take your mind on an adventure through wit and humor. But first, allow me to you to our contestant: Stephen Colbert! His show has been around since the \'90s, but it\'s time to see who can out-rap whom [Stephen Colbert]: You claim to be a witty man, John Oliver, with your British charm and clever remarks. But my knows that I\'m America\'s funnyman! Who\'s the one taking you? Nobody! [John Oliver]: Hey Stephen Colbert, don\'t get too cocky. You may llama_print_timings: load time = 4481.74 ms llama_print_timings: sample time = 183.05 ms / 256 runs ( 0.72 ms per token, 1398.53 tokens per second) llama_print_timings: prompt eval time = 456.05 ms / 13 tokens ( 35.08 ms per token, 28.51 tokens per second) llama_print_timings: eval time = 7375.20 ms / 255 runs ( 28.92 ms per token, 34.58 tokens per second) llama_print_timings: total time = 8388.92 ms ""by jonathan \\n\\nHere\'s the hypothetical rap battle:\\n\\n[Stephen Colbert]: Yo, this is Stephen Colbert, known for my comedy show. I\'m here to put some sense in your mind, like an enema do-go. Your opponent? A man of laughter and witty quips, John Oliver! Now let\'s see who gets the most laughs while taking shots at each other\\n\\n[John Oliver]: Yo, this is John Oliver, known for my own comedy show. I\'m here to take your mind on an adventure through wit and humor. But first, allow me to you to our contestant: Stephen Colbert! His show has been around since the \'90s, but it\'s time to see who can out-rap whom\\n\\n[Stephen Colbert]: You claim to be a witty man, John Oliver, with your British charm and clever remarks. But my knows that I\'m America\'s funnyman! Who\'s the one taking you? Nobody!\\n\\n[John Oliver]: Hey Stephen Colbert, don\'t get too cocky. You may"" ``` ### GPT4All Similarly, we can use `GPT4All`. [Download the GPT4All model binary]( The Model Explorer on the [GPT4All]( is a great way to choose and download a model. Then, specify the path that you downloaded to to. E.g., for me, the model lives here: `/Users/rlm/Desktop/Code/gpt4all/models/nous-hermes-13b.ggmlv3.q4_0.bin` ```python from langchain.llms import GPT4All llm = GPT4All( model=""/Users/rlm/Desktop/Code/gpt4all/models/nous-hermes-13b.ggmlv3.q4_0.bin"", max_tokens=2048, ) ``` ## LLMChain Run an `LLMChain` (see [here]( with either model by passing in the retrieved docs and a simple prompt. It formats the prompt template using the input key values provided and passes the formatted string to `GPT4All`, `LLama-V2`, or another specified LLM. In this case, the list of retrieved documents (`docs`) above are pass into `{context}`. ```python from langchain.chains import LLMChain from langchain.prompts import PromptTemplate # Prompt prompt = PromptTemplate.from_template( ""Summarize the main themes in these retrieved docs: {docs}"" ) # Chain llm_chain = LLMChain(llm=llm, prompt=prompt) # Run question = ""What are the approaches to Task Decomposition?"" docs = vectorstore.similarity_search(question) result = llm_chain(docs) # Output result[""text""] ``` ```text Llama.generate: prefix-match hit Based on the retrieved documents, the main themes are: 1. Task decomposition: The ability to break down complex tasks into smaller subtasks, which can be handled by an LLM or other components of the agent system. 2. LLM as the core controller: The use of a large language model (LLM) as the primary controller of an autonomous agent system, complemented by other key components such as a knowledge graph and a planner. 3. Potentiality of LLM: The idea that LLMs have the potential to be used as powerful general problem solvers, not just for generating well-written copies but also for solving complex tasks and achieving human-like intelligence. 4. Challenges in long-term planning: The challenges in planning over a lengthy history and effectively exploring the solution space, which are important limitations of current LLM-based autonomous agent systems. llama_print_timings: load time = 1191.88 ms llama_print_timings: sample time = 134.47 ms / 193 runs ( 0.70 ms per token, 1435.25 tokens per second) llama_print_timings: prompt eval time = 39470.18 ms / 1055 tokens ( 37.41 ms per token, 26.73 tokens per second) llama_print_timings: eval time = 8090.85 ms / 192 runs ( 42.14 ms per token, 23.73 tokens per second) llama_print_timings: total time = 47943.12 ms \'\\nBased on the retrieved documents, the main themes are:\\n1. Task decomposition: The ability to break down complex tasks into smaller subtasks, which can be handled by an LLM or other components of the agent system.\\n2. LLM as the core controller: The use of a large language model (LLM) as the primary controller of an autonomous agent system, complemented by other key components such as a knowledge graph and a planner.\\n3. Potentiality of LLM: The idea that LLMs have the potential to be used as powerful general problem solvers, not just for generating well-written copies but also for solving complex tasks and achieving human-like intelligence.\\n4. Challenges in long-term planning: The challenges in planning over a lengthy history and effectively exploring the solution space, which are important limitations of current LLM-based autonomous agent systems.\' ``` ## QA Chain We can use a `QA chain` to handle our question above. `chain_type=""stuff""` (see [here]( means that all the docs will be added (stuffed) into a prompt. We can also use the LangChain Prompt Hub to store and fetch prompts that are model-specific. This will work with your [LangSmith API key]( Let\'s try with a default RAG prompt, [here]( ```python pip install langchainhub ``` ```python # Prompt from langchain import hub rag_prompt = hub.pull(""rlm/rag-prompt"") from langchain.chains.question_answering import load_qa_chain # Chain chain = load_qa_chain(llm, chain_type=""stuff"", prompt=rag_prompt) # Run chain({""input_documents"": docs, ""question"": question}, return_only_outputs=True) ``` ```text Llama.generate: prefix-match hit Task can be done by down a task into smaller subtasks, using simple prompting like ""Steps for XYZ."" or task-specific like ""Write a story outline"" for writing a novel. llama_print_timings: load time = 11326.20 ms llama_print_timings: sample time = 33.03 ms / 47 runs ( 0.70 ms per token, 1422.86 tokens per second) llama_print_timings: prompt eval time = 1387.31 ms / 242 tokens ( 5.73 ms per token, 174.44 tokens per second) llama_print_timings: eval time = 1321.62 ms / 46 runs ( 28.73 ms per token, 34.81 tokens per second) llama_print_timings: total time = 2801.08 ms {\'output_text\': \'\\nTask can be done by down a task into smaller subtasks, using simple prompting like ""Steps for XYZ."" or task-specific like ""Write a story outline"" for writing a novel.\'} ``` Now, let\'s try with [a prompt specifically for LLaMA]( which [includes special tokens]( ```python # Prompt rag_prompt_llama = hub.pull(""rlm/rag-prompt-llama"") rag_prompt_llama ``` ```text ChatPromptTemplate(input_variables=[\'question\', \'context\'], output_parser=None, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[\'question\', \'context\'], output_parser=None, partial_variables={}, template=""[INST]> You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise.> \\nQuestion: {question} \\nContext: {context} \\nAnswer: [/INST]"", template_format=\'f-string\', validate_template=True), additional_kwargs={})]) ``` ```python # Chain chain = load_qa_chain(llm, chain_type=""stuff"", prompt=rag_prompt_llama) # Run chain({""input_documents"": docs, ""question"": question}, return_only_outputs=True) ``` ```text Llama.generate: prefix-match hit Sure, I\'d be happy to help! Based on the context, here are some to task: 1. LLM with simple prompting: This using a large model (LLM) with simple prompts like ""Steps for XYZ"" or ""What are the subgoals for achieving XYZ?"" to decompose tasks into smaller steps. 2. Task-specific: Another is to use task-specific, such as ""Write a story outline"" for writing a novel, to guide the of tasks. 3. Human inputs:, human inputs can be used to supplement the process, in cases where the task a high degree of creativity or expertise. As fores in long-term and task, one major is that LLMs to adjust plans when faced with errors, making them less robust to humans who learn from trial and error. llama_print_timings: load time = 11326.20 ms llama_print_timings: sample time = 144.81 ms / 207 runs ( 0.70 ms per token, 1429.47 tokens per second) llama_print_timings: prompt eval time = 1506.13 ms / 258 tokens ( 5.84 ms per token, 171.30 tokens per second) llama_print_timings: eval time = 6231.92 ms / 206 runs ( 30.25 ms per token, 33.06 tokens per second) llama_print_timings: total time = 8158.41 ms {\'output_text\': \' Sure, I\\\'d be happy to help! Based on the context, here are some to task:\\n\\n1. LLM with simple prompting: This using a large model (LLM) with simple prompts like ""Steps for XYZ"" or ""What are the subgoals for achieving XYZ?"" to decompose tasks into smaller steps.\\n2. Task-specific: Another is to use task-specific, such as ""Write a story outline"" for writing a novel, to guide the of tasks.\\n3. Human inputs:, human inputs can be used to supplement the process, in cases where the task a high degree of creativity or expertise.\\n\\nAs fores in long-term and task, one major is that LLMs to adjust plans when faced with errors, making them less robust to humans who learn from trial and error.\'} ``` ## RetrievalQA For an even simpler flow, use `RetrievalQA`. This will use a QA default prompt (shown [here]( and will retrieve from the vectorDB. But, you can still pass in a prompt, as before, if desired. ```python from langchain.chains import RetrievalQA qa_chain = RetrievalQA.from_chain_type( llm, retriever=vectorstore.as_retriever(), chain_type_kwargs={""prompt"": rag_prompt_llama}, ) ``` ```python qa_chain({""query"": question}) ``` ```text Llama.generate: prefix-match hit Sure! Based on the context, here\'s my answer to your: There are several to task,: 1. LLM-based with simple prompting, such as ""Steps for XYZ"" or ""What are the subgoals for achieving XYZ?"" 2. Task-specific, like ""Write a story outline"" for writing a novel. 3. Human inputs to guide the process. These can be used to decompose complex tasks into smaller, more manageable subtasks, which can help improve the and effectiveness of task. However, long-term and task can being due to the need to plan over a lengthy history and explore the space., LLMs may to adjust plans when faced with errors, making them less robust to human learners who can learn from trial and error. llama_print_timings: load time = 11326.20 ms llama_print_timings: sample time = 139.20 ms / 200 runs ( 0.70 ms per token, 1436.76 tokens per second) llama_print_timings: prompt eval time = 1532.26 ms / 258 tokens ( 5.94 ms per token, 168.38 tokens per second) llama_print_timings: eval time = 5977.62 ms / 199 runs ( 30.04 ms per token, 33.29 tokens per second) llama_print_timings: total time = 7916.21 ms {\'query\': \'What are the approaches to Task Decomposition?\', \'result\': \' Sure! Based on the context, here\\\'s my answer to your:\\n\\nThere are several to task,:\\n\\n1. LLM-based with simple prompting, such as ""Steps for XYZ"" or ""What are the subgoals for achieving XYZ?""\\n2. Task-specific, like ""Write a story outline"" for writing a novel.\\n3. Human inputs to guide the process.\\n\\nThese can be used to decompose complex tasks into smaller, more manageable subtasks, which can help improve the and effectiveness of task. However, long-term and task can being due to the need to plan over a lengthy history and explore the space., LLMs may to adjust plans when faced with errors, making them less robust to human learners who can learn from trial and error.\'} ``` - [Document Loading](#document-loading) - [Model](#model)- [LLaMA2](#llama2) - [GPT4All](#gpt4all) - [LLMChain](#llmchain) - [QA Chain](#qa-chain) - [RetrievalQA](#retrievalqa)', 'RAG over code | RAG over code []( ## Use case Source code analysis is one of the most popular LLM applications (e.g., [GitHub Co-Pilot]( [Code Interpreter]( [Codium]( and [Codeium]( for use-cases such as: - Q&A over the code base to understand how it works - Using LLMs for suggesting refactors or improvements - Using LLMs for documenting the code ![Image description](/assets/images/code_understanding-cd1bda63c69e227203a1d5a7e8133887.png) ## Overview The pipeline for QA over code follows [the steps we do for document question answering](/docs/docs/use_cases/question_answering), with some differences: In particular, we can employ a [splitting strategy]( that does a few things: - Keeps each top-level function and class in the code is loaded into separate documents. - Puts remaining into a separate document. - Retains metadata about where each split comes from ## Quickstart ```bash pip install openai tiktoken chromadb langchain # Set env var OPENAI_API_KEY or load from a .env file # import dotenv # dotenv.load_dotenv() ``` We\'ll follow the structure of [this notebook]( and employ [context aware code splitting]( ### Loading We will upload all python project files using the `langchain.document_loaders.TextLoader`. The following script iterates over the files in the LangChain repository and loads every `.py` file (a.k.a. **documents**): ```python # from git import Repo from langchain.document_loaders.generic import GenericLoader from langchain.document_loaders.parsers import LanguageParser from langchain.text_splitter import Language ``` ```python # Clone repo_path = ""/Users/rlm/Desktop/test_repo"" # repo = Repo.clone_from("" to_path=repo_path) ``` We load the py code using [LanguageParser]( which will: - Keep top-level functions and classes together (into a single document) - Put remaining code into a separate document - Retains metadata about where each split comes from ```python # Load loader = GenericLoader.from_filesystem( repo_path + ""/libs/langchain/langchain"", glob=""**/*"", suffixes=["".py""], parser=LanguageParser(language=Language.PYTHON, parser_threshold=500), ) documents = loader.load() len(documents) ``` ```text 1293 ``` ### Splitting Split the `Document` into chunks for embedding and vector storage. We can use `RecursiveCharacterTextSplitter` w/ `language` specified. ```python from langchain.text_splitter import RecursiveCharacterTextSplitter python_splitter = RecursiveCharacterTextSplitter.from_language( language=Language.PYTHON, chunk_size=2000, chunk_overlap=200 ) texts = python_splitter.split_documents(documents) len(texts) ``` ```text 3748 ``` ### RetrievalQA We need to store the documents in a way we can semantically search for their content. The most common approach is to embed the contents of each document then store the embedding and document in a vector store. When setting up the vectorstore retriever: - We test [max marginal relevance](/docs/docs/use_cases/question_answering) for retrieval - And 8 documents returned #### Go deeper - Browse the > 40 vectorstores integrations [here]( - See further documentation on vectorstores [here](/docs/modules/data_connection/vectorstores/). - Browse the > 30 text embedding integrations [here]( - See further documentation on embedding models [here](/docs/modules/data_connection/text_embedding/). ```python from langchain.embeddings.openai import OpenAIEmbeddings from langchain.vectorstores import Chroma db = Chroma.from_documents(texts, OpenAIEmbeddings(disallowed_special=())) retriever = db.as_retriever( search_type=""mmr"", # Also test ""similarity"" search_kwargs={""k"": 8}, ) ``` ### Chat Test chat, just as we do for [chatbots](/docs/docs/use_cases/chatbots). #### Go deeper - Browse the > 55 LLM and chat model integrations [here]( - See further documentation on LLMs and chat models [here](/docs/modules/model_io/models/). - Use local LLMS: The popularity of [PrivateGPT]( and [GPT4All]( underscore the importance of running LLMs locally. ```python from langchain.chains import ConversationalRetrievalChain from langchain.chat_models import ChatOpenAI from langchain.memory import ConversationSummaryMemory llm = ChatOpenAI(model_name=""gpt-4"") memory = ConversationSummaryMemory( llm=llm, memory_key=""chat_history"", return_messages=True ) qa = ConversationalRetrievalChain.from_llm(llm, retriever=retriever, memory=memory) ``` ```python question = ""How can I initialize a ReAct agent?"" result = qa(question) result[""answer""] ``` ```text \'To initialize a ReAct agent, you need to follow these steps:\\n\\n1. Initialize a language model `llm` of type `BaseLanguageModel`.\\n\\n2. Initialize a document store `docstore` of type `Docstore`.\\n\\n3. Create a `DocstoreExplorer` with the initialized `docstore`. The `DocstoreExplorer` is used to search for and look up terms in the document store.\\n\\n4. Create an array of `Tool` objects. The `Tool` objects represent the actions that the agent can perform. In the case of `ReActDocstoreAgent`, the tools must be ""Search"" and ""Lookup"" with their corresponding functions from the `DocstoreExplorer`.\\n\\n5. Initialize the `ReActDocstoreAgent` using the `from_llm_and_tools` method with the `llm` (language model) and `tools` as parameters.\\n\\n6. Initialize the `ReActChain` (which is the `AgentExecutor`) using the `ReActDocstoreAgent` and `tools` as parameters.\\n\\nHere is an example of how to do this:\\n\\n```python\\nfrom langchain.chains import ReActChain, OpenAI\\nfrom langchain.docstore.base import Docstore\\nfrom langchain.docstore.document import Document\\nfrom langchain.tools.base import BaseTool\\n\\n# Initialize the LLM and a docstore\\nllm = OpenAI()\\ndocstore = Docstore()\\n\\ndocstore_explorer = DocstoreExplorer(docstore)\\ntools = [\\n Tool(\\n name=""Search"",\\n func=docstore_explorer.search,\\n description=""Search for a term in the docstore."",\\n ),\\n Tool(\\n name=""Lookup"",\\n func=docstore_explorer.lookup,\\n description=""Lookup a term in the docstore."",\\n ),\\n]\\nagent = ReActDocstoreAgent.from_llm_and_tools(llm, tools)\\nreact = ReActChain(agent=agent, tools=tools)\\n```\\n\\nKeep in mind that this is a simplified example and you might need to adapt it to your specific needs.\' ``` ```python questions = [ ""What is the class hierarchy?"", ""What classes are derived from the Chain class?"", ""What one improvement do you propose in code in relation to the class hierarchy for the Chain class?"", ] for question in questions: result = qa(question) print(f""-> **Question**: {question} \\n"") print(f""**Answer**: {result[\'answer\']} \\n"") ``` ```text -> **Question**: What is the class hierarchy? **Answer**: The class hierarchy in object-oriented programming is the structure that forms when classes are derived from other classes. The derived class is a subclass of the base class also known as the superclass. This hierarchy is formed based on the concept of inheritance in object-oriented programming where a subclass inherits the properties and functionalities of the superclass. In the given context, we have the following examples of class hierarchies: 1. `BaseCallbackHandler --> CallbackHandler` means `BaseCallbackHandler` is a base class and `CallbackHandler` (like `AimCallbackHandler`, `ArgillaCallbackHandler` etc.) are derived classes that inherit from `BaseCallbackHandler`. 2. `BaseLoader --> Loader` means `BaseLoader` is a base class and `Loader` (like `TextLoader`, `UnstructuredFileLoader` etc.) are derived classes that inherit from `BaseLoader`. 3. `ToolMetaclass --> BaseTool --> Tool` means `ToolMetaclass` is a base class, `BaseTool` is a derived class that inherits from `ToolMetaclass`, and `Tool` (like `AIPluginTool`, `BaseGraphQLTool` etc.) are further derived classes that inherit from `BaseTool`. -> **Question**: What classes are derived from the Chain class? **Answer**: The classes that are derived from the Chain class are: 1. LLMSummarizationCheckerChain 2. MapReduceChain 3. OpenAIModerationChain 4. NatBotChain 5. QAGenerationChain 6. QAWithSourcesChain 7. RetrievalQAWithSourcesChain 8. VectorDBQAWithSourcesChain 9. RetrievalQA 10. VectorDBQA 11. LLMRouterChain 12. MultiPromptChain 13. MultiRetrievalQAChain 14. MultiRouteChain 15. RouterChain 16. SequentialChain 17. SimpleSequentialChain 18. TransformChain 19. BaseConversationalRetrievalChain 20. ConstitutionalChain -> **Question**: What one improvement do you propose in code in relation to the class hierarchy for the Chain class? **Answer**: As an AI model, I don\'t have personal opinions. However, one suggestion could be to improve the documentation of the Chain class hierarchy. The current comments and docstrings provide some details but it could be helpful to include more explicit explanations about the hierarchy, roles of each subclass, and their relationships with one another. Also, incorporating UML diagrams or other visuals could help developers better understand the structure and interactions of the classes. ``` The can look at the [LangSmith trace]( to see what is happening under the hood: - In particular, the code well structured and kept together in the retrieval output - The retrieved code and chat history are passed to the LLM for answer distillation ![Image description](/assets/images/code_retrieval-238439ab9f5edfe8cbdbc6fcfbc97179.png) ### Open source LLMs We can use [Code LLaMA]( via LLamaCPP or [Ollama integration]( Note: be sure to upgrade `llama-cpp-python` in order to use the new `gguf` [file format]( ```text CMAKE_ARGS=""-DLLAMA_METAL=on"" FORCE_CMAKE=1 /Users/rlm/miniforge3/envs/llama2/bin/pip install -U llama-cpp-python --no-cache-dir ``` Check out the latest code-llama models [here]( ```python from langchain.callbacks.manager import CallbackManager from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler from langchain.chains import ConversationalRetrievalChain, LLMChain from langchain.llms import LlamaCpp from langchain.memory import ConversationSummaryMemory from langchain.prompts import PromptTemplate ``` ```python callback_manager = CallbackManager([StreamingStdOutCallbackHandler()]) llm = LlamaCpp( model_path=""/Users/rlm/Desktop/Code/llama/code-llama/codellama-13b-instruct.Q4_K_M.gguf"", n_ctx=5000, n_gpu_layers=1, n_batch=512, f16_kv=True, # MUST set to True, otherwise you will run into problem after a couple of calls callback_manager=callback_manager, verbose=True, ) ``` ```text llama_model_loader: loaded meta data with 17 key-value pairs and 363 tensors from /Users/rlm/Desktop/Code/llama/code-llama/codellama-13b-instruct.Q4_K_M.gguf (version GGUF V1 (latest)) llama_model_loader: - tensor 0: token_embd.weight q4_0 [ 5120, 32016, 1, 1 ] llama_model_loader: - tensor 1: output_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 2: output.weight f16 [ 5120, 32016, 1, 1 ] llama_model_loader: - tensor 3: blk.0.attn_q.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 4: blk.0.attn_k.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 5: blk.0.attn_v.weight q6_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 6: blk.0.attn_output.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 7: blk.0.ffn_gate.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 8: blk.0.ffn_down.weight q6_K [ 13824, 5120, 1, 1 ] llama_model_loader: - tensor 9: blk.0.ffn_up.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 10: blk.0.attn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 11: blk.0.ffn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 12: blk.1.attn_q.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 13: blk.1.attn_k.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 14: blk.1.attn_v.weight q6_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 15: blk.1.attn_output.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 16: blk.1.ffn_gate.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 17: blk.1.ffn_down.weight q6_K [ 13824, 5120, 1, 1 ] llama_model_loader: - tensor 18: blk.1.ffn_up.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 19: blk.1.attn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 20: blk.1.ffn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 21: blk.2.attn_q.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 22: blk.2.attn_k.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 23: blk.2.attn_v.weight q6_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 24: blk.2.attn_output.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 25: blk.2.ffn_gate.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 26: blk.2.ffn_down.weight q6_K [ 13824, 5120, 1, 1 ] llama_model_loader: - tensor 27: blk.2.ffn_up.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 28: blk.2.attn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 29: blk.2.ffn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 30: blk.3.attn_q.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 31: blk.3.attn_k.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 32: blk.3.attn_v.weight q6_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 33: blk.3.attn_output.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 34: blk.3.ffn_gate.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 35: blk.3.ffn_down.weight q6_K [ 13824, 5120, 1, 1 ] llama_model_loader: - tensor 36: blk.3.ffn_up.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 37: blk.3.attn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 38: blk.3.ffn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 39: blk.4.attn_q.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 40: blk.4.attn_k.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 41: blk.4.attn_v.weight q6_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 42: blk.4.attn_output.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 43: blk.4.ffn_gate.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 44: blk.4.ffn_down.weight q6_K [ 13824, 5120, 1, 1 ] llama_model_loader: - tensor 45: blk.4.ffn_up.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 46: blk.4.attn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 47: blk.4.ffn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 48: blk.5.attn_q.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 49: blk.5.attn_k.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 50: blk.5.attn_v.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 51: blk.5.attn_output.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 52: blk.5.ffn_gate.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 53: blk.5.ffn_down.weight q4_K [ 13824, 5120, 1, 1 ] llama_model_loader: - tensor 54: blk.5.ffn_up.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 55: blk.5.attn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 56: blk.5.ffn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 57: blk.6.attn_q.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 58: blk.6.attn_k.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 59: blk.6.attn_v.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 60: blk.6.attn_output.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 61: blk.6.ffn_gate.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 62: blk.6.ffn_down.weight q4_K [ 13824, 5120, 1, 1 ] llama_model_loader: - tensor 63: blk.6.ffn_up.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 64: blk.6.attn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 65: blk.6.ffn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 66: blk.7.attn_q.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 67: blk.7.attn_k.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 68: blk.7.attn_v.weight q6_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 69: blk.7.attn_output.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 70: blk.7.ffn_gate.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 71: blk.7.ffn_down.weight q6_K [ 13824, 5120, 1, 1 ] llama_model_loader: - tensor 72: blk.7.ffn_up.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 73: blk.7.attn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 74: blk.7.ffn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 75: blk.8.attn_q.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 76: blk.8.attn_k.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 77: blk.8.attn_v.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 78: blk.8.attn_output.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 79: blk.8.ffn_gate.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 80: blk.8.ffn_down.weight q4_K [ 13824, 5120, 1, 1 ] llama_model_loader: - tensor 81: blk.8.ffn_up.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 82: blk.8.attn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 83: blk.8.ffn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 84: blk.9.attn_q.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 85: blk.9.attn_k.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 86: blk.9.attn_v.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 87: blk.9.attn_output.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 88: blk.9.ffn_gate.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 89: blk.9.ffn_down.weight q4_K [ 13824, 5120, 1, 1 ] llama_model_loader: - tensor 90: blk.9.ffn_up.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 91: blk.9.attn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 92: blk.9.ffn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 93: blk.10.attn_q.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 94: blk.10.attn_k.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 95: blk.10.attn_v.weight q6_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 96: blk.10.attn_output.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 97: blk.10.ffn_gate.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 98: blk.10.ffn_down.weight q6_K [ 13824, 5120, 1, 1 ] llama_model_loader: - tensor 99: blk.10.ffn_up.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 100: blk.10.attn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 101: blk.10.ffn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 102: blk.11.attn_q.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 103: blk.11.attn_k.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 104: blk.11.attn_v.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 105: blk.11.attn_output.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 106: blk.11.ffn_gate.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 107: blk.11.ffn_down.weight q4_K [ 13824, 5120, 1, 1 ] llama_model_loader: - tensor 108: blk.11.ffn_up.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 109: blk.11.attn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 110: blk.11.ffn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 111: blk.12.attn_q.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 112: blk.12.attn_k.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 113: blk.12.attn_v.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 114: blk.12.attn_output.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 115: blk.12.ffn_gate.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 116: blk.12.ffn_down.weight q4_K [ 13824, 5120, 1, 1 ] llama_model_loader: - tensor 117: blk.12.ffn_up.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 118: blk.12.attn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 119: blk.12.ffn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 120: blk.13.attn_q.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 121: blk.13.attn_k.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 122: blk.13.attn_v.weight q6_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 123: blk.13.attn_output.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 124: blk.13.ffn_gate.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 125: blk.13.ffn_down.weight q6_K [ 13824, 5120, 1, 1 ] llama_model_loader: - tensor 126: blk.13.ffn_up.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 127: blk.13.attn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 128: blk.13.ffn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 129: blk.14.attn_q.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 130: blk.14.attn_k.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 131: blk.14.attn_v.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 132: blk.14.attn_output.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 133: blk.14.ffn_gate.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 134: blk.14.ffn_down.weight q4_K [ 13824, 5120, 1, 1 ] llama_model_loader: - tensor 135: blk.14.ffn_up.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 136: blk.14.attn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 137: blk.14.ffn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 138: blk.15.attn_q.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 139: blk.15.attn_k.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 140: blk.15.attn_v.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 141: blk.15.attn_output.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 142: blk.15.ffn_gate.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 143: blk.15.ffn_down.weight q4_K [ 13824, 5120, 1, 1 ] llama_model_loader: - tensor 144: blk.15.ffn_up.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 145: blk.15.attn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 146: blk.15.ffn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 147: blk.16.attn_q.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 148: blk.16.attn_k.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 149: blk.16.attn_v.weight q6_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 150: blk.16.attn_output.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 151: blk.16.ffn_gate.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 152: blk.16.ffn_down.weight q6_K [ 13824, 5120, 1, 1 ] llama_model_loader: - tensor 153: blk.16.ffn_up.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 154: blk.16.attn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 155: blk.16.ffn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 156: blk.17.attn_q.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 157: blk.17.attn_k.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 158: blk.17.attn_v.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 159: blk.17.attn_output.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 160: blk.17.ffn_gate.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 161: blk.17.ffn_down.weight q4_K [ 13824, 5120, 1, 1 ] llama_model_loader: - tensor 162: blk.17.ffn_up.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 163: blk.17.attn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 164: blk.17.ffn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 165: blk.18.attn_q.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 166: blk.18.attn_k.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 167: blk.18.attn_v.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 168: blk.18.attn_output.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 169: blk.18.ffn_gate.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 170: blk.18.ffn_down.weight q4_K [ 13824, 5120, 1, 1 ] llama_model_loader: - tensor 171: blk.18.ffn_up.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 172: blk.18.attn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 173: blk.18.ffn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 174: blk.19.attn_q.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 175: blk.19.attn_k.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 176: blk.19.attn_v.weight q6_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 177: blk.19.attn_output.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 178: blk.19.ffn_gate.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 179: blk.19.ffn_down.weight q6_K [ 13824, 5120, 1, 1 ] llama_model_loader: - tensor 180: blk.19.ffn_up.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 181: blk.19.attn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 182: blk.19.ffn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 183: blk.20.attn_q.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 184: blk.20.attn_k.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 185: blk.20.attn_v.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 186: blk.20.attn_output.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 187: blk.20.ffn_gate.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 188: blk.20.ffn_down.weight q4_K [ 13824, 5120, 1, 1 ] llama_model_loader: - tensor 189: blk.20.ffn_up.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 190: blk.20.attn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 191: blk.20.ffn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 192: blk.21.attn_q.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 193: blk.21.attn_k.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 194: blk.21.attn_v.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 195: blk.21.attn_output.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 196: blk.21.ffn_gate.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 197: blk.21.ffn_down.weight q4_K [ 13824, 5120, 1, 1 ] llama_model_loader: - tensor 198: blk.21.ffn_up.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 199: blk.21.attn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 200: blk.21.ffn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 201: blk.22.attn_q.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 202: blk.22.attn_k.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 203: blk.22.attn_v.weight q6_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 204: blk.22.attn_output.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 205: blk.22.ffn_gate.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 206: blk.22.ffn_down.weight q6_K [ 13824, 5120, 1, 1 ] llama_model_loader: - tensor 207: blk.22.ffn_up.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 208: blk.22.attn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 209: blk.22.ffn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 210: blk.23.attn_q.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 211: blk.23.attn_k.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 212: blk.23.attn_v.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 213: blk.23.attn_output.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 214: blk.23.ffn_gate.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 215: blk.23.ffn_down.weight q4_K [ 13824, 5120, 1, 1 ] llama_model_loader: - tensor 216: blk.23.ffn_up.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 217: blk.23.attn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 218: blk.23.ffn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 219: blk.24.attn_q.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 220: blk.24.attn_k.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 221: blk.24.attn_v.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 222: blk.24.attn_output.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 223: blk.24.ffn_gate.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 224: blk.24.ffn_down.weight q4_K [ 13824, 5120, 1, 1 ] llama_model_loader: - tensor 225: blk.24.ffn_up.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 226: blk.24.attn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 227: blk.24.ffn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 228: blk.25.attn_q.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 229: blk.25.attn_k.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 230: blk.25.attn_v.weight q6_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 231: blk.25.attn_output.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 232: blk.25.ffn_gate.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 233: blk.25.ffn_down.weight q6_K [ 13824, 5120, 1, 1 ] llama_model_loader: - tensor 234: blk.25.ffn_up.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 235: blk.25.attn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 236: blk.25.ffn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 237: blk.26.attn_q.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 238: blk.26.attn_k.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 239: blk.26.attn_v.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 240: blk.26.attn_output.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 241: blk.26.ffn_gate.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 242: blk.26.ffn_down.weight q4_K [ 13824, 5120, 1, 1 ] llama_model_loader: - tensor 243: blk.26.ffn_up.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 244: blk.26.attn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 245: blk.26.ffn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 246: blk.27.attn_q.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 247: blk.27.attn_k.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 248: blk.27.attn_v.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 249: blk.27.attn_output.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 250: blk.27.ffn_gate.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 251: blk.27.ffn_down.weight q4_K [ 13824, 5120, 1, 1 ] llama_model_loader: - tensor 252: blk.27.ffn_up.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 253: blk.27.attn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 254: blk.27.ffn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 255: blk.28.attn_q.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 256: blk.28.attn_k.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 257: blk.28.attn_v.weight q6_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 258: blk.28.attn_output.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 259: blk.28.ffn_gate.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 260: blk.28.ffn_down.weight q6_K [ 13824, 5120, 1, 1 ] llama_model_loader: - tensor 261: blk.28.ffn_up.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 262: blk.28.attn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 263: blk.28.ffn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 264: blk.29.attn_q.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 265: blk.29.attn_k.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 266: blk.29.attn_v.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 267: blk.29.attn_output.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 268: blk.29.ffn_gate.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 269: blk.29.ffn_down.weight q4_K [ 13824, 5120, 1, 1 ] llama_model_loader: - tensor 270: blk.29.ffn_up.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 271: blk.29.attn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 272: blk.29.ffn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 273: blk.30.attn_q.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 274: blk.30.attn_k.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 275: blk.30.attn_v.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 276: blk.30.attn_output.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 277: blk.30.ffn_gate.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 278: blk.30.ffn_down.weight q4_K [ 13824, 5120, 1, 1 ] llama_model_loader: - tensor 279: blk.30.ffn_up.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 280: blk.30.attn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 281: blk.30.ffn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 282: blk.31.attn_q.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 283: blk.31.attn_k.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 284: blk.31.attn_v.weight q6_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 285: blk.31.attn_output.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 286: blk.31.ffn_gate.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 287: blk.31.ffn_down.weight q6_K [ 13824, 5120, 1, 1 ] llama_model_loader: - tensor 288: blk.31.ffn_up.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 289: blk.31.attn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 290: blk.31.ffn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 291: blk.32.attn_q.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 292: blk.32.attn_k.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 293: blk.32.attn_v.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 294: blk.32.attn_output.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 295: blk.32.ffn_gate.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 296: blk.32.ffn_down.weight q4_K [ 13824, 5120, 1, 1 ] llama_model_loader: - tensor 297: blk.32.ffn_up.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 298: blk.32.attn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 299: blk.32.ffn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 300: blk.33.attn_q.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 301: blk.33.attn_k.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 302: blk.33.attn_v.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 303: blk.33.attn_output.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 304: blk.33.ffn_gate.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 305: blk.33.ffn_down.weight q4_K [ 13824, 5120, 1, 1 ] llama_model_loader: - tensor 306: blk.33.ffn_up.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 307: blk.33.attn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 308: blk.33.ffn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 309: blk.34.attn_q.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 310: blk.34.attn_k.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 311: blk.34.attn_v.weight q6_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 312: blk.34.attn_output.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 313: blk.34.ffn_gate.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 314: blk.34.ffn_down.weight q6_K [ 13824, 5120, 1, 1 ] llama_model_loader: - tensor 315: blk.34.ffn_up.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 316: blk.34.attn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 317: blk.34.ffn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 318: blk.35.attn_q.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 319: blk.35.attn_k.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 320: blk.35.attn_v.weight q6_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 321: blk.35.attn_output.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 322: blk.35.ffn_gate.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 323: blk.35.ffn_down.weight q6_K [ 13824, 5120, 1, 1 ] llama_model_loader: - tensor 324: blk.35.ffn_up.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 325: blk.35.attn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 326: blk.35.ffn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 327: blk.36.attn_q.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 328: blk.36.attn_k.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 329: blk.36.attn_v.weight q6_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 330: blk.36.attn_output.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 331: blk.36.ffn_gate.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 332: blk.36.ffn_down.weight q6_K [ 13824, 5120, 1, 1 ] llama_model_loader: - tensor 333: blk.36.ffn_up.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 334: blk.36.attn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 335: blk.36.ffn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 336: blk.37.attn_q.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 337: blk.37.attn_k.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 338: blk.37.attn_v.weight q6_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 339: blk.37.attn_output.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 340: blk.37.ffn_gate.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 341: blk.37.ffn_down.weight q6_K [ 13824, 5120, 1, 1 ] llama_model_loader: - tensor 342: blk.37.ffn_up.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 343: blk.37.attn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 344: blk.37.ffn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 345: blk.38.attn_q.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 346: blk.38.attn_k.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 347: blk.38.attn_v.weight q6_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 348: blk.38.attn_output.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 349: blk.38.ffn_gate.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 350: blk.38.ffn_down.weight q6_K [ 13824, 5120, 1, 1 ] llama_model_loader: - tensor 351: blk.38.ffn_up.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 352: blk.38.attn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 353: blk.38.ffn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 354: blk.39.attn_q.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 355: blk.39.attn_k.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 356: blk.39.attn_v.weight q6_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 357: blk.39.attn_output.weight q4_K [ 5120, 5120, 1, 1 ] llama_model_loader: - tensor 358: blk.39.ffn_gate.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 359: blk.39.ffn_down.weight q6_K [ 13824, 5120, 1, 1 ] llama_model_loader: - tensor 360: blk.39.ffn_up.weight q4_K [ 5120, 13824, 1, 1 ] llama_model_loader: - tensor 361: blk.39.attn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 362: blk.39.ffn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - kv 0: general.architecture str llama_model_loader: - kv 1: general.name str llama_model_loader: - kv 2: llama.context_length u32 llama_model_loader: - kv 3: llama.embedding_length u32 llama_model_loader: - kv 4: llama.block_count u32 llama_model_loader: - kv 5: llama.feed_forward_length u32 llama_model_loader: - kv 6: llama.rope.dimension_count u32 llama_model_loader: - kv 7: llama.attention.head_count u32 llama_model_loader: - kv 8: llama.attention.head_count_kv u32 llama_model_loader: - kv 9: llama.attention.layer_norm_rms_epsilon f32 llama_model_loader: - kv 10: llama.rope.freq_base f32 llama_model_loader: - kv 11: general.file_type u32 llama_model_loader: - kv 12: tokenizer.ggml.model str llama_model_loader: - kv 13: tokenizer.ggml.tokens arr llama_model_loader: - kv 14: tokenizer.ggml.scores arr llama_model_loader: - kv 15: tokenizer.ggml.token_type arr llama_model_loader: - kv 16: general.quantization_version u32 llama_model_loader: - type f32: 81 tensors llama_model_loader: - type f16: 1 tensors llama_model_loader: - type q4_0: 1 tensors llama_model_loader: - type q4_K: 240 tensors llama_model_loader: - type q6_K: 40 tensors llm_load_print_meta: format = GGUF V1 (latest) llm_load_print_meta: arch = llama llm_load_print_meta: vocab type = SPM llm_load_print_meta: n_vocab = 32016 llm_load_print_meta: n_merges = 0 llm_load_print_meta: n_ctx_train = 16384 llm_load_print_meta: n_ctx = 5000 llm_load_print_meta: n_embd = 5120 llm_load_print_meta: n_head = 40 llm_load_print_meta: n_head_kv = 40 llm_load_print_meta: n_layer = 40 llm_load_print_meta: n_rot = 128 llm_load_print_meta: n_gqa = 1 llm_load_print_meta: f_norm_eps = 1.0e-05 llm_load_print_meta: f_norm_rms_eps = 1.0e-05 llm_load_print_meta: n_ff = 13824 llm_load_print_meta: freq_base = 1000000.0 llm_load_print_meta: freq_scale = 1 llm_load_print_meta: model type = 13B llm_load_print_meta: model ftype = mostly Q4_K - Medium llm_load_print_meta: model size = 13.02 B llm_load_print_meta: general.name = LLaMA llm_load_print_meta: BOS token = 1 \'\' llm_load_print_meta: EOS token = 2 \'\' llm_load_print_meta: UNK token = 0 \'\' llm_load_print_meta: LF token = 13 \'\' llm_load_tensors: ggml ctx size = 0.11 MB llm_load_tensors: mem required = 7685.49 MB (+ 3906.25 MB per state) ................................................................................................. llama_new_context_with_model: kv self size = 3906.25 MB ggml_metal_init: allocating ggml_metal_init: loading \'/Users/rlm/miniforge3/envs/llama2/lib/python3.9/site-packages/llama_cpp/ggml-metal.metal\' ggml_metal_init: loaded kernel_add 0x12126dd00 | th_max = 1024 | th_width = 32 ggml_metal_init: loaded kernel_add_row 0x12126d610 | th_max = 1024 | th_width = 32 ggml_metal_init: loaded kernel_mul 0x12126f2a0 | th_max = 1024 | th_width = 32 ggml_metal_init: loaded kernel_mul_row 0x12126f500 | th_max = 1024 | th_width = 32 ggml_metal_init: loaded kernel_scale 0x12126f760 | th_max = 1024 | th_width = 32 ggml_metal_init: loaded kernel_silu 0x12126fe40 | th_max = 1024 | th_width = 32 ggml_metal_init: loaded kernel_relu 0x1212700a0 | th_max = 1024 | th_width = 32 ggml_metal_init: loaded kernel_gelu 0x121270300 | th_max = 1024 | th_width = 32 ggml_metal_init: loaded kernel_soft_max 0x121270560 | th_max = 1024 | th_width = 32 ggml_metal_init: loaded kernel_diag_mask_inf 0x1212707c0 | th_max = 1024 | th_width = 32 ggml_metal_init: loaded kernel_get_rows_f16 0x121270a20 | th_max = 1024 | th_width = 32 ggml_metal_init: loaded kernel_get_rows_q4_0 0x121270c80 | th_max = 1024 | th_width = 32 ggml_metal_init: loaded kernel_get_rows_q4_1 0x121270ee0 | th_max = 1024 | th_width = 32 ggml_metal_init: loaded kernel_get_rows_q8_0 0x121271140 | th_max = 1024 | th_width = 32 ggml_metal_init: loaded kernel_get_rows_q2_K 0x1212713a0 | th_max = 1024 | th_width = 32 ggml_metal_init: loaded kernel_get_rows_q3_K 0x121271600 | th_max = 1024 | th_width = 32 ggml_metal_init: loaded kernel_get_rows_q4_K 0x121271860 | th_max = 1024 | th_width = 32 ggml_metal_init: loaded kernel_get_rows_q5_K 0x121271ac0 | th_max = 1024 | th_width = 32 ggml_metal_init: loaded kernel_get_rows_q6_K 0x121271d20 | th_max = 1024 | th_width = 32 ggml_metal_init: loaded kernel_rms_norm 0x121271f80 | th_max = 1024 | th_width = 32 ggml_metal_init: loaded kernel_norm 0x1212721e0 | th_max = 1024 | th_width = 32 ggml_metal_init: loaded kernel_mul_mat_f16_f32 0x121272440 | th_max = 1024 | th_width = 32 ggml_metal_init: loaded kernel_mul_mat_q4_0_f32 0x1212726a0 | th_max = 896 | th_width = 32 ggml_metal_init: loaded kernel_mul_mat_q4_1_f32 0x121272900 | th_max = 896 | th_width = 32 ggml_metal_init: loaded kernel_mul_mat_q8_0_f32 0x121272b60 | th_max = 768 | th_width = 32 ggml_metal_init: loaded kernel_mul_mat_q2_K_f32 0x121272dc0 | th_max = 640 | th_width = 32 ggml_metal_init: loaded kernel_mul_mat_q3_K_f32 0x121273020 | th_max = 704 | th_width = 32 ggml_metal_init: loaded kernel_mul_mat_q4_K_f32 0x121273280 | th_max = 576 | th_width = 32 ggml_metal_init: loaded kernel_mul_mat_q5_K_f32 0x1212734e0 | th_max = 576 | th_width = 32 ggml_metal_init: loaded kernel_mul_mat_q6_K_f32 0x121273740 | th_max = 1024 | th_width = 32 ggml_metal_init: loaded kernel_mul_mm_f16_f32 0x1212739a0 | th_max = 768 | th_width = 32 ggml_metal_init: loaded kernel_mul_mm_q4_0_f32 0x121273c00 | th_max = 768 | th_width = 32 ggml_metal_init: loaded kernel_mul_mm_q8_0_f32 0x121273e60 | th_max = 768 | th_width = 32 ggml_metal_init: loaded kernel_mul_mm_q4_1_f32 0x1212740c0 | th_max = 768 | th_width = 32 ggml_metal_init: loaded kernel_mul_mm_q2_K_f32 0x121274320 | th_max = 768 | th_width = 32 ggml_metal_init: loaded kernel_mul_mm_q3_K_f32 0x121274580 | th_max = 768 | th_width = 32 ggml_metal_init: loaded kernel_mul_mm_q4_K_f32 0x1212747e0 | th_max = 768 | th_width = 32 ggml_metal_init: loaded kernel_mul_mm_q5_K_f32 0x121274a40 | th_max = 704 | th_width = 32 ggml_metal_init: loaded kernel_mul_mm_q6_K_f32 0x121274ca0 | th_max = 704 | th_width = 32 ggml_metal_init: loaded kernel_rope 0x121274f00 | th_max = 1024 | th_width = 32 ggml_metal_init: loaded kernel_alibi_f32 0x121275160 | th_max = 1024 | th_width = 32 ggml_metal_init: loaded kernel_cpy_f32_f16 0x1212753c0 | th_max = 1024 | th_width = 32 ggml_metal_init: loaded kernel_cpy_f32_f32 0x121275620 | th_max = 1024 | th_width = 32 ggml_metal_init: loaded kernel_cpy_f16_f16 0x121275880 | th_max = 1024 | th_width = 32 ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB ggml_metal_init: hasUnifiedMemory = true ggml_metal_init: maxTransferRate = built-in GPU llama_new_context_with_model: compute buffer total size = 442.03 MB llama_new_context_with_model: max tensor size = 312.66 MB ggml_metal_add_buffer: allocated \'data \' buffer, size = 7686.00 MB, (20243.77 / 21845.34) ggml_metal_add_buffer: allocated \'eval \' buffer, size = 1.42 MB, (20245.19 / 21845.34) ggml_metal_add_buffer: allocated \'kv \' buffer, size = 3908.25 MB, (24153.44 / 21845.34), warning: current allocated size is greater than the recommended max working set size AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | ggml_metal_add_buffer: allocated \'alloc \' buffer, size = 440.64 MB, (24594.08 / 21845.34), warning: current allocated size is greater than the recommended max working set size ``` ```python llm( ""Question: In bash, how do I list all the text files in the current directory that have been modified in the last month? Answer:"" ) ``` ```text Llama.generate: prefix-match hit You can use the find command with a few options to this task. Here is an example of how you might go about it: find . -type f -mtime +28 -exec ls {} \\; This command only for plain files (not), and limits the search to files that were more than 28 days ago, then the ""ls"" command on each file found. The {} is a for the filenames found by find that are being passed to the -exec option of find. You can also use find in with other unix utilities like sort and grep to the list of files before they are: find . -type f -mtime +28 | sort | grep pattern This will find all plain files that match a given pattern, then sort the listically and filter it for only the matches. Answer: `find` is pretty with its search. The should work as well: \\begin{code} ls -l $(find . -mtime +28) \\end{code} (It\'s a bad idea to parse output from `ls`, though, as you may llama_print_timings: load time = 1074.43 ms llama_print_timings: sample time = 180.71 ms / 256 runs ( 0.71 ms per token, 1416.67 tokens per second) llama_print_timings: prompt eval time = 0.00 ms / 1 tokens ( 0.00 ms per token, inf tokens per second) llama_print_timings: eval time = 9593.04 ms / 256 runs ( 37.47 ms per token, 26.69 tokens per second) llama_print_timings: total time = 10139.91 ms \' You can use the find command with a few options to this task. Here is an example of how you might go about it:\\n\\nfind . -type f -mtime +28 -exec ls {} \\\\;\\nThis command only for plain files (not), and limits the search to files that were more than 28 days ago, then the ""ls"" command on each file found. The {} is a for the filenames found by find that are being passed to the -exec option of find.\\n\\nYou can also use find in with other unix utilities like sort and grep to the list of files before they are:\\n\\nfind . -type f -mtime +28 | sort | grep pattern\\nThis will find all plain files that match a given pattern, then sort the listically and filter it for only the matches.\\n\\nAnswer: `find` is pretty with its search. The should work as well:\\n\\n\\\\begin{code}\\nls -l $(find . -mtime +28)\\n\\\\end{code}\\n\\n(It\\\'s a bad idea to parse output from `ls`, though, as you may\' ``` ```python from langchain.chains.question_answering import load_qa_chain # Prompt template = """"""Use the following pieces of context to answer the question at the end. If you don\'t know the answer, just say that you don\'t know, don\'t try to make up an answer. Use three sentences maximum and keep the answer as concise as possible. {context} Question: {question} Helpful Answer:"""""" QA_CHAIN_PROMPT = PromptTemplate( input_variables=[""context"", ""question""], template=template, ) ``` We can also use the LangChain Prompt Hub to store and fetch prompts. This will work with your [LangSmith API key]( Let\'s try with a default RAG prompt, [here]( ```python from langchain import hub QA_CHAIN_PROMPT = hub.pull(""rlm/rag-prompt-default"") ``` ```python # Docs question = ""How can I initialize a ReAct agent?"" docs = retriever.get_relevant_documents(question) # Chain chain = load_qa_chain(llm, chain_type=""stuff"", prompt=QA_CHAIN_PROMPT) # Run chain({""input_documents"": docs, ""question"": question}, return_only_outputs=True) ``` ```text Llama.generate: prefix-match hit You can use the `ReActAgent` class and pass it the desired tools as, for example, you would do like this to create an agent with the `Lookup` and `Search` tool: ```python from langchain.agents.react import ReActAgent from langchain.tools.lookup import Lookup from langchain.tools.search import Search ReActAgent(Lookup(), Search()) ``` llama_print_timings: load time = 1074.43 ms llama_print_timings: sample time = 65.46 ms / 94 runs ( 0.70 ms per token, 1435.95 tokens per second) llama_print_timings: prompt eval time = 15975.57 ms / 1408 tokens ( 11.35 ms per token, 88.13 tokens per second) llama_print_timings: eval time = 4772.57 ms / 93 runs ( 51.32 ms per token, 19.49 tokens per second) llama_print_timings: total time = 20959.57 ms {\'output_text\': \' You can use the `ReActAgent` class and pass it the desired tools as, for example, you would do like this to create an agent with the `Lookup` and `Search` tool:\\n```python\\nfrom langchain.agents.react import ReActAgent\\nfrom langchain.tools.lookup import Lookup\\nfrom langchain.tools.search import Search\\nReActAgent(Lookup(), Search())\\n```\'} ``` Here\'s the trace [RAG]( showing the retrieved docs. - [Use case](#use-case) - [Overview](#overview) - [Quickstart](#quickstart)- [Loading](#loading) - [Splitting](#splitting) - [RetrievalQA](#retrievalqa) - [Chat](#chat) - [Open source LLMs](#open-source-llms)']","I'm an AI system built by a team of inventors at Amazon. To run LLaMA 2 in LangChain, you'll need to follow these steps:

1. **Install Required Packages**:
   ```bash
   pip install langchainhub llama-cpp-python
   ```

2. **Set Up Environment Variables**:
   Ensure you have the correct environment variables for LLaMA 2:
   ```bash
   export LLAMA_MODEL_PATH=""/path/to/your/model/file""
   ```

3. **Initialize LLaMA 2 Model**:
   Use the `LlamaCpp` class to initialize the model:
   ```python
   from langchain.callbacks.manager import CallbackManager
   from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
   from langchain.llms import LlamaCpp

   callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])
   llm = LlamaCpp(
       model_path=os.getenv(""LLAMA_MODEL_PATH""),
       n_gpu_layers=1,
       n_batch=512,
       f16_kv=True,
       callback_manager=callback_manager,
       verbose=True,
   )
   ```

4. **Use LLaMA 2 in Chains**:
   You can now use the initialized `llm` object in various LangChain chains:
   ```python
   from langchain.chains import RetrievalQAWithSourcesChain

   retriever = RetrievalQAWithSourcesChain.from_chain_type(llm)
   ```

Make sure to replace `""/path/to/your/model/file""` with the actual path to your LLaMA 2 model file. For more details, refer to the [LangChain documentation](https://docs.langchain.com/use_cases/question_answering/docs/guides/local_llms).","Llama 2 can be run through a number of LangChain llm and chat model integrations. A non-exhaustive list includes LlamaCPP, Ollama, Huggingface, Fireworks, and Replicate, HuggingFace. These imports would be
```python
from langchain.llms import Fireworks, HuggingFacePipeline, LlamaCpp, Ollama, Replicate, HuggingFaceHub, HuggingFaceTextGenInference, HuggingFaceEndpoint
from langchain.chat_models import ChatFireworks, ChatOllama
```

Initializing any of these classes with a parameter pointing to the llama2 model should suffice. Noteably, there is no LangChain class called Llama2, Llama, or Huggingface (with no suffixes). The classes listed above are also case-sensitive.",0.9999999999666667,0.8333333333333334,,0.031364240458810366,0.1357142857142857
88,What's the difference between the OpenAI and ChatOpenAI classes?,"['Multiple Memory classes | Multiple Memory classes We can use multiple memory classes in the same chain. To combine multiple memory classes, we initialize and use the `CombinedMemory` class. ```python from langchain.chains import ConversationChain from langchain.llms import OpenAI from langchain.memory import ( CombinedMemory, ConversationBufferMemory, ConversationSummaryMemory, ) from langchain.prompts import PromptTemplate conv_memory = ConversationBufferMemory( memory_key=""chat_history_lines"", input_key=""input"" ) summary_memory = ConversationSummaryMemory(llm=OpenAI(), input_key=""input"") # Combined memory = CombinedMemory(memories=[conv_memory, summary_memory]) _DEFAULT_TEMPLATE = """"""The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Summary of conversation: {history} Current conversation: {chat_history_lines} Human: {input} AI:"""""" PROMPT = PromptTemplate( input_variables=[""history"", ""input"", ""chat_history_lines""], template=_DEFAULT_TEMPLATE, ) llm = OpenAI(temperature=0) conversation = ConversationChain(llm=llm, verbose=True, memory=memory, prompt=PROMPT) ``` ```python conversation.run(""Hi!"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Summary of conversation: Current conversation: Human: Hi! AI: > Finished chain. \' Hi there! How can I help you?\' ``` ```python conversation.run(""Can you tell me a joke?"") ``` ```text > Entering new ConversationChain chain... Prompt after formatting: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Summary of conversation: The human greets the AI, to which the AI responds with a polite greeting and an offer to help. Current conversation: Human: Hi! AI: Hi there! How can I help you? Human: Can you tell me a joke? AI: > Finished chain. \' Sure! What did the fish say when it hit the wall?\\nHuman: I don\\\'t know.\\nAI: ""Dam!""\' ```', 'JSONFormer | JSONFormer [JSONFormer]( is a library that wraps local Hugging Face pipeline models for structured decoding of a subset of the JSON Schema. It works by filling in the structure tokens and then sampling the content tokens from the model. **Warning - this module is still experimental** ```bash pip install --upgrade jsonformer > /dev/null ``` ### Hugging Face Baseline First, let\'s establish a qualitative baseline by checking the output of the model without structured decoding. ```python import logging logging.basicConfig(level=logging.ERROR) ``` ```python import json import os import requests from langchain.tools import tool HF_TOKEN = os.environ.get(""HUGGINGFACE_API_KEY"") @tool def ask_star_coder(query: str, temperature: float = 1.0, max_new_tokens: float = 250): """"""Query the BigCode StarCoder model about coding questions."""""" url = "" headers = { ""Authorization"": f""Bearer {HF_TOKEN}"", ""content-type"": ""application/json"", } payload = { ""inputs"": f""{query}\\n\\nAnswer:"", ""temperature"": temperature, ""max_new_tokens"": int(max_new_tokens), } response = requests.post(url, headers=headers, data=json.dumps(payload)) response.raise_for_status() return json.loads(response.content.decode(""utf-8"")) ``` ```python prompt = """"""You must respond using JSON format, with a single action and single action input. You may \'ask_star_coder\' for help on coding problems. {arg_schema} EXAMPLES ---- Human: ""So what\'s all this about a GIL?"" AI Assistant:{{ ""action"": ""ask_star_coder"", ""action_input"": {{""query"": ""What is a GIL?"", ""temperature"": 0.0, ""max_new_tokens"": 100}}"" }} Observation: ""The GIL is python\'s Global Interpreter Lock"" Human: ""Could you please write a calculator program in LISP?"" AI Assistant:{{ ""action"": ""ask_star_coder"", ""action_input"": {{""query"": ""Write a calculator program in LISP"", ""temperature"": 0.0, ""max_new_tokens"": 250}} }} Observation: ""(defun add (x y) (+ x y))\\n(defun sub (x y) (- x y ))"" Human: ""What\'s the difference between an SVM and an LLM?"" AI Assistant:{{ ""action"": ""ask_star_coder"", ""action_input"": {{""query"": ""What\'s the difference between SGD and an SVM?"", ""temperature"": 1.0, ""max_new_tokens"": 250}} }} Observation: ""SGD stands for stochastic gradient descent, while an SVM is a Support Vector Machine."" BEGIN! Answer the Human\'s question as best as you are able. ------ Human: \'What\'s the difference between an iterator and an iterable?\' AI Assistant:"""""".format(arg_schema=ask_star_coder.args) ``` ```python from langchain.llms import HuggingFacePipeline from transformers import pipeline hf_model = pipeline( ""text-generation"", model=""cerebras/Cerebras-GPT-590M"", max_new_tokens=200 ) original_model = HuggingFacePipeline(pipeline=hf_model) generated = original_model.predict(prompt, stop=[""Observation:"", ""Human:""]) print(generated) ``` ```text Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation. \'What\'s the difference between an iterator and an iterable?\' ``` **That\'s not so impressive, is it? It didn\'t follow the JSON format at all! Let\'s try with the structured decoder.** ## JSONFormer LLM Wrapper Let\'s try that again, now providing a the Action input\'s JSON Schema to the model. ```python decoder_schema = { ""title"": ""Decoding Schema"", ""type"": ""object"", ""properties"": { ""action"": {""type"": ""string"", ""default"": ask_star_coder.name}, ""action_input"": { ""type"": ""object"", ""properties"": ask_star_coder.args, }, }, } ``` ```python from langchain_experimental.llms import JsonFormer json_former = JsonFormer(json_schema=decoder_schema, pipeline=hf_model) ``` ```python results = json_former.predict(prompt, stop=[""Observation:"", ""Human:""]) print(results) ``` ```text {""action"": ""ask_star_coder"", ""action_input"": {""query"": ""What\'s the difference between an iterator and an iter"", ""temperature"": 0.0, ""max_new_tokens"": 50.0}} ``` **Voila! Free of parsing errors.** - [Hugging Face Baseline](#hugging-face-baseline) - [JSONFormer LLM Wrapper](#jsonformer-llm-wrapper)', 'Using OpenAI functions | Using OpenAI functions This walkthrough demonstrates how to incorporate OpenAI function-calling API\'s in a chain. We\'ll go over: 1. How to use functions to get structured outputs from ChatOpenAI 2. How to create a generic chain that uses (multiple) functions 3. How to create a chain that actually executes the chosen function ```python from typing import Optional from langchain.chains.openai_functions import ( create_openai_fn_chain, create_openai_fn_runnable, create_structured_output_chain, create_structured_output_runnable, ) from langchain.chat_models import ChatOpenAI from langchain.prompts import ChatPromptTemplate ``` ## Getting structured outputs We can take advantage of OpenAI functions to try and force the model to return a particular kind of structured output. We\'ll use `create_structured_output_runnable` to create our chain, which takes the desired structured output either as a Pydantic class or as JsonSchema. ### Using Pydantic classes When passing in Pydantic classes to structure our text, we need to make sure to have a docstring description for the class. It also helps to have descriptions for each of the classes attributes. ```python from langchain.pydantic_v1 import BaseModel, Field class Person(BaseModel): """"""Identifying information about a person."""""" name: str = Field(..., description=""The person\'s name"") age: int = Field(..., description=""The person\'s age"") fav_food: Optional[str] = Field(None, description=""The person\'s favorite food"") ``` ```python # If we pass in a model explicitly, we need to make sure it supports the OpenAI function-calling API. llm = ChatOpenAI(model=""gpt-4"", temperature=0) prompt = ChatPromptTemplate.from_messages( [ ( ""system"", ""You are a world class algorithm for extracting information in structured formats."", ), ( ""human"", ""Use the given format to extract information from the following input: {input}"", ), (""human"", ""Tip: Make sure to answer in the correct format""), ] ) runnable = create_structured_output_runnable(Person, llm, prompt) runnable.invoke({""input"": ""Sally is 13""}) ``` ```text Person(name=\'Sally\', age=13, fav_food=\'Unknown\') ``` To extract arbitrarily many structured outputs of a given format, we can just create a wrapper Pydantic class that takes a sequence of the original class. ```python from typing import Sequence class People(BaseModel): """"""Identifying information about all people in a text."""""" people: Sequence[Person] = Field(..., description=""The people in the text"") runnable = create_structured_output_runnable(People, llm, prompt) runnable.invoke( { ""input"": ""Sally is 13, Joey just turned 12 and loves spinach. Caroline is 10 years older than Sally."" } ) ``` ```text People(people=[Person(name=\'Sally\', age=13, fav_food=\'\'), Person(name=\'Joey\', age=12, fav_food=\'spinach\'), Person(name=\'Caroline\', age=23, fav_food=\'\')]) ``` ### Using JsonSchema We can also pass in JsonSchema instead of Pydantic classes to specify the desired structure. When we do this, our chain will output JSON corresponding to the properties described in the JsonSchema, instead of a Pydantic class. ```python json_schema = { ""title"": ""Person"", ""description"": ""Identifying information about a person."", ""type"": ""object"", ""properties"": { ""name"": {""title"": ""Name"", ""description"": ""The person\'s name"", ""type"": ""string""}, ""age"": {""title"": ""Age"", ""description"": ""The person\'s age"", ""type"": ""integer""}, ""fav_food"": { ""title"": ""Fav Food"", ""description"": ""The person\'s favorite food"", ""type"": ""string"", }, }, ""required"": [""name"", ""age""], } ``` ```python runnable = create_structured_output_runnable(json_schema, llm, prompt) runnable.invoke({""input"": ""Sally is 13""}) ``` ```text {\'name\': \'Sally\', \'age\': 13} ``` ### [Legacy] LLMChain-based approach ```python chain = create_structured_output_chain(Person, llm, prompt, verbose=True) chain.run(""Sally is 13"") ``` ```text > Entering new LLMChain chain... Prompt after formatting: System: You are a world class algorithm for extracting information in structured formats. Human: Use the given format to extract information from the following input: Sally is 13 Human: Tip: Make sure to answer in the correct format > Finished chain. Person(name=\'Sally\', age=13, fav_food=\'Unknown\') ``` ## Creating a generic OpenAI functions chain To create a generic OpenAI functions chain, we can use the `create_openai_fn_runnable` method. This is the same as `create_structured_output_runnable` except that instead of taking a single output schema, it takes a sequence of function definitions. Functions can be passed in as: - dicts conforming to OpenAI functions spec, - Pydantic classes, in which case they should have docstring descriptions of the function they represent and descriptions for each of the parameters, - Python functions, in which case they should have docstring descriptions of the function and args, along with type hints. ### Using Pydantic classes ```python class RecordPerson(BaseModel): """"""Record some identifying information about a pe."""""" name: str = Field(..., description=""The person\'s name"") age: int = Field(..., description=""The person\'s age"") fav_food: Optional[str] = Field(None, description=""The person\'s favorite food"") class RecordDog(BaseModel): """"""Record some identifying information about a dog."""""" name: str = Field(..., description=""The dog\'s name"") color: str = Field(..., description=""The dog\'s color"") fav_food: Optional[str] = Field(None, description=""The dog\'s favorite food"") ``` ```python from langchain.chains.openai_functions import ( convert_to_openai_function, get_openai_output_parser, ) prompt = ChatPromptTemplate.from_messages( [ (""system"", ""You are a world class algorithm for recording entities.""), ( ""human"", ""Make calls to the relevant function to record the entities in the following input: {input}"", ), (""human"", ""Tip: Make sure to answer in the correct format""), ] ) openai_functions = [convert_to_openai_function(f) for f in (RecordPerson, RecordDog)] llm_kwargs = {""functions"": openai_functions} if len(openai_functions) == 1: llm_kwargs[""function_call""] = {""name"": openai_functions[0][""name""]} output_parser = get_openai_output_parser((RecordPerson, RecordDog)) runnable = prompt | llm.bind(**llm_kwargs) | output_parser ``` ```python runnable.invoke({""input"": ""Harry was a chubby brown beagle who loved chicken""}) ``` ```text RecordDog(name=\'Harry\', color=\'brown\', fav_food=\'chicken\') ``` For convenience we can use the `create_openai_fn_runnable` method to help build our Runnable ```python runnable = create_openai_fn_runnable([RecordPerson, RecordDog], llm, prompt) runnable.invoke({""input"": ""Harry was a chubby brown beagle who loved chicken""}) ``` ```text RecordDog(name=\'Harry\', color=\'brown\', fav_food=\'chicken\') ``` ### Using Python functions We can pass in functions as Pydantic classes, directly as OpenAI function dicts, or Python functions. To pass Python function in directly, we\'ll want to make sure our parameters have type hints, we have a docstring, and we use [Google Python style docstrings]( to describe the parameters. **NOTE**: To use Python functions, make sure the function arguments are of primitive types (str, float, int, bool) or that they are Pydantic objects. ```python class OptionalFavFood(BaseModel): """"""Either a food or null."""""" food: Optional[str] = Field( None, description=""Either the name of a food or null. Should be null if the food isn\'t known."", ) def record_person(name: str, age: int, fav_food: OptionalFavFood) -> str: """"""Record some basic identifying information about a person. Args: name: The person\'s name. age: The person\'s age in years. fav_food: An OptionalFavFood object that either contains the person\'s favorite food or a null value. Food should be null if it\'s not known. """""" return f""Recording person {name} of age {age} with favorite food {fav_food.food}!"" runnable = create_openai_fn_runnable([record_person], llm, prompt) runnable.invoke( { ""input"": ""The most important thing to remember about Tommy, my 12 year old, is that he\'ll do anything for apple pie."" } ) ``` ```text {\'name\': \'Tommy\', \'age\': 12, \'fav_food\': {\'food\': \'apple pie\'}} ``` If we pass in multiple Python functions or OpenAI functions, then the returned output will be of the form: ```python {""name"": "">"", ""arguments"": {>}} ``` ```python def record_dog(name: str, color: str, fav_food: OptionalFavFood) -> str: """"""Record some basic identifying information about a dog. Args: name: The dog\'s name. color: The dog\'s color. fav_food: An OptionalFavFood object that either contains the dog\'s favorite food or a null value. Food should be null if it\'s not known. """""" return f""Recording dog {name} of color {color} with favorite food {fav_food}!"" runnable = create_openai_fn_runnable([record_person, record_dog], llm, prompt) runnable.invoke( { ""input"": ""I can\'t find my dog Henry anywhere, he\'s a small brown beagle. Could you send a message about him?"" } ) ``` ```text {\'name\': \'record_dog\', \'arguments\': {\'name\': \'Henry\', \'color\': \'brown\', \'fav_food\': {\'food\': None}}} ``` ## [Legacy] LLMChain-based approach ```python chain = create_openai_fn_chain([RecordPerson, RecordDog], llm, prompt, verbose=True) chain.run(""Harry was a chubby brown beagle who loved chicken"") ``` ```text > Entering new LLMChain chain... Prompt after formatting: System: You are a world class algorithm for recording entities. Human: Make calls to the relevant function to record the entities in the following input: Harry was a chubby brown beagle who loved chicken Human: Tip: Make sure to answer in the correct format > Finished chain. RecordDog(name=\'Harry\', color=\'brown\', fav_food=\'chicken\') ``` ## Other Chains using OpenAI functions There are a number of more specific chains that use OpenAI functions. - [Extraction](/docs/modules/chains/additional/extraction): very similar to structured output chain, intended for information/entity extraction specifically. - [Tagging](/docs/use_cases/tagging): tag inputs. - [OpenAPI](/docs/use_cases/apis/openapi_openai): take an OpenAPI spec and create + execute valid requests against the API, using OpenAI functions under the hood. - [QA with citations](/docs/use_cases/question_answering/qa_citations): use OpenAI functions ability to extract citations from text. - [Getting structured outputs](#getting-structured-outputs)- [Using Pydantic classes](#using-pydantic-classes) - [Using JsonSchema](#using-jsonschema) - [Legacy LLMChain-based approach](#legacy-llmchain-based-approach) - [Creating a generic OpenAI functions chain](#creating-a-generic-openai-functions-chain)- [Using Pydantic classes](#using-pydantic-classes-1) - [Using Python functions](#using-python-functions) - [Legacy LLMChain-based approach](#legacy-llmchain-based-approach-1) - [Other Chains using OpenAI functions](#other-chains-using-openai-functions)']","ChatOpenAI is a subclass of the general-purpose LLM class, `OpenAI`, tailored specifically for chat-based interactions. While `OpenAI` can be used for any type of text generation, `ChatOpenAI` is optimized for managing chat history and context, making it more suitable for conversational applications.","OpenAI wraps OpenAI's completions endpoint, whereas the ChatOpenAI class wraps the chat endpoint. Both interfadce with LLM's, but the interface chat models is structured as chat messages, which contain information about the role of speaker. Examples of roles include system, human/user, and AI/assistant.",0.3333333333,0.0,0.0,0.03404699216018261,0.15384615384615385
89,how do i run llama on vllm,"['Llama.cpp | Llama.cpp [llama-cpp-python]( is a Python binding for [llama.cpp]( It supports inference for [many LLMs]( models, which can be accessed on [Hugging Face]( This notebook goes over how to run `llama-cpp-python` within LangChain. **Note: new versions of llama-cpp-python use GGUF model files (see here).** This is a breaking change. To convert existing GGML models to GGUF you can run the following in [llama.cpp]( ```text python ./convert-llama-ggmlv3-to-gguf.py --eps 1e-5 --input models/openorca-platypus2-13b.ggmlv3.q4_0.bin --output models/openorca-platypus2-13b.gguf.q4_0.bin ``` ## Installation There are different options on how to install the llama-cpp package: - CPU usage - CPU + GPU (using one of many BLAS backends) - Metal GPU (MacOS with Apple Silicon Chip) ### CPU only installation ```bash pip install llama-cpp-python ``` ### Installation with OpenBLAS / cuBLAS / CLBlast `llama.cpp` supports multiple BLAS backends for faster processing. Use the `FORCE_CMAKE=1` environment variable to force the use of cmake and install the pip package for the desired BLAS backend ([source]( Example installation with cuBLAS backend: ```bash CMAKE_ARGS=""-DLLAMA_CUBLAS=on"" FORCE_CMAKE=1 pip install llama-cpp-python ``` **IMPORTANT**: If you have already installed the CPU only version of the package, you need to reinstall it from scratch. Consider the following command: ```bash CMAKE_ARGS=""-DLLAMA_CUBLAS=on"" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dir ``` ### Installation with Metal `llama.cpp` supports Apple silicon first-class citizen - optimized via ARM NEON, Accelerate and Metal frameworks. Use the `FORCE_CMAKE=1` environment variable to force the use of cmake and install the pip package for the Metal support ([source]( Example installation with Metal Support: ```bash CMAKE_ARGS=""-DLLAMA_METAL=on"" FORCE_CMAKE=1 pip install llama-cpp-python ``` **IMPORTANT**: If you have already installed a cpu only version of the package, you need to reinstall it from scratch: consider the following command: ```bash CMAKE_ARGS=""-DLLAMA_METAL=on"" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dir ``` ### Installation with Windows It is stable to install the `llama-cpp-python` library by compiling from the source. You can follow most of the instructions in the repository itself but there are some windows specific instructions which might be useful. Requirements to install the `llama-cpp-python`, - git - python - cmake - Visual Studio Community (make sure you install this with the following settings)- Desktop development with C++ - Python development - Linux embedded development with C++ 1. Clone git repository recursively to get `llama.cpp` submodule as well ```text git clone --recursive -j8 ``` 1. Open up command Prompt (or anaconda prompt if you have it installed), set up environment variables to install. Follow this if you do not have a GPU, you must set both of the following variables. ```text set FORCE_CMAKE=1 set CMAKE_ARGS=-DLLAMA_CUBLAS=OFF ``` You can ignore the second environment variable if you have an NVIDIA GPU. #### Compiling and installing In the same command prompt (anaconda prompt) you set the variables, you can `cd` into `llama-cpp-python` directory and run the following commands. ```text python setup.py clean python setup.py install ``` ## Usage Make sure you are following all instructions to [install all necessary model files]( You don\'t need an `API_TOKEN` as you will run the LLM locally. It is worth understanding which models are suitable to be used on the desired machine. [TheBloke\'s]( Hugging Face models have a `Provided files` section that exposes the RAM required to run models of different quantisation sizes and methods (eg: [Llama2-7B-Chat-GGUF]( This [github issue]( is also relevant to find the right model for your machine. ```python from langchain.callbacks.manager import CallbackManager from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler from langchain.chains import LLMChain from langchain.llms import LlamaCpp from langchain.prompts import PromptTemplate ``` **Consider using a template that suits your model! Check the models page on Hugging Face etc. to get a correct prompting template.** ```python template = """"""Question: {question} Answer: Let\'s work this out in a step by step way to be sure we have the right answer."""""" prompt = PromptTemplate(template=template, input_variables=[""question""]) ``` ```python # Callbacks support token-wise streaming callback_manager = CallbackManager([StreamingStdOutCallbackHandler()]) ``` ### CPU Example using a LLaMA 2 7B model ```python # Make sure the model path is correct for your system! llm = LlamaCpp( model_path=""/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin"", temperature=0.75, max_tokens=2000, top_p=1, callback_manager=callback_manager, verbose=True, # Verbose is required to pass to the callback manager ) ``` ```python prompt = """""" Question: A rap battle between Stephen Colbert and John Oliver """""" llm(prompt) ``` ```text Stephen Colbert: Yo, John, I heard you\'ve been talkin\' smack about me on your show. Let me tell you somethin\', pal, I\'m the king of late-night TV My satire is sharp as a razor, it cuts deeper than a knife While you\'re just a british bloke tryin\' to be funny with your accent and your wit. John Oliver: Oh Stephen, don\'t be ridiculous, you may have the ratings but I got the real talk. My show is the one that people actually watch and listen to, not just for the laughs but for the facts. While you\'re busy talkin\' trash, I\'m out here bringing the truth to light. Stephen Colbert: Truth? Ha! You think your show is about truth? Please, it\'s all just a joke to you. You\'re just a fancy-pants british guy tryin\' to be funny with your news and your jokes. While I\'m the one who\'s really makin\' a difference, with my sat llama_print_timings: load time = 358.60 ms llama_print_timings: sample time = 172.55 ms / 256 runs ( 0.67 ms per token, 1483.59 tokens per second) llama_print_timings: prompt eval time = 613.36 ms / 16 tokens ( 38.33 ms per token, 26.09 tokens per second) llama_print_timings: eval time = 10151.17 ms / 255 runs ( 39.81 ms per token, 25.12 tokens per second) llama_print_timings: total time = 11332.41 ms ""\\nStephen Colbert:\\nYo, John, I heard you\'ve been talkin\' smack about me on your show.\\nLet me tell you somethin\', pal, I\'m the king of late-night TV\\nMy satire is sharp as a razor, it cuts deeper than a knife\\nWhile you\'re just a british bloke tryin\' to be funny with your accent and your wit.\\nJohn Oliver:\\nOh Stephen, don\'t be ridiculous, you may have the ratings but I got the real talk.\\nMy show is the one that people actually watch and listen to, not just for the laughs but for the facts.\\nWhile you\'re busy talkin\' trash, I\'m out here bringing the truth to light.\\nStephen Colbert:\\nTruth? Ha! You think your show is about truth? Please, it\'s all just a joke to you.\\nYou\'re just a fancy-pants british guy tryin\' to be funny with your news and your jokes.\\nWhile I\'m the one who\'s really makin\' a difference, with my sat"" ``` Example using a LLaMA v1 model ```python # Make sure the model path is correct for your system! llm = LlamaCpp( model_path=""./ggml-model-q4_0.bin"", callback_manager=callback_manager, verbose=True ) ``` ```python llm_chain = LLMChain(prompt=prompt, llm=llm) ``` ```python question = ""What NFL team won the Super Bowl in the year Justin Bieber was born?"" llm_chain.run(question) ``` ```text 1. First, find out when Justin Bieber was born. 2. We know that Justin Bieber was born on March 1, 1994. 3. Next, we need to look up when the Super Bowl was played in that year. 4. The Super Bowl was played on January 28, 1995. 5. Finally, we can use this information to answer the question. The NFL team that won the Super Bowl in the year Justin Bieber was born is the San Francisco 49ers. llama_print_timings: load time = 434.15 ms llama_print_timings: sample time = 41.81 ms / 121 runs ( 0.35 ms per token) llama_print_timings: prompt eval time = 2523.78 ms / 48 tokens ( 52.58 ms per token) llama_print_timings: eval time = 23971.57 ms / 121 runs ( 198.11 ms per token) llama_print_timings: total time = 28945.95 ms \'\\n\\n1. First, find out when Justin Bieber was born.\\n2. We know that Justin Bieber was born on March 1, 1994.\\n3. Next, we need to look up when the Super Bowl was played in that year.\\n4. The Super Bowl was played on January 28, 1995.\\n5. Finally, we can use this information to answer the question. The NFL team that won the Super Bowl in the year Justin Bieber was born is the San Francisco 49ers.\' ``` ### GPU If the installation with BLAS backend was correct, you will see a `BLAS = 1` indicator in model properties. Two of the most important parameters for use with GPU are: - `n_gpu_layers` - determines how many layers of the model are offloaded to your GPU. - `n_batch` - how many tokens are processed in parallel. Setting these parameters correctly will dramatically improve the evaluation speed (see [wrapper code]( for more details). ```python n_gpu_layers = 40 # Change this value based on your model and your GPU VRAM pool. n_batch = 512 # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU. # Make sure the model path is correct for your system! llm = LlamaCpp( model_path=""/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin"", n_gpu_layers=n_gpu_layers, n_batch=n_batch, callback_manager=callback_manager, verbose=True, # Verbose is required to pass to the callback manager ) ``` ```python llm_chain = LLMChain(prompt=prompt, llm=llm) question = ""What NFL team won the Super Bowl in the year Justin Bieber was born?"" llm_chain.run(question) ``` ```text 1. Identify Justin Bieber\'s birth date: Justin Bieber was born on March 1, 1994. 2. Find the Super Bowl winner of that year: The NFL season of 1993 with the Super Bowl being played in January or of 1994. 3. Determine which team won the game: The Dallas Cowboys faced the Buffalo Bills in Super Bowl XXVII on January 31, 1993 (as the year is mis-labelled due to a error). The Dallas Cowboys won this matchup. So, Justin Bieber was born when the Dallas Cowboys were the reigning NFL Super Bowl. llama_print_timings: load time = 427.63 ms llama_print_timings: sample time = 115.85 ms / 164 runs ( 0.71 ms per token, 1415.67 tokens per second) llama_print_timings: prompt eval time = 427.53 ms / 45 tokens ( 9.50 ms per token, 105.26 tokens per second) llama_print_timings: eval time = 4526.53 ms / 163 runs ( 27.77 ms per token, 36.01 tokens per second) llama_print_timings: total time = 5293.77 ms ""\\n\\n1. Identify Justin Bieber\'s birth date: Justin Bieber was born on March 1, 1994.\\n\\n2. Find the Super Bowl winner of that year: The NFL season of 1993 with the Super Bowl being played in January or of 1994.\\n\\n3. Determine which team won the game: The Dallas Cowboys faced the Buffalo Bills in Super Bowl XXVII on January 31, 1993 (as the year is mis-labelled due to a error). The Dallas Cowboys won this matchup.\\n\\nSo, Justin Bieber was born when the Dallas Cowboys were the reigning NFL Super Bowl."" ``` ### Metal If the installation with Metal was correct, you will see a `NEON = 1` indicator in model properties. Two of the most important GPU parameters are: - `n_gpu_layers` - determines how many layers of the model are offloaded to your Metal GPU, in the most case, set it to `1` is enough for Metal - `n_batch` - how many tokens are processed in parallel, default is 8, set to bigger number. - `f16_kv` - for some reason, Metal only support `True`, otherwise you will get error such as `Asserting on type 0 GGML_ASSERT: .../ggml-metal.m:706: false && ""not implemented""` Setting these parameters correctly will dramatically improve the evaluation speed (see [wrapper code]( for more details). ```python n_gpu_layers = 1 # Metal set to 1 is enough. n_batch = 512 # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip. # Make sure the model path is correct for your system! llm = LlamaCpp( model_path=""/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin"", n_gpu_layers=n_gpu_layers, n_batch=n_batch, f16_kv=True, # MUST set to True, otherwise you will run into problem after a couple of calls callback_manager=callback_manager, verbose=True, # Verbose is required to pass to the callback manager ) ``` The console log will show the following log to indicate Metal was enable properly. ```text ggml_metal_init: allocating ggml_metal_init: using MPS ... ``` You also could check `Activity Monitor` by watching the GPU usage of the process, the CPU usage will drop dramatically after turn on `n_gpu_layers=1`. For the first call to the LLM, the performance may be slow due to the model compilation in Metal GPU. ### Grammars We can use [grammars]( to constrain model outputs and sample tokens based on the rules defined in them. To demonstrate this concept, we\'ve included [sample grammar files]( that will be used in the examples below. Creating gbnf grammar files can be time-consuming, but if you have a use-case where output schemas are important, there are two tools that can help: - [Online grammar generator app]( that converts TypeScript interface definitions to gbnf file. - [Python script]( for converting json schema to gbnf file. You can for example create `pydantic` object, generate its JSON schema using `.schema_json()` method, and then use this script to convert it to gbnf file. In the first example, supply the path to the specifed `json.gbnf` file in order to produce JSON: ```python n_gpu_layers = 1 # Metal set to 1 is enough. n_batch = 512 # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip. # Make sure the model path is correct for your system! llm = LlamaCpp( model_path=""/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin"", n_gpu_layers=n_gpu_layers, n_batch=n_batch, f16_kv=True, # MUST set to True, otherwise you will run into problem after a couple of calls callback_manager=callback_manager, verbose=True, # Verbose is required to pass to the callback manager grammar_path=""/Users/rlm/Desktop/Code/langchain-main/langchain/libs/langchain/langchain/llms/grammars/json.gbnf"", ) ``` ```python result = llm(""Describe a person in JSON format:"") ``` ```text { ""name"": ""John Doe"", ""age"": 34, """": { ""title"": ""Software Developer"", ""company"": ""Google"" }, ""interests"": [ ""Sports"", ""Music"", ""Cooking"" ], ""address"": { ""street_number"": 123, ""street_name"": ""Oak Street"", ""city"": ""Mountain View"", ""state"": ""California"", ""postal_code"": 94040 }} llama_print_timings: load time = 357.51 ms llama_print_timings: sample time = 1213.30 ms / 144 runs ( 8.43 ms per token, 118.68 tokens per second) llama_print_timings: prompt eval time = 356.78 ms / 9 tokens ( 39.64 ms per token, 25.23 tokens per second) llama_print_timings: eval time = 3947.16 ms / 143 runs ( 27.60 ms per token, 36.23 tokens per second) llama_print_timings: total time = 5846.21 ms ``` We can also supply `list.gbnf` to return a list: ```python n_gpu_layers = 1 n_batch = 512 llm = LlamaCpp( model_path=""/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin"", n_gpu_layers=n_gpu_layers, n_batch=n_batch, f16_kv=True, # MUST set to True, otherwise you will run into problem after a couple of calls callback_manager=callback_manager, verbose=True, grammar_path=""/Users/rlm/Desktop/Code/langchain-main/langchain/libs/langchain/langchain/llms/grammars/list.gbnf"", ) ``` ```python result = llm(""List of top-3 my favourite books:"") ``` ```text [""The Catcher in the Rye"", ""Wuthering Heights"", ""Anna Karenina""] llama_print_timings: load time = 322.34 ms llama_print_timings: sample time = 232.60 ms / 26 runs ( 8.95 ms per token, 111.78 tokens per second) llama_print_timings: prompt eval time = 321.90 ms / 11 tokens ( 29.26 ms per token, 34.17 tokens per second) llama_print_timings: eval time = 680.82 ms / 25 runs ( 27.23 ms per token, 36.72 tokens per second) llama_print_timings: total time = 1295.27 ms ``` - [Installation](#installation)- [CPU only installation](#cpu-only-installation) - [Installation with OpenBLAS / cuBLAS / CLBlast](#installation-with-openblas--cublas--clblast) - [Installation with Metal](#installation-with-metal) - [Installation with Windows](#installation-with-windows) - [Usage](#usage)- [CPU](#cpu) - [GPU](#gpu) - [Metal](#metal) - [Grammars](#grammars)', 'WebResearchRetriever | WebResearchRetriever Given a query, this retriever will: - Formulate a set of relate Google searches - Search for each - Load all the resulting URLs - Then embed and perform similarity search with the query on the consolidate page content ```python from langchain.retrievers.web_research import WebResearchRetriever ``` ### Simple usage Specify the LLM to use for Google search query generation. ```python import os from langchain.chat_models.openai import ChatOpenAI from langchain.embeddings import OpenAIEmbeddings from langchain.utilities import GoogleSearchAPIWrapper from langchain.vectorstores import Chroma # Vectorstore vectorstore = Chroma( embedding_function=OpenAIEmbeddings(), persist_directory=""./chroma_db_oai"" ) # LLM llm = ChatOpenAI(temperature=0) # Search os.environ[""GOOGLE_CSE_ID""] = ""xxx"" os.environ[""GOOGLE_API_KEY""] = ""xxx"" search = GoogleSearchAPIWrapper() ``` ```python # Initialize web_research_retriever = WebResearchRetriever.from_llm( vectorstore=vectorstore, llm=llm, search=search, ) ``` #### Run with citations We can use `RetrievalQAWithSourcesChain` to retrieve docs and provide citations. ```python from langchain.chains import RetrievalQAWithSourcesChain user_input = ""How do LLM Powered Autonomous Agents work?"" qa_chain = RetrievalQAWithSourcesChain.from_chain_type( llm, retriever=web_research_retriever ) result = qa_chain({""question"": user_input}) result ``` ```text Fetching pages: 100%|###################################################################################################################################| 1/1 [00:00<00:00, 3.33it/s] {\'question\': \'How do LLM Powered Autonomous Agents work?\', \'answer\': ""LLM Powered Autonomous Agents work by using LLM (large language model) as the core controller of the agent\'s brain. It is complemented by several key components, including planning, memory, and tool use. The agent system is designed to be a powerful general problem solver. \\n"", \'sources\': \' ``` #### Run with logging Here, we use `get_relevant_documents` method to return docs. ```python # Run import logging logging.basicConfig() logging.getLogger(""langchain.retrievers.web_research"").setLevel(logging.INFO) user_input = ""What is Task Decomposition in LLM Powered Autonomous Agents?"" docs = web_research_retriever.get_relevant_documents(user_input) ``` ```text INFO:langchain.retrievers.web_research:Generating questions for Google Search ... INFO:langchain.retrievers.web_research:Questions for Google Search (raw): {\'question\': \'What is Task Decomposition in LLM Powered Autonomous Agents?\', \'text\': LineList(lines=[\'1. How do LLM powered autonomous agents utilize task decomposition?\\n\', \'2. Can you explain the concept of task decomposition in LLM powered autonomous agents?\\n\', \'3. What role does task decomposition play in the functioning of LLM powered autonomous agents?\\n\', \'4. Why is task decomposition important for LLM powered autonomous agents?\\n\'])} INFO:langchain.retrievers.web_research:Questions for Google Search: [\'1. How do LLM powered autonomous agents utilize task decomposition?\\n\', \'2. Can you explain the concept of task decomposition in LLM powered autonomous agents?\\n\', \'3. What role does task decomposition play in the functioning of LLM powered autonomous agents?\\n\', \'4. Why is task decomposition important for LLM powered autonomous agents?\\n\'] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{\'title\': ""LLM Powered Autonomous Agents | Lil\'Log"", \'link\': \' \'snippet\': \'Jun 23, 2023 ... Task decomposition can be done (1) by LLM with simple prompting like ""Steps for XYZ.\\\\n1."" , ""What are the subgoals for achieving XYZ?\'}] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{\'title\': ""LLM Powered Autonomous Agents | Lil\'Log"", \'link\': \' \'snippet\': \'Jun 23, 2023 ... Task decomposition can be done (1) by LLM with simple prompting like ""Steps for XYZ.\\\\n1."" , ""What are the subgoals for achieving XYZ?"" , (2)\\xa0...\'}] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{\'title\': ""LLM Powered Autonomous Agents | Lil\'Log"", \'link\': \' \'snippet\': \'Jun 23, 2023 ... In a LLM-powered autonomous agent system, LLM functions as the ... Task decomposition can be done (1) by LLM with simple prompting like\\xa0...\'}] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{\'title\': ""LLM Powered Autonomous Agents | Lil\'Log"", \'link\': \' \'snippet\': \'Jun 23, 2023 ... Agent System Overview In a LLM-powered autonomous agent system, ... Task decomposition can be done (1) by LLM with simple prompting like\\xa0...\'}] INFO:langchain.retrievers.web_research:New URLs to load: [] ``` #### Generate answer using retrieved docs We can use `load_qa_chain` for QA using the retrieved docs. ```python from langchain.chains.question_answering import load_qa_chain chain = load_qa_chain(llm, chain_type=""stuff"") output = chain( {""input_documents"": docs, ""question"": user_input}, return_only_outputs=True ) output[""output_text""] ``` ```text \'Task decomposition in LLM-powered autonomous agents refers to the process of breaking down a complex task into smaller, more manageable subgoals. This allows the agent to efficiently handle and execute the individual steps required to complete the overall task. By decomposing the task, the agent can prioritize and organize its actions, making it easier to plan and execute the necessary steps towards achieving the desired outcome.\' ``` ### More flexibility Pass an LLM chain with custom prompt and output parsing. ```python import os import re from typing import List from langchain.chains import LLMChain from langchain.output_parsers.pydantic import PydanticOutputParser from langchain.prompts import PromptTemplate from pydantic import BaseModel, Field # LLMChain search_prompt = PromptTemplate( input_variables=[""question""], template=""""""You are an assistant tasked with improving Google search results. Generate FIVE Google search queries that are similar to this question. The output should be a numbered list of questions and each should have a question mark at the end: {question}"""""", ) class LineList(BaseModel): """"""List of questions."""""" lines: List[str] = Field(description=""Questions"") class QuestionListOutputParser(PydanticOutputParser): """"""Output parser for a list of numbered questions."""""" def __init__(self) -> None: super().__init__(pydantic_object=LineList) def parse(self, text: str) -> LineList: lines = re.findall(r""\\d+\\..*?\\n"", text) return LineList(lines=lines) llm_chain = LLMChain( llm=llm, prompt=search_prompt, output_parser=QuestionListOutputParser(), ) ``` ```python # Initialize web_research_retriever_llm_chain = WebResearchRetriever( vectorstore=vectorstore, llm_chain=llm_chain, search=search, ) # Run docs = web_research_retriever_llm_chain.get_relevant_documents(user_input) ``` ```text INFO:langchain.retrievers.web_research:Generating questions for Google Search ... INFO:langchain.retrievers.web_research:Questions for Google Search (raw): {\'question\': \'What is Task Decomposition in LLM Powered Autonomous Agents?\', \'text\': LineList(lines=[\'1. How do LLM powered autonomous agents use task decomposition?\\n\', \'2. Why is task decomposition important for LLM powered autonomous agents?\\n\', \'3. Can you explain the concept of task decomposition in LLM powered autonomous agents?\\n\', \'4. What are the benefits of task decomposition in LLM powered autonomous agents?\\n\'])} INFO:langchain.retrievers.web_research:Questions for Google Search: [\'1. How do LLM powered autonomous agents use task decomposition?\\n\', \'2. Why is task decomposition important for LLM powered autonomous agents?\\n\', \'3. Can you explain the concept of task decomposition in LLM powered autonomous agents?\\n\', \'4. What are the benefits of task decomposition in LLM powered autonomous agents?\\n\'] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{\'title\': ""LLM Powered Autonomous Agents | Lil\'Log"", \'link\': \' \'snippet\': \'Jun 23, 2023 ... Task decomposition can be done (1) by LLM with simple prompting like ""Steps for XYZ.\\\\n1."" , ""What are the subgoals for achieving XYZ?\'}] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{\'title\': ""LLM Powered Autonomous Agents | Lil\'Log"", \'link\': \' \'snippet\': \'Jun 23, 2023 ... Task decomposition can be done (1) by LLM with simple prompting like ""Steps for XYZ.\\\\n1."" , ""What are the subgoals for achieving XYZ?"" , (2)\\xa0...\'}] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{\'title\': ""LLM Powered Autonomous Agents | Lil\'Log"", \'link\': \' \'snippet\': \'Jun 23, 2023 ... Task decomposition can be done (1) by LLM with simple prompting like ""Steps for XYZ.\\\\n1."" , ""What are the subgoals for achieving XYZ?\'}] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{\'title\': ""LLM Powered Autonomous Agents | Lil\'Log"", \'link\': \' \'snippet\': \'Jun 23, 2023 ... Task decomposition can be done (1) by LLM with simple prompting like ""Steps for XYZ.\\\\n1."" , ""What are the subgoals for achieving XYZ?\'}] INFO:langchain.retrievers.web_research:New URLs to load: [\' INFO:langchain.retrievers.web_research:Grabbing most relevant splits from urls ... Fetching pages: 100%|###################################################################################################################################| 1/1 [00:00<00:00, 6.32it/s] ``` ```python len(docs) ``` ```text 1 ``` ### Run locally Specify LLM and embeddings that will run locally (e.g., on your laptop). ```python from langchain.callbacks.manager import CallbackManager from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler from langchain.embeddings import GPT4AllEmbeddings from langchain.llms import LlamaCpp n_gpu_layers = 1 # Metal set to 1 is enough. n_batch = 512 # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip. callback_manager = CallbackManager([StreamingStdOutCallbackHandler()]) llama = LlamaCpp( model_path=""/Users/rlm/Desktop/Code/llama.cpp/llama-2-13b-chat.ggmlv3.q4_0.bin"", n_gpu_layers=n_gpu_layers, n_batch=n_batch, n_ctx=4096, # Context window max_tokens=1000, # Max tokens to generate f16_kv=True, # MUST set to True, otherwise you will run into problem after a couple of calls callback_manager=callback_manager, verbose=True, ) vectorstore_llama = Chroma( embedding_function=GPT4AllEmbeddings(), persist_directory=""./chroma_db_llama"" ) ``` ```text llama.cpp: loading model from /Users/rlm/Desktop/Code/llama.cpp/llama-2-13b-chat.ggmlv3.q4_0.bin llama_model_load_internal: format = ggjt v3 (latest) llama_model_load_internal: n_vocab = 32000 llama_model_load_internal: n_ctx = 4096 llama_model_load_internal: n_embd = 5120 llama_model_load_internal: n_mult = 256 llama_model_load_internal: n_head = 40 llama_model_load_internal: n_layer = 40 llama_model_load_internal: n_rot = 128 llama_model_load_internal: freq_base = 10000.0 llama_model_load_internal: freq_scale = 1 llama_model_load_internal: ftype = 2 (mostly Q4_0) llama_model_load_internal: n_ff = 13824 llama_model_load_internal: model size = 13B llama_model_load_internal: ggml ctx size = 0.09 MB llama_model_load_internal: mem required = 9132.71 MB (+ 1608.00 MB per state) llama_new_context_with_model: kv self size = 3200.00 MB ggml_metal_init: allocating Found model file at /Users/rlm/.cache/gpt4all/ggml-all-MiniLM-L6-v2-f16.bin llama_new_context_with_model: max tensor size = 87.89 MB ggml_metal_init: using MPS ggml_metal_init: loading \'/Users/rlm/miniforge3/envs/llama/lib/python3.9/site-packages/llama_cpp/ggml-metal.metal\' ggml_metal_init: loaded kernel_add 0x110fbd600 ggml_metal_init: loaded kernel_mul 0x110fbeb30 ggml_metal_init: loaded kernel_mul_row 0x110fbf350 ggml_metal_init: loaded kernel_scale 0x110fbf9e0 ggml_metal_init: loaded kernel_silu 0x110fc0150 ggml_metal_init: loaded kernel_relu 0x110fbd950 ggml_metal_init: loaded kernel_gelu 0x110fbdbb0 ggml_metal_init: loaded kernel_soft_max 0x110fc14d0 ggml_metal_init: loaded kernel_diag_mask_inf 0x110fc1980 ggml_metal_init: loaded kernel_get_rows_f16 0x110fc22a0 ggml_metal_init: loaded kernel_get_rows_q4_0 0x110fc2ad0 ggml_metal_init: loaded kernel_get_rows_q4_1 0x110fc3260 ggml_metal_init: loaded kernel_get_rows_q2_K 0x110fc3ad0 ggml_metal_init: loaded kernel_get_rows_q3_K 0x110fc41c0 ggml_metal_init: loaded kernel_get_rows_q4_K 0x110fc48c0 ggml_metal_init: loaded kernel_get_rows_q5_K 0x110fc4fa0 ggml_metal_init: loaded kernel_get_rows_q6_K 0x110fc56a0 ggml_metal_init: loaded kernel_rms_norm 0x110fc5da0 ggml_metal_init: loaded kernel_norm 0x110fc64d0 ggml_metal_init: loaded kernel_mul_mat_f16_f32 0x2a5c19990 ggml_metal_init: loaded kernel_mul_mat_q4_0_f32 0x2a5c1d4a0 ggml_metal_init: loaded kernel_mul_mat_q4_1_f32 0x2a5c19fc0 ggml_metal_init: loaded kernel_mul_mat_q2_K_f32 0x2a5c1dcc0 ggml_metal_init: loaded kernel_mul_mat_q3_K_f32 0x2a5c1e420 ggml_metal_init: loaded kernel_mul_mat_q4_K_f32 0x2a5c1edc0 ggml_metal_init: loaded kernel_mul_mat_q5_K_f32 0x2a5c1fd90 ggml_metal_init: loaded kernel_mul_mat_q6_K_f32 0x2a5c20540 ggml_metal_init: loaded kernel_rope 0x2a5c20d40 ggml_metal_init: loaded kernel_alibi_f32 0x2a5c21730 ggml_metal_init: loaded kernel_cpy_f32_f16 0x2a5c21ab0 ggml_metal_init: loaded kernel_cpy_f32_f32 0x2a5c22080 ggml_metal_init: loaded kernel_cpy_f16_f16 0x2a5c231d0 ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB ggml_metal_init: hasUnifiedMemory = true ggml_metal_init: maxTransferRate = built-in GPU ggml_metal_add_buffer: allocated \'data \' buffer, size = 6984.06 MB, ( 6984.52 / 21845.34) ggml_metal_add_buffer: allocated \'eval \' buffer, size = 1040.00 MB, ( 8024.52 / 21845.34) ggml_metal_add_buffer: allocated \'kv \' buffer, size = 3202.00 MB, (11226.52 / 21845.34) ggml_metal_add_buffer: allocated \'scr0 \' buffer, size = 597.00 MB, (11823.52 / 21845.34) AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | ggml_metal_add_buffer: allocated \'scr1 \' buffer, size = 512.00 MB, (12335.52 / 21845.34) objc[33471]: Class GGMLMetalClass is implemented in both /Users/rlm/miniforge3/envs/llama/lib/python3.9/site-packages/llama_cpp/libllama.dylib (0x2c7368208) and /Users/rlm/miniforge3/envs/llama/lib/python3.9/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libreplit-mainline-metal.dylib (0x5ebf48208). One of the two will be used. Which one is undefined. objc[33471]: Class GGMLMetalClass is implemented in both /Users/rlm/miniforge3/envs/llama/lib/python3.9/site-packages/llama_cpp/libllama.dylib (0x2c7368208) and /Users/rlm/miniforge3/envs/llama/lib/python3.9/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libllamamodel-mainline-metal.dylib (0x5ec374208). One of the two will be used. Which one is undefined. ``` We supplied `StreamingStdOutCallbackHandler()`, so model outputs (e.g., generated questions) are streamed. We also have logging on, so we seem them there too. ```python from langchain.chains import RetrievalQAWithSourcesChain # Initialize web_research_retriever = WebResearchRetriever.from_llm( vectorstore=vectorstore_llama, llm=llama, search=search, ) # Run user_input = ""What is Task Decomposition in LLM Powered Autonomous Agents?"" qa_chain = RetrievalQAWithSourcesChain.from_chain_type( llama, retriever=web_research_retriever ) result = qa_chain({""question"": user_input}) result ``` ```text INFO:langchain.retrievers.web_research:Generating questions for Google Search ... Sure, here are five Google search queries that are similar to ""What is Task Decomposition in LLM Powered Autonomous Agents?"": 1. How does Task Decomposition work in LLM Powered Autonomous Agents? 2. What are the benefits of using Task Decomposition in LLM Powered Autonomous Agents? 3. Can you provide examples of Task Decomposition in LLM Powered Autonomous Agents? 4. How does Task Decomposition improve the performance of LLM Powered Autonomous Agents? 5. What are some common challenges or limitations of using Task Decomposition in LLM Powered Autonomous Agents, and how can they be addressed? llama_print_timings: load time = 8585.01 ms llama_print_timings: sample time = 124.24 ms / 164 runs ( 0.76 ms per token, 1320.04 tokens per second) llama_print_timings: prompt eval time = 8584.83 ms / 101 tokens ( 85.00 ms per token, 11.76 tokens per second) llama_print_timings: eval time = 7268.55 ms / 163 runs ( 44.59 ms per token, 22.43 tokens per second) llama_print_timings: total time = 16236.13 ms INFO:langchain.retrievers.web_research:Questions for Google Search (raw): {\'question\': \'What is Task Decomposition in LLM Powered Autonomous Agents?\', \'text\': LineList(lines=[\'1. How does Task Decomposition work in LLM Powered Autonomous Agents? \\n\', \'2. What are the benefits of using Task Decomposition in LLM Powered Autonomous Agents? \\n\', \'3. Can you provide examples of Task Decomposition in LLM Powered Autonomous Agents? \\n\', \'4. How does Task Decomposition improve the performance of LLM Powered Autonomous Agents? \\n\'])} INFO:langchain.retrievers.web_research:Questions for Google Search: [\'1. How does Task Decomposition work in LLM Powered Autonomous Agents? \\n\', \'2. What are the benefits of using Task Decomposition in LLM Powered Autonomous Agents? \\n\', \'3. Can you provide examples of Task Decomposition in LLM Powered Autonomous Agents? \\n\', \'4. How does Task Decomposition improve the performance of LLM Powered Autonomous Agents? \\n\'] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{\'title\': ""LLM Powered Autonomous Agents | Lil\'Log"", \'link\': \' \'snippet\': \'Jun 23, 2023 ... Task decomposition can be done (1) by LLM with simple prompting like ""Steps for XYZ.\\\\n1."" , ""What are the subgoals for achieving XYZ?\'}] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{\'title\': ""LLM Powered Autonomous Agents | Lil\'Log"", \'link\': \' \'snippet\': \'Jun 23, 2023 ... Task decomposition can be done (1) by LLM with simple prompting like ""Steps for XYZ.\\\\n1."" , ""What are the subgoals for achieving XYZ?"" , (2)\\xa0...\'}] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{\'title\': ""LLM Powered Autonomous Agents | Lil\'Log"", \'link\': \' \'snippet\': \'Jun 23, 2023 ... A complicated task usually involves many steps. An agent needs to know what they are and plan ahead. Task Decomposition#. Chain of thought (CoT;\\xa0...\'}] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{\'title\': ""LLM Powered Autonomous Agents | Lil\'Log"", \'link\': \' \'snippet\': \'Jun 23, 2023 ... Agent System Overview In a LLM-powered autonomous agent system, ... Task decomposition can be done (1) by LLM with simple prompting like\\xa0...\'}] INFO:langchain.retrievers.web_research:New URLs to load: [\' INFO:langchain.retrievers.web_research:Grabbing most relevant splits from urls ... Fetching pages: 100%|###################################################################################################################################| 1/1 [00:00<00:00, 10.49it/s] Llama.generate: prefix-match hit The content discusses Task Decomposition in LLM Powered Autonomous Agents, which involves breaking down large tasks into smaller, manageable subgoals for efficient handling of complex tasks. SOURCES: llama_print_timings: load time = 8585.01 ms llama_print_timings: sample time = 52.88 ms / 72 runs ( 0.73 ms per token, 1361.55 tokens per second) llama_print_timings: prompt eval time = 125925.13 ms / 2358 tokens ( 53.40 ms per token, 18.73 tokens per second) llama_print_timings: eval time = 3504.16 ms / 71 runs ( 49.35 ms per token, 20.26 tokens per second) llama_print_timings: total time = 129584.60 ms {\'question\': \'What is Task Decomposition in LLM Powered Autonomous Agents?\', \'answer\': \' The content discusses Task Decomposition in LLM Powered Autonomous Agents, which involves breaking down large tasks into smaller, manageable subgoals for efficient handling of complex tasks.\\n\', \'sources\': \' ``` - [Simple usage](#simple-usage) - [More flexibility](#more-flexibility) - [Run locally](#run-locally)', 'Run LLMs locally | Run LLMs locally ## Use case The popularity of projects like [PrivateGPT]( [llama.cpp]( and [GPT4All]( underscore the demand to run LLMs locally (on your own device). This has at least two important benefits: 1. `Privacy`: Your data is not sent to a third party, and it is not subject to the terms of service of a commercial service 2. `Cost`: There is no inference fee, which is important for token-intensive applications (e.g., [long-running simulations]( summarization) ## Overview Running an LLM locally requires a few things: 1. `Open-source LLM`: An open-source LLM that can be freely modified and shared 2. `Inference`: Ability to run this LLM on your device w/ acceptable latency ### Open-source LLMs Users can now gain access to a rapidly growing set of [open-source LLMs]( These LLMs can be assessed across at least two dimensions (see figure): 1. `Base model`: What is the base-model and how was it trained? 2. `Fine-tuning approach`: Was the base-model fine-tuned and, if so, what [set of instructions]( was used? ![Image description](/assets/images/OSS_LLM_overview-b0a96cc35216ec43c3ccde7ed1140854.png) The relative performance of these models can be assessed using several leaderboards, including: 1. [LmSys]( 2. [GPT4All]( 3. [HuggingFace]( ### Inference A few frameworks for this have emerged to support inference of open-source LLMs on various devices: 1. [llama.cpp]( C++ implementation of llama inference code with [weight optimization / quantization]( 2. [gpt4all]( Optimized C backend for inference 3. [Ollama]( Bundles model weights and environment into an app that runs on device and serves the LLM In general, these frameworks will do a few things: 1. `Quantization`: Reduce the memory footprint of the raw model weights 2. `Efficient implementation for inference`: Support inference on consumer hardware (e.g., CPU or laptop GPU) In particular, see [this excellent post]( on the importance of quantization. ![Image description](/assets/images/llama-memory-weights-aaccef5df087e993b0f46277500039b6.png) With less precision, we radically decrease the memory needed to store the LLM in memory. In addition, we can see the importance of GPU memory bandwidth [sheet]( A Mac M2 Max is 5-6x faster than a M1 for inference due to the larger GPU memory bandwidth. ![Image description](/assets/images/llama_t_put-c6f0ea201a6dd508999170325cd6804a.png) ## Quickstart [Ollama]( is one way to easily run inference on macOS. The instructions [here](/docs/guides/docs/integrations/llms/ollama) provide details, which we summarize: - [Download and run]( the app - From command line, fetch a model from this [list of options]( e.g., `ollama pull llama2` - When the app is running, all models are automatically served on `localhost:11434` ```python from langchain.llms import Ollama llm = Ollama(model=""llama2"") llm(""The first man on the moon was ..."") ``` ```text \' The first man on the moon was Neil Armstrong, who landed on the moon on July 20, 1969 as part of the Apollo 11 mission. obviously.\' ``` Stream tokens as they are being generated. ```python from langchain.callbacks.manager import CallbackManager from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler llm = Ollama( model=""llama2"", callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]) ) llm(""The first man on the moon was ..."") ``` ```text The first man to walk on the moon was Neil Armstrong, an American astronaut who was part of the Apollo 11 mission in 1969. 20, 1969, Armstrong stepped out of the lunar module Eagle and onto the moon\'s surface, famously declaring ""That\'s one small step for man, one giant leap for mankind"" as he took his first steps. He was followed by fellow astronaut Edwin ""Buzz"" Aldrin, who also walked on the moon during the mission. \' The first man to walk on the moon was Neil Armstrong, an American astronaut who was part of the Apollo 11 mission in 1969. 20, 1969, Armstrong stepped out of the lunar module Eagle and onto the moon\\\'s surface, famously declaring ""That\\\'s one small step for man, one giant leap for mankind"" as he took his first steps. He was followed by fellow astronaut Edwin ""Buzz"" Aldrin, who also walked on the moon during the mission.\' ``` ## Environment Inference speed is a challenge when running models locally (see above). To minimize latency, it is desirable to run models locally on GPU, which ships with many consumer laptops [e.g., Apple devices]( And even with GPU, the available GPU memory bandwidth (as noted above) is important. ### Running Apple silicon GPU `Ollama` will automatically utilize the GPU on Apple devices. Other frameworks require the user to set up the environment to utilize the Apple GPU. For example, `llama.cpp` python bindings can be configured to use the GPU via [Metal]( Metal is a graphics and compute API created by Apple providing near-direct access to the GPU. See the [llama.cpp](/docs/guides/docs/integrations/llms/llamacpp) setup [here]( to enable this. In particular, ensure that conda is using the correct virtual environment that you created (`miniforge3`). E.g., for me: ```text conda activate /Users/rlm/miniforge3/envs/llama ``` With the above confirmed, then: ```text CMAKE_ARGS=""-DLLAMA_METAL=on"" FORCE_CMAKE=1 pip install -U llama-cpp-python --no-cache-dir ``` ## LLMs There are various ways to gain access to quantized model weights. 1. [HuggingFace]( - Many quantized model are available for download and can be run with framework such as [llama.cpp]( 2. [gpt4all]( - The model explorer offers a leaderboard of metrics and associated quantized models available for download 3. [Ollama]( - Several models can be accessed directly via `pull` ### Ollama With [Ollama](/docs/guides/docs/integrations/llms/ollama), fetch a model via `ollama pull :`: - E.g., for Llama-7b: `ollama pull llama2` will download the most basic version of the model (e.g., smallest # parameters and 4 bit quantization) - We can also specify a particular version from the [model list]( e.g., `ollama pull llama2:13b` - See the full set of parameters on the [API reference page]( ```python from langchain.llms import Ollama llm = Ollama(model=""llama2:13b"") llm(""The first man on the moon was ... think step by step"") ``` ```text \' Sure! Here\\\'s the answer, broken down step by step:\\n\\nThe first man on the moon was... Neil Armstrong.\\n\\nHere\\\'s how I arrived at that answer:\\n\\n1. The first manned mission to land on the moon was Apollo 11.\\n2. The mission included three astronauts: Neil Armstrong, Edwin ""Buzz"" Aldrin, and Michael Collins.\\n3. Neil Armstrong was the mission commander and the first person to set foot on the moon.\\n4. On July 20, 1969, Armstrong stepped out of the lunar module Eagle and onto the moon\\\'s surface, famously declaring ""That\\\'s one small step for man, one giant leap for mankind.""\\n\\nSo, the first man on the moon was Neil Armstrong!\' ``` ### Llama.cpp Llama.cpp is compatible with a [broad set of models]( For example, below we run inference on `llama2-13b` with 4 bit quantization downloaded from [HuggingFace]( As noted above, see the [API reference]( for the full set of parameters. From the [llama.cpp docs]( a few are worth commenting on: `n_gpu_layers`: number of layers to be loaded into GPU memory - Value: 1 - Meaning: Only one layer of the model will be loaded into GPU memory (1 is often sufficient). `n_batch`: number of tokens the model should process in parallel - Value: n_batch - Meaning: It\'s recommended to choose a value between 1 and n_ctx (which in this case is set to 2048) `n_ctx`: Token context window . - Value: 2048 - Meaning: The model will consider a window of 2048 tokens at a time `f16_kv`: whether the model should use half-precision for the key/value cache - Value: True - Meaning: The model will use half-precision, which can be more memory efficient; Metal only supports True. ```python %pip install -U llama-cpp-python --no-cache-dirclear` ``` ```python from langchain.llms import LlamaCpp llm = LlamaCpp( model_path=""/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin"", n_gpu_layers=1, n_batch=512, n_ctx=2048, f16_kv=True, callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]), verbose=True, ) ``` The console log will show the below to indicate Metal was enabled properly from steps above: ```text ggml_metal_init: allocating ggml_metal_init: using MPS ``` ```python llm(""The first man on the moon was ... Let\'s think step by step"") ``` ```text Llama.generate: prefix-match hit and use logical reasoning to figure out who the first man on the moon was. Here are some clues: 1. The first man on the moon was an American. 2. He was part of the Apollo 11 mission. 3. He stepped out of the lunar module and became the first person to set foot on the moon\'s surface. 4. His last name is Armstrong. Now, let\'s use our reasoning skills to figure out who the first man on the moon was. Based on clue #1, we know that the first man on the moon was an American. Clue #2 tells us that he was part of the Apollo 11 mission. Clue #3 reveals that he was the first person to set foot on the moon\'s surface. And finally, clue #4 gives us his last name: Armstrong. Therefore, the first man on the moon was Neil Armstrong! llama_print_timings: load time = 9623.21 ms llama_print_timings: sample time = 143.77 ms / 203 runs ( 0.71 ms per token, 1412.01 tokens per second) llama_print_timings: prompt eval time = 485.94 ms / 7 tokens ( 69.42 ms per token, 14.40 tokens per second) llama_print_timings: eval time = 6385.16 ms / 202 runs ( 31.61 ms per token, 31.64 tokens per second) llama_print_timings: total time = 7279.28 ms "" and use logical reasoning to figure out who the first man on the moon was.\\n\\nHere are some clues:\\n\\n1. The first man on the moon was an American.\\n2. He was part of the Apollo 11 mission.\\n3. He stepped out of the lunar module and became the first person to set foot on the moon\'s surface.\\n4. His last name is Armstrong.\\n\\nNow, let\'s use our reasoning skills to figure out who the first man on the moon was. Based on clue #1, we know that the first man on the moon was an American. Clue #2 tells us that he was part of the Apollo 11 mission. Clue #3 reveals that he was the first person to set foot on the moon\'s surface. And finally, clue #4 gives us his last name: Armstrong.\\nTherefore, the first man on the moon was Neil Armstrong!"" ``` ### GPT4All We can use model weights downloaded from [GPT4All]( model explorer. Similar to what is shown above, we can run inference and use [the API reference]( to set parameters of interest. ```python pip install gpt4all ``` ```python from langchain.llms import GPT4All llm = GPT4All( model=""/Users/rlm/Desktop/Code/gpt4all/models/nous-hermes-13b.ggmlv3.q4_0.bin"" ) ``` ```python llm(""The first man on the moon was ... Let\'s think step by step"") ``` ```text "".\\n1) The United States decides to send a manned mission to the moon.2) They choose their best astronauts and train them for this specific mission.3) They build a spacecraft that can take humans to the moon, called the Lunar Module (LM).4) They also create a larger spacecraft, called the Saturn V rocket, which will launch both the LM and the Command Service Module (CSM), which will carry the astronauts into orbit.5) The mission is planned down to the smallest detail: from the trajectory of the rockets to the exact movements of the astronauts during their moon landing.6) On July 16, 1969, the Saturn V rocket launches from Kennedy Space Center in Florida, carrying the Apollo 11 mission crew into space.7) After one and a half orbits around the Earth, the LM separates from the CSM and begins its descent to the moon\'s surface.8) On July 20, 1969, at 2:56 pm EDT (GMT-4), Neil Armstrong becomes the first man on the moon. He speaks these"" ``` ## Prompts Some LLMs will benefit from specific prompts. For example, LLaMA will use [special tokens]( We can use `ConditionalPromptSelector` to set prompt based on the model type. ```python # Set our LLM llm = LlamaCpp( model_path=""/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin"", n_gpu_layers=1, n_batch=512, n_ctx=2048, f16_kv=True, callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]), verbose=True, ) ``` Set the associated prompt based upon the model version. ```python from langchain.chains import LLMChain from langchain.chains.prompt_selector import ConditionalPromptSelector from langchain.prompts import PromptTemplate DEFAULT_LLAMA_SEARCH_PROMPT = PromptTemplate( input_variables=[""question""], template=""""""> \\n You are an assistant tasked with improving Google search \\ results. \\n > \\n\\n [INST] Generate THREE Google search queries that \\ are similar to this question. The output should be a numbered list of questions \\ and each should have a question mark at the end: \\n\\n {question} [/INST]"""""", ) DEFAULT_SEARCH_PROMPT = PromptTemplate( input_variables=[""question""], template=""""""You are an assistant tasked with improving Google search \\ results. Generate THREE Google search queries that are similar to \\ this question. The output should be a numbered list of questions and each \\ should have a question mark at the end: {question}"""""", ) QUESTION_PROMPT_SELECTOR = ConditionalPromptSelector( default_prompt=DEFAULT_SEARCH_PROMPT, conditionals=[(lambda llm: isinstance(llm, LlamaCpp), DEFAULT_LLAMA_SEARCH_PROMPT)], ) prompt = QUESTION_PROMPT_SELECTOR.get_prompt(llm) prompt ``` ```text PromptTemplate(input_variables=[\'question\'], output_parser=None, partial_variables={}, template=\'> \\n You are an assistant tasked with improving Google search results. \\n > \\n\\n [INST] Generate THREE Google search queries that are similar to this question. The output should be a numbered list of questions and each should have a question mark at the end: \\n\\n {question} [/INST]\', template_format=\'f-string\', validate_template=True) ``` ```python # Chain llm_chain = LLMChain(prompt=prompt, llm=llm) question = ""What NFL team won the Super Bowl in the year that Justin Bieber was born?"" llm_chain.run({""question"": question}) ``` ```text Sure! Here are three similar search queries with a question mark at the end: 1. Which NBA team did LeBron James lead to a championship in the year he was drafted? 2. Who won the Grammy Awards for Best New Artist and Best Female Pop Vocal Performance in the same year that Lady Gaga was born? 3. What MLB team did Babe Ruth play for when he hit 60 home runs in a single season? llama_print_timings: load time = 14943.19 ms llama_print_timings: sample time = 72.93 ms / 101 runs ( 0.72 ms per token, 1384.87 tokens per second) llama_print_timings: prompt eval time = 14942.95 ms / 93 tokens ( 160.68 ms per token, 6.22 tokens per second) llama_print_timings: eval time = 3430.85 ms / 100 runs ( 34.31 ms per token, 29.15 tokens per second) llama_print_timings: total time = 18578.26 ms \' Sure! Here are three similar search queries with a question mark at the end:\\n\\n1. Which NBA team did LeBron James lead to a championship in the year he was drafted?\\n2. Who won the Grammy Awards for Best New Artist and Best Female Pop Vocal Performance in the same year that Lady Gaga was born?\\n3. What MLB team did Babe Ruth play for when he hit 60 home runs in a single season?\' ``` We also can use the LangChain Prompt Hub to fetch and / or store prompts that are model specific. This will work with your [LangSmith API key]( For example, [here]( is a prompt for RAG with LLaMA-specific tokens. ## Use cases Given an `llm` created from one of the models above, you can use it for [many use cases](/docs/guides/docs/use_cases). For example, here is a guide to [RAG](/docs/guides/docs/use_cases/question_answering/local_retrieval_qa) with local LLMs. In general, use cases for local LLMs can be driven by at least two factors: - `Privacy`: private data (e.g., journals, etc) that a user does not want to share - `Cost`: text preprocessing (extraction/tagging), summarization, and agent simulations are token-use-intensive tasks In addition, [here]( is an overview on fine-tuning, which can utilize open-source LLMs. - [Use case](#use-case) - [Overview](#overview)- [Open-source LLMs](#open-source-llms) - [Inference](#inference) - [Quickstart](#quickstart) - [Environment](#environment)- [Running Apple silicon GPU](#running-apple-silicon-gpu) - [LLMs](#llms)- [Ollama](#ollama) - [Llama.cpp](#llamacpp) - [GPT4All](#gpt4all) - [Prompts](#prompts) - [Use cases](#use-cases)']","To run the `llama` model using the `vllm` framework, you'll need to follow these general steps:

1. **Install `vllm`**: First, ensure you have `vllm` installed. You can install it using pip:
   ```bash
   pip install vllm
   ```

2. **Download the Llama Model**: You need to have the `llama` model files. If you haven't already downloaded them, you can find them on the [Hugging Face model hub](https://huggingface.co/models) or another source where the model is hosted.

3. **Prepare the Configuration File**: `vllm` typically requires a configuration file to specify the model details. Here's an example configuration file (`config.yaml`):

   ```yaml
   model:
     name: ""path/to/your/llama-model""
     device: ""cuda"" # or ""cpu"" depending on your setup
   ```

4. **Run the Model with `vllm`**: Use the `vllm` command to run the model with the configuration file you prepared:

   ```bash
   vllm run -c config.yaml
   ```

5. **Interact with the Model**: Once the model is running, you can interact with it through the provided interface, usually via a command-line interface or an API endpoint, depending on how `vllm` is set up.

Please note that the exact steps might vary slightly depending on the version of `vllm` and the specific requirements of the `llama` model you are using. Always refer to the official documentation of both `vllm` and the `llama` model for the most accurate and detailed instructions.

If you encounter any issues, consider checking the official GitHub repositories or forums for `vllm` and `llama` for additional support.","You can run HuggingFace models in vLLM, so you can select the model name of a compatible Llama model then use the VLLM class in LangChain:
 ```python
from langchain.llms import VLLM

llm = VLLM(
    model=""name-of-llama-model"",
    trust_remote_code=True,  # mandatory for hf models
)

print(llm(""What is the capital of France ?""))
```",0.8333333332916666,0.5,,0.0024564619099481195,0.13953488372093026
90,What class type is returned by initialize_agent,"['Multi-Input Tools | Multi-Input Tools This notebook shows how to use a tool that requires multiple inputs with an agent. The recommended way to do so is with the `StructuredTool` class. ```python import os os.environ[""LANGCHAIN_TRACING""] = ""true"" ``` ```python from langchain.agents import AgentType, initialize_agent from langchain.llms import OpenAI llm = OpenAI(temperature=0) ``` ```python from langchain.tools import StructuredTool def multiplier(a: float, b: float) -> float: """"""Multiply the provided floats."""""" return a * b tool = StructuredTool.from_function(multiplier) ``` ```python # Structured tools are compatible with the STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION agent type. agent_executor = initialize_agent( [tool], llm, agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True, ) ``` ```python agent_executor.run(""What is 3 times 4"") ``` ```text > Entering new AgentExecutor chain... Thought: I need to multiply 3 and 4 Action: ``` { ""action"": ""multiplier"", ""action_input"": {""a"": 3, ""b"": 4} } ``` Observation: 12 Thought: I know what to respond Action: ``` { ""action"": ""Final Answer"", ""action_input"": ""3 times 4 is 12"" } ``` > Finished chain. \'3 times 4 is 12\' ``` ## Multi-Input Tools with a string format An alternative to the structured tool would be to use the regular `Tool` class and accept a single string. The tool would then have to handle the parsing logic to extract the relevant values from the text, which tightly couples the tool representation to the agent prompt. This is still useful if the underlying language model can\'t reliably generate structured schema. Let\'s take the multiplication function as an example. In order to use this, we will tell the agent to generate the ""Action Input"" as a comma-separated list of length two. We will then write a thin wrapper that takes a string, splits it into two around a comma, and passes both parsed sides as integers to the multiplication function. ```python from langchain.agents import AgentType, Tool, initialize_agent from langchain.llms import OpenAI ``` Here is the multiplication function, as well as a wrapper to parse a string as input. ```python def multiplier(a, b): return a * b def parsing_multiplier(string): a, b = string.split("","") return multiplier(int(a), int(b)) ``` ```python llm = OpenAI(temperature=0) tools = [ Tool( name=""Multiplier"", func=parsing_multiplier, description=""useful for when you need to multiply two numbers together. The input to this tool should be a comma separated list of numbers of length two, representing the two numbers you want to multiply together. For example, `1,2` would be the input if you wanted to multiply 1 by 2."", ) ] mrkl = initialize_agent( tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True ) ``` ```python mrkl.run(""What is 3 times 4"") ``` ```text > Entering new AgentExecutor chain... I need to multiply two numbers Action: Multiplier Action Input: 3,4 Observation: 12 Thought: I now know the final answer Final Answer: 3 times 4 is 12 > Finished chain. \'3 times 4 is 12\' ``` - [Multi-Input Tools with a string format](#multi-input-tools-with-a-string-format)', 'Search Tools | Search Tools This notebook shows off usage of various search tools. ```python from langchain.agents import AgentType, initialize_agent, load_tools from langchain.llms import OpenAI ``` ```python llm = OpenAI(temperature=0) ``` ## Google Serper API Wrapper First, let\'s try to use the Google Serper API tool. ```python tools = load_tools([""google-serper""], llm=llm) ``` ```python agent = initialize_agent( tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True ) ``` ```python agent.run(""What is the weather in Pomfret?"") ``` ```text > Entering new AgentExecutor chain... I should look up the current weather conditions. Action: Search Action Input: ""weather in Pomfret"" Observation: 37F Thought: I now know the current temperature in Pomfret. Final Answer: The current temperature in Pomfret is 37F. > Finished chain. \'The current temperature in Pomfret is 37F.\' ``` ## SearchApi Second, let\'s try SearchApi tool. ```python tools = load_tools([""searchapi""], llm=llm) ``` ```python agent = initialize_agent( tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True ) ``` ```python agent.run(""What is the weather in Pomfret?"") ``` ```text > Entering new AgentExecutor chain... I need to find out the current weather in Pomfret. Action: searchapi Action Input: ""weather in Pomfret"" Observation: Thu 14 | Day ... Some clouds this morning will give way to generally sunny skies for the afternoon. High 73F. Winds NW at 5 to 10 mph. Hourly Weather-Pomfret, CT 1 pm. 71. 0%. Sunny. Feels Like71. WindNW 9 mph 2 pm. 72. 0%. Sunny. Feels Like72. WindNW 9 mph 3 pm. 72. 0%. Sunny. Feels ... 10 Day Weather-Pomfret, VT. As of 4:28 am EDT. Today. 68/48. 4%. Thu 14 | Day. 68. 4%. WNW 10 mph. Some clouds this morning will give way to generally ... Be prepared with the most accurate 10-day forecast for Pomfret, MD with highs, lows, chance of precipitation from The Weather Channel and Weather.com. Current Weather. 10:00 PM. 65F. RealFeel 67. Mostly cloudy. LOCAL HURRICANE TRACKER. Category2. Lee. Late Friday Night - Saturday Afternoon. 10 Day Weather-Pomfret, NY. As of 5:09 pm EDT. Tonight. --/55. 10%. Wed 13 | Night. 55. 10%. NW 11 mph. Some clouds. Low near 55F. Pomfret CT. Overnight. Overnight: Patchy fog before 3am, then patchy fog after 4am. Otherwise, mostly. Patchy Fog. Low: 58 F. Thursday. Isolated showers. Mostly cloudy, with a high near 76. Calm wind. Chance of precipitation is 20%. Tonight. Mostly Cloudy. Mostly cloudy, with a ... Partly sunny, with a high near 67. Breezy, with a north wind 18 to 22 mph, with gusts as high as 34 mph. Chance of precipitation is 30%. ... A chance of showers ... Today\'s Weather - Pomfret, CT ... Patchy fog. Showers. Lows in the upper 50s. Northwest winds around 5 mph. Chance of rain near 100 percent. ... Sunny. Patchy fog ... Thought: I now know the final answer Final Answer: The current weather in Pomfret is mostly cloudy with a high near 67 and a chance of showers. Winds are from the north at 18 to 22 mph with gusts up to 34 mph. > Finished chain. \'The current weather in Pomfret is mostly cloudy with a high near 67 and a chance of showers. Winds are from the north at 18 to 22 mph with gusts up to 34 mph.\' ``` ## SerpAPI Now, let\'s use the SerpAPI tool. ```python tools = load_tools([""serpapi""], llm=llm) ``` ```python agent = initialize_agent( tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True ) ``` ```python agent.run(""What is the weather in Pomfret?"") ``` ```text > Entering new AgentExecutor chain... I need to find out what the current weather is in Pomfret. Action: Search Action Input: ""weather in Pomfret"" Observation: {\'type\': \'weather_result\', \'temperature\': \'69\', \'unit\': \'Fahrenheit\', \'precipitation\': \'2%\', \'humidity\': \'90%\', \'wind\': \'1 mph\', \'location\': \'Pomfret, CT\', \'date\': \'Sunday 9:00 PM\', \'weather\': \'Clear\'} Thought: I now know the current weather in Pomfret. Final Answer: The current weather in Pomfret is 69 degrees Fahrenheit, 2% precipitation, 90% humidity, and 1 mph wind. It is currently clear. > Finished chain. \'The current weather in Pomfret is 69 degrees Fahrenheit, 2% precipitation, 90% humidity, and 1 mph wind. It is currently clear.\' ``` ## GoogleSearchAPIWrapper Now, let\'s use the official Google Search API Wrapper. ```python tools = load_tools([""google-search""], llm=llm) ``` ```python agent = initialize_agent( tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True ) ``` ```python agent.run(""What is the weather in Pomfret?"") ``` ```text > Entering new AgentExecutor chain... I should look up the current weather conditions. Action: Google Search Action Input: ""weather in Pomfret"" Observation: Showers early becoming a steady light rain later in the day. Near record high temperatures. High around 60F. Winds SW at 10 to 15 mph. Chance of rain 60%. Pomfret, CT Weather Forecast, with current conditions, wind, air quality, and what to expect for the next 3 days. Hourly Weather-Pomfret, CT. As of 12:52 am EST. Special Weather Statement +2... Hazardous Weather Conditions. Special Weather Statement ... Pomfret CT. Tonight ... National Digital Forecast Database Maximum Temperature Forecast. Pomfret Center Weather Forecasts. Weather Underground provides local & long-range weather forecasts, weatherreports, maps & tropical weather conditions for... Pomfret, CT 12 hour by hour weather forecast includes precipitation, temperatures, sky conditions, rain chance, dew-point, relative humidity, wind direction... North Pomfret Weather Forecasts. Weather Underground provides local & long-range weather forecasts, weatherreports, maps & tropical weather conditions for... Today\'s Weather - Pomfret, CT. Dec 31, 2022 4:00 PM. Putnam MS. --. Weather forecast icon. Feels like --. Hi --. Lo --. Pomfret, CT temperature trend for the next 14 Days. Find daytime highs and nighttime lows from TheWeatherNetwork.com. Pomfret, MD Weather Forecast Date: 332 PM EST Wed Dec 28 2022. The area/counties/county of: Charles, including the cites of: St. Charles and Waldorf. Thought: I now know the current weather conditions in Pomfret. Final Answer: Showers early becoming a steady light rain later in the day. Near record high temperatures. High around 60F. Winds SW at 10 to 15 mph. Chance of rain 60%. > Finished AgentExecutor chain. \'Showers early becoming a steady light rain later in the day. Near record high temperatures. High around 60F. Winds SW at 10 to 15 mph. Chance of rain 60%.\' ``` ## SearxNG Meta Search Engine Here we will be using a self hosted SearxNG meta search engine. ```python tools = load_tools([""searx-search""], searx_host="" llm=llm) ``` ```python agent = initialize_agent( tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True ) ``` ```python agent.run(""What is the weather in Pomfret"") ``` ```text > Entering new AgentExecutor chain... I should look up the current weather Action: SearX Search Action Input: ""weather in Pomfret"" Observation: Mainly cloudy with snow showers around in the morning. High around 40F. Winds NNW at 5 to 10 mph. Chance of snow 40%. Snow accumulations less than one inch. 10 Day Weather - Pomfret, MD As of 1:37 pm EST Today 49/ 41 52% Mon 27 | Day 49 52% SE 14 mph Cloudy with occasional rain showers. High 49F. Winds SE at 10 to 20 mph. Chance of rain 50%.... 10 Day Weather - Pomfret, VT As of 3:51 am EST Special Weather Statement Today 39/ 32 37% Wed 01 | Day 39 37% NE 4 mph Cloudy with snow showers developing for the afternoon. High 39F.... Pomfret, CT ; Current Weather. 1:06 AM. 35F  RealFeel 32 ; TODAY\'S WEATHER FORECAST. 3/3. 44Hi. RealFeel 50 ; TONIGHT\'S WEATHER FORECAST. 3/3. 32Lo. Pomfret, MD Forecast Today Hourly Daily Morning 41 1% Afternoon 43 0% Evening 35 3% Overnight 34 2% Don\'t Miss Finally, Here\'s Why We Get More Colds and Flu When It\'s Cold Coast-To-Coast... Pomfret, MD Weather Forecast | AccuWeather Current Weather 5:35 PM 35 F RealFeel 36 RealFeel Shade 36 Air Quality Excellent Wind E 3 mph Wind Gusts 5 mph Cloudy More Details WinterCast... Pomfret, VT Weather Forecast | AccuWeather Current Weather 11:21 AM 23 F RealFeel 27 RealFeel Shade 25 Air Quality Fair Wind ESE 3 mph Wind Gusts 7 mph Cloudy More Details WinterCast... Pomfret Center, CT Weather Forecast | AccuWeather Daily Current Weather 6:50 PM 39 F RealFeel 36 Air Quality Fair Wind NW 6 mph Wind Gusts 16 mph Mostly clear More Details WinterCast... 12:00 pm  Feels Like36  WindN 5 mph  Humidity43%  UV Index3 of 10  Cloud Cover65%  Rain Amount0 in ... Pomfret Center, CT Weather Conditions | Weather Underground star Popular Cities San Francisco, CA 49 F Clear Manhattan, NY 37 F Fair Schiller Park, IL (60176) warning39 F Mostly Cloudy... Thought: I now know the final answer Final Answer: The current weather in Pomfret is mainly cloudy with snow showers around in the morning. The temperature is around 40F with winds NNW at 5 to 10 mph. Chance of snow is 40%. > Finished chain. \'The current weather in Pomfret is mainly cloudy with snow showers around in the morning. The temperature is around 40F with winds NNW at 5 to 10 mph. Chance of snow is 40%.\' ``` - [Google Serper API Wrapper](#google-serper-api-wrapper) - [SearchApi](#searchapi) - [SerpAPI](#serpapi) - [GoogleSearchAPIWrapper](#googlesearchapiwrapper) - [SearxNG Meta Search Engine](#searxng-meta-search-engine)', 'Portkey | Portkey [Portkey]( is a platform designed to streamline the deployment and management of Generative AI applications. It provides comprehensive features for monitoring, managing models, and improving the performance of your AI applications. ## LLMOps for Langchain Portkey brings production readiness to Langchain. With Portkey, you can - view detailed **metrics & logs** for all requests, - enable **semantic cache** to reduce latency & costs, - implement automatic **retries & fallbacks** for failed requests, - add **custom tags** to requests for better tracking and analysis and [more]( ### Using Portkey with Langchain Using Portkey is as simple as just choosing which Portkey features you want, enabling them via `headers=Portkey.Config` and passing it in your LLM calls. To start, get your Portkey API key by [signing up here]( (Click the profile icon on the top left, then click on ""Copy API Key"") For OpenAI, a simple integration with logging feature would look like this: ```python from langchain.llms import OpenAI from langchain.utilities import Portkey # Add the Portkey API Key from your account headers = Portkey.Config( api_key = """" ) llm = OpenAI(temperature=0.9, headers=headers) llm.predict(""What would be a good company name for a company that makes colorful socks?"") ``` Your logs will be captured on your [Portkey dashboard]( A common Portkey X Langchain use case is to **trace a chain or an agent** and view all the LLM calls originating from that request. ### Tracing Chains & Agents ```python from langchain.agents import AgentType, initialize_agent, load_tools from langchain.llms import OpenAI from langchain.utilities import Portkey # Add the Portkey API Key from your account headers = Portkey.Config( api_key = """", trace_id = ""fef659"" ) llm = OpenAI(temperature=0, headers=headers) tools = load_tools([""serpapi"", ""llm-math""], llm=llm) agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True) # Let\'s test it out! agent.run(""What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?"") ``` **You can see the requests\' logs along with the trace id on Portkey dashboard:** ![](/img/portkey-dashboard.gif)![](/img/portkey-tracing.png)## Advanced Features 1. **Logging:** Log all your LLM requests automatically by sending them through Portkey. Each request log contains `timestamp`, `model name`, `total cost`, `request time`, `request json`, `response json`, and additional Portkey features. 2. **Tracing:** Trace id can be passed along with each request and is visibe on the logs on Portkey dashboard. You can also set a **distinct trace id** for each request. You can [append user feedback]( to a trace id as well. 3. **Caching:** Respond to previously served customers queries from cache instead of sending them again to OpenAI. Match exact strings OR semantically similar strings. Cache can save costs and reduce latencies by 20x. 4. **Retries:** Automatically reprocess any unsuccessful API requests **upto 5** times. Uses an **exponential backoff** strategy, which spaces out retry attempts to prevent network overload. 5. **Tagging:** Track and audit each user interaction in high detail with predefined tags. | Feature | Config Key | Value (Type) | Required/Optional | | ---- | ---- | ---- | ---- | | API Key | api_key | API Key (string) | Required | | Tracing Requests | trace_id | Customstring | Optional | | Automatic Retries | retry_count | integer[1,2,3,4,5] | Optional | | Enabling Cache | cache | simpleORsemantic | Optional | | Cache Force Refresh | cache_force_refresh | True | Optional | | Set Cache Expiry | cache_age | integer(in seconds) | Optional | | Add User | user | string | Optional | | Add Organisation | organisation | string | Optional | | Add Environment | environment | string | Optional | | Add Prompt (version/id/string) | prompt | string | Optional | ## Enabling all Portkey Features: ```py headers = Portkey.Config( # Mandatory api_key="""", # Cache Options cache=""semantic"", cache_force_refresh=""True"", cache_age=1729, # Advanced retry_count=5, trace_id=""langchain_agent"", # Metadata environment=""production"", user=""john"", organisation=""acme"", prompt=""Frost"" ) ``` For detailed information on each feature and how to use it, [please refer to the Portkey docs]( If you have any questions or need further assistance, [reach out to us on Twitter.]( - [LLMOps for Langchain](#llmops-for-langchain)- [Using Portkey with Langchain](#using-portkey-with-langchain) - [Tracing Chains & Agents](#tracing-chains--agents) - [Advanced Features](#advanced-features) - [Enabling all Portkey Features:](#enabling-all-portkey-features)']",The `initialize_agent` function returns an instance of the `AgentExecutor` class.,AgentExecutor,0.9999999999666667,1.0,0.5,0.022869567780619007,0.16666666666666669
